{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\ud83c\udf89 Welcome! \u00b6 Welcome to our open-source project on Generative AI , or GenAI \ud83e\udd16. The field of GenAI is rapidly expanding and becoming increasingly complex, and our mission is to make this topic more accessible and understandable for everyone. This project aims to provide comprehensive, full-spectrum knowledge on a range of GenAI topics \ud83d\udcd8. We've built this documentation in Markdown to encourage collaboration and continuous learning through GitHub pull requests \ud83e\udd1d. One of our main goals is to have this project written by GenAI itself \ud83d\ude80, enabling us to keep up with the complexity of information and advances in the field. To achieve this, we are focusing on: \ud83d\udcdd Creating a useful, base documentation repository that aids authors in generating self-descriptive repositories. \ud83d\udd04 Developing an automated merge and build system for the website that delivers information in an aesthetic and readable manner. \ud83d\udd01 Establishing a self-referential build system using Langchain and learned information, potentially utilizing AutoPR, among others. We are aware this is an ambitious task, but we believe in the power of GenAI to explain itself in a way that everyone can understand and benefit from \ud83d\udcaa. If you share our vision and would like to contribute, we warmly welcome your input. Please follow these guidelines to create a PR. Together, let's push the boundaries of what we can do with GenAI! \ud83c\udf0d","title":"\ud83c\udf89 Welcome!"},{"location":"index.html#welcome","text":"Welcome to our open-source project on Generative AI , or GenAI \ud83e\udd16. The field of GenAI is rapidly expanding and becoming increasingly complex, and our mission is to make this topic more accessible and understandable for everyone. This project aims to provide comprehensive, full-spectrum knowledge on a range of GenAI topics \ud83d\udcd8. We've built this documentation in Markdown to encourage collaboration and continuous learning through GitHub pull requests \ud83e\udd1d. One of our main goals is to have this project written by GenAI itself \ud83d\ude80, enabling us to keep up with the complexity of information and advances in the field. To achieve this, we are focusing on: \ud83d\udcdd Creating a useful, base documentation repository that aids authors in generating self-descriptive repositories. \ud83d\udd04 Developing an automated merge and build system for the website that delivers information in an aesthetic and readable manner. \ud83d\udd01 Establishing a self-referential build system using Langchain and learned information, potentially utilizing AutoPR, among others. We are aware this is an ambitious task, but we believe in the power of GenAI to explain itself in a way that everyone can understand and benefit from \ud83d\udcaa. If you share our vision and would like to contribute, we warmly welcome your input. Please follow these guidelines to create a PR. Together, let's push the boundaries of what we can do with GenAI! \ud83c\udf0d","title":"\ud83c\udf89 Welcome!"},{"location":"to_sort.html","text":"phi 1 https://www.reddit.com/r/LocalLLaMA/comments/14ez6qf/microsoft_makes_new_13b_coding_llm_that/ chat law https://arxiv.org/abs/2306.16092v1?utm_source=tldrai talk codebase https://github.com/rsaryev/talk-codebase?utm_source=tldrai https://martinfowler.com/articles/building-boba.html?utm_source=tldrai rap reproducible analytic pipelines https://digital.nhs.uk/blog/data-points-blog/2023/why-were-getting-our-data-teams-to-rap#:~:text=Reproducible%20analytical%20pipelines%20(RAP)%20are,in%20our%20Data%20Services%20directorate build your own data https://arxiv.org/abs/2306.13651v1?utm_source=tldrai self supervized https://styledrop.github.io/ https://www.youtube.com/watch?v=Ff4fRgnuFgQ&t=9s https://github.com/langchain-ai/streamlit-agent https://github.com/smol-ai/developer https://github.com/vinayprabhu/X-is-all-you-need https://github.com/ThomasEwing04/SMOL_AI https://github.com/kyrolabs/awesome-langchain https://cameronrwolfe.substack.com/p/teaching-language-models-to-use-tools?r=2bjtip&utm_medium=ios&utm_campaign=post https://www.quivr.app/chat","title":"To sort"},{"location":"Background/index.html","text":"Background overview","title":"Index"},{"location":"Background/agent/index.html","text":"Agents Gen(erative) AI \u00b6 Agents in Gen()AI agents have access to 'tools' to provide them 'agency' beyond the ability to generate text or image based responses to the input data. They rely on several important concepts: graph TB Agent((Agent)) -->|makes| decision((Decision)) decision -->|attempts| action((Action)) action -->|passes| execution((Execution)) execution -->|affects| environment((Environment)) execution -->|generates| agentMemory((Agent's Memory)) agentMemory -->|informs and effects| Agent environment -->|provides| observations((Observations)) observations -->|informs and effects| Agent execution -->|queries| environment AgentManager((Agent Manager)) -->|affects| execution Agent --> |informs and effects| AgentManager AgentManager --> |informs and effects| Agent Lists and websites \u00b6 Agents overview by Lilian Weng Awesome AGents of nicely curated AGents that helps to understand the differences they might contain. Basic Concepts \u00b6 (LLM) model : The 'intelligent' component returns an output for a given input. Inceptions: The prompt that orient's and agent's response. Memory access Tool Access Chains and flows Security Features Output formatting Systems of Agents Similar to automota, they may have the ability to run discretely, separately from standard chat-interfaces. Generally they involve the possibility of Human-in-the-loop to help correct odd components. The ability to call programs, APIs, software, cursors, robots, or other non-language systems. Memory Access \u00b6 Tool Access \u00b6 Native function calls and json support with OpenAI AutoLabel A nice pythonic system for generating semantic labels repeatedly for use in downstream datasets Guidance \u00b6 \u203c\ufe0f Guidance Interleaving generation, prompting and logical control to single continuous flow. Security Features \u00b6 \u203c\ufe0f Semantic Kernel \u203c\ufe0f Rebuff a prompt injection detection service. \u203c\ufe0f Guardrails To help format output and prevent improper prompts. Native function calls and json support with OpenAI AutoLabel A nice pythonic system for generating semantic labels repeatedly for use in downstream datasets Agent Networks \u00b6 Generative AI networks involve the interaction of multiple individual Gen()AI elements that can act, to a coordinated degree, independently of other AI Agents. TO ORGANIZE \u00b6 Recurrent and self-improving \u00b6 \u203c\ufe0f ReAct Github Effectively Observe, Think, Act, Repeat. Has limited action space Reflexion : \"Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities\" Github Inspired github Teaching Large Language Models to Self-Debug transcoder Self-play GPT Uses different LLMs and different roles to provide feedback on how to improve and enable autonomous improvement while game playing. Language Models can Solve Computer Tasks , Website , GitHub USes Recursive Criticism and Improvement. Combining with Chain of Thought it is even better. The method: Plan: Critique, Improve Explicit RCI: \"Review your previous answer and find problems with your answer.\" \u2192 \"Based on the problems you found, improve your answer.\" Recursively Criticizes and Improves its output. This sort of prompting outperforms Chain of Thought, and combined it works even better. GPT-Bargaining Uses multiple rounds to improve negotiation tactics based on external feedback. (Manager-like) \u203c\ufe0f RL4L Allen ai Uses smaller critique model feedback to improve larger model output with a policy gradient to fine-tune the critique model while allowing reasonable performance gains. Github Strategic Reasoning with Language Models Uses game trees and observed and inferred beliefs to achieve closer to optimal results. Powerful to consider for inferred beliefs and interacting in situations where negotiation or games are being played. https://arxiv.org/pdf/2306.08640.pdf Agentic \u00b6 Results \u00b6 Toolformer This section describes GPT that has been enabled with more 'agency' or the ability to do better. HuggingGPT of 2023 This paper describes a paradigm where ChatGPT is enabled with the ability to launch other ML models based on input. It does so by creating a Task list, then by identifying appropriate models, and then executing them. \u203c\ufe0f Github repo known as JARVIS here TaskMatrix.ai seemingly from the same authors. AUTO GPT Auto GPT \u203c\ufe0f GPT engineer \u203c\ufe0f BabyAGI \u203c\ufe0f CAMEL inception prompting to guide chat agents toward task completion. Also implemented in Langchain Loop GPT A re-implementation of Auto-GPT with modularity and extensibility in mind. Chameleon GPT A multi-agentic service that is able to accomplish many separate tasks, building it compositionally. (Project Idea: build in Langchain???) Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data Parameter efficient LLama Tuning and risk minimization with a new 'Self Distillation' with Feedback to improve itself even more. RESEARCH ONLY \u203c\ufe0f Robo-GPT (Open source + Product) Agent-GPT and WEbsite \u2192 Doesn't have agency/tools... So it is not good. A fancy wrapper for multi-task planning and execution. Limited at present. \u203c\ufe0f (Open source + product) Super-AGI \u2192 Seemingly better than SuperAGI because more tools accessible and GUI. Allows multiple agents (no communication though) \u203c\ufe0f [AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn] ( https://arxiv.org/pdf/2306.08640.pdf ) Webpage Uses PEIL PLan execute inspect learn. Code coming soon. \u203c\ufe0f GPT Engineer Agent Improvements \u00b6 Learning to Reason and Memorize with Self-Notes \"Allows model to deviate from input context at any time to reason and take notes\" Large language models as tool makers Github Allows high-quality tools to be reused by more lightweight models. CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation Agentic \u00b6 smolai https://www.youtube.com/watch?v=zsxyqz6SYp8&t=1s Chatbots \u00b6 LAION-AI An attempt an open-version of ChatGPT","title":"Agent"},{"location":"Background/agent/index.html#agents-generative-ai","text":"Agents in Gen()AI agents have access to 'tools' to provide them 'agency' beyond the ability to generate text or image based responses to the input data. They rely on several important concepts: graph TB Agent((Agent)) -->|makes| decision((Decision)) decision -->|attempts| action((Action)) action -->|passes| execution((Execution)) execution -->|affects| environment((Environment)) execution -->|generates| agentMemory((Agent's Memory)) agentMemory -->|informs and effects| Agent environment -->|provides| observations((Observations)) observations -->|informs and effects| Agent execution -->|queries| environment AgentManager((Agent Manager)) -->|affects| execution Agent --> |informs and effects| AgentManager AgentManager --> |informs and effects| Agent","title":"Agents Gen(erative) AI"},{"location":"Background/agent/index.html#lists-and-websites","text":"Agents overview by Lilian Weng Awesome AGents of nicely curated AGents that helps to understand the differences they might contain.","title":"Lists and websites"},{"location":"Background/agent/index.html#basic-concepts","text":"(LLM) model : The 'intelligent' component returns an output for a given input. Inceptions: The prompt that orient's and agent's response. Memory access Tool Access Chains and flows Security Features Output formatting Systems of Agents Similar to automota, they may have the ability to run discretely, separately from standard chat-interfaces. Generally they involve the possibility of Human-in-the-loop to help correct odd components. The ability to call programs, APIs, software, cursors, robots, or other non-language systems.","title":"Basic Concepts"},{"location":"Background/agent/index.html#memory-access","text":"","title":"Memory Access"},{"location":"Background/agent/index.html#tool-access","text":"Native function calls and json support with OpenAI AutoLabel A nice pythonic system for generating semantic labels repeatedly for use in downstream datasets","title":"Tool Access"},{"location":"Background/agent/index.html#guidance","text":"\u203c\ufe0f Guidance Interleaving generation, prompting and logical control to single continuous flow.","title":"Guidance"},{"location":"Background/agent/index.html#security-features","text":"\u203c\ufe0f Semantic Kernel \u203c\ufe0f Rebuff a prompt injection detection service. \u203c\ufe0f Guardrails To help format output and prevent improper prompts. Native function calls and json support with OpenAI AutoLabel A nice pythonic system for generating semantic labels repeatedly for use in downstream datasets","title":"Security Features"},{"location":"Background/agent/index.html#agent-networks","text":"Generative AI networks involve the interaction of multiple individual Gen()AI elements that can act, to a coordinated degree, independently of other AI Agents.","title":"Agent Networks"},{"location":"Background/agent/index.html#to-organize","text":"","title":"TO ORGANIZE"},{"location":"Background/agent/index.html#recurrent-and-self-improving","text":"\u203c\ufe0f ReAct Github Effectively Observe, Think, Act, Repeat. Has limited action space Reflexion : \"Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities\" Github Inspired github Teaching Large Language Models to Self-Debug transcoder Self-play GPT Uses different LLMs and different roles to provide feedback on how to improve and enable autonomous improvement while game playing. Language Models can Solve Computer Tasks , Website , GitHub USes Recursive Criticism and Improvement. Combining with Chain of Thought it is even better. The method: Plan: Critique, Improve Explicit RCI: \"Review your previous answer and find problems with your answer.\" \u2192 \"Based on the problems you found, improve your answer.\" Recursively Criticizes and Improves its output. This sort of prompting outperforms Chain of Thought, and combined it works even better. GPT-Bargaining Uses multiple rounds to improve negotiation tactics based on external feedback. (Manager-like) \u203c\ufe0f RL4L Allen ai Uses smaller critique model feedback to improve larger model output with a policy gradient to fine-tune the critique model while allowing reasonable performance gains. Github Strategic Reasoning with Language Models Uses game trees and observed and inferred beliefs to achieve closer to optimal results. Powerful to consider for inferred beliefs and interacting in situations where negotiation or games are being played. https://arxiv.org/pdf/2306.08640.pdf","title":"Recurrent and self-improving"},{"location":"Background/agent/index.html#agentic","text":"","title":"Agentic"},{"location":"Background/agent/index.html#results","text":"Toolformer This section describes GPT that has been enabled with more 'agency' or the ability to do better. HuggingGPT of 2023 This paper describes a paradigm where ChatGPT is enabled with the ability to launch other ML models based on input. It does so by creating a Task list, then by identifying appropriate models, and then executing them. \u203c\ufe0f Github repo known as JARVIS here TaskMatrix.ai seemingly from the same authors. AUTO GPT Auto GPT \u203c\ufe0f GPT engineer \u203c\ufe0f BabyAGI \u203c\ufe0f CAMEL inception prompting to guide chat agents toward task completion. Also implemented in Langchain Loop GPT A re-implementation of Auto-GPT with modularity and extensibility in mind. Chameleon GPT A multi-agentic service that is able to accomplish many separate tasks, building it compositionally. (Project Idea: build in Langchain???) Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data Parameter efficient LLama Tuning and risk minimization with a new 'Self Distillation' with Feedback to improve itself even more. RESEARCH ONLY \u203c\ufe0f Robo-GPT (Open source + Product) Agent-GPT and WEbsite \u2192 Doesn't have agency/tools... So it is not good. A fancy wrapper for multi-task planning and execution. Limited at present. \u203c\ufe0f (Open source + product) Super-AGI \u2192 Seemingly better than SuperAGI because more tools accessible and GUI. Allows multiple agents (no communication though) \u203c\ufe0f [AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn] ( https://arxiv.org/pdf/2306.08640.pdf ) Webpage Uses PEIL PLan execute inspect learn. Code coming soon. \u203c\ufe0f GPT Engineer","title":"Results"},{"location":"Background/agent/index.html#agent-improvements","text":"Learning to Reason and Memorize with Self-Notes \"Allows model to deviate from input context at any time to reason and take notes\" Large language models as tool makers Github Allows high-quality tools to be reused by more lightweight models. CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation","title":"Agent Improvements"},{"location":"Background/agent/index.html#agentic_1","text":"smolai https://www.youtube.com/watch?v=zsxyqz6SYp8&t=1s","title":"Agentic"},{"location":"Background/agent/index.html#chatbots","text":"LAION-AI An attempt an open-version of ChatGPT","title":"Chatbots"},{"location":"Background/agent/actions_and_tools.html","text":"","title":"Actions and tools"},{"location":"Background/agent/chains.html","text":"Chains \u00b6 Chains can be considered linked generative interactions where information can be processed with interepreters, tools, or other agents/GenAIs. Done well, they can be built up to form reasoning systems that can enable more successful reasoning, or task completion. Chain of thought hub Concepts \u00b6 Tree of Thought \u00b6 Large Language Model Guided Tree-of-Thought Github \u203c\ufe0f Tree of Thoughts: Deliberate Problem Solving with Large Language Models Github IDEA: Write Tree of Thoughts into Langchain? \u203c\ufe0f Meta Tree of thought Graph of Thought An excellent thought on what next to consider when dealing with knowledge (or other output like information) generation chains. Certified Reasoning with Language models A 'logical guide' tool that an LLM can use. It \" uses constrained decoding to ensure the model will incrementally generate one of the valid outputs.\" Implementation \u00b6 Langchain \u00b6 \u203c\ufe0f Langchain A primative python or javascript based primitive 'LLM' language that enables planned and agentic AI. \u203c\ufe0f Langflow \u203c\ufe0f Awesome Langchain \u203c\ufe0f Toolkit Generates LangChain plugins Tutorials \u00b6 https://www.pinecone.io/learn/langchain-prompt-templates/ https://learn.deeplearning.ai/langchain/lesson/3/memory Llama index \u00b6 llama index and Github for integrating data ingestion and models. LlamaHub (community library of data loaders) LlamaLab (cutting-edge AGI projects using LlamaIndex) Others \u00b6 \u203c\ufe0f Flowise \uff01 Chain Forge A data flow prompt engineering environment for evaluating ana analyzing LLM responses \u203c\ufe0f EmbedChain Creates embeddings for bots to be used. \u203c\ufe0f llm-chain ChatGPT and Alpaca support. Agentic with bash commands.","title":"Chains"},{"location":"Background/agent/chains.html#chains","text":"Chains can be considered linked generative interactions where information can be processed with interepreters, tools, or other agents/GenAIs. Done well, they can be built up to form reasoning systems that can enable more successful reasoning, or task completion. Chain of thought hub","title":"Chains"},{"location":"Background/agent/chains.html#concepts","text":"","title":"Concepts"},{"location":"Background/agent/chains.html#tree-of-thought","text":"Large Language Model Guided Tree-of-Thought Github \u203c\ufe0f Tree of Thoughts: Deliberate Problem Solving with Large Language Models Github IDEA: Write Tree of Thoughts into Langchain? \u203c\ufe0f Meta Tree of thought Graph of Thought An excellent thought on what next to consider when dealing with knowledge (or other output like information) generation chains. Certified Reasoning with Language models A 'logical guide' tool that an LLM can use. It \" uses constrained decoding to ensure the model will incrementally generate one of the valid outputs.\"","title":"Tree of Thought"},{"location":"Background/agent/chains.html#implementation","text":"","title":"Implementation"},{"location":"Background/agent/chains.html#langchain","text":"\u203c\ufe0f Langchain A primative python or javascript based primitive 'LLM' language that enables planned and agentic AI. \u203c\ufe0f Langflow \u203c\ufe0f Awesome Langchain \u203c\ufe0f Toolkit Generates LangChain plugins","title":"Langchain"},{"location":"Background/agent/chains.html#tutorials","text":"https://www.pinecone.io/learn/langchain-prompt-templates/ https://learn.deeplearning.ai/langchain/lesson/3/memory","title":"Tutorials"},{"location":"Background/agent/chains.html#llama-index","text":"llama index and Github for integrating data ingestion and models. LlamaHub (community library of data loaders) LlamaLab (cutting-edge AGI projects using LlamaIndex)","title":"Llama index"},{"location":"Background/agent/chains.html#others","text":"\u203c\ufe0f Flowise \uff01 Chain Forge A data flow prompt engineering environment for evaluating ana analyzing LLM responses \u203c\ufe0f EmbedChain Creates embeddings for bots to be used. \u203c\ufe0f llm-chain ChatGPT and Alpaca support. Agentic with bash commands.","title":"Others"},{"location":"Background/agent/environments.html","text":"Environments consist of the information that an agent, or system of agents has access too. It can be the set of origins from where the agent gets its observations. These origins can places like be 'a person' as is for a chat-interface, a web-stream, for an alert agent. a simulation of a town. The environment also 'interprets' the output of any agent, possibly changing the environment or executing other sub-commands. Effectively, the environment can be considered as performing tool-execution. In another example, the environment might parse an English-text output into a python-executable, and The environment can be considered,","title":"Environments"},{"location":"Background/agent/implementation.html","text":"Below are suggested implementation requirements, prioritization conditions, and examples codebases enablign ML agents.","title":"Implementation"},{"location":"Background/agent/memory.html","text":"Vector databases \u00b6 Vector Databases (primer by Pinecone.io) Use embeddings to create query vector databases such as: Pinecone, Qdrant, Weaviate, Chroma as well as the incumbents Faiss, Redis, Milvus, ScaNN. https://github.com/Helicone/helicone Memory Augmented \u00b6 Improving language models by retrieving from trillions of tokens","title":"Memory"},{"location":"Background/agent/memory.html#vector-databases","text":"Vector Databases (primer by Pinecone.io) Use embeddings to create query vector databases such as: Pinecone, Qdrant, Weaviate, Chroma as well as the incumbents Faiss, Redis, Milvus, ScaNN. https://github.com/Helicone/helicone","title":"Vector databases"},{"location":"Background/agent/memory.html#memory-augmented","text":"Improving language models by retrieving from trillions of tokens","title":"Memory Augmented"},{"location":"Background/agent/systems.html","text":"When an agent (or model) interacts with a different agent in some way, it becomes a system of agents. This can be created by incepting and equipping different agents and enabling their output to be ingested and returned. It could be considered that an agent's input can be regarded as nother 'tool' where the call to the tool is to the different agent. While a reasonable perspective, because the same considerations can generally be applied to all agents, but not tools, we consider it separately. Binary system (asymmetric calling) ChatGPT calls DallE with a prompt it generated. DallE returns an image that is either returned or otherwise used in Chat GPT's final response. Multi-body system (bidirectional calling) A group of agents discussing their daily affairs and getting periodic environmental updates, like this paper Multi-Agentic Systems \u00b6 Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind Uses Theory fo Mind to try to improve student performance. Github Generative Agents: Interactive Simulacra of Human Behavior A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents. In it they discuss several challenges and solutions: Remembering Observation Memory A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping. Uses, recency , importance and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. Reflection Memory Which is a separate tipe of memory that allow more abstract thoughts for the agent. They can be included alongside the reflections. (Hardcoded when this happens, in relation to sum of importance scores > threshold) Planning and Reacting Recursive Planning used to generate the day into several chunks of goals. These are then broken down to smaller timespaces. Plans can change based on interactions. (perhaps present status, planned and past) Optimizations \u00b6 Multi-Agent Collaboration via Reward Attribution Decomposition Describes optimization of multi agent with distributed reward systems to get SOA performance. It is a joint optimization allowing decentralized Q-function that relies on self and interactive terms.","title":"Systems"},{"location":"Background/agent/systems.html#multi-agentic-systems","text":"Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind Uses Theory fo Mind to try to improve student performance. Github Generative Agents: Interactive Simulacra of Human Behavior A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents. In it they discuss several challenges and solutions: Remembering Observation Memory A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping. Uses, recency , importance and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. Reflection Memory Which is a separate tipe of memory that allow more abstract thoughts for the agent. They can be included alongside the reflections. (Hardcoded when this happens, in relation to sum of importance scores > threshold) Planning and Reacting Recursive Planning used to generate the day into several chunks of goals. These are then broken down to smaller timespaces. Plans can change based on interactions. (perhaps present status, planned and past)","title":"Multi-Agentic Systems"},{"location":"Background/agent/systems.html#optimizations","text":"Multi-Agent Collaboration via Reward Attribution Decomposition Describes optimization of multi agent with distributed reward systems to get SOA performance. It is a joint optimization allowing decentralized Q-function that relies on self and interactive terms.","title":"Optimizations"},{"location":"Background/agent/types.html","text":"Agent types can be described by direct agentic ability to cause a change in the world. Text Agent \u00b6 An agent that can output only language text. Even thought the language can be 'interpreted' into different things, as is done in the environment. Text + Image Agent \u00b6 An agent that can output Robotic Agent \u00b6 A robotic agent can control mechanism impacting the mechanical position or other activity of a device. graph TB Agent((Agent)) -->|makes| decision((Decision)) decision -->|attempts| action((Action)) action -->|passes| execution((Execution)) execution -->|affects| environment((Environment)) execution -->|generates| agentMemory((Agent's Memory)) agentMemory -->|informs and effects| Agent environment -->|provides| observations((Observations)) observations -->|informs and effects| Agent execution -->|queries| environment AgentManager((Agent Manager)) -->|affects| execution Agent --> |informs and effects| AgentManager AgentManager --> |informs and effects| Agent","title":"Types"},{"location":"Background/agent/types.html#text-agent","text":"An agent that can output only language text. Even thought the language can be 'interpreted' into different things, as is done in the environment.","title":"Text Agent"},{"location":"Background/agent/types.html#text-image-agent","text":"An agent that can output","title":"Text + Image Agent"},{"location":"Background/agent/types.html#robotic-agent","text":"A robotic agent can control mechanism impacting the mechanical position or other activity of a device. graph TB Agent((Agent)) -->|makes| decision((Decision)) decision -->|attempts| action((Action)) action -->|passes| execution((Execution)) execution -->|affects| environment((Environment)) execution -->|generates| agentMemory((Agent's Memory)) agentMemory -->|informs and effects| Agent environment -->|provides| observations((Observations)) observations -->|informs and effects| Agent execution -->|queries| environment AgentManager((Agent Manager)) -->|affects| execution Agent --> |informs and effects| AgentManager AgentManager --> |informs and effects| Agent","title":"Robotic Agent"},{"location":"Background/computation/index.html","text":"\ud83d\udea7 This section is under construction \ud83c\udfd7\ufe0f GPUs \u00b6 In order to create models, large volumes of matrix multiplication is necessary. GPUs are designed for this. References \u00b6 Tim Dettmers on GPUs \ufffc","title":"Index"},{"location":"Background/computation/index.html#gpus","text":"In order to create models, large volumes of matrix multiplication is necessary. GPUs are designed for this.","title":"GPUs"},{"location":"Background/computation/index.html#references","text":"Tim Dettmers on GPUs \ufffc","title":"References"},{"location":"Background/data/index.html","text":"Data is the most important part of training any model. Data can be 'real' or 'simulated', though there is general consensus that simulated data can lead to worse models. Data sets \u00b6 RedPajama Pile CommonCrawl (webscrape) C4 (CommonCrawl) Github Books Arxiv StackExchange unarXive 2022: All arXiv Publications Pre-Processed for NLP Redpajama BIG-bench APACHE 2.0 Metaseq For working with Oen pre-trained transformers (from fairseq) Simulated \u00b6 \u201cTextbooks are all you need\u201d A to-be opensourced high-quality model by Microsoft revealing the importance of high-quality input data. only used 4 days on 8 A-100s to train to reach out-performing results. (It also uses a lot of simulated data). Coding-focused model. LLM-extraction \u00b6 \u203c\ufe0f Kor For extracting strucutred data using LLMs. Scaling Laws \u00b6 The 'Chinchilla' paper of 2022 This paper identifies scaling laws that help to understand the volume of data that is needed to obtain 'optimal' performance for a given LLM models size. Use of it in other areas, such as for Llama reveals that the models may have been under-trained. Primary takeaway: **\"All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions.\" **","title":"Data"},{"location":"Background/data/index.html#data-sets","text":"RedPajama Pile CommonCrawl (webscrape) C4 (CommonCrawl) Github Books Arxiv StackExchange unarXive 2022: All arXiv Publications Pre-Processed for NLP Redpajama BIG-bench APACHE 2.0 Metaseq For working with Oen pre-trained transformers (from fairseq)","title":"Data sets"},{"location":"Background/data/index.html#simulated","text":"\u201cTextbooks are all you need\u201d A to-be opensourced high-quality model by Microsoft revealing the importance of high-quality input data. only used 4 days on 8 A-100s to train to reach out-performing results. (It also uses a lot of simulated data). Coding-focused model.","title":"Simulated"},{"location":"Background/data/index.html#llm-extraction","text":"\u203c\ufe0f Kor For extracting strucutred data using LLMs.","title":"LLM-extraction"},{"location":"Background/data/index.html#scaling-laws","text":"The 'Chinchilla' paper of 2022 This paper identifies scaling laws that help to understand the volume of data that is needed to obtain 'optimal' performance for a given LLM models size. Use of it in other areas, such as for Llama reveals that the models may have been under-trained. Primary takeaway: **\"All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions.\" **","title":"Scaling Laws"},{"location":"Background/data/representations.html","text":"Data, represented on disk in binary, though perhaps read in different digital representations is broken up into individual units called tokens. These tokens, corresponding to contiguous strings of Digital Representations \u00b6 Digital representations relates to how data is encoded into memory (short, long) in any way. When it comes to text, the most common representations are utf8 and ascii. It is possible, and potentially useful to consider pure 'binary' representations, especially when considering multimodal data (see below). Tokenization \u00b6 Neural Machine Translation of Rare Words with Subword Units Indicates that breaking up words can offer high value results (2015) Multimodal \u00b6 \u203c\ufe0f Bytes are all you need Reveals that just taking file bytes into transformer technology can directly enable improvements in performance accuracy. The accuracy method varies based on encoding method. Their model is called ByteFormer Github Tokenization \u00b6 Tiktoken uses BPE and is theoretically used in GPT models. Can be used on pure-binary and limited character data (I've checked!). Token Monster Uses 35% fewer tokens and uses a top-down approach, instead of a bottom-up constructive approach. Likely of high value. Special tokens \u00b6 There are special tokens that are used by high-level interpreters on what next to do. START_TOKEN STOP_TOKEN MASK_TOKEN MODALITY_TOKEN mask_token , bos_token (beginning of sequence), eos_token Embeddings \u00b6 \u203c\ufe0f What are Embeddings Github \u00b6","title":"Representations"},{"location":"Background/data/representations.html#digital-representations","text":"Digital representations relates to how data is encoded into memory (short, long) in any way. When it comes to text, the most common representations are utf8 and ascii. It is possible, and potentially useful to consider pure 'binary' representations, especially when considering multimodal data (see below).","title":"Digital Representations"},{"location":"Background/data/representations.html#tokenization","text":"Neural Machine Translation of Rare Words with Subword Units Indicates that breaking up words can offer high value results (2015)","title":"Tokenization"},{"location":"Background/data/representations.html#multimodal","text":"\u203c\ufe0f Bytes are all you need Reveals that just taking file bytes into transformer technology can directly enable improvements in performance accuracy. The accuracy method varies based on encoding method. Their model is called ByteFormer Github","title":"Multimodal"},{"location":"Background/data/representations.html#tokenization_1","text":"Tiktoken uses BPE and is theoretically used in GPT models. Can be used on pure-binary and limited character data (I've checked!). Token Monster Uses 35% fewer tokens and uses a top-down approach, instead of a bottom-up constructive approach. Likely of high value.","title":"Tokenization"},{"location":"Background/data/representations.html#special-tokens","text":"There are special tokens that are used by high-level interpreters on what next to do. START_TOKEN STOP_TOKEN MASK_TOKEN MODALITY_TOKEN mask_token , bos_token (beginning of sequence), eos_token","title":"Special tokens"},{"location":"Background/data/representations.html#embeddings","text":"\u203c\ufe0f What are Embeddings Github","title":"Embeddings"},{"location":"Background/data/representations.html#_1","text":"","title":""},{"location":"Background/deployment/index.html","text":"","title":"Index"},{"location":"Background/engineering/index.html","text":"Engineering and Deployment \u00b6 Deploying on Azure for Embeddings Integrating with Azure Services Langchain service deployment Caching \u00b6 GPTCache","title":"Engineering"},{"location":"Background/engineering/index.html#engineering-and-deployment","text":"Deploying on Azure for Embeddings Integrating with Azure Services Langchain service deployment","title":"Engineering and Deployment"},{"location":"Background/engineering/index.html#caching","text":"GPTCache","title":"Caching"},{"location":"Background/engineering/ecosystem.html","text":"Enabling Companies \u00b6 Building and deploying \u00b6 Fixie LLM Training + Deployment \u00b6 \u203c\ufe0f CodeTF From Salesforce \u203c\ufe0f Azure Open AI samples Sample end-to-end use cases with chatbots, content generation. RLHF with DeepSpeed (Microsoft) vLLM a python repo to help run LLMs.","title":"Enabling Companies"},{"location":"Background/engineering/ecosystem.html#enabling-companies","text":"","title":"Enabling Companies"},{"location":"Background/engineering/ecosystem.html#building-and-deploying","text":"Fixie","title":"Building and deploying"},{"location":"Background/engineering/ecosystem.html#llm-training-deployment","text":"\u203c\ufe0f CodeTF From Salesforce \u203c\ufe0f Azure Open AI samples Sample end-to-end use cases with chatbots, content generation. RLHF with DeepSpeed (Microsoft) vLLM a python repo to help run LLMs.","title":"LLM Training + Deployment"},{"location":"Background/engineering/open_source_frameworks.html","text":"Operational Toolkits for LLMops \u00b6 \u203c\ufe0f Hugging Face Transformers \u203c\ufe0f Adapters for Hugging Face \u203c\ufe0f Open LLM Chatall To interact with multiple chatbots at the same time. \u203c\ufe0f LocalAI drop-in replacement REST API that\u2019s compatible with OpenAI API specifications for local inferencing. For Llama \u00b6 For Llama MedAlpaca","title":"Open source frameworks"},{"location":"Background/engineering/open_source_frameworks.html#operational-toolkits-for-llmops","text":"\u203c\ufe0f Hugging Face Transformers \u203c\ufe0f Adapters for Hugging Face \u203c\ufe0f Open LLM Chatall To interact with multiple chatbots at the same time. \u203c\ufe0f LocalAI drop-in replacement REST API that\u2019s compatible with OpenAI API specifications for local inferencing.","title":"Operational Toolkits for LLMops"},{"location":"Background/engineering/open_source_frameworks.html#for-llama","text":"For Llama MedAlpaca","title":"For Llama"},{"location":"Background/ethics_and_regulation/ethics.html","text":"Bias and Fairness \u00b6 Mitigating bias in data and models Evaluating model fairness Inclusive model development Transparency and Explainability Interpretability \u00b6 Techniques for explainability Right to explanation Safety Risk Mitigation \u00b6 Risk assessment Safeguards against misuse Privacy Data privacy \u00b6 Anonymization and de-identification Encryption and secure computing Governance \u00b6 Internal auditing processes External oversight Accountability measures Access and Inclusion Fair and equitable access \u00b6 Digital divides Participatory design Compliance Laws and regulations \u00b6 Responsible development guidelines Ethics review processes","title":"Ethics and regulation"},{"location":"Background/ethics_and_regulation/ethics.html#bias-and-fairness","text":"Mitigating bias in data and models Evaluating model fairness Inclusive model development Transparency and Explainability","title":"Bias and Fairness"},{"location":"Background/ethics_and_regulation/ethics.html#interpretability","text":"Techniques for explainability Right to explanation Safety","title":"Interpretability"},{"location":"Background/ethics_and_regulation/ethics.html#risk-mitigation","text":"Risk assessment Safeguards against misuse Privacy","title":"Risk Mitigation"},{"location":"Background/ethics_and_regulation/ethics.html#data-privacy","text":"Anonymization and de-identification Encryption and secure computing","title":"Data privacy"},{"location":"Background/ethics_and_regulation/ethics.html#governance","text":"Internal auditing processes External oversight Accountability measures Access and Inclusion","title":"Governance"},{"location":"Background/ethics_and_regulation/ethics.html#fair-and-equitable-access","text":"Digital divides Participatory design Compliance","title":"Fair and equitable access"},{"location":"Background/ethics_and_regulation/ethics.html#laws-and-regulations","text":"Responsible development guidelines Ethics review processes","title":"Laws and regulations"},{"location":"Background/interfaces_and_plugins/plugins.html","text":"Mini Wob++ For web interactive environments for accomplishing different tasks. Quite useful. Plugins \u00b6 \u203c\ufe0f Prompt Genius -\ufe0f Utility: - Reliable GPT A wrapper that prevents failures due to rate limiting requests. \u203c\ufe0f FastChat Conversation This very nice 'multi model' chat interface class allows for effective translation between different models.","title":"Plugins"},{"location":"Background/interfaces_and_plugins/plugins.html#plugins","text":"\u203c\ufe0f Prompt Genius -\ufe0f Utility: - Reliable GPT A wrapper that prevents failures due to rate limiting requests. \u203c\ufe0f FastChat Conversation This very nice 'multi model' chat interface class allows for effective translation between different models.","title":"Plugins"},{"location":"Background/miscellaneous/index.html","text":"","title":"Miscellaneous"},{"location":"Background/miscellaneous/detection.html","text":"Sapling AI content detector","title":"Detection"},{"location":"Background/models/index.html","text":"Models for Gen()AI \u00b6 As of 2023, there are two primary domains of Generative AI, text-based and image-based. Though there is a whole host of other modalities that can be considered. We won't go into detail to the wide variety of generative AI models, but, instead will focus on the model architecture that is presently dominating the market: Transformers. Because prompting will be model dependent we also focus on prompting in this section. Available models \u00b6 Text (first) \u00b6 Bard Claud ChatGPT Medpalm Llama (Non-commercial ??) Open Llama (Non-commercial ??) UAE Falcon (Apache License) Image (first) \u00b6 StableLM: Stability AI Language Models CC BY-SA-4.0 MultiModal \u00b6 Unilm (MSFT Model Overviews \u00b6 Below we provide references to concepts that extend beyond transformer-based models. - A Survey of Large Language Models A very comprehensive paper discussing LLM technology. Self-supervised learning \u00b6 \u203c\ufe0f A cookbook of self-supervised Learning Self-supervised learning \u00b6 \u203c\ufe0f A cookbook of self-supervised Learning Leaderboards and comparisons \u00b6 Hugging Face LLM leaderboard An essential chart for documenting the model peformance across multiple models. lmsys.org leader board Foundation model Providers EU AI compliance TO SORT \u00b6 Token Embedding: Mapping to a vector space. Positional Embedding: Learned or hard-coded mapping to position of sequence to a vector space Attention: Token being predicted is mapped to a query vector and tokens in context are mapped to key and value vectors. Inner products are used to combine to extract information. Bi-directional / unmasked Unidirectional / masked self attetion Cross attention applies attention to the primary sequence and treates the second token sequence the context. Multi-head attention. Multiple attention heads in parallel. Layer normalization. Found to be computationally efficient version sets m = beta = 0 or root mean square layer normalizagion or RMSnorm . Unembedding: Learns to convert vector intot he vocuabulary elements. Architectures: Encoder-Decoder (EDT), is also sequence-to-sequence. Encoder-only: (BERT) Decoder-only (GPT) Next-token Multi-domain decoder-only transformer (Gato)","title":"Models"},{"location":"Background/models/index.html#models-for-genai","text":"As of 2023, there are two primary domains of Generative AI, text-based and image-based. Though there is a whole host of other modalities that can be considered. We won't go into detail to the wide variety of generative AI models, but, instead will focus on the model architecture that is presently dominating the market: Transformers. Because prompting will be model dependent we also focus on prompting in this section.","title":"Models for Gen()AI"},{"location":"Background/models/index.html#available-models","text":"","title":"Available models"},{"location":"Background/models/index.html#text-first","text":"Bard Claud ChatGPT Medpalm Llama (Non-commercial ??) Open Llama (Non-commercial ??) UAE Falcon (Apache License)","title":"Text (first)"},{"location":"Background/models/index.html#image-first","text":"StableLM: Stability AI Language Models CC BY-SA-4.0","title":"Image (first)"},{"location":"Background/models/index.html#multimodal","text":"Unilm (MSFT","title":"MultiModal"},{"location":"Background/models/index.html#model-overviews","text":"Below we provide references to concepts that extend beyond transformer-based models. - A Survey of Large Language Models A very comprehensive paper discussing LLM technology.","title":"Model Overviews"},{"location":"Background/models/index.html#self-supervised-learning","text":"\u203c\ufe0f A cookbook of self-supervised Learning","title":"Self-supervised learning"},{"location":"Background/models/index.html#self-supervised-learning_1","text":"\u203c\ufe0f A cookbook of self-supervised Learning","title":"Self-supervised learning"},{"location":"Background/models/index.html#leaderboards-and-comparisons","text":"Hugging Face LLM leaderboard An essential chart for documenting the model peformance across multiple models. lmsys.org leader board Foundation model Providers EU AI compliance","title":"Leaderboards and comparisons"},{"location":"Background/models/index.html#to-sort","text":"Token Embedding: Mapping to a vector space. Positional Embedding: Learned or hard-coded mapping to position of sequence to a vector space Attention: Token being predicted is mapped to a query vector and tokens in context are mapped to key and value vectors. Inner products are used to combine to extract information. Bi-directional / unmasked Unidirectional / masked self attetion Cross attention applies attention to the primary sequence and treates the second token sequence the context. Multi-head attention. Multiple attention heads in parallel. Layer normalization. Found to be computationally efficient version sets m = beta = 0 or root mean square layer normalizagion or RMSnorm . Unembedding: Learns to convert vector intot he vocuabulary elements. Architectures: Encoder-Decoder (EDT), is also sequence-to-sequence. Encoder-only: (BERT) Decoder-only (GPT) Next-token Multi-domain decoder-only transformer (Gato)","title":"TO SORT"},{"location":"Background/models/metrics.html","text":"Metrics: \u00b6 Exact Match (EM)","title":"Metrics"},{"location":"Background/models/metrics.html#metrics","text":"Exact Match (EM)","title":"Metrics:"},{"location":"Background/models/prompting.html","text":"LLM Prompting \u00b6 \u203c\ufe0f Prompting Guide Wolfram Prompt Repo \u203c\ufe0f Prompt Engine (MSFT) database tool MIT license scale.com/spellbook Prompt engineering \u00b6 Prompting is Programming: A Query Language for Large Language Models Manual \u00b6 OPEN AI best practices Go over all of these! https://www.promptingguide.ai/techniques A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT Examples \u00b6 Pretend you have an IQ of 120 Minimizing AI- plagiarism prompting strategy. \u00b6 \"You are a creative writer, and you like to write everything differently from others. Your task is to follow the instructions below and continue writing at the end of the text given. The instructions (given in markdown format) are \u201cWrite in a way different from the actual continuation, if there is one\u201d, and \u201cNo plagiarism is allowed\u201d.\" https://arxiv.org/pdf/2304.08637.pdf 'According To' \u00b6 \u201cAccording to ...\u201d Prompting Language Models Improves Quoting from Pre-Training Data The grounding prompt According to { some_reputable_source} prompt inception additions increases output quality improves over the null prompt in nearly every dataset and metric, typically by 5-15%. According to {some_reputable_source} ... Summary: \u00b6 Provide several examples to ground it. Good to evaluate this and see if input examples give expected scores. Modify the prompt if it isn't. Consider prompt versioning to keep track of outputs more easily. Breag prompts into smaller prompts Chain of Thought Prompting Generate many outputs and pick final one or use LLM to pick best one. Self consistency technique NOTE: Not model universal and not robust to updated changes: not stable. Automatic \u00b6 Prompt compression \u00b6 Learning to Compress Prompts with Gist Tokens . Can enable 26x compression and 40% FLOP reduction and improvements. Trains 'gist tokens' to summarize information. Resources \u00b6 \u203c\ufe0f Awesome Prompts \u203c\ufe0f Prompt Engineering by Lillian Wang Prompt Engineering Guide Best practices for prompt engineering Chain of Thought Prompting Elicits Reasoning in Large Language Models Automatic Prompt Engineering \u2192 Gave a CoT improvement suggestion \"Let's work this out in a step by step by way to be sure we have the right answer.\" Techniques to improve reliability By OpenAI Give clearer instructions Split complex tasks into simpler subtasks Structure the instruction to keep the model on task Prompt the model to explain before answering Ask for justifications of many possible answers, and then synthesize Generate many outputs, and then use the model to pick the best one Fine-tune custom models to maximize performance Prompt tuning \u00b6 Uses a layer to not change prompts but change the embedding of the prompts. - The Power of Scale for Parameter-Efficient Prompt Tuning Boosted Prompting: few shot prompts that progressively solve more of the problem. Prompt and optimization \u00b6 Large Language Models Can Self Improve Using Chain of thought to provide better examples and then fine-tune the LLM. Refiner Iteratively improves itself based on an LLM critic PROMPT generator To save a few words by just entering a persona and igives prompt output. Manual Prompt optimization \u00b6 Auto Prompt Optimizations \u00b6 A good description of advanced prompt tuning AutoPrompt [5] combines the original prompt input with a set of shared (across all input data) \u201ctrigger tokens\u201d that are selected via a gradient-based search to improve performance. Prefix Tuning [6] adds several \u201cprefix\u201d tokens to the prompt embedding in both input and hidden layers, then trains the parameters of this prefix (leaving model parameters fixed) with gradient descent as a parameter-efficient fine-tuning strategy. Prompt Tuning [7] is similar to prefix tuning, but prefix tokens are only added to the input layer. These tokens are fine-tuned on each task that the language model solves, allowing prefix tokens to condition the model for a given task. P-Tuning [8] adds task-specific anchor tokens to the model\u2019s input layer that are fine-tuned but allows these tokens to be placed at arbitrary locations (e.g., the middle of the prompt), making the approach more flexible than prefix tuning. [5] Shin, Taylor, et al. \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts.\" arXiv preprint arXiv:2010.15980 (2020). [6] Li, Xiang Lisa, and Percy Liang. \"Prefix-tuning: Optimizing continuous prompts for a generation.\" arXiv preprint arXiv:2101.00190 (2021). [7] Lester, Brian, Rami Al-Rfou, and Noah Constant. \"The power of scale for parameter-efficient prompt tuning.\" arXiv preprint arXiv:2104.08691 (2021). [8] Liu, Xiao, et al. \"GPT understands, too.\" arXiv preprint arXiv:2103.10385 (2021).","title":"Prompting"},{"location":"Background/models/prompting.html#llm-prompting","text":"\u203c\ufe0f Prompting Guide Wolfram Prompt Repo \u203c\ufe0f Prompt Engine (MSFT) database tool MIT license scale.com/spellbook","title":"LLM Prompting"},{"location":"Background/models/prompting.html#prompt-engineering","text":"Prompting is Programming: A Query Language for Large Language Models","title":"Prompt engineering"},{"location":"Background/models/prompting.html#manual","text":"OPEN AI best practices Go over all of these! https://www.promptingguide.ai/techniques A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT","title":"Manual"},{"location":"Background/models/prompting.html#examples","text":"Pretend you have an IQ of 120","title":"Examples"},{"location":"Background/models/prompting.html#minimizing-ai-plagiarism-prompting-strategy","text":"\"You are a creative writer, and you like to write everything differently from others. Your task is to follow the instructions below and continue writing at the end of the text given. The instructions (given in markdown format) are \u201cWrite in a way different from the actual continuation, if there is one\u201d, and \u201cNo plagiarism is allowed\u201d.\" https://arxiv.org/pdf/2304.08637.pdf","title":"Minimizing AI- plagiarism prompting strategy."},{"location":"Background/models/prompting.html#according-to","text":"\u201cAccording to ...\u201d Prompting Language Models Improves Quoting from Pre-Training Data The grounding prompt According to { some_reputable_source} prompt inception additions increases output quality improves over the null prompt in nearly every dataset and metric, typically by 5-15%. According to {some_reputable_source} ...","title":"'According To'"},{"location":"Background/models/prompting.html#summary","text":"Provide several examples to ground it. Good to evaluate this and see if input examples give expected scores. Modify the prompt if it isn't. Consider prompt versioning to keep track of outputs more easily. Breag prompts into smaller prompts Chain of Thought Prompting Generate many outputs and pick final one or use LLM to pick best one. Self consistency technique NOTE: Not model universal and not robust to updated changes: not stable.","title":"Summary:"},{"location":"Background/models/prompting.html#automatic","text":"","title":"Automatic"},{"location":"Background/models/prompting.html#prompt-compression","text":"Learning to Compress Prompts with Gist Tokens . Can enable 26x compression and 40% FLOP reduction and improvements. Trains 'gist tokens' to summarize information.","title":"Prompt compression"},{"location":"Background/models/prompting.html#resources","text":"\u203c\ufe0f Awesome Prompts \u203c\ufe0f Prompt Engineering by Lillian Wang Prompt Engineering Guide Best practices for prompt engineering Chain of Thought Prompting Elicits Reasoning in Large Language Models Automatic Prompt Engineering \u2192 Gave a CoT improvement suggestion \"Let's work this out in a step by step by way to be sure we have the right answer.\" Techniques to improve reliability By OpenAI Give clearer instructions Split complex tasks into simpler subtasks Structure the instruction to keep the model on task Prompt the model to explain before answering Ask for justifications of many possible answers, and then synthesize Generate many outputs, and then use the model to pick the best one Fine-tune custom models to maximize performance","title":"Resources"},{"location":"Background/models/prompting.html#prompt-tuning","text":"Uses a layer to not change prompts but change the embedding of the prompts. - The Power of Scale for Parameter-Efficient Prompt Tuning Boosted Prompting: few shot prompts that progressively solve more of the problem.","title":"Prompt tuning"},{"location":"Background/models/prompting.html#prompt-and-optimization","text":"Large Language Models Can Self Improve Using Chain of thought to provide better examples and then fine-tune the LLM. Refiner Iteratively improves itself based on an LLM critic PROMPT generator To save a few words by just entering a persona and igives prompt output.","title":"Prompt and optimization"},{"location":"Background/models/prompting.html#manual-prompt-optimization","text":"","title":"Manual Prompt optimization"},{"location":"Background/models/prompting.html#auto-prompt-optimizations","text":"A good description of advanced prompt tuning AutoPrompt [5] combines the original prompt input with a set of shared (across all input data) \u201ctrigger tokens\u201d that are selected via a gradient-based search to improve performance. Prefix Tuning [6] adds several \u201cprefix\u201d tokens to the prompt embedding in both input and hidden layers, then trains the parameters of this prefix (leaving model parameters fixed) with gradient descent as a parameter-efficient fine-tuning strategy. Prompt Tuning [7] is similar to prefix tuning, but prefix tokens are only added to the input layer. These tokens are fine-tuned on each task that the language model solves, allowing prefix tokens to condition the model for a given task. P-Tuning [8] adds task-specific anchor tokens to the model\u2019s input layer that are fine-tuned but allows these tokens to be placed at arbitrary locations (e.g., the middle of the prompt), making the approach more flexible than prefix tuning. [5] Shin, Taylor, et al. \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts.\" arXiv preprint arXiv:2010.15980 (2020). [6] Li, Xiang Lisa, and Percy Liang. \"Prefix-tuning: Optimizing continuous prompts for a generation.\" arXiv preprint arXiv:2101.00190 (2021). [7] Lester, Brian, Rami Al-Rfou, and Noah Constant. \"The power of scale for parameter-efficient prompt tuning.\" arXiv preprint arXiv:2104.08691 (2021). [8] Liu, Xiao, et al. \"GPT understands, too.\" arXiv preprint arXiv:2103.10385 (2021).","title":"Auto Prompt Optimizations"},{"location":"Background/models/transformers.html","text":"Transformers \u00b6 Components \u00b6 TODO: Describe transformers and components References and Research \u00b6 Amazing Presentation on Transformers Basic Transformer information \u00b6 Attention Is All you Need Initial paper indicating that attention is very powerful and potential replacement of LLM architectures. Neural Machine Translation by Jointly Learning to Align and Translate First paper indicating the notion of 'attention' sort of mechanism. Positional Encoding \u00b6 This component helps to remove the impilcit position-independence that 'vanilla' attention methods have. A Gentle Introduction to Positional Encoding in Transformer Models, pt1 Improvements \u00b6 GPT \u00b6 Illustrated GPT How GPT3 works Five years of progress in GPTs Excellent summary of the progress of GPT over time, revealing core components, optimizations, and essential variations to the major Foundation model architectures. Formal Algorithms for Transformers in 2023 Important discussion revealing the components of Transformers. Improvements and Optimizations \u00b6 Focusing on context-windows \u00b6 Scaling Transformer to 1M tokens and beyond with RMT Github Uses a Recurrent Memory Transformer(RMT) architecture to extend understanding to large lengths. \u203c\ufe0f MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers MEGABYTE segments sequences into patches and uses a local submodel within patches and a global model between patches Hyena Architecture Uses inspiration from FFT to create a drop in replacement for Transformer models. Quite complex and maybe overhyped. Infinite former Uses a representation of input sequence as a continuous signal expressed in a combination of N radial basis functions. Promising but potentially complex. Worth consideration Github To reduce compute \u00b6 SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression Fine Tuning \u00b6 Using examples to fine-tune a model can reduce the number of tokens needed to achieve a sufficiently reasonable response. Can be expensive to retrain though. Symbol Tuning Improves in-context learning in Language Models Modality variations \u00b6 Vision \u00b6 Graphs \u00b6 Transformers Meet Directed Graphs An interesting-if-also-complex variation of Transformer GNNs that uses 'direction-aware' positional encodings to help handle both undirected and directed graphs. Fairness Enablement \u00b6 Concept Erasure Training variations \u00b6 LinkBERT places in the context window hyperlinked references to achieve better performance. Multimodal \u00b6 \u203c\ufe0f Visual GPT \u203c\ufe0f Language is not all you need Abstractions \u00b6 Looped Transformers and Programmable Computers Understanding that transformer networks can simulate complex algorithms when hardcoded with specific weights and made intoa loop. 'Machine Learning' 'Machine code'. \"We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer.\"","title":"Transformers"},{"location":"Background/models/transformers.html#transformers","text":"","title":"Transformers"},{"location":"Background/models/transformers.html#components","text":"TODO: Describe transformers and components","title":"Components"},{"location":"Background/models/transformers.html#references-and-research","text":"Amazing Presentation on Transformers","title":"References and Research"},{"location":"Background/models/transformers.html#basic-transformer-information","text":"Attention Is All you Need Initial paper indicating that attention is very powerful and potential replacement of LLM architectures. Neural Machine Translation by Jointly Learning to Align and Translate First paper indicating the notion of 'attention' sort of mechanism.","title":"Basic Transformer information"},{"location":"Background/models/transformers.html#positional-encoding","text":"This component helps to remove the impilcit position-independence that 'vanilla' attention methods have. A Gentle Introduction to Positional Encoding in Transformer Models, pt1","title":"Positional Encoding"},{"location":"Background/models/transformers.html#improvements","text":"","title":"Improvements"},{"location":"Background/models/transformers.html#gpt","text":"Illustrated GPT How GPT3 works Five years of progress in GPTs Excellent summary of the progress of GPT over time, revealing core components, optimizations, and essential variations to the major Foundation model architectures. Formal Algorithms for Transformers in 2023 Important discussion revealing the components of Transformers.","title":"GPT"},{"location":"Background/models/transformers.html#improvements-and-optimizations","text":"","title":"Improvements and Optimizations"},{"location":"Background/models/transformers.html#focusing-on-context-windows","text":"Scaling Transformer to 1M tokens and beyond with RMT Github Uses a Recurrent Memory Transformer(RMT) architecture to extend understanding to large lengths. \u203c\ufe0f MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers MEGABYTE segments sequences into patches and uses a local submodel within patches and a global model between patches Hyena Architecture Uses inspiration from FFT to create a drop in replacement for Transformer models. Quite complex and maybe overhyped. Infinite former Uses a representation of input sequence as a continuous signal expressed in a combination of N radial basis functions. Promising but potentially complex. Worth consideration Github","title":"Focusing on context-windows"},{"location":"Background/models/transformers.html#to-reduce-compute","text":"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression","title":"To reduce compute"},{"location":"Background/models/transformers.html#fine-tuning","text":"Using examples to fine-tune a model can reduce the number of tokens needed to achieve a sufficiently reasonable response. Can be expensive to retrain though. Symbol Tuning Improves in-context learning in Language Models","title":"Fine Tuning"},{"location":"Background/models/transformers.html#modality-variations","text":"","title":"Modality variations"},{"location":"Background/models/transformers.html#vision","text":"","title":"Vision"},{"location":"Background/models/transformers.html#graphs","text":"Transformers Meet Directed Graphs An interesting-if-also-complex variation of Transformer GNNs that uses 'direction-aware' positional encodings to help handle both undirected and directed graphs.","title":"Graphs"},{"location":"Background/models/transformers.html#fairness-enablement","text":"Concept Erasure","title":"Fairness Enablement"},{"location":"Background/models/transformers.html#training-variations","text":"LinkBERT places in the context window hyperlinked references to achieve better performance.","title":"Training variations"},{"location":"Background/models/transformers.html#multimodal","text":"\u203c\ufe0f Visual GPT \u203c\ufe0f Language is not all you need","title":"Multimodal"},{"location":"Background/models/transformers.html#abstractions","text":"Looped Transformers and Programmable Computers Understanding that transformer networks can simulate complex algorithms when hardcoded with specific weights and made intoa loop. 'Machine Learning' 'Machine code'. \"We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer.\"","title":"Abstractions"},{"location":"Background/overview/index.html","text":"The base components of GENAI","title":"Overview"},{"location":"Background/overview/applications.html","text":"Software \u00b6 Code \u00b6 Wizard Coding AutoPR Component replacements \u00b6 GPT as backend Book Writing \u00b6 Pyprompt chatgpt Motion GPT Science and Tech \u00b6 Emergent autonomous scientific research Robotics \u00b6 CLAIRIFY Translates English to domain-specific languages like robots. https://arxiv.org/abs/2303.14100 Healthcare \u00b6 Health system-scale language models are all-purpose prediction engines Uses LLM based system to integrate real time clinical workflows with note-writing and electronic ordering. Generally quite-performant and. a great indication of how they could be used to predict things such as readmission rates, and many other applications. Chemistry \u00b6 Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction A quality framework using heirarchichal metagraphs to stitch-together molecular nodes resulting in leaves that are 'actual' molecules. Using graph neural-diffusion, it does amazingly well even with minimal data-sets (100 examples). Biology \u00b6 Evolutionary-scale prediction of atomic-level protein structure with a language model End to end Language model enabling structure sequence pairing, coupled with an equivariant transformer structure model at the end. https://arxiv.org/pdf/2303.16416.pdf https://arxiv.org/abs/2304.02496 \uff01 Biomedical simulation Societal simulations \u00b6 Generative Agents: Interactive Simulacra of Human Behavior : They gave 25 AI agents motivations & memory, and put them in a simulated town. Not only did they engage in complex behavior (including throwing a Valentine\u2019s Day party) but the actions were rated more human than humans roleplaying. Demo: https://t.co/pYNF4BBveG Finance \u00b6 ML for trading (NOT LLM based) https://github.com/irgolic/AutoPR Finance GPT LLMs for finance Second Brain \u00b6 \u203c\ufe0f \u203c\ufe0f Quiver A LLM for self Second brain.","title":"Applications"},{"location":"Background/overview/applications.html#software","text":"","title":"Software"},{"location":"Background/overview/applications.html#code","text":"Wizard Coding AutoPR","title":"Code"},{"location":"Background/overview/applications.html#component-replacements","text":"GPT as backend","title":"Component replacements"},{"location":"Background/overview/applications.html#book-writing","text":"Pyprompt chatgpt Motion GPT","title":"Book Writing"},{"location":"Background/overview/applications.html#science-and-tech","text":"Emergent autonomous scientific research","title":"Science and Tech"},{"location":"Background/overview/applications.html#robotics","text":"CLAIRIFY Translates English to domain-specific languages like robots. https://arxiv.org/abs/2303.14100","title":"Robotics"},{"location":"Background/overview/applications.html#healthcare","text":"Health system-scale language models are all-purpose prediction engines Uses LLM based system to integrate real time clinical workflows with note-writing and electronic ordering. Generally quite-performant and. a great indication of how they could be used to predict things such as readmission rates, and many other applications.","title":"Healthcare"},{"location":"Background/overview/applications.html#chemistry","text":"Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction A quality framework using heirarchichal metagraphs to stitch-together molecular nodes resulting in leaves that are 'actual' molecules. Using graph neural-diffusion, it does amazingly well even with minimal data-sets (100 examples).","title":"Chemistry"},{"location":"Background/overview/applications.html#biology","text":"Evolutionary-scale prediction of atomic-level protein structure with a language model End to end Language model enabling structure sequence pairing, coupled with an equivariant transformer structure model at the end. https://arxiv.org/pdf/2303.16416.pdf https://arxiv.org/abs/2304.02496 \uff01 Biomedical simulation","title":"Biology"},{"location":"Background/overview/applications.html#societal-simulations","text":"Generative Agents: Interactive Simulacra of Human Behavior : They gave 25 AI agents motivations & memory, and put them in a simulated town. Not only did they engage in complex behavior (including throwing a Valentine\u2019s Day party) but the actions were rated more human than humans roleplaying. Demo: https://t.co/pYNF4BBveG","title":"Societal simulations"},{"location":"Background/overview/applications.html#finance","text":"ML for trading (NOT LLM based) https://github.com/irgolic/AutoPR Finance GPT LLMs for finance","title":"Finance"},{"location":"Background/overview/applications.html#second-brain","text":"\u203c\ufe0f \u203c\ufe0f Quiver A LLM for self Second brain.","title":"Second Brain"},{"location":"Background/overview/extra_resources.html","text":"Videos \u00b6 State of GPT by Andrej Karpathy Lex Fridman David Shapiro AI Explained Yannic Kilcher Whole system \u00b6 \u203c\ufe0f Emerging Architectures for LLM Applications A very nice high overview of the component market for LLM architectures. LLMs \u00b6 A Survey of Large Language Models A very comprehensive paper discussing LLM technology. Understanding Large Language Models LLM Prompting \u00b6 \u203c\ufe0f LLM Practical Guide based on paper . \u203c\ufe0f Prompting Guide Wolfram Prompt Repo \u203c\ufe0f Prompt Engine (MSFT) database tool MIT license Video + Podcasts \u00b6 Lex Fridman David Shapiro AI Explained Yannic Kilcher State of GPT by Andrej Karpathy","title":"Extra resources"},{"location":"Background/overview/extra_resources.html#videos","text":"State of GPT by Andrej Karpathy Lex Fridman David Shapiro AI Explained Yannic Kilcher","title":"Videos"},{"location":"Background/overview/extra_resources.html#whole-system","text":"\u203c\ufe0f Emerging Architectures for LLM Applications A very nice high overview of the component market for LLM architectures.","title":"Whole system"},{"location":"Background/overview/extra_resources.html#llms","text":"A Survey of Large Language Models A very comprehensive paper discussing LLM technology. Understanding Large Language Models","title":"LLMs"},{"location":"Background/overview/extra_resources.html#llm-prompting","text":"\u203c\ufe0f LLM Practical Guide based on paper . \u203c\ufe0f Prompting Guide Wolfram Prompt Repo \u203c\ufe0f Prompt Engine (MSFT) database tool MIT license","title":"LLM Prompting"},{"location":"Background/overview/extra_resources.html#video-podcasts","text":"Lex Fridman David Shapiro AI Explained Yannic Kilcher State of GPT by Andrej Karpathy","title":"Video + Podcasts"},{"location":"Background/training/index.html","text":"LLM Engineering by Huyen Chip The whole training process by Huyen Chip Frameworks \u00b6 Levanter (not just LLMS) Codebase for training FMs with JAX. Using Haliax for naming tensors field-names instead of indexes. (for example Batch, Feature....). Full sharding and distributable / parallelizable. DeepSpeed ZeRO++ A framework for accelerating model pre-training, finetuning, RLHF updating. by minimizing communication overhead. A likely essential concept to be very familiar with. \u203c\ufe0f RL4LMs by microsoft A modular RL library to fine-tune language models to human preferences. paper Methods and IMprovements \u00b6 Fine Tuning using Distillation \u00b6 Train on model trains a new model on the output of a new model. - Alpaca Fine tuning Optimizations \u00b6 Full Parameter Fine-Tuning for Large Language Models with Limited Resources. Introduces LOMO: LOw-Memory Optimization to fuse Adapter layers \u00b6 AdapterHub: A Framework for Adapting Transformers Website Adapters are efficient and performant layers that can optimize performance without needing to do inefficient fine-tuning. RLHF \u00b6 \u203c\ufe0f RLHF basics by hugging face A realy good intro to parse again. RLHF for Palm in Pytorch AligningLargeLanguageModelsthroughSyntheticFeedback Using a heirarchy of systems to AI-enabled ranking \u00b6 Can foundation models label data like humans? using GPT to review model outputs produced biased results. Changing the prompt doesn't really help to de-bias it. Lots of additional considerations surrounding model evaluation Multi-model methods \u00b6 Scaling Expert Language Models with Unsupervised Domain Discovery \"parse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. \" Pruning and compression \u00b6 SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot Remove up to ~50% parameters preserving performance Scaling Expert Language Models with Unsupervised Domain Discovery Cluster-Branch-Train-Merge (c-BTM), a new way to scale sparse expert LLMs on any dataset. Github SqueezeLLM They are able to have 2x fold in model size for equivalent performance in perplexity. They use 'Dense and SParce Quantization' Github","title":"Training"},{"location":"Background/training/index.html#frameworks","text":"Levanter (not just LLMS) Codebase for training FMs with JAX. Using Haliax for naming tensors field-names instead of indexes. (for example Batch, Feature....). Full sharding and distributable / parallelizable. DeepSpeed ZeRO++ A framework for accelerating model pre-training, finetuning, RLHF updating. by minimizing communication overhead. A likely essential concept to be very familiar with. \u203c\ufe0f RL4LMs by microsoft A modular RL library to fine-tune language models to human preferences. paper","title":"Frameworks"},{"location":"Background/training/index.html#methods-and-improvements","text":"","title":"Methods and IMprovements"},{"location":"Background/training/index.html#fine-tuning-using-distillation","text":"Train on model trains a new model on the output of a new model. - Alpaca","title":"Fine Tuning using Distillation"},{"location":"Background/training/index.html#fine-tuning-optimizations","text":"Full Parameter Fine-Tuning for Large Language Models with Limited Resources. Introduces LOMO: LOw-Memory Optimization to fuse","title":"Fine tuning Optimizations"},{"location":"Background/training/index.html#adapter-layers","text":"AdapterHub: A Framework for Adapting Transformers Website Adapters are efficient and performant layers that can optimize performance without needing to do inefficient fine-tuning.","title":"Adapter layers"},{"location":"Background/training/index.html#rlhf","text":"\u203c\ufe0f RLHF basics by hugging face A realy good intro to parse again. RLHF for Palm in Pytorch AligningLargeLanguageModelsthroughSyntheticFeedback Using a heirarchy of systems to","title":"RLHF"},{"location":"Background/training/index.html#ai-enabled-ranking","text":"Can foundation models label data like humans? using GPT to review model outputs produced biased results. Changing the prompt doesn't really help to de-bias it. Lots of additional considerations surrounding model evaluation","title":"AI-enabled ranking"},{"location":"Background/training/index.html#multi-model-methods","text":"Scaling Expert Language Models with Unsupervised Domain Discovery \"parse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. \"","title":"Multi-model methods"},{"location":"Background/training/index.html#pruning-and-compression","text":"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot Remove up to ~50% parameters preserving performance Scaling Expert Language Models with Unsupervised Domain Discovery Cluster-Branch-Train-Merge (c-BTM), a new way to scale sparse expert LLMs on any dataset. Github SqueezeLLM They are able to have 2x fold in model size for equivalent performance in perplexity. They use 'Dense and SParce Quantization' Github","title":"Pruning and compression"},{"location":"this_project/index.html","text":"Thanks for being interested in this project. This relays how we are putting together the components that enable this project to exist.","title":"This project"},{"location":"this_project/contributing.html","text":"In order to contribute...","title":"Contributing"}]}