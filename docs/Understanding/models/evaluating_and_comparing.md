There are many ways that you can evaluate your model, and the manner of evaluation will depend on your use-case. 

While single-LLM calls are useful to evaluate, [comparing and evaluating](../agents/evaluating_and_comparing.md) system-evaluation will likely be essential to ensure successful [deployment](../../Using/deploying/index.md).

## Evaluation tools

!!! code "[Legal Bench](https://github.com/HazyResearch/legalbench/) is an ongoing open science effort to collaboratively curate tasks for evaluating LLM legal reasoning in English."

