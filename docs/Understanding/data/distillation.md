Dataset distillation reduces the storage and computational consumption of training a network by generating a small surrogate dataset that encapsulates rich information
of the original large-scale one)

??? abstract "![GitHub Repo stars](https://badgen.net/github/stars/vimar-gu/MinimaxDiffusion) [Efficient Dataset Distillation via Minimax Diffusion](https://github.com/vimar-gu/MinimaxDiffusion)"

    [Paper](https://arxiv.org/pdf/2311.15529v1.pdf)
    <img width="333" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/5b5bee3e-f079-4437-b094-eb7e39bc3aec">
