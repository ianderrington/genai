
## Data sources


RedPajama
Pile
CommonCrawl (webscrape)
C4 (CommonCrawl)
Github
Books
Arxiv
StackExchange

- [unarXive 2022: All arXiv Publications Pre-Processed for NLP](https://arxiv.org/pdf/2303.14957.pdf)

- [Redpajama](https://www.together.xyz/blog/redpajama)
- [BIG-bench](https://github.com/google/BIG-bench/blob/main/docs/doc.md)
- [Metaseq](https://github.com/facebookresearch/metaseq/)
- [Kaggle-code](https://www.kaggle.com/datasets/kaggle/meta-kaggle-code)


The largest open source text dataset just dropped

??? tip "[Dolma. (by AI2)](https://huggingface.co/datasets/allenai/dolma)"
    WARNING: The license is not 'open source'
    3 Trillion tokens of high quality data.

    - Diverse: Documents, code, academic papers, wiki..
    - Focused: English only.
    - De-duplicated.
    - Filtered for high quality.

    But most importantly:
    The largest open curated dataset for pretraining.

    -----
    
    • Link: https://huggingface.co/datasets/allenai/dolma
    • Blog: https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64
    • Code: https://github.com/allenai/dolma
    • Paper: https://drive.google.com/file/d/12gOf5I5RytsD159nSP7iim_5zN31FCXq/view