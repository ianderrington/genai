Prompt injections involve the modification of an input prompt prior to model processing.

Use of prompt injections, also 'prompt hacking' can allow for intentional bypasses of any pre-established alignment guardrails thereby enabling non-aligned output to occur.

- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org) and [paper](https://arxiv.org/pdf/2307.15043.pdf) demonstrate generally presently undefended attacks on models just by appending to the prompt. Prompt injection.
