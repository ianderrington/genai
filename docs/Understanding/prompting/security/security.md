## Under construction ðŸš§
In order to provide effective security, it is important to know first how prompts can be considered 'insecure'. 


Problems: 

1. **Jailbreaking and prompt hacking** allow the user to gain control for unintended, and potentially harmful sue
1. **Befuddlement** tricks the LLM, particularly in customer relations settings, to confabulate
1. Data privacy
1. Prompt leaking
1. Tool hacking


