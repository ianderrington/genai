Here we share novel and promising architectures that may supplement or supplant other presently established models.

## Models


!!! "[Hyena Architecture](https://arxiv.org/pdf/2302.10866.pdf) Uses inspiration from FFT to create a drop in replacement for Transformer models."
    [Github implementation for PyTorch](https://github.com/lucidrains/MEGABYTE-pytorch)


!!! tip "[Retentive Network: A successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf) Important LLM-like system using similar components that may help it to be more scaleable than `O(N^2)` memory and `O(N)` inference complexity."


!!! tip "[Bayesian Flow Networks](https://arxiv.org/pdf/2308.07037.pdf) A new class of generative models for discrete and continuous data and generation"


!!! tip "[Retentive Network: A successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf) Important LLM-like system using similar components that may help it to be more scaleable than `O(N^2)` memory and `O(N)` inference complexity."

??? code "[Memoria](https://github.com/cosmoquester/memoria) stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebbâ€™s rule. "
    [Paper](https://arxiv.org/pdf/2310.03052.pdf)
    <img width="778" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/2a0bc1b1-9409-45a3-b8b4-08d363619354">
    <img width="628" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/a2cd82b8-b92a-446e-bc8f-95116dfe15ea">
    <img width="688" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/fe79add1-6748-45d8-a187-1db22c74a185">


