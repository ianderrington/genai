??? code "[Langfuse](https://github.com/langfuse/langfuse?tab=readme-ov-file)"


??? code "[DeepSpeed ZeRO++](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/) A framework for accelerating model pre-training, finetuning, RLHF updating." deepspeed
     By minimizing communication overhead. A likely essential concept to be very familiar with.

??? abstract "![GitHub Repo stars](https://badgen.net/github/stars/stanford-crfm/levanter) [Levanter (not just LLMS) ](https://github.com/stanford-crfm/levanter) Codebase for training FMs with JAX."
    [Release](https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html) 
    Using Haliax for naming tensors field names instead of indexes. (for example Batch, Feature....). Full sharding and distributable/parallelizable.

??? abstract "[RL4LMs by microsoft](https://github.com/allenai/RL4LMs/tree/main) A modular RL library to fine-tune language models to human preferences."
    [paper](https://arxiv.org/pdf/2305.08844.pdf)

### References

- [Distributed training frameworks and tools](https://neptune.ai/blog/distributed-training-frameworks-and-tools)



