ğŸš§ This section is under construction ğŸ—ï¸

## GPUs

In order to create models, large volumes of matrix multiplication is necessary. GPUs are designed for this. 

[Tim Dettmers on GPUs](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)

## Cloud computation 

## Local computation

- [Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&A](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)

## References
ï¿¼
