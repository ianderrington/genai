Data is the most important part of training any model. 

Data can be 'real' or 'simulated', though there is general consensus that simulated data can lead to worse models. 

Data, represented on disk in binary, though perhaps read in different formats (utf8, ascii, pure-binary) is broken up into individual units called tokens. These tokens have an integer values

## Binary Representations
- ‼️ [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf) Reveals that just taking file bytes into transformer technology can directly enable improvements in performance accuracy. The accuracy method varies based on encoding method. Their model is called ByteFormer [Github](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)
- 

## Tokenization

- [Tiktoken](https://github.com/openai/tiktoken) uses BPE and is theoretically used in GPT models. 
- [Token Monster](https://github.com/alasdairforsythe/tokenmonster) Uses 35% fewer tokens and uses a top-down approach, instead of a bottom-up constructive approach. Likely of high value. 

### Special tokens




## Embeddings
‼️[What are Embeddings](http://vickiboykis.com/what_are_embeddings/)[Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)

##