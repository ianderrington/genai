<img width="1719" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/c5ae3eb1-03e3-41d2-ba2d-0061c5a3b41d">### Biology
[Genetics Language Models](https://arxiv.org/pdf/2311.07621.pdf)



### Healthcare 

??? "[A study of generative large language model for medical research and healthcare](https://www.nature.com/articles/s41746-023-00958-w)"
    The authors train and use a larger 80G GPT-3 style to generate quality synthetic datasets mimicking doctor's notes. These words are used to train a smaller GPT that could generate very high-quality datasets to improve performance. In essence, it offers a very profound general way to retrain LLMs with synthetic data.
* train(Large+Vague model) -> generate highly specific data
* train(small specific model) on specific data.
    
    ![image](https://github.com/ianderrington/genai/assets/76016868/5dd829ee-f8f0-4bec-b02d-4941d1d289b7)



??? "[Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist](https://arxiv.org/pdf/2311.02107.pdf) provides a framework/checklist for evaluating GenAI in healthcare"
    [TREGAI Github](https://github.com/nliulab/GenAI-Ethical-Checklist)
    [DocX checklist](https://drive.google.com/file/d/1ro_-GqITKHfNpHYTegUQdE-xm5t0Rvm6/view)
