
Like other applications, deployment of LLM technologies will rely on front-end and back-end components. Front-end will allow for ease-of-use of the components. Back-end components enable the hosting, serving, and recording of any information that is needed for [observability](./observability.md). 

## Back-End (Model serving)
??? tip "[Text Generation Inference](https://github.com/Preemo-Inc/text-generation-inference) an open-sourced implementation forked from HF"
    "A Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets."    
    ![image](https://github.com/ianderrington/genai/assets/76016868/a3f5ddbf-a2e3-45ae-bca4-200c07c9dd91)

!!! tip "[Lit-Gpt](https://github.com/Lightning-AI/lit-gpt#setup) Hackable implementation of state-of-the-art open-source large language models released under the Apache 2.0 license."

!!! tip "[Azure-Chat-GPT](https://github.com/davidxw/azurechatgpt) to run GPT on Azure services"

!!! code "[Torch Serve](https://pytorch.org/serve/large_model_inference.html) enable efficient serving.

??? tip "[Triton Inference Server](https://github.com/triton-inference-server/server) Part of NVIDIA AI Inference" 
    [Tutorial](https://github.com/triton-inference-server/server)

### Tutorials
- [](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)

## Front-End Interfaces
People have to access it to be useful

- [GPT Graph](https://github.com/m-elbably/gpt-graph) Allows for a graphical network representation of chat interactions.

### Open source methods

- [Streamlit](https://blog.streamlit.io/langchain-streamlit/)
- [DemoGPT](https://github.com/melih-unsal/DemoGPT) Connects Langchain and streamlit to create dynamic apps that can be repeatedly used for interacting with Chat- GPTs. 



