
Like other applications, deployment of LLM technologies will rely on front-end and back-end components. Front-end will allow for ease-of-use of the components. Back-end components enable the hosting, serving, and recording of any information that is needed for [observability](./observability.md). 

## Back-End serving
??? tip "[Text Generation Inference](https://github.com/Preemo-Inc/text-generation-inference) an open-sourced implementation forked from HF"
    "A Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets."    
    ![image](https://github.com/ianderrington/genai/assets/76016868/a3f5ddbf-a2e3-45ae-bca4-200c07c9dd91)




## Front-End Interfaces
People have to access it to be useful

- [GPT Graph](https://github.com/m-elbably/gpt-graph) Allows for a graphical network representation of chat interactions.

### Open source methods

- [Streamlit](https://blog.streamlit.io/langchain-streamlit/)
- [DemoGPT](https://github.com/melih-unsal/DemoGPT) Connects Langchain and streamlit to create dynamic apps that can be repeatedly used for interacting with Chat- GPTs. 



