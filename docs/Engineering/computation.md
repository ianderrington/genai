## GPUS

In order to create models, large volumes of matrix multiplication is necessary. GPUs are designed for this. 

[Tim Dettmers on GPUs](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)

## Cloud computation 

## Local computation

- [Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&A](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)

## API-Access

!!! "[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)"
TODO:

### Distributed

!!! "[Petals](https://github.com/bigscience-workshop/petals) Run large language models at home, BitTorrent-style."

    Generate text with distributed LLaMA 2 (70B), Stable Beluga 2, Guanaco-65B or BLOOM-176B and fine‑tune them for your own tasks — right from your desktop computer or Google Cola
    [Launch your own swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm)

