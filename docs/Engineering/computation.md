## GPUS

In order to create models, large volumes of matrix multiplication is necessary. GPUs are designed for this. 

[Tim Dettmers on GPUs](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)

## Cloud computation 

## Local computation

- [Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&A](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference)

## API-Access

[Petals](https://github.com/bigscience-workshop/petals)
[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
