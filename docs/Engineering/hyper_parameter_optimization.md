<>
!!! code [Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit
](https://arxiv.org/abs/2308.01814)
    "We show that optimal hyperparameters become stable across neural network sizes when we parametrize the model in maximal update parametrization (μP). This can be used to tune extremely large neural networks such as large pretrained transformers, as we have done in our work. More generally, μP reduces the fragility and uncertainty when transitioning from exploration to scaling up, which are not often talked about explicitly in the deep learning literature."