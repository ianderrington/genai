Because of potential pitfals with Generative AI technology, it is essential to evaluate, compare, and test models such that they meet the indendent requirements. 

Below are some tools that you can use to help with this!

## Tools

??? code "[Arthur.ai Bench](https://github.com/arthur-ai/bench) Bench is a tool for evaluating LLMs for production use cases. "
    
    ![image](https://github.com/ianderrington/genai/assets/76016868/377d86c7-9ebf-4828-8e6a-582b86a499f9)

    ![image](https://github.com/ianderrington/genai/assets/76016868/081f14b5-f2b7-47e6-985b-a886ed66eaf1)

??? code "[DeepEval](https://github.com/mr-gpt/deepeval) provides a Pythonic way to run offline evaluations on your LLM pipelines"
    "... so you can launch comfortably into production. The guiding philosophy is a "Pytest for LLM" that aims to make productionizing and evaluating LLMs as easy as ensuring all tests pass."
    ![image](https://github.com/mr-gpt/deepeval/blob/main/assets/synthetic-query-generation.png)



