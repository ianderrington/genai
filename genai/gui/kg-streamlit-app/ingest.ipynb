{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "# !pip install -U langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain.document_loaders import WikipediaLoader, PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader, PyPDFLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.embeddings.azure_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# from langchain.graphs import Neo4jGraph\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.text_splitter import TokenTextSplitter, CharacterTextSplitter\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "from neo4j.exceptions import ClientError\n",
    "import os\n",
    "from langchain.schema import HumanMessage, ChatMessage, AIMessage\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 6 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 2 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 3 children\n",
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 3 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 4 children\n",
      "Processing 7 children\n",
      "Processing 7 children\n",
      "Processing 1 children\n",
      "Processing 7 children\n",
      "Processing 5 children\n",
      "Done;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_data = WikipediaLoader(query=\"Removal_of_Sam_Altman_from_OpenAI\").load()\n",
    "\n",
    "# Embeddings & LLM models\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_dimension = 1536\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    deployment_name=\"gpt-4\",\n",
    ")\n",
    "\n",
    "# Process All Data\n",
    "parent_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "child_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=24)\n",
    "\n",
    "# Ingest Parent-Child node pairs\n",
    "for document in all_data:\n",
    "    parent_documents = parent_splitter.split_documents([document])\n",
    "    for i, parent in enumerate(parent_documents):\n",
    "        child_documents = child_splitter.split_documents([parent])\n",
    "        params = {\n",
    "            \"parent_text\": parent.page_content,\n",
    "            \"parent_id\": i,\n",
    "            \"parent_embedding\": embeddings.embed_query(parent.page_content),\n",
    "            \"children\": [\n",
    "                {\n",
    "                    \"text\": c.page_content,\n",
    "                    \"id\": f\"{i}-{ic}\",\n",
    "                    \"embedding\": embeddings.embed_query(c.page_content),\n",
    "                }\n",
    "                for ic, c in enumerate(child_documents)\n",
    "            ],\n",
    "        }\n",
    "        print(f\"Processing {len(params['children'])} children\")\n",
    "        # Ingest data\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "        MERGE (p:Parent {id: $parent_id})\n",
    "        SET p.text = $parent_text\n",
    "        WITH p\n",
    "        CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding)\n",
    "        YIELD node\n",
    "        WITH p \n",
    "        UNWIND $children AS child\n",
    "        MERGE (c:Child {id: child.id})\n",
    "        SET c.text = child.text\n",
    "        MERGE (c)<-[:HAS_CHILD]-(p)\n",
    "        WITH c, child\n",
    "        CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n",
    "        YIELD node\n",
    "        RETURN count(*)\n",
    "        \"\"\",\n",
    "            params,\n",
    "        )\n",
    "        # Create vector index for child\n",
    "        try:\n",
    "            graph.query(\n",
    "                \"CALL db.index.vector.createNodeIndex('parent_document', \"\n",
    "                \"'Child', 'embedding', $dimension, 'cosine')\",\n",
    "                {\"dimension\": embedding_dimension},\n",
    "            )\n",
    "        except ClientError:  # already exists\n",
    "            pass\n",
    "        # Create vector index for parents\n",
    "        try:\n",
    "            graph.query(\n",
    "                \"CALL db.index.vector.createNodeIndex('typical_rag', \"\n",
    "                \"'Parent', 'embedding', $dimension, 'cosine')\",\n",
    "                {\"dimension\": embedding_dimension},\n",
    "            )\n",
    "        except ClientError:  # already exists\n",
    "            pass\n",
    "    # Ingest hypothethical questions\n",
    "print('Done;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser, PydanticOutputParser, NumberedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your desired data structure.\n",
    "# class Questions(BaseModel):\n",
    "#     \"\"\"Generating hypothetical questions about text.\"\"\"\n",
    "\n",
    "#     questions: List[str] = Field(\n",
    "#         ...,\n",
    "#         description=(\n",
    "#             \"Generated hypothetical questions based on \" \"the information from the text\"\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "# pydantic_parser = PydanticOutputParser(pydantic_object=Questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[AIMessage(content='You are generating hypothetical questions based on the information found in the text. Make sure to provide full context in the generated questions.'), HumanMessage(content='Use the given format to generate hypothetical questions from the following input: test data')]\n"
     ]
    }
   ],
   "source": [
    "## This is a barebones manner, without 'chat' contexts\n",
    "# prompt = \"\"\"You are generating hypothetical questions based on the information \n",
    "#             found in the text. Make sure to provide full context in the generated \n",
    "#             questions. \n",
    "\n",
    "#             Use the given format to generate hypothetical questions from the \"\n",
    "#                 \"following input: {input}\"\"\"\n",
    "# questions_prompt = ChatPromptTemplate.from_template(prompt) \n",
    "\n",
    "## This is preferred for chat contexts\n",
    "questions_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        AIMessage(content=\"You are generating hypothetical questions based on the information \"\n",
    "                \"found in the text. Make sure to provide full context in the generated \"\n",
    "                \"questions.\" \n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template( \"Use the given format to generate hypothetical questions from the \"\n",
    "                \"following input: {input}\"\n",
    "        )\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(questions_prompt.invoke({'input':'test data'}))\n",
    "\n",
    "question_chain = questions_prompt | llm | NumberedListOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]\n"
     ]
    }
   ],
   "source": [
    "## This is just an example of how to use  the invoke of the chains\n",
    "# from langchain.prompts import HumanMessagePromptTemplate\n",
    "# from langchain_core.messages import SystemMessage\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# chat_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=(\n",
    "#                 \"You are a helpful assistant that re-writes the user's text to \"\n",
    "#                 \"sound more upbeat.\"\n",
    "#             )\n",
    "#         ),\n",
    "#         HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "#     ]\n",
    "# )\n",
    "# # messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "# messages = chat_template.invoke({\"text\":\"I don't like eating tasty things\"})\n",
    "# print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' its hyperbolic trajectory and estimated initial high velocity, to be from beyond the Solar System. The 2014 meteorite was detected three years earlier than the more recent and widely known interstellar objects, ʻOumuamua in 2017 and  2I/Borisov in 2019. Further related studies were reported on 1 September 2023.\\nThe first known dinosaur fossil linked to the very day of the Chicxulub impact is reported by paleontologists at the Tanis site in North Dakota.\\nOne science journalist reflects on the global management of the COVID-19 pandemic in relation to science, investigating the question \"Why the WHO took two years to say COVID is airborne\" – a finding hundreds of scientists reaffirmed in an open letter in July 2020 – with one indication that this may be one valid major concern to many expert scientists being several writings published by news outlets.\\nA study decodes electrical communication between fungi into word-like components via spiking characteristics.\\nResearchers demonstrate semi-automated testing for reproducibility (which is lacking especially in cancer research) via extraction of statements about experimental results in, as of 2022 non-semantic, gene expression cancer research papers and subsequent testing with breast cancer cell lines via robot scientist \"Eve\".7 April\\nAstronomers report the discovery of HD1, considered to be the earliest and most distant known galaxy yet identified in the observable universe, located only about 330 million years after the Big Bang 13.8 billion years ago, a light-travel distance of 13.5 billion light-years from Earth, and, due to the expansion of th'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parent.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing parent 0\n",
      "prompt:\n",
      " messages=[AIMessage(content='You are generating hypothetical questions based on the information found in the text. Make sure to provide full context in the generated questions.'), HumanMessage(content='Use the given format to generate hypothetical questions from the following input: The following scientific events occurred in 2022.\\n\\n\\n== Events ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n1 April\\nBiochemists report finishing the complete sequence of the human genome.\\nA study shows that, contrary to widespread belief, body sizes of mammal extinction survivors of the dinosaur-times extinction event were the first to evolutionarily increase, with brain sizes increasing later in the Eocene.\\n4 April\\nThe Intergovernmental Panel on Climate Change (IPCC) releases the third and final part of its Sixth Assessment Report on climate change, warning that greenhouse gas emissions must peak before 2025 at the latest and decline 43% by 2030, in order to likely limit global warming to 1.5 °C (2.7 °F).\\nResearchers announce a new technique for accelerating the development of vaccines and other pharmaceutical products by up to a million times, using much smaller quantities based on DNA nanotechnology.\\nAlzheimer\\'s disease (AD) research progress:A study reports 42 new genes linked to an increased risk of AD. Researchers report a potential primary mechanism of sleep disturbance as an early-stage effect of neurodegenerative diseases. Researchers identify several genes associated with changes in brain structure over lifetime and potential AD therapy-targets (5 Apr).5 April\\nCOVID-19 pandemic: Preclinical data for a new vaccine developed at the Medical University of Vienna indicates it is effective against all SARS-CoV-2 variants known to date, including Omicron.\\nA study presents a mechanism by which the hypothesized potential dark-energy-explaining quintessence, if true, would smoothly cause the accelerating expansion of the Universe to inverse to contraction, possibly within the cosmic near-future (100 My) given current data. It concludes that its end-time scenario theory fits \"naturally with cyclic cosmologies [(each a theory of cycles of universe originations and ends, rather than the theories of one Big Bang beginning of the Universe/multiverse, to which authors were major contributors)] and recent conjectures about quantum gravity\".6 April\\nU.S. Space Command, based on information collected from its planetary defense sensors, confirms the detection of the first known interstellar object. The purported interstellar meteorite, technically known as CNEOS 2014-01-08, impacted Earth in 2014, and was determined, based on its hyperbolic trajectory and estimated initial high velocity, to be from beyond the Solar System. The 2014 meteorite was')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'questions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(questions_prompt\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;250m \u001b[39mparent\u001b[38;5;241m.\u001b[39mpage_content}))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m questions \u001b[38;5;241m=\u001b[39m question_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: parent\u001b[38;5;241m.\u001b[39mpage_content})\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere were \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mquestions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestions\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m questions generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: i,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     ],\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     16\u001b[0m graph\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mMERGE (p:Parent {id: $parent_id})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     params,\n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'questions'"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, parent in enumerate(parent_documents):\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(f\"Processing parent {i}\")# with page content\\n {parent.page_content}\")\n",
    "    print(f\"prompt:\\n {str(questions_prompt.invoke({'input': parent.page_content}))}\")\n",
    "    questions = question_chain.invoke({'input': parent.page_content})\n",
    "    print(f\"There were {len(questions)} questions generated\")\n",
    "    params = {\n",
    "        \"parent_id\": i,\n",
    "        \"questions\": [\n",
    "            {\"text\": q, \"id\": f\"{i}-{iq}\", \"embedding\": embeddings.embed_query(q)}\n",
    "            for iq, q in enumerate(questions)\n",
    "            if q\n",
    "        ],\n",
    "    }\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    WITH p\n",
    "    UNWIND $questions AS question\n",
    "    CREATE (q:Question {id: question.id})\n",
    "    SET q.text = question.text\n",
    "    MERGE (q)<-[:HAS_QUESTION]-(p)\n",
    "    WITH q, question\n",
    "    CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
    "            \"'Question', 'embedding', $dimension, 'cosine')\",\n",
    "            {\"dimension\": embedding_dimension},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"\"\"\n",
    "MERGE (p:Parent {id: $parent_id})\n",
    "WITH p\n",
    "UNWIND $questions AS question\n",
    "CREATE (q:Question {id: question.id})\n",
    "SET q.text = question.text\n",
    "MERGE (q)<-[:HAS_QUESTION]-(p)\n",
    "WITH q, question\n",
    "CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
    "YIELD node\n",
    "RETURN count(*)\n",
    "\"\"\",\n",
    "    params,\n",
    ")\n",
    "# Create vector index\n",
    "try:\n",
    "    graph.query(\n",
    "        \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
    "        \"'Question', 'embedding', $dimension, 'cosine')\",\n",
    "        {\"dimension\": embedding_dimension},\n",
    "    )\n",
    "except ClientError:  # already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ingest summaries\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        AIMessage(\n",
    "            content=\"You are generating concise and accurate summaries based on the \"\\\n",
    "                \"information found in the text.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template( \n",
    "         \"Generate a summary of the following input: \\n {input}\\n\" \"Summary:\\n\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "for i, parent in enumerate(parent_documents):\n",
    "    if i > 0:\n",
    "        break\n",
    "    summary = summary_chain.invoke({\"input\": parent.page_content})\n",
    "    params = {\n",
    "        \"parent_id\": i,\n",
    "        \"summary\": summary,\n",
    "        \"embedding\": embeddings.embed_query(summary),\n",
    "    }\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "    MERGE (p:Parent {id: $parent_id})\n",
    "    MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n",
    "    SET s.text = $summary\n",
    "    WITH s\n",
    "    CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n",
    "    YIELD node\n",
    "    RETURN count(*)\n",
    "    \"\"\",\n",
    "        params,\n",
    "    )\n",
    "    # Create vector index\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"CALL db.index.vector.createNodeIndex('summary', \"\n",
    "            \"'Summary', 'embedding', $dimension, 'cosine')\",\n",
    "            {\"dimension\": embedding_dimension},\n",
    "        )\n",
    "    except ClientError:  # already exists\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' its hyperbolic trajectory and estimated initial high velocity, to be from beyond the Solar System. The 2014 meteorite was detected three years earlier than the more recent and widely known interstellar objects, ʻOumuamua in 2017 and  2I/Borisov in 2019. Further related studies were reported on 1 September 2023.\\nThe first known dinosaur fossil linked to the very day of the Chicxulub impact is reported by paleontologists at the Tanis site in North Dakota.\\nOne science journalist reflects on the global management of the COVID-19 pandemic in relation to science, investigating the question \"Why the WHO took two years to say COVID is airborne\" – a finding hundreds of scientists reaffirmed in an open letter in July 2020 – with one indication that this may be one valid major concern to many expert scientists being several writings published by news outlets.\\nA study decodes electrical communication between fungi into word-like components via spiking characteristics.\\nResearchers demonstrate semi-automated testing for reproducibility (which is lacking especially in cancer research) via extraction of statements about experimental results in, as of 2022 non-semantic, gene expression cancer research papers and subsequent testing with breast cancer cell lines via robot scientist \"Eve\".7 April\\nAstronomers report the discovery of HD1, considered to be the earliest and most distant known galaxy yet identified in the observable universe, located only about 330 million years after the Big Bang 13.8 billion years ago, a light-travel distance of 13.5 billion light-years from Earth, and, due to the expansion of th'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_chain.invoke({\"input\": parent.page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text doesn't provide any information to summarize. Please provide a text.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='You are generating concise and accurate summaries based on the information found in the text.'),\n",
       " HumanMessage(content='Generate a summary of the following input: \\n {input}\\nSummary:\\n')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_prompt.format_messages(input=parent.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
