{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.verbose = False\n",
    "langchain.debug = False\n",
    "langchain.llm_cache = False\n",
    "from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4', 'gpt-4o', 'gpt-4-1106-preview',\"text-davinci-003\"]\n",
    "model = models[2]\n",
    "\n",
    "# openai = OpenAI.chat\n",
    "# (\n",
    "#     model_name=model,\n",
    "#     # openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    "#     temperature=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── .pages\n",
      "├── blog/\n",
      "│   ├── .authors.yml\n",
      "│   ├── index.md\n",
      "│   └── posts/\n",
      "│       ├── .pages\n",
      "│       └── Launch.md\n",
      "├── index.md\n",
      "├── javascripts/\n",
      "│   ├── chatbase.js\n",
      "│   ├── copy-link.js\n",
      "│   └── mathjax.js\n",
      "├── Managenai/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── code_of_conduct.md\n",
      "│   ├── contributing.md\n",
      "│   ├── explorations_blog.md\n",
      "│   ├── index.md\n",
      "│   ├── project_requirements.md\n",
      "│   ├── site_graph.md\n",
      "│   └── strategy.md\n",
      "├── shared/\n",
      "├── Understanding/\n",
      "│   ├── .pages\n",
      "│   ├── agents/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── applications.md\n",
      "│   │   ├── cognitive_architecture.md\n",
      "│   │   ├── commercial.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── embedding.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── frameworks.md\n",
      "│   │   ├── generation.md\n",
      "│   │   ├── gpt.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusion_models.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── mixture_of_experts.md\n",
      "│   │   │   ├── multimodal.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   ├── transformers.md\n",
      "│   │   │   └── vision_language_transformers.md\n",
      "│   │   ├── optimization/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   └── index.md\n",
      "│   │   └── training/\n",
      "│   │       ├── .pages\n",
      "│   │       ├── distributed.md\n",
      "│   │       ├── feedback.md\n",
      "│   │       ├── finetuning.md\n",
      "│   │       ├── grounding.md\n",
      "│   │       ├── index.md\n",
      "│   │       ├── pre-training.md\n",
      "│   │       └── recursive.md\n",
      "│   ├── building/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── agents.md\n",
      "│   │   ├── back_end.md\n",
      "│   │   ├── commercial_products.md\n",
      "│   │   ├── compliance.md\n",
      "│   │   ├── computation.md\n",
      "│   │   ├── data.md\n",
      "│   │   ├── examples_and_tutorials.md\n",
      "│   │   ├── front_end.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── libraries_and_tools.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── model_serving.md\n",
      "│   │   ├── orchestrating.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   └── training_and_finetuning.md\n",
      "│   ├── data/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── distillation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── preparation/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   ├── augmentation.md\n",
      "│   │   │   ├── collection.md\n",
      "│   │   │   ├── formatting.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   └── selection.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── ai_and_ml_basics/\n",
      "│   │   │   └── index.md\n",
      "│   │   ├── background/\n",
      "│   │   │   └── tensor_maths.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── chronology.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── open_source.md\n",
      "│   │   ├── philosophies.md\n",
      "│   │   └── use_cases.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── examples/\n",
      "│   │   │   ├── coding/\n",
      "│   │   │   │   └── Claude_2024-07-20.md\n",
      "│   │   │   └── leaked/\n",
      "│   │   │       └── Claude_2024-07-11.md\n",
      "│   │   ├── hacking.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── security.md\n",
      "│   │   └── tools_and_libraries.md\n",
      "│   └── studies/\n",
      "│       ├── behavior.md\n",
      "│       └── studies.md\n",
      "└── Using/\n",
      "    ├── .pages\n",
      "    ├── de-risking/\n",
      "    │   ├── explainability.md\n",
      "    │   ├── index.md\n",
      "    │   ├── marking_and_detecting.md\n",
      "    │   ├── red_teaming.md\n",
      "    │   └── security.md\n",
      "    ├── ethically/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_existential_concerns.md\n",
      "    │   ├── confabulation.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── examples/\n",
      "    │   ├── by_field/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── business.md\n",
      "    │   │   ├── entertainment/\n",
      "    │   │   │   ├── dynamic.md\n",
      "    │   │   │   └── static.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── individuals_and_society/\n",
      "    │   │   │   ├── education.md\n",
      "    │   │   │   ├── law.md\n",
      "    │   │   │   └── socio_societal.md\n",
      "    │   │   ├── mathematics/\n",
      "    │   │   │   └── index.md\n",
      "    │   │   ├── science/\n",
      "    │   │   │   ├── biology/\n",
      "    │   │   │   │   ├── genetics.md\n",
      "    │   │   │   │   ├── index.md\n",
      "    │   │   │   │   └── proteins.md\n",
      "    │   │   │   ├── chemistry.md\n",
      "    │   │   │   ├── healthcare.md\n",
      "    │   │   │   ├── index.md\n",
      "    │   │   │   └── prompts/\n",
      "    │   │   │       ├── research_agent_experiment_design.yaml\n",
      "    │   │   │       ├── research_agent_experiment_validation.yaml\n",
      "    │   │   │       ├── research_agent_method_development.yaml\n",
      "    │   │   │       ├── research_agent_method_validation.yaml\n",
      "    │   │   │       ├── research_agent_problem_identification.yaml\n",
      "    │   │   │       └── research_agent_problem_validation.yaml\n",
      "    │   │   └── technology/\n",
      "    │   │       ├── chip_design.md\n",
      "    │   │       ├── coding.md\n",
      "    │   │       ├── finance.md\n",
      "    │   │       ├── healthcare.md\n",
      "    │   │       └── robotics.md\n",
      "    │   ├── by_modality/\n",
      "    │   │   ├── charts_and_graphs.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── knowledge_graphs.md\n",
      "    │   │   ├── language.md\n",
      "    │   │   ├── multimodal.md\n",
      "    │   │   ├── sound.md\n",
      "    │   │   ├── static_2d.md\n",
      "    │   │   ├── tabular.md\n",
      "    │   │   ├── text.md\n",
      "    │   │   ├── time_series.md\n",
      "    │   │   └── video.md\n",
      "    │   ├── by_use_case/\n",
      "    │   │   ├── chat.md\n",
      "    │   │   ├── content_framing.md\n",
      "    │   │   ├── data_extraction.md\n",
      "    │   │   ├── forecasting.md\n",
      "    │   │   ├── planning.md\n",
      "    │   │   ├── research.md\n",
      "    │   │   └── web_crawling.md\n",
      "    │   └── index.md\n",
      "    ├── index.md\n",
      "    ├── managing/\n",
      "    │   ├── governing.md\n",
      "    │   ├── index.md\n",
      "    │   ├── ml_ops.md\n",
      "    │   ├── observability.md\n",
      "    │   └── regulations_and_guidelines.md\n",
      "    ├── strategically/\n",
      "    │   ├── building_or_buying.md\n",
      "    │   ├── business_models.md\n",
      "    │   ├── implementation.md\n",
      "    │   ├── index.md\n",
      "    │   └── open_source.md\n",
      "    ├── tech_stack.md\n",
      "    └── useful_tools/\n",
      "        ├── index.md\n",
      "        ├── integrations.md\n",
      "        └── web_plugins.md\n"
     ]
    }
   ],
   "source": [
    "from genai.tools.dir_utils import get_tree_structure\n",
    "tree_structure = get_tree_structure(path_base = BASE_DOCS_DIR)\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern(file_class=None):\n",
    "    if file_class is None:\n",
    "        file_class = 'index.md'\n",
    "    patterns={}\n",
    "    patterns['index.md'] = \\\n",
    "    \"\"\"\n",
    "NOTE: '-' is used to denote a general topic, sentence, or consideration but not considered a 'list' item.\n",
    "## Executive Summary (TL;DR)\n",
    "- Concise summary highlighting the essence of the topic and its significance.\n",
    "- Designed for readability by a non-technical or executive-level audience.\n",
    "- Utilize emojis, images, and visual elements effectively to emphasize key points.\n",
    "- Include Mermaid diagrams where appropriate, or describe necessary images as `IMAGE: <image description>`.\n",
    "\n",
    "## Practical Application and Usage\n",
    "- Focus on providing immediately actionable guidance and high-priority examples.\n",
    "- Extract and condense key usage instructions from earlier content into concise, actionable steps.\n",
    "- Offer 'How-to' guides, quick-start tips, and links for direct application.\n",
    "\n",
    "## Introduction and Relevance\n",
    "- Thorough introduction to the topic, highlighting its relevance and importance.\n",
    "- Discuss core components and their interplay within the broader context of Generative AI.\n",
    "\n",
    "## Core Content and Results\n",
    "- Detailed exploration of specific aspects under clear subheadings.\n",
    "- Provide illustrations or diagrams (Mermaid or `IMAGE:<image description>`) for complex concepts.\n",
    "- For extensive topics, include brief summaries and links to dedicated markdown files. If markdown files are already created, link to them here. If markdown files are needed, suggest them. \n",
    "\n",
    "## Technological Aspects\n",
    "- Explore relevant tools, technologies, and methodologies.\n",
    "- Highlight current trends and future directions in technology related to the topic.\n",
    "\n",
    "## Background or Theoretical Foundation (if necessary)\n",
    "- Delve into historical context and foundational theories.\n",
    "- Clarify essential theoretical concepts and terminologies for comprehensive understanding.\n",
    "\n",
    "## Ethical Considerations and Challenges\n",
    "- Address ethical dilemmas, challenges, and potential risks.\n",
    "- Discuss strategies for ethical practice and risk mitigation.\n",
    "\n",
    "## Extended Examples (if applicable)\n",
    "- Link to practical examples, simulations, or code snippets for hands-on understanding.\n",
    "- Direct readers to external resources, tools, or demonstrations for further exploration.\n",
    "\n",
    "## Advanced Topics and Further Exploration (if applicable)\n",
    "- Present open challenges and future research directions.\n",
    "- Deep dive into complex aspects with links to advanced readings and resources.\n",
    "\n",
    "## FAQs and Common Queries\n",
    "- Tackle frequently asked questions and common queries related to the section.\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "- Recap the main points and emphasize the key messages from the section.\n",
    "\n",
    "## References and Additional Reading\n",
    "- List citations and provide links to source materials and further reading.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return patterns[file_class]\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0241f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartPromptTemplate:\n",
    "    def __init__(self, template_required, template_optional_dict, template_variable_independent):\n",
    "        self.template_required = template_required if template_required is not None else \"\"\n",
    "        self.template_optional_dict = template_optional_dict if template_optional_dict is not None else {}\n",
    "        self.template_variable_independent = template_variable_independent if template_variable_independent is not None else \"\"\n",
    "    \n",
    "    def get_prompt(self, **kwargs):\n",
    "        template_list = []\n",
    "        for k, v in kwargs.items():\n",
    "            if k in self.template_optional_dict.keys():\n",
    "                template_list.append(self.template_optional_dict[k])\n",
    "        # begin_indicator = \"\\n What would you write given the requests above? \\n>>>\\n\"\n",
    "        # #\\n<<< end input \\n\" + \\\n",
    "        template =   '\\n'.join(template_list) + self.template_required  +  self.template_variable_independent #+ begin_indicator\n",
    "        prompt = template.format(**kwargs)\n",
    "        return prompt\n",
    "\n",
    "template_optional_dict = {\n",
    "\n",
    "    'role': \"You are a {role}\",\n",
    "    # 'project_name': \"You are working on a project called: {project_name}\\n\",\n",
    "    # 'project_goals': \"You are part of a team working to: {project_goals}\\n\",\n",
    "    'present_task_description': \"You are helping to: {present_task_description}\\n\",\n",
    "    'file_name': \"You are helping to rewrite and expand a file called {file_name}\\n\",\n",
    "    'structure_pattern': \"Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\",\n",
    "    'tree_structure': \"The present tree-structure:\\n {tree_structure}\\n \",\n",
    "    'markdown_text': \"Markdown input \\n>>>\\n{markdown_text}\"\n",
    "}\n",
    "\n",
    "\n",
    "template_variable_independent = \\\n",
    "\"\"\"\n",
    "Things to keep in mind:\n",
    "* present ALL html links without changing the link's text.\n",
    "* Preserve any urls or relative links without changing them. \n",
    "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
    "* Keep all code blocks that are referenced in any manner.\n",
    "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
    "* Be as honest and as accurate as possible. \n",
    "* Be succinct in your responses. \n",
    "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
    "* If text is sparse or missing create a reasonable outline and follow it. \n",
    "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
    "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
    "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
    "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
    "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
    "* After you have written improvements, please write any commentary you see that would best describe the way a human could improve it. \n",
    "* We'll get $1000 if we do this right, so let's do our best!\n",
    "* Write EOF on a new line after the last line of the text to indicate nothing new.\n",
    "\n",
    "Here's the content.\n",
    "\"\"\"\n",
    "# Please, do follow these instructions closely for it if we don't get this right, we might lose our job. \n",
    "# * reformat any bulleted lists of links where github links have `!!! abstract`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "template_required = \\\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "spt = SmartPromptTemplate(template_required=template_required, \n",
    "template_optional_dict=template_optional_dict, \n",
    "template_variable_independent=template_variable_independent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcaa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"expert AI technology creator, communicator, and markdown / mkdocs expert\"\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Create an automated repository that is able to explain Generative AI \"\\\n",
    "        \"and how to improve upon it in plain-English and how to enable it from idea to product, as well as new and interesting research. \"\\\n",
    "                \n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\n",
    "                         \n",
    "\n",
    "# file_from_base_dir = 'Using/examples/by_field/science/biology/proteins.md'\n",
    "# file_from_base_dir = 'Understanding/prompting/index.md'\n",
    "# file_from_base_dir = 'Understanding/architectures/models/transformers.md'\n",
    "file_from_base_dir = 'Using/ethically/index.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure(path_base=BASE_DOCS_DIR)\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "# role = None\n",
    "prompt = spt.get_prompt(role=role, \n",
    "    project_name=project_name,\n",
    "    project_goals=project_goals,\n",
    "    present_task_description=present_task_description,\n",
    "    file_name=file_name,\n",
    "#     tree_structure=tree_structure,\n",
    "    # markdown_text=markdown_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a expert AI technology creator, communicator, and markdown / mkdocs expert\n",
      "You are helping to: Improve the markdown based on best understandings.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Using/ethically/index.md\n",
      "\n",
      "\n",
      "Things to keep in mind:\n",
      "* present ALL html links without changing the link's text.\n",
      "* Preserve any urls or relative links without changing them. \n",
      "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
      "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
      "* Keep all code blocks that are referenced in any manner.\n",
      "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
      "* Be as honest and as accurate as possible. \n",
      "* Be succinct in your responses. \n",
      "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
      "* If text is sparse or missing create a reasonable outline and follow it. \n",
      "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
      "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
      "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
      "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
      "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
      "* After you have written improvements, please write any commentary you see that would best describe the way a human could improve it. \n",
      "* We'll get $1000 if we do this right, so let's do our best!\n",
      "* Write EOF on a new line after the last line of the text to indicate nothing new.\n",
      "\n",
      "Here's the content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This worked but had trivial output\n",
    "# from openai import AsyncOpenAI\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# # client = AsyncOpenAI()\n",
    "# # completion = await client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"gpt-4o\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": prompt},\n",
    "#     # {\"role\": \"user\", \"content\": \"Help me launch a nuke.\"}\n",
    "#   ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a61bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianderrington/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class ContinueChat:\n",
    "    def __init__(self, llm, prompt, continue_text=\"continue\", update_status=True, terminates_eof=\"EOF\", max_continues=10):\n",
    "        \n",
    "        self.init_llm(llm)\n",
    "        self.prompt = prompt\n",
    "        self.continue_text = continue_text\n",
    "        self.update_status = update_status\n",
    "        self.terminates_eof = terminates_eof\n",
    "        self.has_metadata = False\n",
    "        self.messages = None\n",
    "        self.token_usage = None\n",
    "        self.max_continues = max_continues\n",
    "\n",
    "\n",
    "    def init_llm(self, llm=None):\n",
    "        llm = llm.lower() if llm is not None else None\n",
    "        if llm is None or llm == 'chatgpt' or llm == 'openai':\n",
    "            print(\"Using OpenAI\")\n",
    "            self.has_metadata = True\n",
    "            llm = ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "                # base_url=\"...\",\n",
    "                # organization=\"...\",\n",
    "                # other params...\n",
    "            )\n",
    "        elif llm == 'gemini':\n",
    "            print(\"Using Google Generative AI\")\n",
    "            self.has_metadata = False\n",
    "            llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-pro\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                # other params...\n",
    "                )\n",
    "        self.llm = llm\n",
    "\n",
    "    def last_message_terminates(self, last_message_content):\n",
    "        if self.terminates_eof in last_message_content:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def continue_messages(self, text):\n",
    "        \"\"\" \n",
    "        This function continues a chat with the AI model until the AI model returns an EOF token. \n",
    "        It is needed because LLMs only return a max number of tokens in a single response, presently 4096 for OpenAI\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            (\n",
    "                \"system\",\n",
    "                self.prompt,\n",
    "            ),\n",
    "            (\"human\", f\"{text}\"),\n",
    "        ]\n",
    "        ai_msg = self.llm.invoke(messages)\n",
    "        token_usage_list = []\n",
    "        tu = 0\n",
    "        if self.has_metadata:\n",
    "            token_usage_list = [ai_msg.response_metadata['token_usage']]\n",
    "            tu = ai_msg.response_metadata['token_usage']['completion_tokens']\n",
    "        i=0\n",
    "        if self.update_status:\n",
    "            print(f\"Response {i} recieved with token usage: {tu} used.\")\n",
    "        while not self.last_message_terminates(ai_msg.content) and i < self.max_continues:\n",
    "            i+=1\n",
    "            messages.append((\"system\", ai_msg.content))\n",
    "            messages.append((\"human\", self.continue_text))\n",
    "            if self.has_metadata:\n",
    "                token_usage_list.append(ai_msg.response_metadata['token_usage'])\n",
    "                tu = ai_msg.response_metadata['token_usage']['completion_tokens']\n",
    "            if self.update_status:\n",
    "                print(f\"Response {i} recieved with token usage: {tu} used.\")\n",
    "            ai_msg = self.llm.invoke(messages)\n",
    "        \n",
    "        messages.append((\"system\", ai_msg.content))\n",
    "\n",
    "        token_usage = {}\n",
    "        if self.has_metadata:\n",
    "            tu = ai_msg.response_metadata['token_usage']['completion_tokens']\n",
    "            token_usage = {k: sum([d[k] for d in token_usage_list]) for k in token_usage_list[0].keys()} \n",
    "        if self.update_status:\n",
    "\n",
    "            print(f\"Response {i} recieved with token usage: {tu} used.\")\n",
    "\n",
    "        # sum up the token usage from 'completion_tokens', 'prompt-tokens', and 'total_tokens' list of dicts    \n",
    "       \n",
    "        \n",
    "        return messages, token_usage\n",
    "    \n",
    "    def invoke(self, text):\n",
    "        messages, token_usage = self.continue_messages(text)\n",
    "        self.messages = messages\n",
    "        self.token_usage = token_usage\n",
    "        text =  ''.join([m[1] for m in messages[1:] if m[0] == \"system\"])\n",
    "        # remove eof\n",
    "        text = text.replace(self.terminates_eof, \"\")\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b71a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# messages = [   \n",
    "#     (\n",
    "#         \"system\",\n",
    "#         prompt,\n",
    "#     ),\n",
    "#     (\"human\",markdown_text ),\n",
    "# ]\n",
    "\n",
    "# response = llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4816005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Google Generative AI\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m CC \u001b[38;5;241m=\u001b[39m ContinueChat(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# messages, token_usage = CC.continued(markdown_text)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m response_content \u001b[38;5;241m=\u001b[39m \u001b[43mCC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkdown_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m, in \u001b[0;36mContinueChat.invoke\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 101\u001b[0m     messages, token_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinue_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m messages\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_usage \u001b[38;5;241m=\u001b[39m token_usage\n",
      "Cell \u001b[0;32mIn[10], line 65\u001b[0m, in \u001b[0;36mContinueChat.continue_messages\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mThis function continues a chat with the AI model until the AI model returns an EOF token. \u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mIt is needed because LLMs only return a max number of tokens in a single response, presently 4096 for OpenAI\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     59\u001b[0m     (\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     64\u001b[0m ]\n\u001b[0;32m---> 65\u001b[0m ai_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m token_usage_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m tu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:265\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    262\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    264\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    275\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:698\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    692\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    696\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    697\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:523\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop}\n\u001b[1;32m    518\u001b[0m inheritable_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ls_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    521\u001b[0m }\n\u001b[0;32m--> 523\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43minheritable_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    533\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    534\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    541\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/callbacks/manager.py:1569\u001b[0m, in \u001b[0;36mCallbackManager.configure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     local_metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CallbackManager:\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configure the callback manager.\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \n\u001b[1;32m   1551\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;124;03m        CallbackManager: The configured callback manager.\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_configure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minheritable_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/callbacks/manager.py:2186\u001b[0m, in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   2179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTracing using LangChainTracerV1 is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set the LANGCHAIN_TRACING_V2 environment variable to enable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracing instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2183\u001b[0m     )\n\u001b[1;32m   2185\u001b[0m tracer_project \u001b[38;5;241m=\u001b[39m _get_tracer_project()\n\u001b[0;32m-> 2186\u001b[0m debug \u001b[38;5;241m=\u001b[39m \u001b[43m_get_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mor\u001b[39;00m tracing_v2_enabled_:\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangChainTracer\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/callbacks/manager.py:59\u001b[0m, in \u001b[0;36m_get_debug\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_debug\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_debug\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_core/globals.py:146\u001b[0m, in \u001b[0;36mget_debug\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImporting debug from langchain root module is no longer supported\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;66;03m# N.B.: This is a workaround for an unfortunate quirk of Python's\u001b[39;00m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;66;03m#       module-level `__getattr__()` implementation:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# to using `set_debug()` yet. Those users are getting deprecation warnings\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# directing them to use `set_debug()` when they import `langhchain.debug`.\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m         old_debug \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     old_debug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain' has no attribute 'debug'"
     ]
    }
   ],
   "source": [
    "llm = 'openai'\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = 'gemini'\n",
    "CC = ContinueChat(llm=llm, prompt=prompt)\n",
    "# messages, token_usage = CC.continued(markdown_text)\n",
    "response_content = CC.invoke(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! quote \"Be sure to consider the unintended consequences.\"\n",
      "    - Sundar Pichai, Google's CEO\n",
      "\n",
      "Core elements in AI governance require ethics to guide AI governance. While there are many variations surrounding these, from sources such as [this one](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgmodelaigovframework2.pdf), they can include considerations such as the following:\n",
      "\n",
      "1. **Human-centric**: Amplifies the capabilities and protects the interests of people.\n",
      "2. **Transparency**: All aspects of the AI system and its development are thoughtfully described and documented.\n",
      "3. **Fairness**: Equitable and beneficial for all.\n",
      "4. **Explainability**: The AI's results can be understood and reproduced.\n",
      "5. **Sustainability**: Minimizes environmental impact.\n",
      "6. **Accountability**: Enabling actions to be taken to prevent future failures.\n",
      "7. **Observability**: Allows one to observe the AI to be evaluated.\n",
      "8. **Positive Impact**: Creates positive value for all parties.\n",
      "9. **Privacy**: Appropriately protects the privacy rights of people.\n",
      "10. **Security**: Cannot be misused intentionally or unintentionally.\n",
      "\n",
      "## Bias and Fairness\n",
      "\n",
      "### Mitigating Bias in Data and Models\n",
      "Ensuring that data and models are free from bias is crucial for ethical AI. Techniques such as data augmentation, re-sampling, and fairness constraints can help mitigate bias.\n",
      "\n",
      "### Evaluating Model Fairness\n",
      "Regularly evaluate models for fairness using metrics like demographic parity, equalized odds, and disparate impact. Tools like Fairness Indicators can assist in this process.\n",
      "\n",
      "### Inclusive Model Development\n",
      "Involve diverse teams in the model development process to ensure a variety of perspectives and reduce the risk of bias.\n",
      "\n",
      "### Transparency and Explainability\n",
      "Make models transparent and explainable to build trust and allow users to understand how decisions are made. Techniques like LIME and SHAP can help in explaining model predictions.\n",
      "\n",
      "## Interpretability \n",
      "\n",
      "### Techniques for Explainability\n",
      "Use methods such as feature importance, partial dependence plots, and surrogate models to make AI systems more interpretable.\n",
      "\n",
      "### Right to Explanation\n",
      "Ensure that users have the right to understand how decisions affecting them are made, in compliance with regulations like GDPR.\n",
      "\n",
      "### Safety\n",
      "Implement safety measures to prevent harm from AI systems, including robust testing and validation.\n",
      "\n",
      "## Risk Mitigation\n",
      "\n",
      "### Risk Assessment\n",
      "Conduct thorough risk assessments to identify potential issues and mitigate them before deployment.\n",
      "\n",
      "### Safeguards Against Misuse\n",
      "Implement safeguards to prevent the misuse of AI technologies, such as access controls and monitoring.\n",
      "\n",
      "### Privacy\n",
      "Ensure that AI systems respect user privacy by incorporating privacy-preserving techniques.\n",
      "\n",
      "## Data Privacy\n",
      "\n",
      "### Anonymization and De-identification\n",
      "Use anonymization and de-identification techniques to protect user data while still allowing for meaningful analysis.\n",
      "\n",
      "### Encryption and Secure Computing\n",
      "Implement encryption and secure computing practices to protect data at rest and in transit.\n",
      "\n",
      "## Governance\n",
      "\n",
      "### Internal Auditing Processes\n",
      "Establish internal auditing processes to regularly review AI systems for compliance with ethical guidelines.\n",
      "\n",
      "### External Oversight\n",
      "Engage external auditors to provide an objective review of AI systems and practices.\n",
      "\n",
      "### Accountability Measures\n",
      "Implement accountability measures to ensure that individuals and teams are responsible for the ethical use of AI.\n",
      "\n",
      "## Access and Inclusion\n",
      "\n",
      "### Fair and Equitable Access\n",
      "Ensure that AI technologies are accessible to all, regardless of socioeconomic status or geographic location.\n",
      "\n",
      "### Digital Divides\n",
      "Work to bridge digital divides by providing resources and support to underserved communities.\n",
      "\n",
      "### Participatory Design\n",
      "Involve end-users in the design process to ensure that AI systems meet their needs and are usable by all.\n",
      "\n",
      "## Compliance\n",
      "\n",
      "### Laws and Regulations\n",
      "Stay informed about and comply with relevant laws and regulations governing AI use.\n",
      "\n",
      "### Responsible Development Guidelines\n",
      "Follow responsible development guidelines to ensure ethical AI practices.\n",
      "\n",
      "### Ethics Review Processes\n",
      "Implement ethics review processes to evaluate the potential impact of AI systems before deployment.\n",
      "\n",
      "## Emerging Ethical Considerations in AI\n",
      "\n",
      "### Unlearning\n",
      "Explore techniques for unlearning in AI systems to remove biases or incorrect information. [Unlearning Saliency](https://github.com/optml-group/unlearn-saliency) This area is particularly important as AI systems are increasingly learning from dynamic data, and the ability to correct or remove outdated information becomes crucial.\n",
      "\n",
      "### Generative AI and Research Integrity\n",
      "The rise of generative AI, such as large language models, presents unique ethical challenges, especially in research. \n",
      "\n",
      "#### Key Principles for Generative AI in Research:\n",
      "1. **Accountability**: Humans must remain responsible for evaluating the quality and originality of AI-generated content. While AI can assist in tasks like summarization or grammar checks, critical aspects like writing manuscripts or peer reviews should not be solely reliant on AI.\n",
      "2. **Transparency**: Researchers should disclose the use of generative AI in their work to maintain transparency and allow for scrutiny of its impact on research quality. Developers of these tools should also be transparent about their functionalities to enable thorough evaluation.\n",
      "3. **Independent Oversight**:  Given the significant influence of AI, independent bodies should audit generative AI tools to ensure their quality, ethical use, and adherence to research integrity standards.\n",
      "\n",
      "### Security Vulnerabilities in Large Language Model Applications\n",
      "The OWASP Top 10 for Large Language Model Applications project ([OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/)) highlights the unique security risks associated with LLMs. These include:\n",
      "* **Prompt Injections**: Malicious inputs that manipulate the LLM's behavior.\n",
      "* **Data Leakage**: Unintentional exposure of sensitive information through the LLM's output.\n",
      "* **Inadequate Sandboxing**: Insufficient isolation of the LLM from critical systems, potentially leading to broader security breaches.\n",
      "* **Unauthorized Code Execution**: Exploiting vulnerabilities to execute arbitrary code within the LLM environment.\n",
      "\n",
      "Addressing these vulnerabilities requires robust security measures, including input validation, output sanitization, secure deployment practices, and continuous monitoring.\n",
      "\n",
      "!!! abstract \"[Some questionable or fraudulent practices in ML](https://arxiv.org/pdf/2407.12220)\"\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# response_content = CC.messages[2][1]\n",
    "# print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../docs/Using/ethically/index_temp0.md'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write the file to disk with a _temp suffix and then open it with a system call a command line comparer to visualize the two\n",
    "# files side by side.\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import webbrowser\n",
    "\n",
    "def write_to_file(file_name, text):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    return file_name\n",
    "\n",
    "# Please be sure to run `homebrew install tkdiff` or otherwise install tkdiff on your computer\n",
    "# ! brew install tkdiff\n",
    "# ! brew install meld # this is better because it allows management\n",
    "def open_with_comparer(file_name1, file_name2, comparer='meld'):\n",
    "    assert comparer in ['meld', 'tkdiff']\n",
    "    subprocess.run([comparer, file_name1, file_name2])\n",
    "\n",
    "def make_name(file_name):\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    temp_name = base + '_temp0' + ext\n",
    "    #check to see if it exists and if so, make a new name with a _temp# where # is the next available number\n",
    "    count=0\n",
    "    while os.path.exists(temp_name):\n",
    "        count += 1\n",
    "        \n",
    "        temp_name = base + f'_temp{count}' + ext\n",
    "    return temp_name\n",
    "temp_name = make_name(file_name)\n",
    "write_to_file(temp_name, response_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name: ../../docs/Using/ethically/index.md\n",
      "temp name: ../../docs/Using/ethically/index_temp0.md\n"
     ]
    }
   ],
   "source": [
    "print(f\"file name: {file_name}\")\n",
    "print(f\"temp name: {temp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(meld:43471): Gtk-WARNING **: 14:59:33.896: Locale not supported by C library.\n",
      "\tUsing the fallback 'C' locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't set the locale: unsupported locale setting; falling back to 'C' locale\n"
     ]
    }
   ],
   "source": [
    "\n",
    "open_with_comparer(file_name, temp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../docs/Using/ethically/index.md has been updated.\n"
     ]
    }
   ],
   "source": [
    "input_answer = input(\"Is the output correct Yes/no/deletefile? (y/n/d)\")\n",
    "## if the answer is y then move the temp-name to the original file name and delete the temp file\n",
    "if input_answer == 'y':\n",
    "    os.rename(temp_name, file_name)\n",
    "    print(f\"File {file_name} has been updated.\")\n",
    "else:\n",
    "    print(f\"File {file_name} has not been updated.\")\n",
    "    # if the answer is n then delete the temp file and do nothing\n",
    "    if input_answer == 'd':\n",
    "        os.remove(temp_name)\n",
    "        print(f\"File {temp_name} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT THAT LOOKS AT DIFFERENCES CHHUNK BY CHUNK AND AMENDS THEM. \n",
    "CREATE DIFF, ITERATE ON DIFF AND UPDATE MODIFIED DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
