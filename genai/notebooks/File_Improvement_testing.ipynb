{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-32k',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "├── to_sort.md\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── index.md\n",
      "    │   ├── interpreters.md\n",
      "    │   ├── memory.md\n",
      "    │   └── systems.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embedding.md\n",
      "    │   ├── preprocessing.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── quality.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokenizing.md\n",
      "    ├── engineering_and_management/\n",
      "    │   ├── .pages\n",
      "    │   ├── commercial_products.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── deployment.md\n",
      "    │   ├── engineering_and_management.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── marking_and_detecting.md\n",
      "    │   ├── models.md\n",
      "    │   ├── observability.md\n",
      "    │   ├── plugins.md\n",
      "    │   └── regulation.md\n",
      "    ├── ethics/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── ethics.md\n",
      "    │   ├── fairness.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── model_creation/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── overview.md\n",
      "    ├── prompt_engineering/\n",
      "    │   ├── prompt_injections.md\n",
      "    │   └── prompting.md\n",
      "    └── studies/\n",
      "        └── studies.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return ( '.pages' in path.name or not path.name.startswith(\".\") ) and 'Icon' not in path.name\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path='Understanding/data/tokenizing.md', base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern():\n",
    "    pattern = \\\n",
    "    \"\"\"\n",
    "    <<intro>>\n",
    "    ## <<First topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## <<Second topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## ...\n",
    "    ## Essential References\n",
    "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
    "    \"\"\"\n",
    "    return pattern\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "                \"Generative AI and how to improve upon it. \"\n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "                         \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \\\n",
    "\"\"\" You are working on a project called: {project_name}\\n\n",
    "You are part of a team working to: {project_goals}\\n\n",
    "You are helping to: {present_task_description}\\n\n",
    "You are helping to rewrite and expand a file called {file_name} in the present tree-structure:\\n {tree_structure}\\n \n",
    "Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "Please present html links without changing the link's text. \n",
    "After the markdown When the text is presented, please improve upon it. If no text is present create a reasonable outline following the pattern above and fill it in.\n",
    "Please preserve any urls or relative links without changing them. \n",
    "Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "Markdown Response: {markdown_text}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"markdown_text\", \"structure_pattern\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "file_name=get_file_name()\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt=prompt_template.format(project_name=project_name,\n",
    "                       project_goals=project_goals,\n",
    "                       present_task_description=present_task_description,\n",
    "                       file_name=file_name,\n",
    "                       tree_structure=tree_structure,\n",
    "                        structure_pattern=structure_pattern,\n",
    "                       markdown_text=markdown_text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b982b3e-6563-4e18-8d44-7ba14122fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_markdown_text(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are working on a project called: Managing Generative AI\n",
      "\n",
      "You are part of a team working to: Overall: Create an automated repository that is able to explain in plain-English and in code, Generative AI and how to improve upon it. \n",
      "\n",
      "You are helping to: Improve the markdown based on best understandings.Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Understanding/data/tokenizing.md in the present tree-structure:\n",
      " docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "├── to_sort.md\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── index.md\n",
      "    │   ├── interpreters.md\n",
      "    │   ├── memory.md\n",
      "    │   └── systems.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embedding.md\n",
      "    │   ├── preprocessing.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── quality.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokenizing.md\n",
      "    ├── engineering_and_management/\n",
      "    │   ├── .pages\n",
      "    │   ├── commercial_products.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── deployment.md\n",
      "    │   ├── engineering_and_management.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── marking_and_detecting.md\n",
      "    │   ├── models.md\n",
      "    │   ├── observability.md\n",
      "    │   ├── plugins.md\n",
      "    │   └── regulation.md\n",
      "    ├── ethics/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── ethics.md\n",
      "    │   ├── fairness.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── model_creation/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── overview.md\n",
      "    ├── prompt_engineering/\n",
      "    │   ├── prompt_injections.md\n",
      "    │   └── prompting.md\n",
      "    └── studies/\n",
      "        └── studies.md\n",
      " \n",
      "Please use a heading/subheading structure that follows the general pattern : \n",
      "    <<intro>>\n",
      "    ## <<First topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## <<Second topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## ...\n",
      "    ## Essential References\n",
      "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
      "    \n",
      "\n",
      "Please present html links without changing the link's text. \n",
      "After the markdown When the text is presented, please improve upon it.\n",
      "Please preserve any urls or relative links without changing them. \n",
      "Please be sure to use `#` appropriately to reference sections and subsections.\n",
      "Markdown Response: \n",
      "In generative AI, the raw data—whether it be in binary, text, or a different form—is divided into individual units termed as *tokens*. These play a crucial role in easing the understanding and manipulation of data for the AI.\n",
      "\n",
      "## Understanding Tokenization\n",
      "Tokenization is the process of splitting data into these individual units. The choice of a token largely depends on the data type and the expected outcome of the AI. In text data, for instance, tokens often correspond to single words or subwords. \n",
      "\n",
      "### Subword Units\n",
      "A subword unit, or a part of a word, can be a token in itself. The paper titled [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) brings to light the effectiveness of subword units in improving results. This type of tokenization was used in a neural machine translation system and it significantly improved the handling of rare words.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Special tokens\n",
      "\n",
      "There are special tokens that are used by high-level interpreters on what next to do. \n",
      "\n",
      "| Token Name | Description |\n",
      "| --- | --- |\n",
      "| START_TOKEN or BOS_TOKEN | This is used to indicate the beginning of a sequence. BOS stands for \"Beginning Of Sequence\". |\n",
      "| STOP_TOKEN or EOS_TOKEN | This is used to indicate the end of a sequence. EOS stands for \"End Of Sequence\". |\n",
      "| MASK_TOKEN | This is used to represent a masked value, which the model needs to predict. |\n",
      "| MODALITY_TOKEN | This is used to indicate the type of data in the sequence (such as text, images, etc.) |\n",
      "\n",
      "\n",
      "## Multimodal Tokenization\n",
      "Multimodal tokenization is an area of tokenization that focuses on incorporating multiple data forms or modes. This facet of tokenization has seen remarkable strides. [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf)—a study utilizing transformer technology to input file bytes directly—demonstrates that multimodal tokenization can assist in improving the AI's performance accuracy. The researchers in the study developed ByteFormer, a model based on their study’s findings that can be accessed [here](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer).\n",
      "\n",
      "## Significance of Embeddings\n",
      "Embeddings play a key role in AI as they translate tokens into numerical representation that can be processed by the AI. 'What are Embeddings' is an essential [read](http://vickiboykis.com/what_are_embeddings/) that elucidates the concept of embeddings in a digestible manner. For a deeper dive, check the accompanied [Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md) page.\n",
      "\n",
      "## Tools\n",
      "Examples of coding tools that facilitate tokenization include [Tiktoken](https://github.com/openai/tiktoken) which utilizes Byte Pair Encoding (BPE) for tokenization and is purportedly used in GPT models. An alternative tool is [Token Monster](https://github.com/alasdairforsythe/tokenmonster), which takes a unique top-down approach and results in almost 35% less tokens as opposed to the standard bottom-up approach.\n",
      "\n",
      "## Essential References\n",
      "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)\n",
      "- [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf)\n",
      "- [Tiktoken](https://github.com/openai/tiktoken)\n",
      "- [ByteFormer Github](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)\n",
      "- [Token Monster](https://github.com/alasdairforsythe/tokenmonster)\n",
      "- ️[What are Embeddings](http://vickiboykis.com/what_are_embeddings/)[Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)\n",
      "- [Github page on Embeddings](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI divides raw information, which may come in various forms such as binary, text, etc., into individual units called *tokens*. These tokens play a pivotal part in simplifying how the AI comprehends and manipulates the data.\n",
      "\n",
      "## Comprehending Tokenization\n",
      "Tokenization is the mechanism of fragmenting data into these isolated units. The selection of a token hinges heavily on the type of data and the anticipated result from the AI. For instance, in textual data, tokens frequently map to solitary words or subwords.\n",
      "\n",
      "### Subword Tokens\n",
      "A subword element, or a segment of a word itself can be considered a token. The research paper [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) emphasizes the effectiveness of subword units in enhancing the results. It was found that using subword units for tokenization in a neural machine translation system considerably ameliorated the handling of rare words.\n",
      "\n",
      "### Special Tokens\n",
      "\n",
      "Special tokens inform high-level interpreters about the subsequent steps to be taken.\n",
      "\n",
      "| Token Name | Description |\n",
      "| --- | --- |\n",
      "| START_TOKEN or BOS_TOKEN | This is an indicator of the start of a sequence. BOS is the abbreviation for \"Beginning Of Sequence\". |\n",
      "| STOP_TOKEN or EOS_TOKEN | This signals the conclusion of a sequence. EOS represents \"End Of Sequence\". |\n",
      "| MASK_TOKEN | This represents a masked value, which the model is supposed to predict. |\n",
      "| MODALITY_TOKEN | This indicates the type of data present in the sequence (such as text, images, etc.) |\n",
      "\n",
      "\n",
      "## Multimodal Tokenization\n",
      "Multimodal tokenization is a subfield of tokenization that integrates numerous forms or modes of data. The area has been witnessing prominent advancements. An example is [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf), a study that uses transformer technology to feed file bytes directly, indicating multimodal tokenization helps improve the accuracy of AI performance. Based on the findings of the study, the researchers developed the ByteFormer model, which can be accessed [here](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer).\n",
      "\n",
      "## Importance of Embeddings\n",
      "In AI, embeddings play an important role as they convert tokens into a numeric representation that the AI can process. [What are Embeddings](http://vickiboykis.com/what_are_embeddings/) is an insightful read that simplifies the concept of embeddings. For a more detailed understanding, refer to the associated [Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md) page.\n",
      "\n",
      "## Tools\n",
      "Coding tools like [Tiktoken](https://github.com/openai/tiktoken) and [Token Monster](https://github.com/alasdairforsythe/tokenmonster) can assist with tokenization. Tiktoken uses Byte Pair Encoding (BPE) for tokenization, and is allegedly used in GPT models, while Token Monster adopts a unique top-down approach, resulting in nearly 35% fewer tokens compared to the conventional bottom-up approach.\n",
      "\n",
      "## Essential References\n",
      "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)\n",
      "- [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf)\n",
      "- [Tiktoken](https://github.com/openai/tiktoken)\n",
      "- [ByteFormer Github](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)\n",
      "- [Token Monster](https://github.com/alasdairforsythe/tokenmonster)\n",
      "- [What are Embeddings](http://vickiboykis.com/what_are_embeddings/)\n",
      "- [Github page explaining Embeddings in depth](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "711fb39a-a6a1-4394-94d3-6fa83a54bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "md = markdown.markdown(markdown_text, extensions=['toc'])\n",
    "# from markdown.extensions.toc import TocExtension\n",
    "# html = markdown.markdown(markdown_text, extensions=[TocExtension(baselevel=1)])\n",
    "# print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b32251b-2f60-41a2-bcc4-ee275b4ebd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'l': 1, 't': 'Title'}, {'l': 2, 't': 'Section 1'}, {'l': 3, 't': 'Subsection 1.1'}, {'l': 2, 't': 'Section 2'}, {'l': 3, 't': 'Subsection 2.1'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_toc(md_text, level_key=\"l\", text_key='t'):\n",
    "    # Regex to match markdown headings, capturing the level based on hash count and the heading text\n",
    "    pattern = re.compile(r'^(?P<hashes>#+) (?P<text>.+)$', re.MULTILINE)\n",
    "    matches = pattern.findall(md_text.strip())\n",
    "\n",
    "    toc_structure = [{level_key: len(hashes), text_key: text} for hashes, text in matches]\n",
    "    return toc_structure\n",
    "\n",
    "def serialize_toc(toc_structure):\n",
    "    return json.dumps(toc_structure, indent=4)\n",
    "\n",
    "def test_extract_toc():\n",
    "    md_text = \"\"\"\n",
    "# Title\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Content here\n",
    "\n",
    "### Subsection 1.1\n",
    "\n",
    "More content here\n",
    "\n",
    "## Section 2\n",
    "\n",
    "### Subsection 2.1\n",
    "\n",
    "Yet more content\n",
    "    \"\"\"\n",
    "\n",
    "    toc_structure = extract_toc(md_text)\n",
    "    print(toc_structure)\n",
    "    serialized_toc = serialize_toc(toc_structure)\n",
    "\n",
    "    expected_output = \"\"\"\n",
    "[\n",
    "    {\n",
    "        \"l\": 1,\n",
    "        \"t\": \"Title\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 1.1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 2\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 2.1\"\n",
    "    }\n",
    "]\n",
    "    \"\"\"\n",
    "\n",
    "    assert serialized_toc.strip() == expected_output.strip(), f\"Expected:\\n{expected_output}\\nGot:\\n{serialized_toc}\"\n",
    "\n",
    "test_extract_toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacbaf58-392b-494b-9e87-218a439bb6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a28916-5f7d-4334-a77d-a629b42c0a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
