{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-32k',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    openai_api_key=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = 'docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "├── to_sort.md\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── index.md\n",
      "    │   ├── interpreters.md\n",
      "    │   ├── memory.md\n",
      "    │   ├── systems.md\n",
      "    │   └── tools.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embeddings.md\n",
      "    │   ├── preprocessing.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── quality.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokens_and_embeddings.md\n",
      "    ├── engineering_and_management/\n",
      "    │   ├── .pages\n",
      "    │   ├── commercial_products.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── deployment.md\n",
      "    │   ├── engineering_and_management.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── genai_detection.md\n",
      "    │   ├── models.md\n",
      "    │   ├── observability.md\n",
      "    │   ├── plugins.md\n",
      "    │   └── regulation.md\n",
      "    ├── ethics/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── ethics.md\n",
      "    │   ├── fairness.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── model_creation/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── overview.md\n",
      "    ├── prompt_engineering/\n",
      "    │   ├── prompt_injections.md\n",
      "    │   └── prompting.md\n",
      "    └── studies/\n",
      "        └── studies.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return ( '.pages' in path.name or not path.name.startswith(\".\") ) and 'Icon' not in path.name\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return 'docs/Understanding/data/tokens_and_embeddings.md'\n",
    "\n",
    "def get_structure_pattern():\n",
    "    pattern = \\\n",
    "    \"\"\"\n",
    "    <<intro>>\n",
    "    ## <<First topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## <<Second topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## ...\n",
    "    ## Essential References\n",
    "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
    "    \"\"\"\n",
    "    return pattern\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "                \"Generative AI and how to improve upon it. \"\n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "                         \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \\\n",
    "\"\"\" You are working on a project called: {project_name}\\n\n",
    "You are part of a team working to: {project_goals}\\n\n",
    "You are helping to: {present_task_description}\\n\n",
    "You are helping to rewrite and expand a file called {file_name} in the present tree-structure:\\n {tree_structure}\\n \n",
    "Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "Please present html links without changing the link's text. \n",
    "After the markdown When the text is presented, please improve upon it.\n",
    "Please preserve any urls or relative links without changing them. \n",
    "Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "Markdown Response: {markdown_text}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"markdown_text\", \"structure_pattern\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "file_name=get_file_name()\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt=prompt_template.format(project_name=project_name,\n",
    "                       project_goals=project_goals,\n",
    "                       present_task_description=present_task_description,\n",
    "                       file_name=file_name,\n",
    "                       tree_structure=tree_structure,\n",
    "                        structure_pattern=structure_pattern,\n",
    "                       markdown_text=markdown_text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b982b3e-6563-4e18-8d44-7ba14122fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_markdown_text(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are working on a project called: Managing Generative AI\n",
      "\n",
      "You are part of a team working to: Overall: Create an automated repository that is able to explain in plain-English and in code, Generative AI and how to improve upon it. \n",
      "\n",
      "You are helping to: Improve the markdown based on best understandings.Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\n",
      "\n",
      "You are helping to rewrite and expand a file called docs/Understanding/data/tokens_and_embeddings.md in the present tree-structure:\n",
      " docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "├── to_sort.md\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── index.md\n",
      "    │   ├── interpreters.md\n",
      "    │   ├── memory.md\n",
      "    │   ├── systems.md\n",
      "    │   └── tools.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embeddings.md\n",
      "    │   ├── preprocessing.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── quality.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokens_and_embeddings.md\n",
      "    ├── engineering_and_management/\n",
      "    │   ├── .pages\n",
      "    │   ├── commercial_products.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── deployment.md\n",
      "    │   ├── engineering_and_management.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── genai_detection.md\n",
      "    │   ├── models.md\n",
      "    │   ├── observability.md\n",
      "    │   ├── plugins.md\n",
      "    │   └── regulation.md\n",
      "    ├── ethics/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── ethics.md\n",
      "    │   ├── fairness.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── model_creation/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── overview.md\n",
      "    ├── prompt_engineering/\n",
      "    │   ├── prompt_injections.md\n",
      "    │   └── prompting.md\n",
      "    └── studies/\n",
      "        └── studies.md\n",
      " \n",
      "Please use a heading/subheading structure that follows the general pattern : \n",
      "    <<intro>>\n",
      "    ## <<First topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## <<Second topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## ...\n",
      "    ## Essential References\n",
      "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
      "    \n",
      "\n",
      "Please present html links without changing the link's text. \n",
      "After the markdown When the text is presented, please improve upon it.\n",
      "Please preserve any urls or relative links without changing them. \n",
      "Please be sure to use `#` appropriately to reference sections and subsections.\n",
      "Markdown Response: \n",
      "\n",
      "Data, represented on disk in binary, though perhaps read in different digital representations  is broken up into individual units called tokens. These tokens, corresponding to contiguous strings of \n",
      "\n",
      "## Digital Representations\n",
      "Digital representations relates to how data is encoded into memory (short, long) in any way. When it comes to text, the most common representations are utf8 and ascii. It is possible, and potentially useful to consider pure 'binary' representations, especially when considering multimodal data (see below).\n",
      "\n",
      "## Tokenization\n",
      "\n",
      "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) Indicates that breaking up words can offer high value results (2015)\n",
      "\n",
      "\n",
      "### Multimodal\n",
      "\n",
      "- ‼️ [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf) Reveals that just taking file bytes into transformer technology can directly enable improvements in performance accuracy. The accuracy method varies based on encoding method. Their model is called ByteFormer [Github](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)\n",
      " \n",
      "## Tokenization\n",
      "\n",
      "- [Tiktoken](https://github.com/openai/tiktoken) uses BPE and is theoretically used in GPT models. Can be used on pure-binary and limited character data (I've checked!). \n",
      "- [Token Monster](https://github.com/alasdairforsythe/tokenmonster) Uses 35% fewer tokens and uses a top-down approach, instead of a bottom-up constructive approach. Likely of high value. \n",
      "\n",
      "### Special tokens\n",
      "\n",
      "There are special tokens that are used by high-level interpreters on what next to do. \n",
      "\n",
      "```markdown\n",
      "START_TOKEN\n",
      "STOP_TOKEN \n",
      "MASK_TOKEN\n",
      "MODALITY_TOKEN\n",
      "```\n",
      "\n",
      "`mask_token`, `bos_token` (beginning of sequence), `eos_token`\n",
      "  \n",
      "## Embeddings\n",
      "‼️[What are Embeddings](http://vickiboykis.com/what_are_embeddings/)[Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential tokens and embeddings\n",
      "\n",
      "In generative AI, the raw data—whether it be in binary, text, or a different form—is divided into individual units termed as *tokens*. These play a crucial role in easing the understanding and manipulation of data for the AI.\n",
      "\n",
      "## Understanding Tokenization\n",
      "Tokenization is the process of splitting data into these individual units. The choice of a token largely depends on the data type and the expected outcome of the AI. In text data, for instance, tokens often correspond to single words or subwords. \n",
      "\n",
      "### Subword Units\n",
      "A subword unit, or a part of a word, can be a token in itself. The paper titled [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) brings to light the effectiveness of subword units in improving results. This type of tokenization was used in a neural machine translation system and it significantly improved the handling of rare words.\n",
      "\n",
      "### Special Tokens \n",
      "In certain situations, special tokens are utilized by interpreters to designate specific actions. These include `START_TOKEN`, `STOP_TOKEN`, `MASK_TOKEN`, `MODALITY_TOKEN`, `bos_token` (beginning of sequence), and `eos_token` (end of sequence). \n",
      "\n",
      "Examples of coding tools that facilitate tokenization include [Tiktoken](https://github.com/openai/tiktoken) which utilizes Byte Pair Encoding (BPE) for tokenization and is purportedly used in GPT models. An alternative tool is [Token Monster](https://github.com/alasdairforsythe/tokenmonster), which takes a unique top-down approach and results in almost 35% less tokens as opposed to the standard bottom-up approach.\n",
      "\n",
      "## Multimodal Tokenization\n",
      "Multimodal tokenization is an area of tokenization that focuses on incorporating multiple data forms or modes. This facet of tokenization has seen remarkable strides. [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf)—a study utilizing transformer technology to input file bytes directly—demonstrates that multimodal tokenization can assist in improving the AI's performance accuracy. The researchers in the study developed ByteFormer, a model based on their study’s findings that can be accessed [here](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer).\n",
      "\n",
      "## Significance of Embeddings\n",
      "Embeddings play a key role in AI as they translate tokens into numerical representation that can be processed by the AI. 'What are Embeddings' is an essential [read](http://vickiboykis.com/what_are_embeddings/) that elucidates the concept of embeddings in a digestible manner. For a deeper dive, check the accompanied [Github](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md) page.\n",
      "\n",
      "## Essential References\n",
      "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)\n",
      "- [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf)\n",
      "- [Tiktoken](https://github.com/openai/tiktoken)\n",
      "- [ByteFormer Github](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)\n",
      "- [Token Monster](https://github.com/alasdairforsythe/tokenmonster)\n",
      "- [What are Embeddings](http://vickiboykis.com/what_are_embeddings/)\n",
      "- [Github page on Embeddings](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)\n"
     ]
    }
   ],
   "source": [
    "print(ansa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c41409da-359a-4028-9845-6dab3ea78716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data and Digital Representations\\n\\nData, in its digital form, is represented on disk in binary format. However, it can be read and interpreted in different digital representations. The most common representations for text data are utf8 and ascii. In some cases, it may be useful to consider pure \\'binary\\' representations, especially when dealing with multimodal data.\\n\\nTokenization and Multimodal Data\\n\\nTokenization involves breaking up data into individual units called tokens. This process can offer high-value results, as shown in the paper \"Neural Machine Translation of Rare Words with Subword Units.\" This technique is particularly beneficial for languages with complex word formations (2015).\\n\\nIn the context of multimodal data, a study called \"Bytes are all you need\" reveals that using file bytes directly in transformer technology can improve performance accuracy. This study introduces a model called ByteFormer and provides implementation details on GitHub.\\n\\nTokenization Techniques\\n\\nThere are various tokenization techniques available, depending on the specific requirements of the task at hand. For GPT models, the Tiktoken library is commonly used, which utilizes Byte-Pair Encoding (BPE). It can be applied to both pure-binary and limited character data. Another tool, Token Monster, offers a top-down approach to tokenization and boasts a 35% reduction in token usage compared to traditional bottom-up approaches.\\n\\nSpecial Tokens\\n\\nIn the process of tokenization, special tokens are often used by high-level interpreters to guide the sequence of actions. Some common special tokens include `start_token`, `stop_token`, `mask_token`, and `modality_token`. These tokens play a crucial role in instructing the behavior of generative AI models.\\n\\nEmbeddings\\n\\nEmbeddings refer to the representation of data in a lower-dimensional space. They capture the semantic meaning of data and enable effective analysis and processing. To gain a deeper understanding of embeddings, refer to the article \"What are Embeddings\" by Vicki Boykis. Additional information and examples can be found in the GitHub repository associated with the article. \\n\\nEssential References:\\n\\n- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909): This paper highlights the benefits of breaking up words into subword units for translation tasks.\\n- [Bytes are all you need](https://arxiv.org/pdf/2306.00238.pdf) ([GitHub repository](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)): This study demonstrates the use of file bytes in transformer technology for improved performance accuracy.\\n- [Tiktoken](https://github.com/openai/tiktoken): This library utilizes Byte-Pair Encoding (BPE) and is commonly used in GPT models for tokenization.\\n- [Token Monster](https://github.com/alasdairforsythe/tokenmonster): This tool offers a top-down approach to tokenization and reduces token usage by 35% compared to traditional methods.\\n- [What are Embeddings](http://vickiboykis.com/what_are_embeddings/) ([GitHub repository](https://github.com/veekaybee/what_are_embeddings/blob/main/README.md)): This article provides insights into the concept of embeddings and their applications.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fb39a-a6a1-4394-94d3-6fa83a54bac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
