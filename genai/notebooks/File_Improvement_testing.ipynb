{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain_community/llms/openai.py:248: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-1106-preview',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    # openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── index.md\n",
      "├── javascripts/\n",
      "│   └── mathjax.js\n",
      "├── Managenai/\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   ├── index.md\n",
      "│   ├── managing.md\n",
      "│   ├── requirements.md\n",
      "│   └── site_graph.md\n",
      "├── Understanding/\n",
      "│   ├── agents/\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── chains.md\n",
      "│   │   ├── cognitive_architecture.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── feedback.md\n",
      "│   │   ├── finetuning.md\n",
      "│   │   ├── generation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── llm_systems.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── components.md\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusers.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── gpt.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── multimodal.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   └── transformers.md\n",
      "│   │   ├── optimization.md\n",
      "│   │   ├── pre-training.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   ├── recurrent_training.md\n",
      "│   │   └── training.md\n",
      "│   ├── background/\n",
      "│   │   └── tensor_maths.md\n",
      "│   ├── data/\n",
      "│   │   ├── augmentation.md\n",
      "│   │   ├── distillation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── selection.md\n",
      "│   │   ├── simulation.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── deploying/\n",
      "│   │   ├── back_end.md\n",
      "│   │   ├── commercial_products.md\n",
      "│   │   ├── computation.md\n",
      "│   │   ├── examples_and_tutorials.md\n",
      "│   │   ├── frameworks.md\n",
      "│   │   ├── front_end.md\n",
      "│   │   ├── index.md\n",
      "│   │   └── libraries_and_tools.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── ai_and_ml_basics/\n",
      "│   │   │   └── index.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── chronology.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── open_source.md\n",
      "│   │   └── use_cases.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── index.md\n",
      "│   │   └── prompt_injections.md\n",
      "│   └── studies/\n",
      "│       ├── behavior.md\n",
      "│       └── studies.md\n",
      "└── Using/\n",
      "    ├── building_and_buying.md\n",
      "    ├── de-risking/\n",
      "    │   ├── redteaming.md\n",
      "    │   └── security.md\n",
      "    ├── ethically/\n",
      "    │   ├── alignment.md\n",
      "    │   ├── alignment_and_exestential_concerns.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── examples/\n",
      "    │   ├── by_field/\n",
      "    │   │   ├── business.md\n",
      "    │   │   ├── entertainment/\n",
      "    │   │   │   ├── dynamic.md\n",
      "    │   │   │   └── static.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── individuals_and_society/\n",
      "    │   │   │   ├── education.md\n",
      "    │   │   │   ├── law.md\n",
      "    │   │   │   └── socio_societal.md\n",
      "    │   │   ├── mathematics/\n",
      "    │   │   │   └── index.md\n",
      "    │   │   ├── science/\n",
      "    │   │   │   ├── biology.md\n",
      "    │   │   │   ├── chemistry.md\n",
      "    │   │   │   ├── healthcare.md\n",
      "    │   │   │   └── index.md\n",
      "    │   │   └── technology/\n",
      "    │   │       ├── coding.md\n",
      "    │   │       ├── finance.md\n",
      "    │   │       ├── healthcare.md\n",
      "    │   │       └── robotics.md\n",
      "    │   ├── by_modality/\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── knowledge_graphs.md\n",
      "    │   │   ├── language.md\n",
      "    │   │   ├── multimodal.md\n",
      "    │   │   ├── sound.md\n",
      "    │   │   ├── static_2d.md\n",
      "    │   │   ├── tabular.md\n",
      "    │   │   ├── text.md\n",
      "    │   │   ├── time_series.md\n",
      "    │   │   └── video.md\n",
      "    │   └── index.md\n",
      "    ├── index.md\n",
      "    ├── interfacing_layers/\n",
      "    │   └── web_plugins.md\n",
      "    ├── managing/\n",
      "    │   ├── governing.md\n",
      "    │   ├── index.md\n",
      "    │   ├── ml_ops.md\n",
      "    │   ├── observability.md\n",
      "    │   └── regulations_and_guidelines.md\n",
      "    └── marking_and_detecting.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return  not ( 'Icon' in path.name or '.DS_Store' in path.name or 'stylesheets'  in path.name or \\\n",
    "        'CNAME' in path.name or 'assets' in path.name or '.svg' in path.name or '.pages' in path.name)\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern(file_class=None):\n",
    "    if file_class is None:\n",
    "        file_class = 'index.md'\n",
    "    patterns={}\n",
    "    patterns['index.md'] = \\\n",
    "    \"\"\"\n",
    "NOTE: '-' is used to denote a general topic, sentence, or consideration but not considered a 'list' item.\n",
    "## Executive Summary (TL;DR)\n",
    "- Concise summary highlighting the essence of the topic and its significance.\n",
    "- Designed for readability by a non-technical or executive-level audience.\n",
    "- Utilize emojis, images, and visual elements effectively to emphasize key points.\n",
    "- Include Mermaid diagrams where appropriate, or describe necessary images as `IMAGE: <image description>`.\n",
    "\n",
    "## Practical Application and Usage\n",
    "- Focus on providing immediately actionable guidance and high-priority examples.\n",
    "- Extract and condense key usage instructions from earlier content into concise, actionable steps.\n",
    "- Offer 'How-to' guides, quick-start tips, and links for direct application.\n",
    "\n",
    "## Introduction and Relevance\n",
    "- Thorough introduction to the topic, highlighting its relevance and importance.\n",
    "- Discuss core components and their interplay within the broader context of Generative AI.\n",
    "\n",
    "## Core Content and Results\n",
    "- Detailed exploration of specific aspects under clear subheadings.\n",
    "- Provide illustrations or diagrams (Mermaid or `IMAGE:<image description>`) for complex concepts.\n",
    "- For extensive topics, include brief summaries and links to dedicated markdown files. If markdown files are already created, link to them here. If markdown files are needed, suggest them. \n",
    "\n",
    "## Technological Aspects\n",
    "- Explore relevant tools, technologies, and methodologies.\n",
    "- Highlight current trends and future directions in technology related to the topic.\n",
    "\n",
    "## Background or Theoretical Foundation (if necessary)\n",
    "- Delve into historical context and foundational theories.\n",
    "- Clarify essential theoretical concepts and terminologies for comprehensive understanding.\n",
    "\n",
    "## Ethical Considerations and Challenges\n",
    "- Address ethical dilemmas, challenges, and potential risks.\n",
    "- Discuss strategies for ethical practice and risk mitigation.\n",
    "\n",
    "## Extended Examples (if applicable)\n",
    "- Link to practical examples, simulations, or code snippets for hands-on understanding.\n",
    "- Direct readers to external resources, tools, or demonstrations for further exploration.\n",
    "\n",
    "## Advanced Topics and Further Exploration (if applicable)\n",
    "- Present open challenges and future research directions.\n",
    "- Deep dive into complex aspects with links to advanced readings and resources.\n",
    "\n",
    "## FAQs and Common Queries\n",
    "- Tackle frequently asked questions and common queries related to the section.\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "- Recap the main points and emphasize the key messages from the section.\n",
    "\n",
    "## References and Additional Reading\n",
    "- List citations and provide links to source materials and further reading.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # patterns['glossary.md'] = \\\n",
    "    # \"\"\"\n",
    "    # \"\"\"\n",
    "    return patterns[file_class]\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role = \"expert AI technology creator and communicator\"\n",
    "\n",
    "# project_name = \"Managing Generative AI\"\n",
    "# project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "#                 \"Generative AI and how to improve upon it. \"\n",
    "# present_task_description=\"Improve the markdown based on best understandings.\\n\"\\\n",
    "#                          \"Be as honest and as accurate as possible. Be succinct in your responses. \"\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "# # Idea\n",
    "# # Select between prompt patterns\n",
    "# # Chain select them more effectively. \n",
    "# # The present tree-structure:\\n {tree_structure}\\n \n",
    "# # Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "# template = \\\n",
    "# \"\"\" \n",
    "# You are a {role}\n",
    "# You are working on a project called: {project_name}\\n\n",
    "# You are part of a team working to: {project_goals}\\n\n",
    "# You are helping to: {present_task_description}\\n\n",
    "# You are helping to rewrite and expand a file called {file_name} \n",
    "# Here are some things we'd like you to be sure to do:\n",
    "# * Please present ALL html links without changing the link's text. \n",
    "# * Preserve any urls or relative links without changing them. \n",
    "# * Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "# * Please reformat any bulleted lists of links where github links have `!!! code`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "\n",
    "# After the markdown When the text is presented (after >>>), please improve upon it. \n",
    "# If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
    "\n",
    "# Markdown Input:\\n\n",
    "# >>>\\n\n",
    "# {markdown_text}\"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate(\n",
    "#     # input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"structure_pattern\", \"markdown_text\", ],\n",
    "#         input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\",  \"markdown_text\", ],\n",
    "#     template=template\n",
    "# )\n",
    "# file_from_base_dir = 'Using/deploying/index.md'\n",
    "# file_from_base_dir = 'Using/redteaming.md'\n",
    "# file_from_base_dir = 'Understanding/agents/rag.md'\n",
    "# file_name=get_file_name(file_from_base_dir)\n",
    "# tree_structure=get_tree_structure()\n",
    "# markdown_text=get_markdown_text(file_name)\n",
    "# structure_pattern = get_structure_pattern()\n",
    "# prompt=prompt_template.format(role=role,\n",
    "#                               project_name=project_name,\n",
    "#                        project_goals=project_goals,\n",
    "#                        present_task_description=present_task_description,\n",
    "#                        file_name=file_name,\n",
    "#                        # tree_structure=tree_structure,\n",
    "#                         structure_pattern=structure_pattern,\n",
    "#                        markdown_text=markdown_text,)\n",
    "# # The above is very verboase especially as it requires a lot of repeated typing of the same variables.\n",
    "# # It also needs to work for variables that are only specified in the template. If they are not specified in the template, then they should be ignored.\n",
    "# # There will a list of template lines that are appended to create the final prompt.\n",
    "# # The template lines will be specified as a list of dictionaries.\n",
    "# #  \n",
    "# # Let's write this as a class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  write above but realizing template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0241f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartPromptTemplate:\n",
    "    def __init__(self, template_required, template_optional_dict, template_variable_independent):\n",
    "        self.template_required = template_required if template_required is not None else \"\"\n",
    "        self.template_optional_dict = template_optional_dict if template_optional_dict is not None else {}\n",
    "        self.template_variable_independent = template_variable_independent if template_variable_independent is not None else \"\"\n",
    "    \n",
    "    def get_prompt(self, **kwargs):\n",
    "        template_list = []\n",
    "        for k, v in kwargs.items():\n",
    "            if k in self.template_optional_dict.keys():\n",
    "                template_list.append(self.template_optional_dict[k])\n",
    "        begin_indicator = \"\\n What would you write given the requests above? \\n>>>\\n\"\n",
    "        \n",
    "        template =   '\\n'.join(template_list) + self.template_required  + \"\\n<<< end input \\n\" + \\\n",
    "                        self.template_variable_independent + begin_indicator\n",
    "        prompt = template.format(**kwargs)\n",
    "        return prompt\n",
    "\n",
    "template_optional_dict = {\n",
    "\n",
    "    'role': \"You are a {role}\",\n",
    "    # 'project_name': \"You are working on a project called: {project_name}\\n\",\n",
    "    # 'project_goals': \"You are part of a team working to: {project_goals}\\n\",\n",
    "    'present_task_description': \"You are helping to: {present_task_description}\\n\",\n",
    "    'file_name': \"You are helping to rewrite and expand a file called {file_name}\\n\",\n",
    "    'structure_pattern': \"Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\",\n",
    "    'tree_structure': \"The present tree-structure:\\n {tree_structure}\\n \",\n",
    "    'markdown_text': \"Markdown input \\n>>>\\n{markdown_text}\"\n",
    "}\n",
    "\n",
    "\n",
    "template_variable_independent = \\\n",
    "\"\"\"\n",
    "Things to keep in mind:\n",
    "* present ALL html links without changing the link's text.\n",
    "* Preserve any urls or relative links without changing them. \n",
    "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
    "* Keep all code blocks that are referenced in any manner.\n",
    "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
    "* Be as honest and as accurate as possible. \n",
    "* Be succinct in your responses. \n",
    "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
    "* If text is sparse or missing create a reasonable outline and follow it. \n",
    "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
    "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
    "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
    "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
    "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
    "* We'll get $1000 if we do this right, so let's do our best!\n",
    "\"\"\"\n",
    "# Please, do follow these instructions closely for it if we don't get this right, we might lose our job. \n",
    "# * reformat any bulleted lists of links where github links have `!!! code`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "template_required = \\\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "spt = SmartPromptTemplate(template_required=template_required, \n",
    "template_optional_dict=template_optional_dict, \n",
    "template_variable_independent=template_variable_independent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfcaa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"expert AI technology creator and communicator\"\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Create an automated repository that is able to explain Generative AI \"\\\n",
    "        \"and how to improve upon it in plain-English and how to enable it from idea to product, as well as new and interesting research. \"\\\n",
    "                \n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\n",
    "                         \n",
    "# file_from_base_dir = 'Using/deploying/index.md'\n",
    "# file_from_base_dir = 'Using/deploying/libraries_and_tools.md'\n",
    "file_from_base_dir = 'Understanding/data/simulation.md'\n",
    "file_from_base_dir = 'Understanding/prompting/index.md'\n",
    "# file_from_base_dir = 'Understanding/architectures/optimization.md'\n",
    "# file_from_base_dir = 'Understanding/architectures/rlhf.md'\n",
    "# file_from_base_dir = 'Understanding/agents/rag.md'\n",
    "# file_from_base_dir = 'Understanding/Overview/index.md'\n",
    "# file_from_base_dir = 'Understanding/architectures/feedback.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt = spt.get_prompt(role=role, \n",
    "    project_name=project_name,\n",
    "    project_goals=project_goals,\n",
    "    present_task_description=present_task_description,\n",
    "    file_name=file_name,\n",
    "#     tree_structure=tree_structure,\n",
    "    markdown_text=markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a expert AI technology creator and communicator\n",
      "You are helping to: Improve the markdown based on best understandings.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Understanding/prompting/index.md\n",
      "\n",
      "Markdown input \n",
      ">>>\n",
      "Prompts detail the manner in which a Generative AI model should be producing output. Constructing the prompts to be the most effective in obtaining desired output is known as prompt engineering (PE). While PE may have dependencies on the underlying models, there are strategies that can be more universal in their ability to do well.\n",
      "\n",
      "Because often an individual query or generation may be insufficient to produce the desired outputs, it may be necessary to use [cognitive architectures](../agents/cognitive_architecture.md) as part of [chains](../agents/chains.md). Here, we describe one-shot prompting methods, may function without multiple LLM-calls.\n",
      "\n",
      "It is also important to note, that while [manual methods](#manual-methods) are essential and may continue, [automatic methods](#automatic-methods) have become common and may help to reduce burdens of identifying sufficiently optimal prompts for certain models and situations.\n",
      "\n",
      "\n",
      "## Manual Methods\n",
      "\n",
      "- Give clearer instructions\n",
      "- Split complex tasks into simpler subtasks\n",
      "- Structure the instruction to keep the model on task\n",
      "- Prompt the model to explain before answering\n",
      "- Ask for justifications of many possible answers, and then synthesize\n",
      "- Generate many outputs, and then use the model to pick the best one\n",
      "- Fine-tune custom models to maximize performance\n",
      "- Provide several examples to ground it.\n",
      "  -  Good to evaluate this and see if input examples give expected scores. Modify the prompt if it isn't.\n",
      "- Consider prompt versioning to keep track of outputs more easily.\n",
      "- Break prompts into smaller prompts\n",
      "- Chain of Thought Prompting\n",
      "- Generate many outputs and pick final one or use LLM to pick best one.\n",
      "\n",
      "\n",
      "??? important \"[Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171 )\"\n",
      "\n",
      "    26 Prompting Tips\n",
      "\n",
      "    1 - No need to be polite with LLM so there is no need to add phrases like “please”, “if you don’t mind”, “thank you”, “I would like to”, etc., and get straight to the point.\n",
      "    2 -  Integrate the intended audience in the prompt, e.g., the audience is an expert in the field.\n",
      "    3 - Break down complex tasks into a sequence of simpler prompts in an interactive conversation.\n",
      "    4 - Employ affirmative directives such as ‘do,’ while steering clear of negative language like ‘don’t’.\n",
      "    5 - When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts:\n",
      "        * Explain [insert specific topic] in simple terms.\n",
      "        * Explain to me like I’m 11 years old.\n",
      "        * Explain to me as if I’m a beginner in [field].\n",
      "        * Write the [essay/text/paragraph] using simple English like you’re explaining something to a 5-year-old.\n",
      "\n",
      "    6 - Add “I’m going to tip $xxx for a better solution!”\n",
      "    7 -  Implement example-driven prompting (Use few-shot prompting).\n",
      "    8 - When formatting your prompt, start with ‘###Instruction###’, followed by either ‘###Example###’ or ‘###Question###’ if relevant. Subsequently, present your content. Use one or more\n",
      "    line breaks to separate instructions, examples, questions, context, and input data.\n",
      "    9 -  Incorporate the following phrases: “Your task is” and “You MUST”.\n",
      "    10 - Incorporate the following phrases: “You will be penalized”.\n",
      "    11 -  Use the phrase ”Answer a question given in a natural, human-like manner” in your prompts.\n",
      "    12 - Use leading words like writing “think step by step”.\n",
      "    13 -  Add to your prompt the following phrase “Ensure that your answer is unbiased and does not rely on stereotypes”.\n",
      "    14 - Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output (for example, “From now on, I would like you to ask me questions to...”).\n",
      "    15 - To inquire about a specific topic or idea or any information and you want to test your understanding, you can use the following phrase: “Teach me the [Any theorem/topic/rule name] and include a test at the end, but don’t\n",
      "    give me the answers and then tell me if I got the answer right when I respond”.\n",
      "    16 - Assign a role to the large language models.\n",
      "    17 - Use Delimiters.\n",
      "    18 - Repeat a specific word or phrase multiple times within a prompt.\n",
      "    19 - Combine Chain-of-thought (CoT) with few-Shot prompts.\n",
      "    20 - \n",
      "    Use output primers, which involve concluding your prompt with the beginning of the desired output. Utilize output primers by ending your prompt with the start of the anticipated response. \n",
      "    21 - To write an essay /text /paragraph /article or any type of text that should be detailed: “Write a detailed [essay/text /paragraph] for me on [topic] in detail by adding all the information necessary”.\n",
      "    22 - To correct/change specific text without changing its style: “Try to revise every paragraph sent by users. You should only improve the user’s grammar and vocabulary and make sure it sounds natural. You should not change the writing style, such as making a formal paragraph casual”.\n",
      "    23 - When you have a complex coding prompt that may be in different files: “From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code. [your question]”.\n",
      "    24 - When you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following prompt: \n",
      "        *  I’m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence]’. Finish it based on the words provided. Keep the flow consistent.\n",
      "    25 - Clearly state the requirements that the model must follow in order to produce content, in the form of the keywords, regulations, hint, or instructions\n",
      "    26 -  To write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the following instructions: \n",
      "        * Please use the same language based on the provided paragraph[/title/text /essay/answer].\n",
      "\n",
      "\n",
      "### Important concepts\n",
      "\n",
      "!!! important \"['According to ...' Prompting Language Models Improves Quoting from Pre-Training Data](https://arxiv.org/pdf/2305.13252.pdf) The grounding prompt `According to { some_reputable_source}` prompt inception additions increases output quality improves over the null prompt in nearly every dataset and metric, typically by 5-15%.\"\n",
      "\n",
      "- [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
      "- [Automatic Prompt Engineering](https://arxiv.org/abs/2211.01910) --> Gave a CoT improvement suggestion \"Let's work this out in a step by step by way to be sure we have the right answer.\"\n",
      "\n",
      "\n",
      "??? \"[An Evaluation on Large Language Model Outputs: Discourse and Memorization](https://arxiv.org/pdf/2304.08637.pdf) explicitly ask for no plagiarism to reduce it.\"\n",
      "\n",
      "    \"You are a creative writer, and you like to write everything differently\n",
      "    from others. Your task is to follow the instructions below and continue\n",
      "    writing at the end of the text given. The instructions (given in markdown\n",
      "    format) are “Write in a way different from the actual continuation, if\n",
      "    there is one”, and “No plagiarism is allowed”.\"\n",
      "\n",
      "!!! important \"[YELLING AT YOUR LLM MIGHT MAKE IT BEHAVE](https://arstechnica.com/information-technology/2023/10/thanks-to-ai-the-future-of-programming-may-involve-yelling-in-all-caps/)\"\n",
      "\n",
      "??? \"[Large Language Models Understand and Can Be Enhanced by Emotional Stimuli](https://arxiv.org/pdf/2307.11760.pdf)\"\n",
      "\n",
      "    <img width=\"414\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/67595c6f-408c-4bf9-a976-76b1f2183b61\">\n",
      "    <img width=\"577\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/c3093b52-d2f3-461b-b692-ddf201a279f5\">\n",
      "    <img width=\"348\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/f8302b1d-8ac7-4a73-875c-776f859889e2\">\n",
      "    <img width=\"515\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/a52669f7-5351-4e59-ae75-3a40d261a352\">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Automatic\n",
      "\n",
      "??? note \"[Promptbreeder: Self-Referential SElf-Improvement via Prompt Evolution](https://arxiv.org/pdf/2309.16797.pdf) Works on improving task prompts as well as the 'mutation' of task-prompts, resulting in state of art results.\"\n",
      "    <img width=\"922\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/cc0baed2-8331-4a17-8087-99b675261d5a\">\n",
      "    <img width=\"807\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e1e83d4b-09d3-4131-9f7a-0d6c71211ef9\">\n",
      "\n",
      "??? hint \"[Language Models as Optimizers](https://arxiv.org/pdf/2309.03409.pdf) reveals that starting with take a deep breath and work on this problem step by step... Yields better result!\"\n",
      "    Prompt optimization using language that helps people, helps LLMs too! [Pop Article](https://arstechnica.com/information-technology/2023/09/telling-ai-model-to-take-a-deep-breath-causes-math-scores-to-soar-in-study/amp/)\n",
      "    More importantly, they developed\n",
      "    ```\n",
      "    \"Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs)\n",
      "    as optimizers, where the optimization task is described in natural language\"\n",
      "    ```\n",
      "    to optimize prompts:\n",
      "    <img width=\"418\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/b82fd195-db43-48bb-9014-f5395329aa9a\">\n",
      "\n",
      "\n",
      "\n",
      "!!! note \"[GPT Prompt Engineer](https://github.com/mshumer/gpt-prompt-engineer)\"\n",
      "    A fairly simple automation tool to create the best prompts\n",
      "\n",
      "    ```python\n",
      "        description = \"Given a prompt, generate a landing page headline.\" # this style of description tends to work well\n",
      "\n",
      "        test_cases = [\n",
      "            {\n",
      "                'prompt': 'Promoting an innovative new fitness app, Smartly',\n",
      "            },\n",
      "            {\n",
      "                'prompt': 'Why a vegan diet is beneficial for your health',\n",
      "            },\n",
      "            ...\n",
      "        ]\n",
      "    ```\n",
      "\n",
      "    ![image](https://github.com/ianderrington/genai/assets/76016868/f02a9f3e-4f4c-49de-9b35-1702df65d618)\n",
      "\n",
      "\n",
      "### Prompt Compression\n",
      "\n",
      "Prompt compression provides methods of compressing prompt inputs in such a way that it will yield equivalent results for downstream result generation. \n",
      "\n",
      "??? code \"[(Long)LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://github.com/microsoft/LLMLingua?)\"\n",
      "\n",
      "    [Paper: LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/pdf/2310.06839.pdf)\n",
      "    [Paper: LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/pdf/2310.05736.pdf)\n",
      "    The authors demonstrate the use of smaller language models to identify and remove non-essential tokens in prompts, enabling up to 20x compression with minimal performance loss. The method is designed to generate a compressed prompt from an original prompt. Using a budget controller to dynamically allocate compression ratios for different components prompts to maintain semantic integrity under high compression ratios. \n",
      "    \n",
      "    ![image](https://github.com/ianderrington/genai/assets/76016868/fa37f948-b1c0-4886-a1fb-1dad2ca435c0)\n",
      "    <img width=\"544\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/ea698dc3-2d05-4b40-9e77-722bf5ccbd79\">\n",
      "\n",
      "    **Pseudo Code**\n",
      "    <img width=\"321\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/0817d223-e806-4d16-9c31-c85124b248a7\">\n",
      "    <img width=\"307\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/40ef2794-7924-4882-a1bd-2d090428c017\">\n",
      "    \n",
      "\n",
      "## Useful Resources for LLM Prompting\n",
      "\n",
      "- [Prompting is Programming: A Query Language for Large Language Models](https://arxiv.org/pdf/2212.06094.pdf)\n",
      "\n",
      "### Best practices and guides\n",
      "\n",
      "!!! tip \"[Techniques to improve reliability](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md#how-to-improve-reliability-on-complex-tasks) By OpenAI\"\n",
      "\n",
      "\n",
      "### Libraries \n",
      "\n",
      "!!! tip \"[Prompt Royale](https://github.com/meistrari/prompts-royale) Provides the ability to automatically generate prompts to test around the same general theme.\"\n",
      "\n",
      "\n",
      "- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/pdf/2302.11382.pdf)\n",
      "- [LLM Practical Guide](https://github.com/Mooler0410/LLMsPracticalGuide) based on [paper](https://arxiv.org/abs/2304.13712).\n",
      "- [Prompting Guide](https://www.promptingguide.ai/)\n",
      "\n",
      "- [Prompt Engineering by Lillian Wang](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
      "- [OPEN AI best practices](https://platform.openai.com/docs/guides/gpt-best-practices/)\n",
      "- [Prompting Guide](https://www.promptingguide.ai/techniques)\n",
      "\n",
      "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
      "- [Best practices for prompt engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
      "\n",
      "### Repositories and Collections\n",
      "- [Awesome Prompts](https://github.com/f/awesome-chatgpt-prompts/blob/main/README.md)\n",
      "- [Prompt Hub](https://app.prompthub.studio/) For Generating image prompts\n",
      "- [Wolfram Prompt Repo](https://writings.stephenwolfram.com/2023/06/prompts-for-work-play-launching-the-wolfram-prompt-repository/?mibextid=Zxz2cZ)\n",
      "\n",
      "### Tools and Services\n",
      "- [Notion.io plugin](https://haonmade.gumroad.com/l/ozuvb)\n",
      "- [PROMPT generator](https://huggingface.co/spaces/merve/ChatGPT-prompt-generator) To save a few words by just entering a persona and gives prompt output.\n",
      "- [Prompt Engine (MSFT) database tool](https://github.com/microsoft/prompt-engine) MIT license\n",
      "- [Scale spellbook](https://www.scale.com/spellbook)\n",
      "\n",
      "\n",
      "### Prompt tuning\n",
      "\n",
      "Uses a layer to not change prompts but change the embedding of the prompts.\n",
      "- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)\n",
      "Boosted Prompting: few shot prompts that progressively solve more of the problem.\n",
      "\n",
      "## Prompt and optimization\n",
      "- [Large Language Models Can Self Improve](https://arxiv.org/pdf/2210.11610.pdf) Using Chain of thought to provide better examples and then fine-tune the LLM.\n",
      "- [Refiner](https://arxiv.org/pdf/2304.01904.pdf) Iteratively improves itself based on an LLM critic\n",
      "<img width=\"713\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/3ac44e13-2444-4f1e-ae3b-800c9d32ce59\">\n",
      "\n",
      "\n",
      "## To Sort\n",
      "\n",
      "[A good description of advanced prompt tuning](https://cameronrwolfe.substack.com/p/advanced-prompt-engineering)\n",
      "\n",
      "\n",
      "```\n",
      "AutoPrompt [5] combines the original prompt input with a set of shared (across all input data) “trigger tokens” that are selected via a gradient-based search to improve performance.\n",
      "\n",
      "Prefix Tuning [6] adds several “prefix” tokens to the prompt embedding in both input and hidden layers, then trains the parameters of this prefix (leaving model parameters fixed) with gradient descent as a parameter-efficient fine-tuning strategy.\n",
      "\n",
      "Prompt Tuning [7] is similar to prefix tuning, but prefix tokens are only added to the input layer. These tokens are fine-tuned on each task that the language model solves, allowing prefix tokens to condition the model for a given task.\n",
      "\n",
      "P-Tuning [8] adds task-specific anchor tokens to the model’s input layer that are fine-tuned but allows these tokens to be placed at arbitrary locations (e.g., the middle of the prompt), making the approach more flexible than prefix tuning.\n",
      "\n",
      "[5] Shin, Taylor, et al. \"Autoprompt: Eliciting knowledge from language models with automatically generated prompts.\" arXiv preprint arXiv:2010.15980 (2020).\n",
      "\n",
      "[6] Li, Xiang Lisa, and Percy Liang. \"Prefix-tuning: Optimizing continuous prompts for a generation.\" arXiv preprint arXiv:2101.00190 (2021).\n",
      "\n",
      "[7] Lester, Brian, Rami Al-Rfou, and Noah Constant. \"The power of scale for parameter-efficient prompt tuning.\" arXiv preprint arXiv:2104.08691 (2021).\n",
      "\n",
      "[8] Liu, Xiao, et al. \"GPT understands, too.\" arXiv preprint arXiv:2103.10385 (2021).\n",
      "\n",
      "[Self consistency technique](https://arxiv.org/pdf/2203.11171.pdf)\n",
      "```\n",
      "\n",
      "\n",
      "<<< end input \n",
      "\n",
      "Things to keep in mind:\n",
      "* present ALL html links without changing the link's text.\n",
      "* Preserve any urls or relative links without changing them. \n",
      "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
      "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
      "* Keep all code blocks that are referenced in any manner.\n",
      "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
      "* Be as honest and as accurate as possible. \n",
      "* Be succinct in your responses. \n",
      "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
      "* If text is sparse or missing create a reasonable outline and follow it. \n",
      "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
      "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
      "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
      "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
      "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
      "* We'll get $1000 if we do this right, so let's do our best!\n",
      "\n",
      " What would you write given the requests above? \n",
      ">>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../docs/Understanding/prompting/index_temp0.md'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write the file to disk with a _temp suffix and then open it with a system call to tkdiff to visualize the two\n",
    "# files side by side.\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import webbrowser\n",
    "\n",
    "def write_to_file(file_name, text):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    return file_name\n",
    "\n",
    "# Please be sure to run `homebrew install tkdiff` or otherwise install tkdiff on your computer\n",
    "def open_with_tkdiff(file_name1, file_name2):\n",
    "    subprocess.run(['tkdiff', file_name1, file_name2])\n",
    "\n",
    "def make_name(file_name):\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    temp_name = base + '_temp0' + ext\n",
    "    #check to see if it exists and if so, make a new name with a _temp# where # is the next available number\n",
    "    count=0\n",
    "    while os.path.exists(temp_name):\n",
    "        count += 1\n",
    "        \n",
    "        temp_name = base + f'_temp{count}' + ext\n",
    "    return temp_name\n",
    "temp_name = make_name(file_name)\n",
    "write_to_file(temp_name, markdown_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "open_with_tkdiff(file_name, temp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../docs/Using/index.md has been updated.\n"
     ]
    }
   ],
   "source": [
    "input_answer = input(\"Is the output correct Yes/no/deletefile? (y/n/d)\")\n",
    "## if the answer is y then move the temp-name to the original file name and delete the temp file\n",
    "if input_answer == 'y':\n",
    "    os.rename(temp_name, file_name)\n",
    "    print(f\"File {file_name} has been updated.\")\n",
    "else:\n",
    "    print(f\"File {file_name} has not been updated.\")\n",
    "    # if the answer is n then delete the temp file and do nothing\n",
    "    if input_answer == 'd':\n",
    "        os.remove(temp_name)\n",
    "        print(f\"File {temp_name} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT THAT LOOKS AT DIFFERENCES CHHUNK BY CHUNK AND AMENDS THEM. \n",
    "CREATE DIFF, ITERATE ON DIFF AND UPDATE MODIFIED DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
