{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ian/miniforge3/envs/genai/lib/python3.10/site-packages/langchain/llms/openai.py:173: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ian/miniforge3/envs/genai/lib/python3.10/site-packages/langchain/llms/openai.py:751: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-32k',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── Engineering/\n",
      "│   ├── .pages\n",
      "│   ├── actions_and_tools.md\n",
      "│   ├── by_application.md\n",
      "│   ├── commercial_products.md\n",
      "│   ├── computation.md\n",
      "│   ├── deployment.md\n",
      "│   ├── examples.md\n",
      "│   ├── frameworks_and_tools.md\n",
      "│   ├── hyper_parameter_optimization.md\n",
      "│   ├── implementations.md\n",
      "│   ├── index.md\n",
      "│   ├── interpreters.md\n",
      "│   ├── marking_and_detecting.md\n",
      "│   ├── memory.md\n",
      "│   ├── models.md\n",
      "│   ├── observability.md\n",
      "│   ├── regulation.md\n",
      "│   └── web_plugins.md\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── agents.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── index.md\n",
      "    │   ├── memory.md\n",
      "    │   └── systems.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embedding.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── selection.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokenizing.md\n",
      "    ├── ethical_concerns/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── models/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── RL.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   ├── prompt_engineering/\n",
      "    │   │   ├── prompt_injections.md\n",
      "    │   │   └── prompting.md\n",
      "    │   ├── rag.md\n",
      "    │   ├── reinforcement_feedback.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── index.md\n",
      "    └── studies/\n",
      "        └── studies.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return ( '.pages' in path.name or not path.name.startswith(\".\") ) and 'Icon' not in path.name\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern():\n",
    "    pattern = \\\n",
    "    \"\"\"\n",
    "    Please write a thorough response that is pedagogical yes succinct. Use a heading/subheading structure that follows the general pattern : \n",
    "    <<intro>>\n",
    "    ## <<First topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## <<Second topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## ...\n",
    "    ## Essential References\n",
    "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
    "    \"\"\"\n",
    "    return pattern\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "                \"Generative AI and how to improve upon it. \"\n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "                         \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \\\n",
    "\"\"\" You are working on a project called: {project_name}\\n\n",
    "You are part of a team working to: {project_goals}\\n\n",
    "You are helping to: {present_task_description}\\n\n",
    "You are helping to rewrite and expand a file called {file_name} in the present tree-structure:\\n {tree_structure}\\n \n",
    "{structure_pattern}\\n\n",
    "Please present html links without changing the link's text. \n",
    "After the markdown When the text is presented (after >>>), please improve upon it. If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
    "Please preserve any urls or relative links without changing them. \n",
    "Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "Please use balanced use of italics, boldface, and occasional use of emojis to help increase readability. \n",
    "Please write this to be read by someone with a college degree, but minimize unnecessarily large words. \n",
    "Please preserve any admonitions like ??? and !!! and consider introducing others for similar references.\n",
    "Please use bullets, lists, and other information as well as any admonitions  from markdown to make it easy to read and understand.\n",
    "Markdown Response: \n",
    ">>>\\n\n",
    "{markdown_text}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"markdown_text\", \"structure_pattern\"],\n",
    "    template=template\n",
    ")\n",
    "file_from_base_dir = 'Engineering/deployment.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt=prompt_template.format(project_name=project_name,\n",
    "                       project_goals=project_goals,\n",
    "                       present_task_description=present_task_description,\n",
    "                       file_name=file_name,\n",
    "                       tree_structure=tree_structure,\n",
    "                        structure_pattern=structure_pattern,\n",
    "                       markdown_text=markdown_text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b982b3e-6563-4e18-8d44-7ba14122fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# project_name = \"Managing Generative AI\"\n",
    "# project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "#                 \"Generative AI and how to improve upon it. \"\n",
    "# present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "#                          \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "\n",
    "# template = \\\n",
    "# \"\"\" You are working on a project called: {project_name}\\n\n",
    "# You are part of a team working to: {project_goals}\\n\n",
    "# You are helping to: {present_task_description}\\n\n",
    "# You are helping to rewrite and expand a file called {file_name}\\n\n",
    "# Primarily you are tasked with adding admonitions to the markdown document to make it more nice to read. \n",
    "# for instance, you will see formats like this:\n",
    "# '''\n",
    "# - [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf) A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents.   In it they discuss several challenges and solutions:\n",
    "\n",
    "#     **Remembering**\n",
    "    \n",
    "#     _Observation Memory_ A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping.\n",
    "#     Uses, _recency_, _importance_ and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. \n",
    "# '''\n",
    "# This needs to be reformatted in the following manner:\n",
    "# '''\n",
    "# <div class=\"result\" markdown>\n",
    "# !!! tip \"[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf)\"\n",
    "#     A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents.   In it they discuss several challenges and solutions:\n",
    "\n",
    "# ??? example \n",
    "#      **Remembering**\n",
    "    \n",
    "#     _Observation Memory_ A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping.\n",
    "#     Uses, _recency_, _importance_ and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. \n",
    "# </div> \n",
    "\n",
    "# You should make this modification for EVERY link that is presented that doesn't have admonitions already there. \n",
    "# After the markdown When the text is presented, please improve upon it. If no text is present create a reasonable outline following the pattern above and fill it in.\n",
    "# Please preserve any urls or relative links without changing them. \n",
    "# Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "# Please be sure to use appropriate spacing to make admonitions work.\n",
    "\n",
    "# Here is the markdown text:\n",
    "# {markdown_text}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"markdown_text\"],\n",
    "#     template=template\n",
    "# )\n",
    "# file_from_base_dir = 'Understanding/agents/systems.md'\n",
    "# file_name=get_file_name(file_from_base_dir)\n",
    "\n",
    "# markdown_text=get_markdown_text(file_name)\n",
    "# prompt=prompt_template.format(project_name=project_name,\n",
    "#                        project_goals=project_goals,\n",
    "#                        present_task_description=present_task_description,\n",
    "#                        file_name=file_name,\n",
    "#                        markdown_text=markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are working on a project called: Managing Generative AI\n",
      "\n",
      "You are part of a team working to: Overall: Create an automated repository that is able to explain in plain-English and in code, Generative AI and how to improve upon it. \n",
      "\n",
      "You are helping to: Improve the markdown based on best understandings.Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Engineering/deployment.md in the present tree-structure:\n",
      " docs/\n",
      "├── .pages\n",
      "├── assets/\n",
      "│   ├── genai_logo_edited.svg\n",
      "│   └── genai_logo_v1.png\n",
      "├── CNAME\n",
      "├── Engineering/\n",
      "│   ├── .pages\n",
      "│   ├── actions_and_tools.md\n",
      "│   ├── by_application.md\n",
      "│   ├── commercial_products.md\n",
      "│   ├── computation.md\n",
      "│   ├── deployment.md\n",
      "│   ├── examples.md\n",
      "│   ├── frameworks_and_tools.md\n",
      "│   ├── hyper_parameter_optimization.md\n",
      "│   ├── implementations.md\n",
      "│   ├── index.md\n",
      "│   ├── interpreters.md\n",
      "│   ├── marking_and_detecting.md\n",
      "│   ├── memory.md\n",
      "│   ├── models.md\n",
      "│   ├── observability.md\n",
      "│   ├── regulation.md\n",
      "│   └── web_plugins.md\n",
      "├── index.md\n",
      "├── Managing/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   └── managing.md\n",
      "├── stylesheets/\n",
      "│   └── extra.css\n",
      "└── Understanding/\n",
      "    ├── .pages\n",
      "    ├── agents/\n",
      "    │   ├── .pages\n",
      "    │   ├── actions_and_tools.md\n",
      "    │   ├── agents.md\n",
      "    │   ├── chains.md\n",
      "    │   ├── environments.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── examples.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── index.md\n",
      "    │   ├── memory.md\n",
      "    │   └── systems.md\n",
      "    ├── data/\n",
      "    │   ├── .pages\n",
      "    │   ├── augmentation.md\n",
      "    │   ├── data.md\n",
      "    │   ├── embedding.md\n",
      "    │   ├── privacy.md\n",
      "    │   ├── selection.md\n",
      "    │   ├── sources.md\n",
      "    │   └── tokenizing.md\n",
      "    ├── ethical_concerns/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── index.md\n",
      "    ├── models/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── classes/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── diffusers.md\n",
      "    │   │   ├── gans.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── RL.md\n",
      "    │   │   └── transformers.md\n",
      "    │   ├── distillation.md\n",
      "    │   ├── evaluation.md\n",
      "    │   ├── index.md\n",
      "    │   ├── models.md\n",
      "    │   ├── prompt_engineering/\n",
      "    │   │   ├── prompt_injections.md\n",
      "    │   │   └── prompting.md\n",
      "    │   ├── rag.md\n",
      "    │   ├── reinforcement_feedback.md\n",
      "    │   └── training.md\n",
      "    ├── overview/\n",
      "    │   ├── .pages\n",
      "    │   ├── ai_in_general.md\n",
      "    │   ├── applications.md\n",
      "    │   ├── challenges.md\n",
      "    │   ├── extra_resources.md\n",
      "    │   └── index.md\n",
      "    └── studies/\n",
      "        └── studies.md\n",
      " \n",
      "\n",
      "    Please write a thorough response that is pedagogical yes succinct. Use a heading/subheading structure that follows the general pattern : \n",
      "    <<intro>>\n",
      "    ## <<First topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## <<Second topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## ...\n",
      "    ## Essential References\n",
      "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
      "    \n",
      "\n",
      "Please present html links without changing the link's text. \n",
      "After the markdown When the text is presented (after >>>), please improve upon it. If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
      "Please preserve any urls or relative links without changing them. \n",
      "Please be sure to use `#` appropriately to reference sections and subsections.\n",
      "Please use balanced use of italics, boldface, and occasional use of emojis to help increase readability. \n",
      "Please write this to be read by someone with a college degree, but minimize unnecessarily large words. \n",
      "Please preserve any admonitions like ??? and !!! and consider introducing others for similar references.\n",
      "Please use bullets, lists, and other information as well as any admonitions  from markdown to make it easy to read and understand.\n",
      "Markdown Response: \n",
      ">>>\n",
      "\n",
      "The deployment of models enables people to use them for their intended purpose. There are many component touchpoints along the way, and more so for customers that have higher requirements.\n",
      "\n",
      "Models of the desired specs must be stored in a file and then loaded for serving. Serving as user inputs that are routed to the served model, optionally batched to improve average request latency, and outputs returned routed appropriately to users. \n",
      "\n",
      "Breaking this down, you will need to determine, at least as a first guess, at the following:\n",
      "\n",
      "1. Customer needs\n",
      "2. Servable model to appropriately service customer and environmental requirements.\n",
      "3. Compute needed to enable service\n",
      "4. Budget needed to fund compute\n",
      "5. Compute Host that will work with the budget\n",
      "6. Visualization needs of the customer\n",
      "7. GUI Framework, and visualization service\n",
      "\n",
      "Keep in mind the needs will change as the understanding of all of the answers above shifts. Still, it is important to get _something_ that you can iterate from, particularly if your solution involves a data flywheel (which it should!).\n",
      "\n",
      "##  Customer needs\n",
      "< common customer needs and themes>\n",
      "<other important topics and sub-bullets\n",
      "\n",
      "## Servable model\n",
      "< common requirements for servable models>\n",
      "<other important topics and sub-bullets\n",
      "## ...\n",
      "\n",
      "![image](https://github.com/ianderrington/genai/assets/76016868/9b379996-e311-4b9b-a35e-9020702fa050)\n",
      "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html\n",
      "\n",
      "## Back-End (Model serving)\n",
      "\n",
      "\n",
      "### Hosting services\n",
      "- [GCP Tutorial](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "- \n",
      "??? tip \"[Text Generation Inference](https://github.com/Preemo-Inc/text-generation-inference) an open-sourced implementation forked from HF\"\n",
      "    \"A Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.\"    \n",
      "    ![image](https://github.com/ianderrington/genai/assets/76016868/a3f5ddbf-a2e3-45ae-bca4-200c07c9dd91)\n",
      "\n",
      "!!! tip \"[Lit-Gpt](https://github.com/Lightning-AI/lit-gpt#setup) Hackable implementation of state-of-the-art open-source large language models released under the Apache 2.0 license.\"\n",
      "\n",
      "!!! tip \"[Azure-Chat-GPT](https://github.com/davidxw/azurechatgpt) to run GPT on Azure services\"\n",
      "\n",
      "!!! code \"[Torch Serve](https://pytorch.org/serve/large_model_inference.html) enable efficient serving.\n",
      "\n",
      "??? tip \"[Triton Inference Server](https://github.com/triton-inference-server/server) Part of NVIDIA AI Inference\" \n",
      "    [Tutorial](https://github.com/triton-inference-server/server)\n",
      "\n",
      "### Tutorials\n",
      "- [](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "\n",
      "## Front-End Interfaces\n",
      "People have to access it to be useful\n",
      "\n",
      "- [GPT Graph](https://github.com/m-elbably/gpt-graph) Allows for a graphical network representation of chat interactions.\n",
      "\n",
      "### Open source methods\n",
      "\n",
      "- [Streamlit](https://blog.streamlit.io/langchain-streamlit/)\n",
      "- [DemoGPT](https://github.com/melih-unsal/DemoGPT) Connects Langchain and streamlit to create dynamic apps that can be repeatedly used for interacting with Chat- GPTs. \n",
      "\n",
      "\n",
      "\n",
      "## Overview Lit\n",
      "\n",
      "- [Neptune-nlp-models-infrastructure](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization#:~:text=Use%20a%20lightweight%20deployment%20framework,serve%20predictions%20over%20a%20network.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Deployment of Generative AI Models\n",
      "\n",
      "The deployment process of Generative AI models is critical to ensuring the models can be used effectively. This involves numerous steps and crucial decisions, ranging from recognizing customer needs to choosing an appropriate host for computation that fits within the budget. The process is typically dynamic and iterative, often pivoting with changes in understanding and evolving requirements.\n",
      "\n",
      "## Understanding the Customer needs\n",
      "Understanding what your customer needs is the foundation of a successful AI deployment. Taking into account the customer's requirements, environmental constraints, intended use cases, and budgetary limitations will help shape the entire deployment process. \n",
      "\n",
      "### Common themes\n",
      "Some common themes in customer needs include:\n",
      "- Accuracy and efficiency of the model\n",
      "- Speed of response\n",
      "- Scalability\n",
      "- Safety and reliability\n",
      "- Data privacy and regulatory compliance\n",
      "\n",
      "\n",
      "## Creating a Serveable Model\n",
      "Ideally, the model should be tailored to meet the customer's needs and environmental requirements. The chosen model should be accurate, fast, scalable, reliable, and compliant with relevant regulations.\n",
      "\n",
      "### Key Considerations\n",
      "In deploying a servable model, consider the following:\n",
      "- Model architecture and complexity\n",
      "- Model size and efficiency\n",
      "- Implementation language and libraries\n",
      "- Hardware requirements\n",
      "\n",
      "![Deploying AI Models](https://github.com/ianderrington/genai/assets/76016868/9b379996-e311-4b9b-a35e-9020702fa050)\n",
      "\n",
      "\n",
      "## Back-End (Model Serving)\n",
      "Model hosting and serving infrastructure come into play after the model has been created. Choosing the appropriate hosting service affects the performance, scalability, and cost of maintaining the model.\n",
      "\n",
      "### Hosting Services\n",
      "Numerous services offer model hosting capabilities. Here are a few popular choices:\n",
      "- Check out this detailed tutorial about deploying large AI models in the Google Cloud Platform: [GCP Tutorial](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "\n",
      "**Noteworthy Tools** \n",
      "\n",
      "??? tip \"[Text Generation Inference](https://github.com/Preemo-Inc/text-generation-inference)\"\n",
      "    \"An open-source implementation powered by Rust, Python, and gRPC for text generation inference, used in production at HuggingFace.\"\n",
      "\n",
      "!!! tip \"[Lit-Gpt](https://github.com/Lightning-AI/lit-gpt#setup)\"\n",
      "    \"A hackable implementation of cutting-edge, large language models released under the Apache 2.0 license.\"\n",
      "\n",
      "!!! tip \"[Azure-Chat-GPT](https://github.com/davidxw/azurechatgpt)\"\n",
      "    \"A straightforward guide to deploying GPT models on Azure services.\"\n",
      "\n",
      "!!! code \"[Torch Serve](https://pytorch.org/serve/large_model_inference.html)\"\n",
      "    \"A service offered by PyTorch to enable efficient serving of AI models.\"\n",
      "\n",
      "??? tip \"[Triton Inference Server](https://github.com/triton-inference-server/server)\"\n",
      "    \"Part of NVIDIA's AI Suite, this server handles inference for AI models.\" \n",
      "\n",
      "### Tutorials\n",
      "For detailed tutorials, you can visit:\n",
      "- [Deploying Large AI Models](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "\n",
      "## Front-End Interfaces\n",
      "To make the AI model truly useful, it should be accessible. The design of the front-end interfaces is crucial for user experience and adoption.\n",
      "\n",
      "### Open Source Methods\n",
      "Open-source methods allow for more customizable and flexible deployment of the front-end interface for AI models. Some notable options include:\n",
      "- [Streamlit](https://blog.streamlit.io/langchain-streamlit/)\n",
      "- [DemoGPT](https://github.com/melih-unsal/DemoGPT) which connects Langchain and streamlit to create dynamic apps for interacting with Chat- GPTs.\n",
      "\n",
      "### GPT Graph\n",
      "[GPT-Graph](https://github.com/m-elbably/gpt-graph) offers an innovative approach to chat interaction with a graphical network representation. \n",
      "\n",
      "## Essential References\n",
      "For a more comprehensive understanding of AI model deployment, refer to the following resources:\n",
      "- [Neptune-nlp-models-infrastructure](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization#:~:text=Use%20a%20lightweight%20deployment%20framework,serve%20predictions%20over%20a%20network.) provides cost optimization strategies for NLP model deployments.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fb39a-a6a1-4394-94d3-6fa83a54bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "md = markdown.markdown(markdown_text, extensions=['toc'])\n",
    "# from markdown.extensions.toc import TocExtension\n",
    "# html = markdown.markdown(markdown_text, extensions=[TocExtension(baselevel=1)])\n",
    "# print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32251b-2f60-41a2-bcc4-ee275b4ebd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_toc(md_text, level_key=\"l\", text_key='t'):\n",
    "    # Regex to match markdown headings, capturing the level based on hash count and the heading text\n",
    "    pattern = re.compile(r'^(?P<hashes>#+) (?P<text>.+)$', re.MULTILINE)\n",
    "    matches = pattern.findall(md_text.strip())\n",
    "\n",
    "    toc_structure = [{level_key: len(hashes), text_key: text} for hashes, text in matches]\n",
    "    return toc_structure\n",
    "\n",
    "def serialize_toc(toc_structure):\n",
    "    return json.dumps(toc_structure, indent=4)\n",
    "\n",
    "def test_extract_toc():\n",
    "    md_text = \"\"\"\n",
    "# Title\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Content here\n",
    "\n",
    "### Subsection 1.1\n",
    "\n",
    "More content here\n",
    "\n",
    "## Section 2\n",
    "\n",
    "### Subsection 2.1\n",
    "\n",
    "Yet more content\n",
    "    \"\"\"\n",
    "\n",
    "    toc_structure = extract_toc(md_text)\n",
    "    print(toc_structure)\n",
    "    serialized_toc = serialize_toc(toc_structure)\n",
    "\n",
    "    expected_output = \"\"\"\n",
    "[\n",
    "    {\n",
    "        \"l\": 1,\n",
    "        \"t\": \"Title\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 1.1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 2\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 2.1\"\n",
    "    }\n",
    "]\n",
    "    \"\"\"\n",
    "\n",
    "    assert serialized_toc.strip() == expected_output.strip(), f\"Expected:\\n{expected_output}\\nGot:\\n{serialized_toc}\"\n",
    "\n",
    "test_extract_toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbaf58-392b-494b-9e87-218a439bb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a28916-5f7d-4334-a77d-a629b42c0a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
