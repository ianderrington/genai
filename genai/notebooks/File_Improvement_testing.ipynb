{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain/llms/openai.py:202: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain/llms/openai.py:790: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-32k',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── index.md\n",
      "├── Managen.ai/\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   ├── index.md\n",
      "│   ├── managing.md\n",
      "│   └── requirements.md\n",
      "├── Understanding/\n",
      "│   ├── agents/\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── agents.md\n",
      "│   │   ├── chain_optimization.md\n",
      "│   │   ├── chains.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── evaluation.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── interpreters.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── alignment.md\n",
      "│   │   ├── embedding.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── finetuning.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── components.md\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusers.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   └── transformers.md\n",
      "│   │   ├── optimization.md\n",
      "│   │   ├── optimizing_hyper_parameters.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   ├── recurrent_training.md\n",
      "│   │   ├── reinforcement_feedback.md\n",
      "│   │   └── training.md\n",
      "│   ├── background/\n",
      "│   │   └── tensor_maths.md\n",
      "│   ├── data/\n",
      "│   │   ├── augmentation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── selection.md\n",
      "│   │   ├── simulation.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── examples/\n",
      "│   │   ├── by_domain.md\n",
      "│   │   └── index.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── ai_in_general.md\n",
      "│   │   ├── applications.md\n",
      "│   │   ├── business.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── foundation_models.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── knowledge_graphs.md\n",
      "│   │   └── open_source.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── index.md\n",
      "│   │   └── prompt_injections.md\n",
      "│   └── studies/\n",
      "│       └── studies.md\n",
      "└── Using/\n",
      "    ├── by_application.md\n",
      "    ├── commercial_products.md\n",
      "    ├── deploying/\n",
      "    │   ├── back_end.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── front_end.md\n",
      "    │   ├── index.md\n",
      "    │   └── libraries_and_tools.md\n",
      "    ├── ethically/\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── governing.md\n",
      "    ├── index.md\n",
      "    ├── marking_and_detecting.md\n",
      "    ├── ml_ops.md\n",
      "    ├── observability.md\n",
      "    ├── regulation.md\n",
      "    ├── responsibly/\n",
      "    │   └── index.md\n",
      "    ├── using_external_vendors.md\n",
      "    └── web_plugins.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return  not ( 'Icon' in path.name or '.DS_Store' in path.name or 'stylesheets'  in path.name or \\\n",
    "        'CNAME' in path.name or 'assets' in path.name or '.svg' in path.name or '.pages' in path.name)\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern():\n",
    "    pattern = \\\n",
    "    \"\"\"\n",
    "    <<intro>>\n",
    "    ## <<First topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## <<Second topic>>\n",
    "    ### <<topic sub component>>\n",
    "    ### ...\n",
    "    ## ...\n",
    "    ## Essential References\n",
    "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
    "    \"\"\"\n",
    "    return pattern\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "                \"Generative AI and how to improve upon it. \"\n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "                         \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \\\n",
    "\"\"\" You are working on a project called: {project_name}\\n\n",
    "You are part of a team working to: {project_goals}\\n\n",
    "You are helping to: {present_task_description}\\n\n",
    "You are helping to rewrite and expand a file called {file_name} in the present tree-structure:\\n {tree_structure}\\n \n",
    "Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "Please present html links without changing the link's text. \n",
    "After the markdown When the text is presented (after >>>), please improve upon it. If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
    "Please preserve any urls or relative links without changing them. \n",
    "Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "Please be sure to keep any amonitions like `!!! tip` and use others when appropriate. \n",
    "Markdown Response: \n",
    ">>>\\n\n",
    "{markdown_text}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"markdown_text\", \"structure_pattern\"],\n",
    "    template=template\n",
    ")\n",
    "file_from_base_dir = 'Using/deploying/index.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt=prompt_template.format(project_name=project_name,\n",
    "                       project_goals=project_goals,\n",
    "                       present_task_description=present_task_description,\n",
    "                       file_name=file_name,\n",
    "                       tree_structure=tree_structure,\n",
    "                        structure_pattern=structure_pattern,\n",
    "                       markdown_text=markdown_text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b982b3e-6563-4e18-8d44-7ba14122fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# project_name = \"Managing Generative AI\"\n",
    "# project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "#                 \"Generative AI and how to improve upon it. \"\n",
    "# present_task_description=\"Improve the markdown based on best understandings.\"\\\n",
    "#                          \"Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\"\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "\n",
    "# template = \\\n",
    "# \"\"\" You are working on a project called: {project_name}\\n\n",
    "# You are part of a team working to: {project_goals}\\n\n",
    "# You are helping to: {present_task_description}\\n\n",
    "# You are helping to rewrite and expand a file called {file_name}\\n\n",
    "# Primarily you are tasked with adding admonitions to the markdown document to make it more nice to read. \n",
    "# for instance, you will see formats like this:\n",
    "# '''\n",
    "# - [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf) A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents.   In it they discuss several challenges and solutions:\n",
    "\n",
    "#     **Remembering**\n",
    "    \n",
    "#     _Observation Memory_ A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping.\n",
    "#     Uses, _recency_, _importance_ and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. \n",
    "# '''\n",
    "# This needs to be reformatted in the following manner:\n",
    "# '''\n",
    "# <div class=\"result\" markdown>\n",
    "# !!! tip \"[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf)\"\n",
    "#     A simulation of different agents of different personalities with a time-evolving environment that could be manipulated by the agents.   In it they discuss several challenges and solutions:\n",
    "\n",
    "# ??? example \n",
    "#      **Remembering**\n",
    "    \n",
    "#     _Observation Memory_ A memory stream maintaining a record of experience: memory objects with a description in natural language, and timestamping.\n",
    "#     Uses, _recency_, _importance_ and relevance_ to add weight to information that is more recent, how the memory is compared in relation to other memories, and how the information pertains to the present situation. \n",
    "# </div> \n",
    "\n",
    "# You should make this modification for EVERY link that is presented that doesn't have admonitions already there. \n",
    "# After the markdown When the text is presented, please improve upon it. If no text is present create a reasonable outline following the pattern above and fill it in.\n",
    "# Please preserve any urls or relative links without changing them. \n",
    "# Please be sure to use `#` appropriately to reference sections and subsections.\n",
    "# Please be sure to use appropriate spacing to make admonitions work.\n",
    "\n",
    "# Here is the markdown text:\n",
    "# {markdown_text}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"markdown_text\"],\n",
    "#     template=template\n",
    "# )\n",
    "# file_from_base_dir = 'Understanding/agents/systems.md'\n",
    "# file_name=get_file_name(file_from_base_dir)\n",
    "\n",
    "# markdown_text=get_markdown_text(file_name)\n",
    "# prompt=prompt_template.format(project_name=project_name,\n",
    "#                        project_goals=project_goals,\n",
    "#                        present_task_description=present_task_description,\n",
    "#                        file_name=file_name,\n",
    "#                        markdown_text=markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are working on a project called: Managing Generative AI\n",
      "\n",
      "You are part of a team working to: Overall: Create an automated repository that is able to explain in plain-English and in code, Generative AI and how to improve upon it. \n",
      "\n",
      "You are helping to: Improve the markdown based on best understandings.Be as honest and as accurate as possible. Be succinct in your responses. Preserve any URLS.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Using/deploying/index.md in the present tree-structure:\n",
      " docs/\n",
      "├── index.md\n",
      "├── Managen.ai/\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── contributing.md\n",
      "│   ├── index.md\n",
      "│   ├── managing.md\n",
      "│   └── requirements.md\n",
      "├── Understanding/\n",
      "│   ├── agents/\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── agents.md\n",
      "│   │   ├── chain_optimization.md\n",
      "│   │   ├── chains.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── evaluation.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── interpreters.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── alignment.md\n",
      "│   │   ├── embedding.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── finetuning.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── components.md\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusers.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   └── transformers.md\n",
      "│   │   ├── optimization.md\n",
      "│   │   ├── optimizing_hyper_parameters.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   ├── recurrent_training.md\n",
      "│   │   ├── reinforcement_feedback.md\n",
      "│   │   └── training.md\n",
      "│   ├── background/\n",
      "│   │   └── tensor_maths.md\n",
      "│   ├── data/\n",
      "│   │   ├── augmentation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── selection.md\n",
      "│   │   ├── simulation.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── examples/\n",
      "│   │   ├── by_domain.md\n",
      "│   │   └── index.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── ai_in_general.md\n",
      "│   │   ├── applications.md\n",
      "│   │   ├── business.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── foundation_models.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── knowledge_graphs.md\n",
      "│   │   └── open_source.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── index.md\n",
      "│   │   └── prompt_injections.md\n",
      "│   └── studies/\n",
      "│       └── studies.md\n",
      "└── Using/\n",
      "    ├── by_application.md\n",
      "    ├── commercial_products.md\n",
      "    ├── deploying/\n",
      "    │   ├── back_end.md\n",
      "    │   ├── computation.md\n",
      "    │   ├── frameworks.md\n",
      "    │   ├── front_end.md\n",
      "    │   ├── index.md\n",
      "    │   └── libraries_and_tools.md\n",
      "    ├── ethically/\n",
      "    │   ├── alignment_and_exential_concerns.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── governing.md\n",
      "    ├── index.md\n",
      "    ├── marking_and_detecting.md\n",
      "    ├── ml_ops.md\n",
      "    ├── observability.md\n",
      "    ├── regulation.md\n",
      "    ├── responsibly/\n",
      "    │   └── index.md\n",
      "    ├── using_external_vendors.md\n",
      "    └── web_plugins.md\n",
      " \n",
      "Please use a heading/subheading structure that follows the general pattern : \n",
      "    <<intro>>\n",
      "    ## <<First topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## <<Second topic>>\n",
      "    ### <<topic sub component>>\n",
      "    ### ...\n",
      "    ## ...\n",
      "    ## Essential References\n",
      "    << List with '-' of references with each reference providing written as [link_title](link_address) and a thoughtful but succinct output>>\n",
      "    \n",
      "\n",
      "Please present html links without changing the link's text. \n",
      "After the markdown When the text is presented (after >>>), please improve upon it. If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
      "Please preserve any urls or relative links without changing them. \n",
      "Please be sure to use `#` appropriately to reference sections and subsections.\n",
      "Please be sure to keep any amonitions like `!!! tip` and use others when appropriate. \n",
      "Specific request: Please combine the number-list and the ### headings into an effective relay to the different documents.\n",
      "Markdown Response: \n",
      ">>>\n",
      "\n",
      "The deployment of models enables callers, people, or other software, to use them. While deployment may initially consist of only 'making a model available for calling'. \n",
      "\n",
      "Because the model may be one limiting- consider the deployment of the model to be separate from the deployment of the model's encapsulating project, though they are directly connected.  \n",
      "\n",
      "There are many component touchpoints along the way, and more so for customers that have higher requirements.\n",
      "\n",
      "Quickly, models of the desired specs must be stored in a file and then loaded for serving. Serving as user inputs that are routed to the served model, optionally batched to improve average request latency, and outputs returned routed appropriately to users. \n",
      "\n",
      "As would be done for other AI-enabled products, you will need to have in mind the following\n",
      "\n",
      "1. [Caller needs](#caller-needs) (customer requirements)\n",
      "2. [Servable model](#servable-model) to appropriately service customer and environmental requirements.\n",
      "3. [Compute needed](#compute-needs) to enable service\n",
      "4. [Budget available](#budget-available) the compute\n",
      "5. [Compute back end](#compute-back-end) service and framework that will work with the budget you have\n",
      "7. [Front End](./front_end.md) that provides the appropriate visualization\n",
      "\n",
      "Keep in mind the needs will change as the understanding of all of the answers above shifts. Still, it is important to get _something_ that you can iterate from, particularly if your solution involves some form of a [data flywheel](https://brightdata.com/blog/brightdata-in-practice/using-data-flywheel-to-scale-your-business).\n",
      "\n",
      "###  Caller needs\n",
      "\n",
      "What the caller requires will depend on the target audience your offering is provided. Focusing on narrower audiences allow you to have fewer (initial) requirements and may enable MVP generation quickly. These audiences can expand or shift as needed. Often needs will require 'rapid' results that are 'good'. \n",
      "\n",
      "### Servable model\n",
      "\n",
      "The models must be sufficient to provide the content that the model have a sufficiently reasonable latency that it can enable the throughput requirements of your model. \n",
      "\n",
      "To enable a properly servable model, it may likely be required to [optimize](../../Understanding/architectures/optimization.md) the serving of your models.\n",
      "\n",
      "### Compute needs\n",
      "\n",
      "Here are some general considerations (from AWS) regarding how to consider the requirements of model deployment.\n",
      "\n",
      "![[image](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)](https://github.com/ianderrington/genai/assets/76016868/9b379996-e311-4b9b-a35e-9020702fa050)\n",
      "    \n",
      "\n",
      "### Budget available\n",
      "\n",
      "Your calculated budget will be useful to consider the monetization strategy of your tool. While highly dependant on your business model, knowing when to inspire greater [model serving optimization](../../Understanding/architectures/optimization.md) to prevent 'too much compute'. \n",
      "\n",
      "### [Compute back end](back_end.md)\n",
      "\n",
      "To determining your back-end will involve selecting from both DIY and full-service [frameworks](./frameworks.md) that you use on some compute host solution and perhaps connected with other [tools and libraries](libraries_and_tools.md) that can help your solution. \n",
      "\n",
      "\n",
      "### [Front end](front_end.md)\n",
      "\n",
      "At the end of a model that is ready to be deployed, you'll need to get the results to the end-user in a useful manner. Look into the discussion on [front ends](./front_end.md) for some quality solutions and best practices to for your model output.\n",
      "\n",
      "## Tips \n",
      "\n",
      "??? tip \"[State of GPT by Andrej Karpathy](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2) A stellar presentation to update on the general state of Genai enabled by GPT\"\n",
      "\n",
      "    <img width=\"925\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/de2d3b33-9e79-407d-b3c7-5b795f330722\" loading=\"lazy\">\n",
      "    <img width=\"918\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/0ecb56de-966a-40c5-8d14-1df3b4a5a89f\">\n",
      "    <img width=\"282\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/7cea8be4-26dd-46c3-9001-fcf625e5975d\">\n",
      "    <img width=\"918\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/a32295bd-9d88-4b31-bd10-134e11e6c546\">\n",
      "    <img width=\"886\" alt=\"image\" src=\"https://github.com/ianderrington/general/assets/76016868/7b1c6c4b-3778-4536-8d10-03696f3624c5\">\n",
      "\n",
      "## References\n",
      "\n",
      "??? tip \"[Emerging Architectures for LLM Applications](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) A very nice discussion of the components and their interactions via orchestration systems.\"\n",
      "\n",
      "    ![image](https://github.com/ianderrington/genai/assets/76016868/f287eaef-6b86-4846-8885-2b3ad3cd614b) [^n1]\n",
      "\n",
      "??? tip \"[Challenges and Applications of Large Language Models Kaddour et al](https://arxiv.org/abs/2307.10169) Well done and thorough.\"\n",
      "\n",
      "## Overview Literature\n",
      "\n",
      "Below are some overviews to help with practical aspects of Generative AI, particularly GPT and LLMs.\n",
      "\n",
      "- [Neptune-nlp-models-infrastructure](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization#:~:text=Use%20a%20lightweight%20deployment%20framework,serve%20predictions%20over%20a%20network.)\n",
      "\n",
      "- [How to Deploy Large Size Deep Learning Models Into Production](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Deploying Generative AI Models\n",
      "\n",
      "Deploying generative AI models involves several important considerations. This may initially seem as simple as 'making the model available for use', but it is significantly more complex in reality as models, and their requirements can differ greatly. \n",
      "\n",
      "This document will guide you through key considerations when deploying your generative AI model. These include understanding your caller's needs, selecting a servable model, detailing computational needs, available budget, computational backend, and finally the front-end visualisation. \n",
      "\n",
      "## Understanding Caller Needs\n",
      "\n",
      "Caller needs and requirements depend on your target audience. Initially, catering to a narrower audience will simplify your model requirements. Throughout this process, you will often encounter needs for 'rapid' and 'good' results. As your audience base expands or changes, so will these needs and requirements. \n",
      "\n",
      "## Selecting a Servable Model\n",
      "\n",
      "Generative AI models should be capable of providing content at a latency that enables the throughput requirements of the specific use case. \n",
      "\n",
      "Servable models require optimization to properly serve your customers. You can refer to our document on [Optimization](../../Understanding/architectures/optimization.md) for further details.\n",
      "\n",
      "## Compute Needs and Budget\n",
      "\n",
      "Compute needs are specific to the model requirements. It is important to map out the computational resources required for deployment. Below is a general guideline provided by AWS on model deployment considerations.\n",
      "\n",
      "![Compute Frame](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)\n",
      "\n",
      "It is equally important to consider your budget. Understanding your monetization strategy and business model can help you get more clarity on this aspect.\n",
      "\n",
      "[Model serving optimization](../../Understanding/architectures/optimization.md) can ensure you have sufficient computational power while adhering to your budget constraints.\n",
      "\n",
      "## Back-end and Front-end Selection\n",
      "\n",
      "Back-end selection involves choosing from DIY and full-service [frameworks](./frameworks.md). Depending on your budget, you may need to choose the right compute host solution along with other [tools and libraries](libraries_and_tools.md) that complement your model deployment.\n",
      "\n",
      "Lastly, the user interface or front-end is crucial to deliver model results to the user in a meaningful way. Take a look at our document on [front ends](./front_end.md) for optimal solutions and approaches to visualizing your model output.\n",
      "\n",
      "## References and Much More\n",
      "\n",
      "Most importantly, always keep researching and referencing to continuously improve and iterate on your deployment strategies.\n",
      "\n",
      "- [State of GPT by Andrej Karpathy](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2) \n",
      "\n",
      "- [Emerging Architectures for LLM Applications](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/)\n",
      "\n",
      "- [Challenges and Applications of Large Language Models by Kaddour et al](https://arxiv.org/abs/2307.10169)\n",
      "\n",
      "- [Neptune-nlp-models-infrastructure](https://neptune.ai/blog/nlp-models-infrastructure-cost-optimization#:~:text=Use%20a%20lightweight%20deployment%20framework,serve%20predictions%20over%20a%20network.)\n",
      "\n",
      "- [How to Deploy Large Size Deep Learning Models Into Production](https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33)\n",
      "\n",
      "This is, by no means, an exhaustive list, but these resources provide a good starting point for deploying Generative AI models.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "711fb39a-a6a1-4394-94d3-6fa83a54bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "md = markdown.markdown(markdown_text, extensions=['toc'])\n",
    "# from markdown.extensions.toc import TocExtension\n",
    "# html = markdown.markdown(markdown_text, extensions=[TocExtension(baselevel=1)])\n",
    "# print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b32251b-2f60-41a2-bcc4-ee275b4ebd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'l': 1, 't': 'Title'}, {'l': 2, 't': 'Section 1'}, {'l': 3, 't': 'Subsection 1.1'}, {'l': 2, 't': 'Section 2'}, {'l': 3, 't': 'Subsection 2.1'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_toc(md_text, level_key=\"l\", text_key='t'):\n",
    "    # Regex to match markdown headings, capturing the level based on hash count and the heading text\n",
    "    pattern = re.compile(r'^(?P<hashes>#+) (?P<text>.+)$', re.MULTILINE)\n",
    "    matches = pattern.findall(md_text.strip())\n",
    "\n",
    "    toc_structure = [{level_key: len(hashes), text_key: text} for hashes, text in matches]\n",
    "    return toc_structure\n",
    "\n",
    "def serialize_toc(toc_structure):\n",
    "    return json.dumps(toc_structure, indent=4)\n",
    "\n",
    "def test_extract_toc():\n",
    "    md_text = \"\"\"\n",
    "# Title\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Content here\n",
    "\n",
    "### Subsection 1.1\n",
    "\n",
    "More content here\n",
    "\n",
    "## Section 2\n",
    "\n",
    "### Subsection 2.1\n",
    "\n",
    "Yet more content\n",
    "    \"\"\"\n",
    "\n",
    "    toc_structure = extract_toc(md_text)\n",
    "    print(toc_structure)\n",
    "    serialized_toc = serialize_toc(toc_structure)\n",
    "\n",
    "    expected_output = \"\"\"\n",
    "[\n",
    "    {\n",
    "        \"l\": 1,\n",
    "        \"t\": \"Title\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 1.1\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 2,\n",
    "        \"t\": \"Section 2\"\n",
    "    },\n",
    "    {\n",
    "        \"l\": 3,\n",
    "        \"t\": \"Subsection 2.1\"\n",
    "    }\n",
    "]\n",
    "    \"\"\"\n",
    "\n",
    "    assert serialized_toc.strip() == expected_output.strip(), f\"Expected:\\n{expected_output}\\nGot:\\n{serialized_toc}\"\n",
    "\n",
    "test_extract_toc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacbaf58-392b-494b-9e87-218a439bb6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a28916-5f7d-4334-a77d-a629b42c0a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
