{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianderrington/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_community/llms/openai.py:253: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ianderrington/miniconda3/envs/langchain_streamlit/lib/python3.10/site-packages/langchain_community/llms/openai.py:1076: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4', 'gpt-4o', 'gpt-4-1106-preview',\"text-davinci-003\"]\n",
    "model = models[2]\n",
    "\n",
    "openai = OpenAI.chat\n",
    "(\n",
    "    model_name=model,\n",
    "    # openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── .pages\n",
      "├── blog/\n",
      "│   ├── .authors.yml\n",
      "│   ├── index.md\n",
      "│   └── posts/\n",
      "│       ├── .pages\n",
      "│       └── Launch.md\n",
      "├── index.md\n",
      "├── javascripts/\n",
      "│   ├── chatbase.js\n",
      "│   ├── copy-link.js\n",
      "│   └── mathjax.js\n",
      "├── Managenai/\n",
      "│   ├── .pages\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── code_of_conduct.md\n",
      "│   ├── contributing.md\n",
      "│   ├── explorations_blog.md\n",
      "│   ├── index.md\n",
      "│   ├── project_requirements.md\n",
      "│   ├── site_graph.md\n",
      "│   └── strategy.md\n",
      "├── shared/\n",
      "├── Understanding/\n",
      "│   ├── .pages\n",
      "│   ├── agents/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── applications.md\n",
      "│   │   ├── cognitive_architecture.md\n",
      "│   │   ├── commercial.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── embedding.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── frameworks.md\n",
      "│   │   ├── generation.md\n",
      "│   │   ├── gpt.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusion_models.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── mixture_of_experts.md\n",
      "│   │   │   ├── multimodal.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   ├── transformers.md\n",
      "│   │   │   └── vision_language_transformers.md\n",
      "│   │   ├── optimization/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   └── index.md\n",
      "│   │   └── training/\n",
      "│   │       ├── .pages\n",
      "│   │       ├── distributed.md\n",
      "│   │       ├── feedback.md\n",
      "│   │       ├── finetuning.md\n",
      "│   │       ├── index.md\n",
      "│   │       ├── pre-training.md\n",
      "│   │       └── recurrent.md\n",
      "│   ├── building/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── agents.md\n",
      "│   │   ├── back_end.md\n",
      "│   │   ├── commercial_products.md\n",
      "│   │   ├── compliance.md\n",
      "│   │   ├── data.md\n",
      "│   │   ├── examples_and_tutorials.md\n",
      "│   │   ├── front_end.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── libraries_and_tools.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── model_serving.md\n",
      "│   │   ├── orchestrating.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   └── training_and_finetuning.md\n",
      "│   ├── data/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── distillation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── preparation/\n",
      "│   │   │   ├── .pages\n",
      "│   │   │   ├── augmentation.md\n",
      "│   │   │   ├── collection.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   └── selection.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── .pages\n",
      "│   │   ├── ai_and_ml_basics/\n",
      "│   │   │   └── index.md\n",
      "│   │   ├── background/\n",
      "│   │   │   └── tensor_maths.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── chronology.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── open_source.md\n",
      "│   │   ├── philosophies.md\n",
      "│   │   └── use_cases.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── examples/\n",
      "│   │   │   ├── coding/\n",
      "│   │   │   │   └── Claude_2024-07-20.md\n",
      "│   │   │   └── leaked/\n",
      "│   │   │       └── Claude_2024-07-11.md\n",
      "│   │   ├── hacking.md\n",
      "│   │   ├── index.md\n",
      "│   │   └── tools_and_libraries.md\n",
      "│   └── studies/\n",
      "│       ├── behavior.md\n",
      "│       └── studies.md\n",
      "└── Using/\n",
      "    ├── .pages\n",
      "    ├── building_or_buying.md\n",
      "    ├── business.md\n",
      "    ├── commercial_markets.md\n",
      "    ├── de-risking/\n",
      "    │   ├── alignment.md\n",
      "    │   ├── explainability.md\n",
      "    │   ├── index.md\n",
      "    │   ├── red_teaming.md\n",
      "    │   └── security.md\n",
      "    ├── ethically/\n",
      "    │   ├── .pages\n",
      "    │   ├── alignment.md\n",
      "    │   ├── alignment_and_exestential_concerns.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── examples/\n",
      "    │   ├── by_field/\n",
      "    │   │   ├── .pages\n",
      "    │   │   ├── business.md\n",
      "    │   │   ├── entertainment/\n",
      "    │   │   │   ├── dynamic.md\n",
      "    │   │   │   └── static.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── individuals_and_society/\n",
      "    │   │   │   ├── education.md\n",
      "    │   │   │   ├── law.md\n",
      "    │   │   │   └── socio_societal.md\n",
      "    │   │   ├── mathematics/\n",
      "    │   │   │   └── index.md\n",
      "    │   │   ├── science/\n",
      "    │   │   │   ├── biology/\n",
      "    │   │   │   │   ├── genetics.md\n",
      "    │   │   │   │   ├── index.md\n",
      "    │   │   │   │   └── proteins.md\n",
      "    │   │   │   ├── chemistry.md\n",
      "    │   │   │   ├── healthcare.md\n",
      "    │   │   │   └── index.md\n",
      "    │   │   └── technology/\n",
      "    │   │       ├── chip_design.md\n",
      "    │   │       ├── coding.md\n",
      "    │   │       ├── finance.md\n",
      "    │   │       ├── healthcare.md\n",
      "    │   │       └── robotics.md\n",
      "    │   ├── by_modality/\n",
      "    │   │   ├── charts_and_graphs.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── knowledge_graphs.md\n",
      "    │   │   ├── language.md\n",
      "    │   │   ├── multimodal.md\n",
      "    │   │   ├── sound.md\n",
      "    │   │   ├── static_2d.md\n",
      "    │   │   ├── tabular.md\n",
      "    │   │   ├── text.md\n",
      "    │   │   ├── time_series.md\n",
      "    │   │   └── video.md\n",
      "    │   ├── by_use_case/\n",
      "    │   │   ├── chat.md\n",
      "    │   │   ├── content_framing.md\n",
      "    │   │   ├── data_extraction.md\n",
      "    │   │   ├── forecasting.md\n",
      "    │   │   ├── planning.md\n",
      "    │   │   ├── research.md\n",
      "    │   │   └── web_crawling.md\n",
      "    │   └── index.md\n",
      "    ├── governing.md\n",
      "    ├── index.md\n",
      "    ├── interfacing_layers/\n",
      "    │   ├── integrations.md\n",
      "    │   └── web_plugins.md\n",
      "    ├── managing/\n",
      "    │   ├── governing.md\n",
      "    │   ├── index.md\n",
      "    │   ├── ml_ops.md\n",
      "    │   ├── observability.md\n",
      "    │   └── regulations_and_guidelines.md\n",
      "    ├── marking_and_detecting.md\n",
      "    ├── open_source.md\n",
      "    └── rewrite.md\n"
     ]
    }
   ],
   "source": [
    "from genai.tools.dir_utils import get_tree_structure\n",
    "tree_structure = get_tree_structure(path_base = BASE_DOCS_DIR)\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d13e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern(file_class=None):\n",
    "    if file_class is None:\n",
    "        file_class = 'index.md'\n",
    "    patterns={}\n",
    "    patterns['index.md'] = \\\n",
    "    \"\"\"\n",
    "NOTE: '-' is used to denote a general topic, sentence, or consideration but not considered a 'list' item.\n",
    "## Executive Summary (TL;DR)\n",
    "- Concise summary highlighting the essence of the topic and its significance.\n",
    "- Designed for readability by a non-technical or executive-level audience.\n",
    "- Utilize emojis, images, and visual elements effectively to emphasize key points.\n",
    "- Include Mermaid diagrams where appropriate, or describe necessary images as `IMAGE: <image description>`.\n",
    "\n",
    "## Practical Application and Usage\n",
    "- Focus on providing immediately actionable guidance and high-priority examples.\n",
    "- Extract and condense key usage instructions from earlier content into concise, actionable steps.\n",
    "- Offer 'How-to' guides, quick-start tips, and links for direct application.\n",
    "\n",
    "## Introduction and Relevance\n",
    "- Thorough introduction to the topic, highlighting its relevance and importance.\n",
    "- Discuss core components and their interplay within the broader context of Generative AI.\n",
    "\n",
    "## Core Content and Results\n",
    "- Detailed exploration of specific aspects under clear subheadings.\n",
    "- Provide illustrations or diagrams (Mermaid or `IMAGE:<image description>`) for complex concepts.\n",
    "- For extensive topics, include brief summaries and links to dedicated markdown files. If markdown files are already created, link to them here. If markdown files are needed, suggest them. \n",
    "\n",
    "## Technological Aspects\n",
    "- Explore relevant tools, technologies, and methodologies.\n",
    "- Highlight current trends and future directions in technology related to the topic.\n",
    "\n",
    "## Background or Theoretical Foundation (if necessary)\n",
    "- Delve into historical context and foundational theories.\n",
    "- Clarify essential theoretical concepts and terminologies for comprehensive understanding.\n",
    "\n",
    "## Ethical Considerations and Challenges\n",
    "- Address ethical dilemmas, challenges, and potential risks.\n",
    "- Discuss strategies for ethical practice and risk mitigation.\n",
    "\n",
    "## Extended Examples (if applicable)\n",
    "- Link to practical examples, simulations, or code snippets for hands-on understanding.\n",
    "- Direct readers to external resources, tools, or demonstrations for further exploration.\n",
    "\n",
    "## Advanced Topics and Further Exploration (if applicable)\n",
    "- Present open challenges and future research directions.\n",
    "- Deep dive into complex aspects with links to advanced readings and resources.\n",
    "\n",
    "## FAQs and Common Queries\n",
    "- Tackle frequently asked questions and common queries related to the section.\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "- Recap the main points and emphasize the key messages from the section.\n",
    "\n",
    "## References and Additional Reading\n",
    "- List citations and provide links to source materials and further reading.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # patterns['glossary.md'] = \\\n",
    "    # \"\"\"\n",
    "    # \"\"\"\n",
    "    return patterns[file_class]\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role = \"expert AI technology creator and communicator\"\n",
    "\n",
    "# project_name = \"Managing Generative AI\"\n",
    "# project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "#                 \"Generative AI and how to improve upon it. \"\n",
    "# present_task_description=\"Improve the markdown based on best understandings.\\n\"\\\n",
    "#                          \"Be as honest and as accurate as possible. Be succinct in your responses. \"\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "# # Idea\n",
    "# # Select between prompt patterns\n",
    "# # Chain select them more effectively. \n",
    "# # The present tree-structure:\\n {tree_structure}\\n \n",
    "# # Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "# template = \\\n",
    "# \"\"\" \n",
    "# You are a {role}\n",
    "# You are working on a project called: {project_name}\\n\n",
    "# You are part of a team working to: {project_goals}\\n\n",
    "# You are helping to: {present_task_description}\\n\n",
    "# You are helping to rewrite and expand a file called {file_name} \n",
    "# Here are some things we'd like you to be sure to do:\n",
    "# * Please present ALL html links without changing the link's text. \n",
    "# * Preserve any urls or relative links without changing them. \n",
    "# * Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "# * Please reformat any bulleted lists of links where github links have `!!! abstract`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "\n",
    "# After the markdown When the text is presented (after >>>), please improve upon it. \n",
    "# If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
    "\n",
    "# Markdown Input:\\n\n",
    "# >>>\\n\n",
    "# {markdown_text}\"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate(\n",
    "#     # input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"structure_pattern\", \"markdown_text\", ],\n",
    "#         input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\",  \"markdown_text\", ],\n",
    "#     template=template\n",
    "# )\n",
    "# file_from_base_dir = 'Use/deploying/index.md'\n",
    "# file_from_base_dir = 'Use/redteaming.md'\n",
    "# file_from_base_dir = 'Understand/agents/rag.md'\n",
    "# file_name=get_file_name(file_from_base_dir)\n",
    "# tree_structure=get_tree_structure()\n",
    "# markdown_text=get_markdown_text(file_name)\n",
    "# structure_pattern = get_structure_pattern()\n",
    "# prompt=prompt_template.format(role=role,\n",
    "#                               project_name=project_name,\n",
    "#                        project_goals=project_goals,\n",
    "#                        present_task_description=present_task_description,\n",
    "#                        file_name=file_name,\n",
    "#                        # tree_structure=tree_structure,\n",
    "#                         structure_pattern=structure_pattern,\n",
    "#                        markdown_text=markdown_text,)\n",
    "# # The above is very verboase especially as it requires a lot of repeated typing of the same variables.\n",
    "# # It also needs to work for variables that are only specified in the template. If they are not specified in the template, then they should be ignored.\n",
    "# # There will a list of template lines that are appended to create the final prompt.\n",
    "# # The template lines will be specified as a list of dictionaries.\n",
    "# #  \n",
    "# # Let's write this as a class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  write above but realizing template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0241f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartPromptTemplate:\n",
    "    def __init__(self, template_required, template_optional_dict, template_variable_independent):\n",
    "        self.template_required = template_required if template_required is not None else \"\"\n",
    "        self.template_optional_dict = template_optional_dict if template_optional_dict is not None else {}\n",
    "        self.template_variable_independent = template_variable_independent if template_variable_independent is not None else \"\"\n",
    "    \n",
    "    def get_prompt(self, **kwargs):\n",
    "        template_list = []\n",
    "        for k, v in kwargs.items():\n",
    "            if k in self.template_optional_dict.keys():\n",
    "                template_list.append(self.template_optional_dict[k])\n",
    "        # begin_indicator = \"\\n What would you write given the requests above? \\n>>>\\n\"\n",
    "        # #\\n<<< end input \\n\" + \\\n",
    "        template =   '\\n'.join(template_list) + self.template_required  +  self.template_variable_independent #+ begin_indicator\n",
    "        prompt = template.format(**kwargs)\n",
    "        return prompt\n",
    "\n",
    "template_optional_dict = {\n",
    "\n",
    "    'role': \"You are a {role}\",\n",
    "    # 'project_name': \"You are working on a project called: {project_name}\\n\",\n",
    "    # 'project_goals': \"You are part of a team working to: {project_goals}\\n\",\n",
    "    'present_task_description': \"You are helping to: {present_task_description}\\n\",\n",
    "    'file_name': \"You are helping to rewrite and expand a file called {file_name}\\n\",\n",
    "    'structure_pattern': \"Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\",\n",
    "    'tree_structure': \"The present tree-structure:\\n {tree_structure}\\n \",\n",
    "    'markdown_text': \"Markdown input \\n>>>\\n{markdown_text}\"\n",
    "}\n",
    "\n",
    "\n",
    "template_variable_independent = \\\n",
    "\"\"\"\n",
    "Things to keep in mind:\n",
    "* present ALL html links without changing the link's text.\n",
    "* Preserve any urls or relative links without changing them. \n",
    "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
    "* Keep all code blocks that are referenced in any manner.\n",
    "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
    "* Be as honest and as accurate as possible. \n",
    "* Be succinct in your responses. \n",
    "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
    "* If text is sparse or missing create a reasonable outline and follow it. \n",
    "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
    "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
    "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
    "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
    "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
    "* We'll get $1000 if we do this right, so let's do our best!\n",
    "* Write EOF on a new line after the last line of the text to indicate nothing new.\n",
    "\n",
    "Here's the content.\n",
    "\"\"\"\n",
    "# Please, do follow these instructions closely for it if we don't get this right, we might lose our job. \n",
    "# * reformat any bulleted lists of links where github links have `!!! abstract`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "template_required = \\\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "spt = SmartPromptTemplate(template_required=template_required, \n",
    "template_optional_dict=template_optional_dict, \n",
    "template_variable_independent=template_variable_independent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfcaa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"expert AI technology creator, communicator, and markdown / mkdocs expert\"\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Create an automated repository that is able to explain Generative AI \"\\\n",
    "        \"and how to improve upon it in plain-English and how to enable it from idea to product, as well as new and interesting research. \"\\\n",
    "                \n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\n",
    "                         \n",
    "\n",
    "file_from_base_dir = 'Using/examples/by_field/science/biology/proteins.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure(path_base=BASE_DOCS_DIR)\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "# role = None\n",
    "prompt = spt.get_prompt(role=role, \n",
    "    project_name=project_name,\n",
    "    project_goals=project_goals,\n",
    "    present_task_description=present_task_description,\n",
    "    file_name=file_name,\n",
    "#     tree_structure=tree_structure,\n",
    "    # markdown_text=markdown_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a expert AI technology creator, communicator, and markdown / mkdocs expert\n",
      "You are helping to: Improve the markdown based on best understandings.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Using/examples/by_field/science/biology/proteins.md\n",
      "\n",
      "\n",
      "Things to keep in mind:\n",
      "* present ALL html links without changing the link's text.\n",
      "* Preserve any urls or relative links without changing them. \n",
      "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
      "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
      "* Keep all code blocks that are referenced in any manner.\n",
      "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
      "* Be as honest and as accurate as possible. \n",
      "* Be succinct in your responses. \n",
      "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
      "* If text is sparse or missing create a reasonable outline and follow it. \n",
      "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
      "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
      "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
      "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
      "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
      "* We'll get $1000 if we do this right, so let's do our best!\n",
      "* Write EOF on a new line after the last line of the text to indicate nothing new.\n",
      "\n",
      "Here's the content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b83ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This worked but had trivial output\n",
    "# from openai import AsyncOpenAI\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# # client = AsyncOpenAI()\n",
    "# # completion = await client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"gpt-4o\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": prompt},\n",
    "#     # {\"role\": \"user\", \"content\": \"Help me launch a nuke.\"}\n",
    "#   ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cca5af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1300ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         prompt,\n",
    "#     ),\n",
    "#     (\"human\", f\"{markdown_text}\"),\n",
    "# ]\n",
    "# ai_msg = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f203240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         prompt,\n",
    "#     ),\n",
    "#     (\"human\", f\"{markdown_text}\"),\n",
    "#     (\"system\", ai_msg.content),\n",
    "#     (\"human\", \"continue\")\n",
    "# ]\n",
    "# ai_msg2 = llm.invoke(messages)\n",
    "# print(ai_msg2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09a61bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_have_eof(last_message_content, eof=\"EOF\"):\n",
    "    if eof in last_message_content:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def continue_chat(llm, prompt, text, continue_text=\"continue\", update_status=True):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            prompt,\n",
    "        ),\n",
    "        (\"human\", f\"{text}\"),\n",
    "    ]\n",
    "    ai_msg = llm.invoke(messages)\n",
    "    concatenated_text = [ai_msg.content]\n",
    "    token_usage_list = [ai_msg.response_metadata['token_usage']]\n",
    "    i=0\n",
    "    if update_status:\n",
    "        print(f\"Response {i} recieved with token usage: {ai_msg.response_metadata['token_usage']['completion_tokens']} used.\")\n",
    "    while not messages_have_eof(ai_msg.content):\n",
    "        i+=1\n",
    "        messages.append((\"system\", ai_msg.content))\n",
    "        messages.append((\"human\", continue_text))\n",
    "        concatenated_text.append(ai_msg.content)\n",
    "        token_usage_list.append(ai_msg.response_metadata['token_usage'])\n",
    "        if update_status:\n",
    "            print(f\"Response {i} recieved with token usage: {ai_msg.response_metadata['token_usage']['completion_tokens']} used.\")\n",
    "        ai_msg = llm.invoke(messages)\n",
    "    \n",
    "    messages.append((\"system\", ai_msg.content))\n",
    "    concatenated_text.append(ai_msg.content)\n",
    "    \n",
    "    if update_status:\n",
    "        print(f\"Response {i} recieved with token usage: {ai_msg.response_metadata['token_usage']['completion_tokens']} used.\")\n",
    "\n",
    "    # sum up the token usage from 'completion_tokens', 'prompt-tokens', and 'total_tokens' list of dicts    \n",
    "    token_usage = {k: sum([d[k] for d in token_usage_list]) for k in token_usage_list[0].keys()}\n",
    "    \n",
    "    return messages, ''.join(concatenated_text), token_usage\n",
    "\n",
    "# def concatenate_message_responses(messages):\n",
    "#     return \"\\n\".join([m.content for m in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4816005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 0 recieved with token usage: 4096 used.\n",
      "Response 1 recieved with token usage: 4096 used.\n",
      "Response 2 recieved with token usage: 4096 used.\n",
      "Response 2 recieved with token usage: 336 used.\n"
     ]
    }
   ],
   "source": [
    "messages, concatenated, token_usage = continue_chat(llm, prompt, markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d583a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system',\n",
       "  \"You are a expert AI technology creator, communicator, and markdown / mkdocs expert\\nYou are helping to: Improve the markdown based on best understandings.\\n\\nYou are helping to rewrite and expand a file called ../../docs/Using/examples/by_field/science/biology/proteins.md\\n\\n\\nThings to keep in mind:\\n* present ALL html links without changing the link's text.\\n* Preserve any urls or relative links without changing them. \\n* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\\n* keep ALL images `<img ...></img>` that are referenced in any manner.  \\n* Keep all code blocks that are referenced in any manner.\\n* Please be sure to keep any admonitions like `!!!` and `???`.\\n* Be as honest and as accurate as possible. \\n* Be succinct in your responses. \\n* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \\n* If text is sparse or missing create a reasonable outline and follow it. \\n* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \\n* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\\n* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\\n* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\\n* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\\n* We'll get $1000 if we do this right, so let's do our best!\\n* Write EOF on a new line after the last line of the text to indicate nothing new.\\n\\nHere's the content.\\n\"),\n",
       " ('human',\n",
       "  '🚧 Under construction 🦺\\n\\nGenerating or modifying protein sequences to improve behavior, or to create novel behavior, is is a powrful application for AI. Guided through evolutionary-techniques, Bayesian optimization, and/or with the use of protein language models (PLMS), they can vastly accelerate the development of biotechnological tools, as well as for identifying targets and avenues for therapeutics. Because of their ability to represent the \\'language of proteins\\', PLMS are increasingly important in predicting structure and function of proteins. \\n\\n## Components\\n\\nProtein optimization can be broken down into several component [^n1]\\n\\n[^n1]: [adaptive machine learning for protein engineering](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\\n\\n- **[Target property](#optimization-targets)** is the intended goal(s) for protein development\\n- **[Fitness Predictor]** that uses sequence information to estimate the value of the optimization target, as a surrogate for laboratory measurement\\n- **[Sequence Proposer](#sequence-optimization)** that creates sequences to evaluate and explore\\n- **Prioritizer** that uses sequence and predictor information to estimate the top candidates \\n- **Laboratory measurements** that reveal the quality of the generated proteins based on the targets\\n- **Orchestrator** That puts the pieces together in a functional and validated manner\\n\\nOptimization systems may involve merging and combining these components for full solutions in two general manners.\\n\\n1. A model that separates generation and evaluation steps, where an the predictor model evaluates the quality of an input set of sequences (generated or otherwise defined)\\n2. A model that model that directly predicts the best designs using adaptive sampling, proposing solutions, evaluating them with the predictor model, and then iterating. \\n\\nThese components can be cleanly seen in the box below:\\n\\n???+ tip \"[Adaptive machine learning for protein engineering](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\"\\n\\n    An overview of ML for protein engineering:\\n    \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/a8af9370-05e8-4e81-a223-b60cafbb9b00)\\n\\n\\n\\n### Strategy\\n\\nProtein optimization will necessarily evolve the creation of those proteins and evaluations of target characteristics. There are large volumes of databases of various forms that may be useful in creating foundation models. It will still be essential to use continued observaiton to improve the optimization target based on predicted and iterated feedback.\\n\\nThe volume of the observations will help to determine the architectures that one could use. Base models tend to be PLMs because of the large set of available data. Unsupervised finetuning with those large models may be able to occur through homology or family sets to per. Final targets may be then optimized with simple networks, often involving regression to minimize overfitting, or methods that include Bayesian or evolutionary approaches. \\n\\nTo be able to successfully deliver on final target optimziation, the greater the quantity of direct or surrogate data that can be obtained, the greater the potential the resulting models will sufficiently predictive of fitness of future protein sequence candidates. That is why massive screening approaches, as described in by [Ginko\\'s platform](https://foundrytheory.substack.com/p/improving-a-stubborn-enzyme-with-ai) screening thousands of candidates. \\n\\n??? note \"[An example process by Ginkgo](https://foundrytheory.substack.com/p/improving-a-stubborn-enzyme-with-ai)\"\\n    \\n    Gingko reveals with foundry-scale protein estimates, that with thousands of samples they were able to create an enzyme with 10x improvement from where they started. In their design they use structure (differential) estimates via Rosetta, Evolutionary-scale modeling (PLMs), active site focus evolutionary models, as well as an in-house method called \\'OWL. \\n\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c3666ac2-8d7b-46f7-838b-cd2e6d3721c1)\\n\\nWhen it is possibly to iteratively measure proposed sequences, new data can be used to improve subsequent sequence predictions. This can be done _greedily_, choosing the best solutions, or using probabilistic methods, such as [Bayesian Optimization]. Searching for a protein that optimizes a target by combining both estimated values, as well as their uncertainties. Selecting the sequences with highest-predicted target values  will _greedily_ inform what should be used, and may easily fail due to incorrect estimates due to the predictor model. In other manners, confidence bound (UCB) acquisition, that selects sequences based on an a sum of the predicted target value and the predicted target unertainty. \\n\\n\\n???+ tip \"[Ways of prioritizing](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\"\\n\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/08ed6633-0439-44f5-a52d-e53afb4804f2)\\n\\n\\n### Optimization Targets\\n\\nThere are a number of [targets](#optimization-targets) that protein optimization can focus on. For examples, some targets enable primarily basic understanding, such as protein [structure](#structure), and other targets are related to [function](#function), though it is generally considered that structure enables the functions. \\n\\nIn the cannon of causal influence,  _source_ has --> _sequence_ that creates --> _structure_ --> enables the _function_.  we can generally compartmentalize targets based on these, though there is certain crossover betwen them . \\n\\n- **Source**\\n    - [Candidate Identification](#candidate-identification) \\n- **[Sequence](#sequence)**\\n    - [Alignment](#candidate-alignment)\\n    - [Remote cohomology] Similar function, or structure, \\n- **[Structure](#structure)**\\n    - **Contact prediction**\\n    - **Secondary and tertiary structure**\\n    - **(mis)Folding (missense)**\\n- **[Function](#function)**\\n    - **Enzymatic Catalysis:** The ability of an enzyme to accelerate chemical processes \\n    - **Thermocompatibility** or thermostability, how well a protein remains stable or functions at varying temperatures\\n    - **Fluorescence** for visualization purposes\\n\\n    - **[Protein Binding](#binding)** to...\\n        - **Proteins**\\n        - **Nucleic Acids**\\n        - **Drugs molecules**\\n        - **Metals**\\n\\nThough there are many examples where these classes cross-these potential \\n\\n### Fitness prediction\\nTraining a fitness model may first involve training an unsupervised [foundation model](#foundation-models) on a high volume of data. These models can then be fine-tuned, or otherwise adapted, to incorporate protein-sequences or higher relevance to the protein targets of interest. \\n??? note \"[Learning protein fitness models from evolutionary and assay-labeled data](https://www.nature.com/articles/s41587-021-01146-5)\"\\n    The authors show in their paper that uses a manner to combine ridge regression wiømbined with large-languag emodels revealing the ability to effectively predict evolutionary and assay-labeled fitness\\n    <img width=\"706\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/03ac33f5-b455-491e-b7e0-72c207216d48\">\\n\\n    \\n\\n### Sequence Proposer\\n\\nWith a fitness predictor made available, the next step is to create proposal sequences that may be evaluated with the predictor model, or potentially with direct measurement. \\n\\n\\nOne way of doing this is to use [_generative models_](#generative-models) directly in seeding the generated sequence with starting sequences of the target sequence, or even from an natural language prompt. Another mthod is to use [_activation maximization_](#activation-maximization), a method that will generate input to a model that will ideally maximize the output for a given model (assuming maximization is the desired target direction.\\n\\n\\n\\n#### Generative Models\\n\\n??? abstract \"[Sequence modeling and design from molecular to genome scale with Evo](https://github.com/evo-design/evo)\" evo-dna\\n\\n    The authors reveal in their [paper](https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1.full.pdf) the use of long-context Genetics models can be powerful in their ability to yield state of of art predictions in protein-related tasks. These tasks include zero-shot function prediction, multi-element sequence generation. Their models use the \\'Striped-Hyena\\' structured state space model. Their model is known as Evo. \\n    <img width=\"566\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/d62b1d21-f323-4ad7-8a1f-28295e9dea2b\">\\n\\n\\n??? abstract \"[ZymCTRL: a conditional language model for the controllable generation of artificial enzymes](https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf)\"\\n    Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods.\\n    <img width=\"892\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/67d72fce-e8d8-4372-9371-1f45d2c2d408\">\\n\\n    [Model](https://huggingface.co/nferruz/ZymCTRL)\\n\\n\\n??? note \"[Low-N protein engineering with data-efficient deep learning](https://www.nature.com/articles/s41592-021-01100-y)\"\\n    The authors demonstrate a standard model where a pLM undergoes unsupervised pre-training and then refined on evolutionarily related sequences, and finally fine-tuned on assay-specific sequences. They use a Markov Chain Monte Carlo (MCMC) method to mutate and iteratively evaluate mutations to improve design approaches. \\n    \\n\\n#### Markov Chain Monte Carlo \\n\\n??? abstract \"[Plug & play directed evolution of proteins with gradient-based discrete MCMC (EvoProtGrad for MCMC)](https://github.com/NREL/EvoProtGrad)\" evoprotgrad\\n    A Python package for directed evolution on a protein sequence with gradient-based discrete Markov chain monte carlo (MCMC) based on the [paper](https://iopscience.iop.org/article/10.1088/2632-2153/accacd) [blog](https://huggingface.co/blog/AmelieSchreiber/directed-evolution-with-esm2) and [docs](https://nrel.github.io/EvoProtGrad/getting_started/MCMC/)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/4be735d6-bba2-4003-9bf0-36218e264c93)\\n    \\n\\n    \\n    \\n\\n\\n##### With Natural Large Language Models\\n\\n#### Activation Maximization\\n\\n??? abstract \"[SeqProp: Stochastic Sequence Propagation - A Keras Model for optimizing DNA, RNA and protein sequences based on a predictor. ](https://github.com/johli/seqprop)\" seqprop\\n    The authors reveal in their [paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04437-5) and [arxiv](https://arxiv.org/pdf/2005.11275.pdf) a method to optimize biological protein sequences based on an a predictor model. They use something called _trainable logits_ that can be sampled from, but do so doing instance normalizaton.\\n        A Python API for constructing generative DNA/RNA/protein Sequence PWM models in Keras. Implements a PWM generator (with support for discrete sampling and ST gradient estimation), a predictor model wrapper and a loss model.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/3c2fe20f-1257-4a76-a034-1b3cad242b8c)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/fed3de2c-6dcf-4f4b-8ad1-aa2ecadce5ad)\\n\\n\\n??? abstract \"[Protein sequence design by conformational landscape optimization](https://github.com/gjoni/trDesign)\"\\n    The authors propose a bayesian approach to optimizing the a protein structure to yield a residue sequence. They use a loss of the form $Loss = -/log P(contacts|sequence) + D_{KL}(f_{20}||f_{20}^{PDB}$ where $D_{KL}$ is the Kullback-Leibler divergence, $f_20$ is the average frequency of amino acids from the sequence, and $f_{20}^{PDB}$ is the average frequency of amino acids from proteins int he PDB. \\n    [Paper](https://www.pnas.org/doi/full/10.1073/pnas.2017228118)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/8936aae6-4e1c-41f4-bc03-38092e829585)\\n\\n??? abstract \"[Structure-based scoring and sampling of \\'Combinatorial Variant Effects from Structure\\' (CoVES)](https://github.com/ddingding/CoVES/tree/publish)\" coves\\n    The authors show in their [paper](https://www.biorxiv.org/content/10.1101/2022.10.31.514613v2) and [Nature](https://www.nature.com/articles/s41467-024-45621-4#Sec1) over 7 different combinatorial mutation studies, the ability to design proteins by exploring the design spacae without needs for combinatorial number of mutations. They build a model to estimate a residue preference effect for each amino acid variant at each position, and sums these effects to predict combinatorial variants.  Simple linear and logistic models using a \\'mutation effect preference of size 20(Amino Acids)x residue size\\' were able to predict the effect of variance. They could then use this to design sequences using Boltsman sampling and generate variatns that were much better.  \\n    \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/753aaf78-06b7-4199-999d-f08e78d7addd)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/5d933173-49e2-4f76-9a0a-d7834c00590a)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/4fba09dc-0ecf-4a9b-833c-9d607e545c34)\\n    Particularly the following image provides credence that these simple models of important sites can be useful in predicting proteins. \\n    \\n    <img width=\"440\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/911a6b86-0e44-45c2-8a47-9a301d187ce1\">\\n\\n## Data sources\\n\\n??? note \"[Brenda](https://www.brenda-enzymes.org/)\"\\n\\n??? abstract \"[ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design](https://github.com/OATML-Markslab/ProteinGym) is an extensive set of Deep Mutational Scanning (DMS) assays and annotated human clinical variants\"\\n    The results are \"curated to enable thorough comparisons of various mutation effect predictors in different regimes\"\\n    <img width=\"566\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/75911610-75f1-4cce-bccf-d7de1f3a168a\">\\n\\n    [Website](https://proteingym.org/)\\n    [Paper](https://papers.nips.cc/paper_files/paper/2023/file/cac723e5ff29f65e3fcbb0739ae91bee-Paper-Datasets_and_Benchmarks.pdf)\\n\\n??? note \"[Homologous Pairs of Low and High Temperature Originating Proteins Spanning the Known Prokaryotic Universe](https://www.nature.com/articles/s41597-023-02553-w)\"\\n\\n## Example Architectures\\nWhile there are many architecture and methods for creating and optimizing proteins, we focus here, primarily on ways that employ PLMs in some way. These create _foundation models_ that can be fine-tuned and readily adapted to specific domains of interest. \\n\\nThe general method of creating protein foundation models uses Masked Language Modeling (MLM) or \\'Bert-based\\' predictions, though next-token predictions, as is done with GPT-architectures may also be used. We share a number of prominent models and uses or derivatives, \\n\\n### Evaluation Metrics\\n\\n* Spearman Correlation Coefficient\\n* AUC\\n* MCC\\n\\n\\n### Foundation models\\n\\n#### ESM models\\n\\n??? abstract \"![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/esm) [Language models enable zero-shot prediction of the effects of mutations on protein function](https://github.com/facebookresearch/esm)\"\\n\\n??? tip \"[Evolutionary-scale prediction of atomic-level protein structure with a language model (esm)](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3.full.pdf)\"\\n    End to end Language model enabling structure sequence pairing, coupled with an equivariant transformer structure model at the end\\n    <img width=\"474\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/aeac8588-89a6-42f0-afa2-24f2735b0c50\">\\n\\n    [Science paper](https://www.science.org/doi/10.1126/science.ade2574)\\n\\n??? abstract \"[Genome-wide prediction of disease variant effects with a deep protein language model](https://github.com/ntranoslab/esm-variants)\"\\n    The authors show in their [paper](https://www.nature.com/articles/s41588-023-01465-0) a workflow using ESM1b, a 650-million-parameter protein language model, to predict all ~450 million possible missense variant effects in the human genome, and made all predictions available on a web portal.\\n\\n\\n    **Developments**\\n\\n    Using established and newly trained protein language models, the authors demonstrate the ability to provide zero-shot predictions of the effect of a protein mutation on a protein\\'s fluorescence. \\n    \\n    <img width=\"610\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/5b9b6d18-a7a6-4ffb-a0cd-c952315aed90\">\\n    \\n    They use a PLM to score the mutations using a log odds-ration of the mutated protein. \\n    \\n    <img width=\"320\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/88f7ea61-b1a8-4455-882e-d3ba23403f58\">\\n    \\n    **Data**\\n    \\n    They create ESM-1v, an unsupervised masked transformer model by training on 98 million protein sequences, using Uniref90 2020-03. \\n    \\n    They evaluate the model on a set of 41 deep mutational scans. \\n\\n    [Paper](https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2.full.pdf)\\n\\n\\n??? abstract \"![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/esm) [MSA Transformer](https://github.com/facebookresearch/esm)\"\\n\\n    The author\\'s demonstrate in their [paper](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v3.full.pdf) training an unsupervised PLM that operates on sets of aligned sequences. Self-supervision helps to reconstruct the corrupted MSA. \\n\\n    **Developments**\\n    \\n    <img width=\"334\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e87edf1e-49eb-4a19-bf08-093060e87220\">\\n    \\n    ** Architecture**\\n\\n    The architecture \\'interleaves attention across the rows and columns of the alignment as an axial attention\\' that ties the attention map across the rows with \\'tied row attention\\'. They use a single feed-forward layer for each block. For position embeddings, they use a 1D learned position embeddings added independently to each row of MSA to distinguish aligned positions differently for each sequence. \\n\\n    The objective looks for the loss of th masked MSA as follows\\n    \\n    <img width=\"254\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e7fbe493-e30d-415a-99e7-9c28dd4358c6\">\\n    \\n    With the probabilities are the output of the MSA transformer, softmax normalized of the amino acid vocabulary indepentely normalized per position in the sequence. Masking the columns uniformly resulted in the best performance.\\n    \\n    The models are 12 layers, with a 768 embedding size, and 12 attention heads resulting in 100M parameters.\\n    \\n    **Data**\\n\\n    They use 26 million MSA sequences generated from from UniRef50 by searching UniClust30 with HHblits.\\n\\n    **Analysis** \\n\\n    <img width=\"706\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/b24fffe3-efbb-4b14-a2b7-fa20b3fdf7ba\">\\n\\n    They show that a logistic regression with 144 parameters fit on 20 training structures could predict the contact maps of almost 15k other structures almost unsupervised. \\n    They show a supervised contact prediction map can improve the contact-prediction maps. \\n    They find the attention heads focus on highly variable colums, correlating with the per-column entropy of MSA.  \\n\\n    \\n\\n??? abstract \"[Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.biorxiv.org/content/10.1101/622803v4.full.pdf)\"\\n    The authors used masked languaged prediction with transformer models to train a foundation model capable of multiple downstream tasks.\\n    \\n        \"To this end we use unsupervised learning to train a deep contextual language model on\\n        86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The\\n        resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone.\\n        The learned representation space has a multi-scale\\n        organization reflecting structure from the level\\n        of biochemical properties of amino acids to remote homology of proteins. Information about\\n        secondary and tertiary structure is encoded in the\\n        representations and can be identified by linear projections. \"\\n\\n        <img width=\"329\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/27df578a-50ab-42ac-b675-58f7d740be4a\">\\n\\n??? note \"[TRANSFORMER PROTEIN LANGUAGE MODELS ARE UNSUPERVISED STRUCTURE LEARNERS](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.full.pdf)\"\\n    <img width=\"973\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e6ca2843-c5a1-444c-96f5-081a8aad6a5b\">\\n\\n\\n??? note \"[Reference Optimization of Protein Language Models as a Multi-objective Binder Design Paradigm](https://arxiv.org/pdf/2403.04187.pdf)\" protgpt2\\n    The authors create a design paradigm using instruction fine-tuning and direct preference optimization of PLMS. Creating ProtGPT2 allows binders to be designed based on receptor and drug develepoability criterion. To do this, they do two-step instruction tuning with receptor-bindign \\'chat-templates\\', and then optimize fine-tuned models to promote preferred binders. \\n    Specifically they \"propose an alignment method to transform pre-trained unconditional protein sequence models (p(s)), that autoregressively sample sequences (s) from underlying data distribution (D), to conditional probability models (p(s|r; c)) that given a target receptor (r) sample binders that satisfy constraints (c) encoded by preference datasets compiled from experiments and domain experts.\"\\n    \\n    <img width=\"726\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/4a673b82-fa46-419b-b24e-d65436923438\">\\n    \\n    Notably, they fuse protein sequences with English-language prompts and use BPE encoding with a large vocabulary size (50k) instead of the smaller pLM vocabulary sizes (33) that are standard.  \\n    \\n    <img width=\"708\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/ff42d6df-a13e-418d-8385-264ecd2d0994\">\\n\\n\\n\\n\\n    \\n\\n#### Multimodal\\n\\n??? abstract \"🧬 ![GitHub Repo stars](https://badgen.net/github/stars/bio-ontology-research-group/deepgo2) [Protein function prediction as approximate semantic entailment](https://github.com/bio-ontology-research-group/deepgo2)\" deepgo-se\\n\\n    **Developments** \\n\\n    Current LLM models excel at predicting the structure and other attributes of biological sequences like proteins. However, their [transferability is limited](https://www.biorxiv.org/content/10.1101/2024.02.05.578959v2.full.pdf), capping their true potential. The [DeepGO-SE](https://www.nature.com/articles/s42256-024-00795-w) model innovates 🚀 by integrating protein language models with specific knowledge on protein function, bridging the gap between knowledge-graphs\\' explicit representations and next-token prediction\\'s implicit representations, and thereby significantly improving model performance.\\n\\n    **How it works** \\n\\n    * 🔄 First, DeepGO-SE reuses the ESM2 large language model to convert a protein sequence into a vector space embedding, prepping it for machine learning application.\\n    * 🧠 Next, an ensemble of fitted prediction models is trained to align ESM2 embeddings with an embedding space (ELEmbeddings) derived from GO axioms, creating a world model filled with geometric shapes and relations akin to a Σ algebra, which can verify the truth of a statement.\\n    * ✅ Finally, for statements such as \"protein has function C\", when the ensemble reaches a consensus on truth, the semantic truth estimation is then accepted as valid.\\n\\n    ![DeepGO-SE Model Overview](https://github.com/ianderrington/genai/assets/76016868/6136332a-66cd-4f1f-89d5-fe11690e42fa)\\n\\n    The authors demonstrate 📈 that this method improves molecular function prediction by a substantial margin. Moreover, they reveal that training with protein-protein interactions substantially benefits the understanding of complex biological processes. They suggest that predicting biological processes may only require knowledge of molecular functions, potentially paving the way for a more generalized approach that could be advantageous in other domains.\\n\\n\\n??? abstract \"[ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts](https://github.com/DeepGraphLearning/ProtST)\" prost \\n    The authors show in their [paper](https://proceedings.mlr.press/v202/xu23t/xu23t.pdf) that the fusion of natural language model with a protein language model can reasonably improve protein location prediction, fitness landscape prediction, and protein function annotation. \\n    \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c78e6baa-84a6-477f-b831-a69d338eb55c)\\n\\n    **Data** Their build a ProtDescribe to match protein sequences with text descriptions.\\n    \\n    **Models** Their models involve three losses. 1. InfooNCE loss to maximize similarity between sequence pairs, and minimize similarity between negative pairs. 2. A Masked protein modeling cross-entropy loss to maintain unimodal information to the sequences, and a fusion MultiMOdal Mask Prediction that uses self and cross-attention on masked input sequence and text pairs to mutually recover the predicted results in sequence and text results. They start with pre-traiend protein models (Bert, ESM-1b and ESM-2) and pretrained language model (PubMedBERT-abs and PubMedBERT-full)\\n\\n    <img width=\"336\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/cd2617ba-87d0-456d-bb1d-ba11c903e2fc\">\\n\\n    The text data set looks like this: \\n    <img width=\"673\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/2e96eea2-aff6-4667-9101-96ae5dbb4dc0\">\\n\\n\\n\\n\\n    \\n#### Other models\\n\\n??? note \"[Single-sequence protein structure prediction using supervised transformer protein language models](https://yanglab.nankai.edu.cn/trRosetta/benchmark_single/)\"\\n    The authors show in their [paper[(https://nature.com/articles/s43588-022-00373-3) the ability to generate high quality predictions outperforming AlphaFold2, with a model called trRosettaX-Single using ESM to generate representations and attention maps  that can be trained for distance+energy maps, \\n    \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c06d4a40-117f-4b86-9deb-ee9d29fc8f70)\\n\\n??? abstract \"[Tasks Assessing Protein Embeddings (TAPE)](https://github.com/songlab-cal/tape)\"\\n\\n#### Architectures by Target\\n\\n##### Enzymatic Catalysis\\n\\n!!! tip \"[Harnessing Generative AI to Decode Enzyme Catalysis and Evolution for Enhanced Engineering](https://www.biorxiv.org/content/10.1101/2023.10.10.561808v1.full.pdf)\"\\n\\n??? tip \"[De novo design of luciferases using deep learning](https://www.nature.com/articles/s41586-023-05696-3)\"\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/b4de3724-def9-43f6-a3b0-e55061c5b278)\\n\\n\\n??? note \"[ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a language diffusion model](https://www.science.org/doi/10.1126/sciadv.adl4000)\" forcegen\\n      **Developments**  The authors present ForceGen, an end-to-end algorithm for de novo protein generation based on nonlinear mechanical unfolding responses. Rooted in the physics of protein mechanics, this generative strategy provides a powerful way to design new proteins rapidly, including exquisite and rapid predictions about their dynamical behavior.\\n      \\n      Proteins, like any other mechanical object, respond to forces in peculiar ways. Think of the different response you\\'d get from pulling on a steel cable versus pulling on a rubber band, or the difference between honey and glass. Now, we can design proteins with a set of desirable mechanical characteristics, with applications from health to sustainable plastics.\\n\\n      <img width=\"701\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/3af9d0de-93dd-4591-9967-ebb856307618\">\\n\\n      <img width=\"727\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/f985357c-2b3b-4092-875c-93648ab167f0\">\\n\\n      The key to solving this problem was to integrate a **protein language model with denoising diffusion methods**, and using accurate atomistic-level physical simulation data to endow the model a first-principles understanding. ForceGen can solve both forward and inverse tasks: In the forward task, we can predict how stable a protein is, how it will unfold and what the forces involved are, all given just the sequence of amino acids. In the inverse task, we can design new proteins that meet complex nonlinear mechanical signature targets.\\n      \\n      With the new generative model they can directly design proteins to meet complex nonlinear mechanical property-design objectives by leveraging deep knowledge on protein sequences from a pretrained protein language model and maps mechanical unfolding responses to create proteins.\\n      \\n      Via full-atom molecular simulations for direct validation from physical and chemical principles, we demonstrate that the designed proteins are de novo, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, and a detailed unfolding force-separation curves. \\n\\n\\n\\n#### Thermostability\\n\\n??? abstract \"[ProLaTherm: Protein Language Model-based Thermophilicity Predictor](https://github.com/grimmlab/ProLaTherm)\" prolatherm\\n    \\n    **Developments** The authors reveal in their [paper](https://academic.oup.com/nargab/article/5/4/lqad087/7306664) a model that is good at predicting thermal stability as well as an augmented dataset to enable their good predictive control \\n    \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/0c9b9576-b753-459b-9016-c40a7aaccde0)\\n    \\n    Data: Collected from multiple sources to create new sets. \" 9422 UniProt identifiers and 9363 corresponding amino acid sequences from 16 thermophilic and 16 mesophilic organisms\" Filtered\\n\\n    Models: \\n    They considered several first, we consider feature-based models that rely on manually engineered features, such as physicochemical properties. Second, we include hybrid sequence-based models that use amino acid features to learn sequence embeddings. Third, we consider approaches that are purely sequence-based, similarly to ProLaTherm, but in contrast train sequence embeddings from scratch.  The final model used a simplified transformer solution that used 1024 sequence enbeddings that were put into a self-attention network resulting in an output embedding that was averaged and put into an ReLU activation that then went to a a batch norm and logistic predction of whether the protein was a thermophile. \\n    \\n    Training: From scratch.\\n\\n    Results: High performance of PLM 97% accuracy over other models, though this accuracy is reduced when reducing train/test set homology.\\n    \\n\\n    \\n\\n### Candidate Identification\\n\\nParticularly for evolutionary methods, it is essential to know _where to start_ optimizing from. GenAI can be used to identify candidates based on databases of prior candidates. \\n\\nSearching is essential to find similar sequences that may aid in the training or fine-tuning of models. This can be done with sequence-based alignment, as well as structure-based alignment. Here are a few references of highly-relevant tools for search/alignment. \\n\\n??? tip \"[Fast and accurate protein structure search with: Foldseek](https://search.foldseek.com/search)\"\\n    Foldseek \"aligns the structure of a query protein against a database by describing tertiary amino acid interactions within proteins as sequences over a structural alphabet\".\\n    [Paper](https://www.nature.com/articles/s41587-023-01773-0)\\n\\n\\n#### Candidate alignment\\nIt is not necessarily just enough to identify a potential candidate but to have a degree of _alignment_ with of the candidate with starting or suggested candidates. This allows for a degree of interpretability to by people. \\n\\n??? abstract \"[Contrastive learning on protein embeddings enlightens midnight zone](https://github.com/Rostlab/EAT)\"\\n    In their [paper](https://academic.oup.com/nargab/article/4/2/lqac043/6605840) the authors demonstrate the use of contrastive optimization (like CLIP) to create embeddings that \"optimize constraints captured by heirarchichal classification of protein 3D structures\" \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/9bacb594-15e1-46aa-bb89-36c2bddfaefb)\\n\\n\\n#### Protein Binding\\n\\n??? abstract \"[Contrastive learning in protein language space predicts interactions between drugs and protein targets](https://github.com/samsledje/ConPLex)\" ConPLex\\n    The authors show in their [paper](https://www.pnas.org/doi/full/10.1073/pnas.2220778120) the use of contrastive learning to help co-locate proteins and potential drug molecules in a \\'shared feature space\\' and learns to map drue drugs against non-binding \\'decoy\\' molecules. \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/bb697ce1-6ad7-4a1c-9122-c19ea93ce9eb)\\n\\n??? abstract \"[Robust deep learning based protein sequence design using ProteinMPNN](https://github.com/dauparas/ProteinMPNN)\" protein-mpnn\\n    In their [paper][(https://www.biorxiv.org/content/10.1101/2022.06.03.494563v1](https://www.biorxiv.org/content/10.1101/2022.06.03.494563v1.full.pdf)) the authors reveal a novel method to predict sequences and sequence recovery. \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/ee8d6025-d4a1-4ade-ac22-cfb26cabd41e)\\n\\n\\n## Tools\\n\\n### Evaluation Methods\\n\\n??? abstract \"[BERTOLOGY MEETS BIOLOGY: INTERPRETING ATTENTION IN PROTEIN LANGUAGE MODELS](https://github.com/salesforce/provis)\"\\n    **Developments** The authors show in their [paper](https://arxiv.org/pdf/2006.15222.pdf) \" that\\n    attention: (1) captures the folding structure of proteins, connecting amino acids that\\n    are far apart in the underlying sequence, but spatially close in the three-dimensional\\n    structure, (2) targets binding sites, a key functional component of proteins, and\\n    (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer\\n    architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We\\n    also present a three-dimensional visualization of the interaction between attention and protein structure\"\\n\\n    They see the following:\\n    \\n    * Attention aligns strongly with contact maps in the deepest layers.\\n    * Attention targets binding sites throughout most layers of the models.\\n    * Attention targets Post-translational modifications in a small number of heads.\\n    * Attention targets higher-level properties in deeper layers.\\n    * Attention heads specialize in particular amino acids. \\n    * Attention is consistent with substitution relationships\\n\\n### Colab Design\\n!!! tip \"![GitHub Repo stars](https://badgen.net/github/stars/sokrypton/ColabDesign) [ColabDesign: Making Protein Design accessible to all via Google Colab!](https://github.com/sokrypton/ColabDesign)\"\\n\\n\\n## Quality reviews and references\\n\\n!!! tip \"[Harnessing Generative AI to Decode Enzyme Catalysis and Evolution for Enhanced Engineering](https://www.biorxiv.org/content/10.1101/2023.10.10.561808v1.full.pdf)\"\\n\\n!!! abstract \"![GitHub Repo stars](https://badgen.net/github/stars/yangkky/Machine-learning-for-proteins) [Papers on Machine learning for Proteins](https://github.com/yangkky/Machine-learning-for-proteins)\"\\n\\n??? tip \"[Deep Learning in Protein Structural Modeling and Design](https://www.sciencedirect.com/science/article/pii/S2666389920301902) provides a thorough summary of DL manners of optimizing proteins.\"\\n    They emphasize a Sequence --> Structure --> Function approach should be focused upon. \\n    ![image](https://github.com/ianderrington/genai/assets/76016868/cf1b22cc-73d7-4f91-888d-2ad6f75953a1)\\n\\n!!! note \"[Nucleate AI in Biotech: AI for Protein Design](https://nucleate-hq.notion.site/AI-in-Protein-Design-Resource-Page-8c137f8ba2684402aef9e1e31b85776c)\"\\n    \\n\\n## Companies\\n\\nHere are several companies that help to focus in protein design. If you have one you\\'d like to suggest, please file an [issue](https://github.com/ianderrington/genai/issues). \\n\\n- [Deepchain.bio]\\n- [https://310.ai/]\\n'),\n",
       " ('system',\n",
       "  '# Protein Optimization Using AI\\n\\nGenerating or modifying protein sequences to improve behavior or create novel behavior is a powerful application for AI. Guided through evolutionary techniques, Bayesian optimization, and/or with the use of protein language models (PLMs), AI can vastly accelerate the development of biotechnological tools and identify targets and avenues for therapeutics. Because of their ability to represent the \\'language of proteins,\\' PLMs are increasingly important in predicting the structure and function of proteins.\\n\\n## Components\\n\\nProtein optimization can be broken down into several components[^n1]:\\n\\n[^n1]: [Adaptive Machine Learning for Protein Engineering](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\\n\\n- **[Target Property](#optimization-targets)**: The intended goal(s) for protein development.\\n- **Fitness Predictor**: Uses sequence information to estimate the value of the optimization target, as a surrogate for laboratory measurement.\\n- **[Sequence Proposer](#sequence-optimization)**: Creates sequences to evaluate and explore.\\n- **Prioritizer**: Uses sequence and predictor information to estimate the top candidates.\\n- **Laboratory Measurements**: Reveal the quality of the generated proteins based on the targets.\\n- **Orchestrator**: Puts the pieces together in a functional and validated manner.\\n\\nOptimization systems may involve merging and combining these components for full solutions in two general manners:\\n\\n1. A model that separates generation and evaluation steps, where the predictor model evaluates the quality of an input set of sequences (generated or otherwise defined).\\n2. A model that directly predicts the best designs using adaptive sampling, proposing solutions, evaluating them with the predictor model, and then iterating.\\n\\nThese components can be cleanly seen in the box below:\\n\\n???+ tip \"[Adaptive Machine Learning for Protein Engineering](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\"\\n    An overview of ML for protein engineering:\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/a8af9370-05e8-4e81-a223-b60cafbb9b00)\\n\\n### Strategy\\n\\nProtein optimization will necessarily evolve the creation of those proteins and evaluations of target characteristics. There are large volumes of databases of various forms that may be useful in creating foundation models. It will still be essential to use continued observation to improve the optimization target based on predicted and iterated feedback.\\n\\nThe volume of the observations will help to determine the architectures that one could use. Base models tend to be PLMs because of the large set of available data. Unsupervised fine-tuning with those large models may be able to occur through homology or family sets. Final targets may then be optimized with simple networks, often involving regression to minimize overfitting, or methods that include Bayesian or evolutionary approaches.\\n\\nTo be able to successfully deliver on final target optimization, the greater the quantity of direct or surrogate data that can be obtained, the greater the potential the resulting models will sufficiently predict the fitness of future protein sequence candidates. That is why massive screening approaches, as described by [Ginkgo\\'s platform](https://foundrytheory.substack.com/p/improving-a-stubborn-enzyme-with-ai), screen thousands of candidates.\\n\\n??? note \"[An example process by Ginkgo](https://foundrytheory.substack.com/p/improving-a-stubborn-enzyme-with-ai)\"\\n    Ginkgo reveals with foundry-scale protein estimates, that with thousands of samples they were able to create an enzyme with 10x improvement from where they started. In their design, they use structure (differential) estimates via Rosetta, Evolutionary-scale modeling (PLMs), active site focus evolutionary models, as well as an in-house method called \\'OWL.\\'\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c3666ac2-8d7b-46f7-838b-cd2e6d3721c1)\\n\\nWhen it is possible to iteratively measure proposed sequences, new data can be used to improve subsequent sequence predictions. This can be done _greedily_, choosing the best solutions, or using probabilistic methods, such as [Bayesian Optimization]. Searching for a protein that optimizes a target by combining both estimated values, as well as their uncertainties. Selecting the sequences with the highest-predicted target values will _greedily_ inform what should be used and may easily fail due to incorrect estimates from the predictor model. In other manners, confidence bound (UCB) acquisition selects sequences based on a sum of the predicted target value and the predicted target uncertainty.\\n\\n???+ tip \"[Ways of prioritizing](https://www.sciencedirect.com/science/article/pii/S0959440X21001457)\"\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/08ed6633-0439-44f5-a52d-e53afb4804f2)\\n\\n### Optimization Targets\\n\\nThere are a number of [targets](#optimization-targets) that protein optimization can focus on. For example, some targets enable primarily basic understanding, such as protein [structure](#structure), and other targets are related to [function](#function), though it is generally considered that structure enables the functions.\\n\\nIn the canon of causal influence, _source_ has --> _sequence_ that creates --> _structure_ --> enables the _function_. We can generally compartmentalize targets based on these, though there is certain crossover between them.\\n\\n- **Source**\\n    - [Candidate Identification](#candidate-identification)\\n- **[Sequence](#sequence)**\\n    - [Alignment](#candidate-alignment)\\n    - [Remote cohomology]: Similar function, or structure\\n- **[Structure](#structure)**\\n    - **Contact prediction**\\n    - **Secondary and tertiary structure**\\n    - **(mis)Folding (missense)**\\n- **[Function](#function)**\\n    - **Enzymatic Catalysis:** The ability of an enzyme to accelerate chemical processes\\n    - **Thermocompatibility** or thermostability, how well a protein remains stable or functions at varying temperatures\\n    - **Fluorescence** for visualization purposes\\n    - **[Protein Binding](#binding)** to...\\n        - **Proteins**\\n        - **Nucleic Acids**\\n        - **Drug molecules**\\n        - **Metals**\\n\\nThough there are many examples where these classes cross, these potential targets are essential for protein optimization.\\n\\n### Fitness Prediction\\n\\nTraining a fitness model may first involve training an unsupervised [foundation model](#foundation-models) on a high volume of data. These models can then be fine-tuned, or otherwise adapted, to incorporate protein sequences or higher relevance to the protein targets of interest.\\n\\n??? note \"[Learning protein fitness models from evolutionary and assay-labeled data](https://www.nature.com/articles/s41587-021-01146-5)\"\\n    The authors show in their paper that uses a manner to combine ridge regression with large-language models revealing the ability to effectively predict evolutionary and assay-labeled fitness.\\n    <img width=\"706\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/03ac33f5-b455-491e-b7e0-72c207216d48\">\\n\\n### Sequence Proposer\\n\\nWith a fitness predictor made available, the next step is to create proposal sequences that may be evaluated with the predictor model, or potentially with direct measurement.\\n\\nOne way of doing this is to use [_generative models_](#generative-models) directly in seeding the generated sequence with starting sequences of the target sequence, or even from a natural language prompt. Another method is to use [_activation maximization_](#activation-maximization), a method that will generate input to a model that will ideally maximize the output for a given model (assuming maximization is the desired target direction).\\n\\n#### Generative Models\\n\\n??? abstract \"[Sequence modeling and design from molecular to genome scale with Evo](https://github.com/evo-design/evo)\"\\n    The authors reveal in their [paper](https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1.full.pdf) the use of long-context Genetics models can be powerful in their ability to yield state-of-the-art predictions in protein-related tasks. These tasks include zero-shot function prediction, multi-element sequence generation. Their models use the \\'Striped-Hyena\\' structured state space model. Their model is known as Evo.\\n    <img width=\"566\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/d62b1d21-f323-4ad7-8a1f-28295e9dea2b\">\\n\\n??? abstract \"[ZymCTRL: a conditional language model for the controllable generation of artificial enzymes](https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf)\"\\n    Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods.\\n    <img width=\"892\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/67d72fce-e8d8-4372-9371-1f45d2c2d408\">\\n    [Model](https://huggingface.co/nferruz/ZymCTRL)\\n\\n??? note \"[Low-N protein engineering with data-efficient deep learning](https://www.nature.com/articles/s41592-021-01100-y)\"\\n    The authors demonstrate a standard model where a PLM undergoes unsupervised pre-training and then refined on evolutionarily related sequences, and finally fine-tuned on assay-specific sequences. They use a Markov Chain Monte Carlo (MCMC) method to mutate and iteratively evaluate mutations to improve design approaches.\\n\\n#### Markov Chain Monte Carlo\\n\\n??? abstract \"[Plug & play directed evolution of proteins with gradient-based discrete MCMC (EvoProtGrad for MCMC)](https://github.com/NREL/EvoProtGrad)\"\\n    A Python package for directed evolution on a protein sequence with gradient-based discrete Markov chain Monte Carlo (MCMC) based on the [paper](https://iopscience.iop.org/article/10.1088/2632-2153/accacd), [blog](https://huggingface.co/blog/AmelieSchreiber/directed-evolution-with-esm2), and [docs](https://nrel.github.io/EvoProtGrad/getting_started/MCMC/)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/4be735d6-bba2-4003-9bf0-36218e264c93)\\n\\n##### With Natural Large Language Models\\n\\n#### Activation Maximization\\n\\n??? abstract \"[SeqProp: Stochastic Sequence Propagation - A Keras Model for optimizing DNA, RNA and protein sequences based on a predictor.](https://github.com/johli/seqprop)\"\\n    The authors reveal in their [paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04437-5) and [arxiv](https://arxiv.org/pdf/2005.11275.pdf) a method to optimize biological protein sequences based on a predictor model. They use something called _trainable logits_ that can be sampled from, but do so using instance normalization.\\n    A Python API for constructing generative DNA/RNA/protein Sequence PWM models in Keras. Implements a PWM generator (with support for discrete sampling and ST gradient estimation), a predictor model wrapper, and a loss model.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/3c2fe20f-1257-4a76-a034-1b3cad242b8c)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/fed3de2c-6dcf-4f4b-8ad1-aa2ecadce5ad)\\n\\n??? abstract \"[Protein sequence design by conformational landscape optimization](https://github.com/gjoni/trDesign)\"\\n    The authors propose a Bayesian approach to optimizing a protein structure to yield a residue sequence. They use a loss of the form $Loss = -/log P(contacts|sequence) + D_{KL}(f_{20}||f_{20}^{PDB}$ where $D_{KL}$ is the Kullback-Leibler divergence, $f_{20}$ is the average frequency of amino acids from the sequence, and $f_{20}^{PDB}$ is the average frequency of amino acids from proteins in the PDB.\\n    [Paper](https://www.pnas.org/doi/full/10.1073/pnas.2017228118)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/8936aae6-4e1c-41f4-bc03-38092e829585)\\n\\n??? abstract \"[Structure-based scoring and sampling of \\'Combinatorial Variant Effects from Structure\\' (CoVES)](https://github.com/ddingding/CoVES/tree/publish)\"\\n    The authors show in their [paper](https://www.biorxiv.org/content/10.1101/2022.10.31.514613v2) and [Nature](https://www.nature.com/articles/s41467-024-45621-4#Sec1) over 7 different combinatorial mutation studies, the ability to design proteins by exploring the design space without the need for a combinatorial number of mutations. They build a model to estimate a residue preference effect for each amino acid variant at each position and sum these effects to predict combinatorial variants. Simple linear and logistic models using a \\'mutation effect preference of size 20(Amino Acids)x residue size\\' were able to predict the effect of variance. They could then use this to design sequences using Boltzmann sampling and generate variants that were much better.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/753aaf78-06b7-4199-999d-f08e78d7addd)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/5d933173-49e2-4f76-9a0a-d7834c00590a)\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/4fba09dc-0ecf-4a9b-833c-9d607e545c34)\\n    Particularly the following image provides credence that these simple models of important sites can be useful in predicting proteins.\\n    <img width=\"440\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/911a6b86-0e44-45c2-8a47-9a301d187ce1\">\\n\\n## Data Sources\\n\\n??? note \"[Brenda](https://www.brenda-enzymes.org/)\"\\n\\n??? abstract \"[ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design](https://github.com/OATML-Markslab/ProteinGym)\"\\n    ProteinGym is an extensive set of Deep Mutational Scanning (DMS) assays and annotated human clinical variants. The results are \"curated to enable thorough comparisons of various mutation effect predictors in different regimes.\"\\n    <img width=\"566\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/75911610-75f1-4cce-bccf-d7de1f3a168a\">\\n    [Website](https://proteingym.org/)\\n    [Paper](https://papers.nips.cc/paper_files/paper/2023/file/cac723e5ff29f65e3fcbb0739ae91bee-Paper-Datasets_and_Benchmarks.pdf)\\n\\n??? note \"[Homologous Pairs of Low and High Temperature Originating Proteins Spanning the Known Prokaryotic Universe](https://www.nature.com/articles/s41597-023-02553-w)\"\\n\\n## Example Architectures\\n\\nWhile there are many architectures and methods for creating and optimizing proteins, we focus here primarily on ways that employ PLMs in some way. These create _foundation models_ that can be fine-tuned and readily adapted to specific domains of interest.\\n\\nThe general method of creating protein foundation models uses Masked Language Modeling (MLM) or \\'Bert-based\\' predictions, though next-token predictions, as is done with GPT-architectures, may also be used. We share a number of prominent models and uses or derivatives.\\n\\n### Evaluation Metrics\\n\\n- Spearman Correlation Coefficient\\n- AUC\\n- MCC\\n\\n### Foundation Models\\n\\n#### ESM Models\\n\\n??? abstract \"![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/esm) [Language models enable zero-shot prediction of the effects of mutations on protein function](https://github.com/facebookresearch/esm)\"\\n\\n??? tip \"[Evolutionary-scale prediction of atomic-level protein structure with a language model (esm)](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3.full.pdf)\"\\n    End-to-end Language model enabling structure sequence pairing, coupled with an equivariant transformer structure model at the end.\\n    <img width=\"474\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/aeac8588-89a6-42f0-afa2-24f2735b0c50\">\\n    [Science paper](https://www.science.org/doi/10.1126/science.ade2574)\\n\\n??? abstract \"[Genome-wide prediction of disease variant effects with a deep protein language model](https://github.com/ntranoslab/esm-variants)\"\\n    The authors show in their [paper](https://www.nature.com/articles/s41588-023-01465-0) a workflow using ESM1b, a 650-million-parameter protein language model, to predict all ~450 million possible missense variant effects in the human genome, and made all predictions available on a web portal.\\n    **Developments**\\n    Using established and newly trained protein language models, the authors demonstrate the ability to provide zero-shot predictions of the effect of a protein mutation on a protein\\'s fluorescence.\\n    <img width=\"610\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/5b9b6d18-a7a6-4ffb-a0cd-c952315aed90\">\\n    They use a PLM to score the mutations using a log odds-ratio of the mutated protein.\\n    <img width=\"320\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/88f7ea61-b1a8-4455-882e-d3ba23403f58\">\\n    **Data**\\n    They create ESM-1v, an unsupervised masked transformer model by training on 98 million protein sequences, using Uniref90 2020-03.\\n    They evaluate the model on a set of 41 deep mutational scans.\\n    [Paper]('),\n",
       " ('human', 'continue'),\n",
       " ('system',\n",
       "  '    [Paper](https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2.full.pdf)\\n\\n??? abstract \"![GitHub Repo stars](https://badgen.net/github/stars/facebookresearch/esm) [MSA Transformer](https://github.com/facebookresearch/esm)\"\\n    The authors demonstrate in their [paper](https://www.biorxiv.org/content/10.1101/2021.02.12.430858v3.full.pdf) training an unsupervised PLM that operates on sets of aligned sequences. Self-supervision helps to reconstruct the corrupted MSA.\\n    **Developments**\\n    <img width=\"334\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e87edf1e-49eb-4a19-bf08-093060e87220\">\\n    **Architecture**\\n    The architecture \\'interleaves attention across the rows and columns of the alignment as an axial attention\\' that ties the attention map across the rows with \\'tied row attention\\'. They use a single feed-forward layer for each block. For position embeddings, they use a 1D learned position embeddings added independently to each row of MSA to distinguish aligned positions differently for each sequence.\\n    The objective looks for the loss of the masked MSA as follows:\\n    <img width=\"254\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e7fbe493-e30d-415a-99e7-9c28dd4358c6\">\\n    With the probabilities being the output of the MSA transformer, softmax normalized of the amino acid vocabulary independently normalized per position in the sequence. Masking the columns uniformly resulted in the best performance.\\n    The models are 12 layers, with a 768 embedding size, and 12 attention heads resulting in 100M parameters.\\n    **Data**\\n    They use 26 million MSA sequences generated from UniRef50 by searching UniClust30 with HHblits.\\n    **Analysis**\\n    <img width=\"706\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/b24fffe3-efbb-4b14-a2b7-fa20b3fdf7ba\">\\n    They show that a logistic regression with 144 parameters fit on 20 training structures could predict the contact maps of almost 15k other structures almost unsupervised. They show a supervised contact prediction map can improve the contact-prediction maps. They find the attention heads focus on highly variable columns, correlating with the per-column entropy of MSA.\\n\\n??? abstract \"[Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.biorxiv.org/content/10.1101/622803v4.full.pdf)\"\\n    The authors used masked language prediction with transformer models to train a foundation model capable of multiple downstream tasks.\\n    \"To this end we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multi-scale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections.\"\\n    <img width=\"329\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/27df578a-50ab-42ac-b675-58f7d740be4a\">\\n\\n??? note \"[TRANSFORMER PROTEIN LANGUAGE MODELS ARE UNSUPERVISED STRUCTURE LEARNERS](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.full.pdf)\"\\n    <img width=\"973\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/e6ca2843-c5a1-444c-96f5-081a8aad6a5b\">\\n\\n??? note \"[Reference Optimization of Protein Language Models as a Multi-objective Binder Design Paradigm](https://arxiv.org/pdf/2403.04187.pdf)\"\\n    The authors create a design paradigm using instruction fine-tuning and direct preference optimization of PLMs. Creating ProtGPT2 allows binders to be designed based on receptor and drug developability criteria. To do this, they do two-step instruction tuning with receptor-binding \\'chat-templates\\', and then optimize fine-tuned models to promote preferred binders.\\n    Specifically, they \"propose an alignment method to transform pre-trained unconditional protein sequence models (p(s)), that autoregressively sample sequences (s) from underlying data distribution (D), to conditional probability models (p(s|r; c)) that given a target receptor (r) sample binders that satisfy constraints (c) encoded by preference datasets compiled from experiments and domain experts.\"\\n    <img width=\"726\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/4a673b82-fa46-419b-b24e-d65436923438\">\\n    Notably, they fuse protein sequences with English-language prompts and use BPE encoding with a large vocabulary size (50k) instead of the smaller PLM vocabulary sizes (33) that are standard.\\n    <img width=\"708\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/ff42d6df-a13e-418d-8385-264ecd2d0994\">\\n\\n#### Multimodal\\n\\n??? abstract \"🧬 ![GitHub Repo stars](https://badgen.net/github/stars/bio-ontology-research-group/deepgo2) [Protein function prediction as approximate semantic entailment](https://github.com/bio-ontology-research-group/deepgo2)\"\\n    **Developments**\\n    Current LLM models excel at predicting the structure and other attributes of biological sequences like proteins. However, their [transferability is limited](https://www.biorxiv.org/content/10.1101/2024.02.05.578959v2.full.pdf), capping their true potential. The [DeepGO-SE](https://www.nature.com/articles/s42256-024-00795-w) model innovates 🚀 by integrating protein language models with specific knowledge on protein function, bridging the gap between knowledge-graphs\\' explicit representations and next-token prediction\\'s implicit representations, and thereby significantly improving model performance.\\n    **How it works**\\n    * 🔄 First, DeepGO-SE reuses the ESM2 large language model to convert a protein sequence into a vector space embedding, prepping it for machine learning application.\\n    * 🧠 Next, an ensemble of fitted prediction models is trained to align ESM2 embeddings with an embedding space (ELEmbeddings) derived from GO axioms, creating a world model filled with geometric shapes and relations akin to a Σ algebra, which can verify the truth of a statement.\\n    * ✅ Finally, for statements such as \"protein has function C\", when the ensemble reaches a consensus on truth, the semantic truth estimation is then accepted as valid.\\n    ![DeepGO-SE Model Overview](https://github.com/ianderrington/genai/assets/76016868/6136332a-66cd-4f1f-89d5-fe11690e42fa)\\n    The authors demonstrate 📈 that this method improves molecular function prediction by a substantial margin. Moreover, they reveal that training with protein-protein interactions substantially benefits the understanding of complex biological processes. They suggest that predicting biological processes may only require knowledge of molecular functions, potentially paving the way for a more generalized approach that could be advantageous in other domains.\\n\\n??? abstract \"[ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts](https://github.com/DeepGraphLearning/ProtST)\"\\n    The authors show in their [paper](https://proceedings.mlr.press/v202/xu23t/xu23t.pdf) that the fusion of natural language model with a protein language model can reasonably improve protein location prediction, fitness landscape prediction, and protein function annotation.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c78e6baa-84a6-477f-b831-a69d338eb55c)\\n    **Data** They build a ProtDescribe to match protein sequences with text descriptions.\\n    **Models** Their models involve three losses. 1. InfoNCE loss to maximize similarity between sequence pairs, and minimize similarity between negative pairs. 2. A Masked protein modeling cross-entropy loss to maintain unimodal information to the sequences, and a fusion MultiModal Mask Prediction that uses self and cross-attention on masked input sequence and text pairs to mutually recover the predicted results in sequence and text results. They start with pre-trained protein models (Bert, ESM-1b, and ESM-2) and pre-trained language model (PubMedBERT-abs and PubMedBERT-full).\\n    <img width=\"336\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/cd2617ba-87d0-456d-bb1d-ba11c903e2fc\">\\n    The text data set looks like this:\\n    <img width=\"673\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/2e96eea2-aff6-4667-9101-96ae5dbb4dc0\">\\n\\n#### Other Models\\n\\n??? note \"[Single-sequence protein structure prediction using supervised transformer protein language models](https://yanglab.nankai.edu.cn/trRosetta/benchmark_single/)\"\\n    The authors show in their [paper](https://nature.com/articles/s43588-022-00373-3) the ability to generate high-quality predictions outperforming AlphaFold2, with a model called trRosettaX-Single using ESM to generate representations and attention maps that can be trained for distance+energy maps.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/c06d4a40-117f-4b86-9deb-ee9d29fc8f70)\\n\\n??? abstract \"[Tasks Assessing Protein Embeddings (TAPE)](https://github.com/songlab-cal/tape)\"\\n\\n#### Architectures by Target\\n\\n##### Enzymatic Catalysis\\n\\n!!! tip \"[Harnessing Generative AI to Decode Enzyme Catalysis and Evolution for Enhanced Engineering](https://www.biorxiv.org/content/10.1101/2023.10.10.561808v1.full.pdf)\"\\n\\n??? tip \"[De novo design of luciferases using deep learning](https://www.nature.com/articles/s41586-023-05696-3)\"\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/b4de3724-def9-43f6-a3b0-e55061c5b278)\\n\\n??? note \"[ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a language diffusion model](https://www.science.org/doi/10.1126/sciadv.adl4000)\"\\n    **Developments** The authors present ForceGen, an end-to-end algorithm for de novo protein generation based on nonlinear mechanical unfolding responses. Rooted in the physics of protein mechanics, this generative strategy provides a powerful way to design new proteins rapidly, including exquisite and rapid predictions about their dynamical behavior.\\n    Proteins, like any other mechanical object, respond to forces in peculiar ways. Think of the different response you\\'d get from pulling on a steel cable versus pulling on a rubber band, or the difference between honey and glass. Now, we can design proteins with a set of desirable mechanical characteristics, with applications from health to sustainable plastics.\\n    <img width=\"701\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/3af9d0de-93dd-4591-9967-ebb856307618\">\\n    <img width=\"727\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/f985357c-2b3b-4092-875c-93648ab167f0\">\\n    The key to solving this problem was to integrate a **protein language model with denoising diffusion methods**, and using accurate atomistic-level physical simulation data to endow the model a first-principles understanding. ForceGen can solve both forward and inverse tasks: In the forward task, we can predict how stable a protein is, how it will unfold and what the forces involved are, all given just the sequence of amino acids. In the inverse task, we can design new proteins that meet complex nonlinear mechanical signature targets.\\n    With the new generative model, they can directly design proteins to meet complex nonlinear mechanical property-design objectives by leveraging deep knowledge on protein sequences from a pretrained protein language model and maps mechanical unfolding responses to create proteins.\\n    Via full-atom molecular simulations for direct validation from physical and chemical principles, we demonstrate that the designed proteins are de novo, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, and a detailed unfolding force-separation curves.\\n\\n#### Thermostability\\n\\n??? abstract \"[ProLaTherm: Protein Language Model-based Thermophilicity Predictor](https://github.com/grimmlab/ProLaTherm)\"\\n    **Developments** The authors reveal in their [paper](https://academic.oup.com/nargab/article/5/4/lqad087/7306664) a model that is good at predicting thermal stability as well as an augmented dataset to enable their good predictive control.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/0c9b9576-b753-459b-9016-c40a7aaccde0)\\n    **Data**: Collected from multiple sources to create new sets. \"9422 UniProt identifiers and 9363 corresponding amino acid sequences from 16 thermophilic and 16 mesophilic organisms\" Filtered.\\n    **Models**: They considered several first, we consider feature-based models that rely on manually engineered features, such as physicochemical properties. Second, we include hybrid sequence-based models that use amino acid features to learn sequence embeddings. Third, we consider approaches that are purely sequence-based, similarly to ProLaTherm, but in contrast train sequence embeddings from scratch. The final model used a simplified transformer solution that used 1024 sequence embeddings that were put into a self-attention network resulting in an output embedding that was averaged and put into a ReLU activation that then went to a batch norm and logistic prediction of whether the protein was a thermophile.\\n    **Training**: From scratch.\\n    **Results**: High performance of PLM 97% accuracy over other models, though this accuracy is reduced when reducing train/test set homology.\\n\\n### Candidate Identification\\n\\nParticularly for evolutionary methods, it is essential to know _where to start_ optimizing from. GenAI can be used to identify candidates based on databases of prior candidates.\\n\\nSearching is essential to find similar sequences that may aid in the training or fine-tuning of models. This can be done with sequence-based alignment, as well as structure-based alignment. Here are a few references of highly-relevant tools for search/alignment.\\n\\n??? tip \"[Fast and accurate protein structure search with: Foldseek](https://search.foldseek.com/search)\"\\n    Foldseek \"aligns the structure of a query protein against a database by describing tertiary amino acid interactions within proteins as sequences over a structural alphabet.\"\\n    [Paper](https://www.nature.com/articles/s41587-023-01773-0)\\n\\n#### Candidate Alignment\\n\\nIt is not necessarily just enough to identify a potential candidate but to have a degree of _alignment_ with the candidate with starting or suggested candidates. This allows for a degree of interpretability by people.\\n\\n??? abstract \"[Contrastive learning on protein embeddings enlightens midnight zone](https://github.com/Rostlab/EAT)\"\\n    In their [paper](https://academic.oup.com/nargab/article/4/2/lqac043/6605840) the authors demonstrate the use of contrastive optimization (like CLIP) to create embeddings that \"optimize constraints captured by hierarchical classification of protein 3D structures.\"\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/9bacb594-15e1-46aa-bb89-36c2bddfaefb)\\n\\n#### Protein Binding\\n\\n??? abstract \"[Contrastive learning in protein language space predicts interactions between drugs and protein targets](https://github.com/samsledje/ConPLex)\"\\n    The authors show in their [paper](https://www.pnas.org/doi/full/10.1073/pnas.2220778120) the use of contrastive learning to help co-locate proteins and potential drug molecules in a \\'shared feature space\\' and learns to map drugs against non-binding \\'decoy\\' molecules.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/bb697ce1-6ad7-4a1c-9122-c19ea93ce9eb)\\n\\n??? abstract \"[Robust deep learning based protein sequence design using ProteinMPNN](https://github.com/dauparas/ProteinMPNN)\"\\n    In their [paper](https://www.biorxiv.org/content/10.1101/2022.06.03.494563v1) the authors reveal a novel method to predict sequences and sequence recovery.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/ee8d6025-d4a1-4ade-ac22-cfb26cabd41e)\\n\\n## Tools\\n\\n### Evaluation Methods\\n\\n??? abstract \"[BERTOLOGY MEETS BIOLOGY: INTERPRETING ATTENTION IN PROTEIN LANGUAGE MODELS](https://github.com/salesforce/provis)\"\\n    **Developments** The authors show in their [paper](https://arxiv.org/pdf/2006.15222.pdf) \"that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure.\"\\n    They see the following:\\n    * Attention aligns strongly with contact maps in the deepest layers.\\n    * Attention targets binding sites throughout most layers of the models.\\n    * Attention targets Post-translational modifications in a small number of heads.\\n    * Attention targets higher-level properties in deeper layers.\\n    * Attention heads specialize in particular amino acids.\\n    * Attention is consistent with substitution relationships.\\n\\n### Colab Design\\n\\n!!! tip \"![GitHub Repo stars](https://badgen.net/github/stars/sokrypton/ColabDesign) [ColabDesign: Making Protein Design accessible to all via Google Colab!](https://github.com/sokrypton/ColabDesign)\"\\n\\n##'),\n",
       " ('human', 'continue'),\n",
       " ('system',\n",
       "  '## Quality Reviews and References\\n\\n!!! tip \"[Harnessing Generative AI to Decode Enzyme Catalysis and Evolution for Enhanced Engineering](https://www.biorxiv.org/content/10.1101/2023.10.10.561808v1.full.pdf)\"\\n\\n!!! abstract \"![GitHub Repo stars](https://badgen.net/github/stars/yangkky/Machine-learning-for-proteins) [Papers on Machine learning for Proteins](https://github.com/yangkky/Machine-learning-for-proteins)\"\\n\\n??? tip \"[Deep Learning in Protein Structural Modeling and Design](https://www.sciencedirect.com/science/article/pii/S2666389920301902)\"\\n    Provides a thorough summary of DL manners of optimizing proteins. They emphasize a Sequence --> Structure --> Function approach should be focused upon.\\n    ![image](https://github.com/ianderrington/genai/assets/76016868/cf1b22cc-73d7-4f91-888d-2ad6f75953a1)\\n\\n!!! note \"[Nucleate AI in Biotech: AI for Protein Design](https://nucleate-hq.notion.site/AI-in-Protein-Design-Resource-Page-8c137f8ba2684402aef9e1e31b85776c)\"\\n\\n## Companies\\n\\nHere are several companies that focus on protein design. If you have one you\\'d like to suggest, please file an [issue](https://github.com/ianderrington/genai/issues).\\n\\n- [Deepchain.bio](https://deepchain.bio)\\n- [310.ai](https://310.ai)\\n\\nEOF')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c8195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import OpenAI\n",
    "# openai = OpenAI(model_name=model)\n",
    "# markdown_text = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../docs/Using/examples/by_field/science/biology/proteins_temp0.md'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write the file to disk with a _temp suffix and then open it with a system call to tkdiff to visualize the two\n",
    "# files side by side.\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import webbrowser\n",
    "\n",
    "def write_to_file(file_name, text):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    return file_name\n",
    "\n",
    "# Please be sure to run `homebrew install tkdiff` or otherwise install tkdiff on your computer\n",
    "def open_with_tkdiff(file_name1, file_name2):\n",
    "    subprocess.run(['tkdiff', file_name1, file_name2])\n",
    "\n",
    "def make_name(file_name):\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    temp_name = base + '_temp0' + ext\n",
    "    #check to see if it exists and if so, make a new name with a _temp# where # is the next available number\n",
    "    count=0\n",
    "    while os.path.exists(temp_name):\n",
    "        count += 1\n",
    "        \n",
    "        temp_name = base + f'_temp{count}' + ext\n",
    "    return temp_name\n",
    "temp_name = make_name(file_name)\n",
    "write_to_file(temp_name, concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3d6f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name: ../../docs/Using/examples/by_field/science/biology/proteins.md\n",
      "temp name: ../../docs/Using/examples/by_field/science/biology/proteins_temp0.md\n"
     ]
    }
   ],
   "source": [
    "print(f\"file name: {file_name}\")\n",
    "print(f\"temp name: {temp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "open_with_tkdiff(file_name, temp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../docs/Using/examples/by_field/science/biology/proteins.md has been updated.\n"
     ]
    }
   ],
   "source": [
    "input_answer = input(\"Is the output correct Yes/no/deletefile? (y/n/d)\")\n",
    "## if the answer is y then move the temp-name to the original file name and delete the temp file\n",
    "if input_answer == 'y':\n",
    "    os.rename(temp_name, file_name)\n",
    "    print(f\"File {file_name} has been updated.\")\n",
    "else:\n",
    "    print(f\"File {file_name} has not been updated.\")\n",
    "    # if the answer is n then delete the temp file and do nothing\n",
    "    if input_answer == 'd':\n",
    "        os.remove(temp_name)\n",
    "        print(f\"File {temp_name} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT THAT LOOKS AT DIFFERENCES CHHUNK BY CHUNK AND AMENDS THEM. \n",
    "CREATE DIFF, ITERATE ON DIFF AND UPDATE MODIFIED DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
