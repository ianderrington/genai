{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851ef27-17a2-42ac-ba6a-e9b5fb6bb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6335ae2-c900-437a-b731-d0226558f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain_community/llms/openai.py:248: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/ianderrington/miniconda3/envs/genai/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "# initialize the models\n",
    "\n",
    "models = ['gpt-3.5-turbo-16k', 'gpt-4','gpt-4-1106-preview',\"text-davinci-003\"]\n",
    "model = models[1]\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=model,\n",
    "    # openai_api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b915662-678f-40b3-ba10-710432d1c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DOCS_DIR = '../../docs/'\n",
    "# file_name = dosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae838ef5-36a1-4095-8982-28dc6f119812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/\n",
      "├── index.md\n",
      "├── javascripts/\n",
      "│   ├── copy-link.js\n",
      "│   └── mathjax.js\n",
      "├── Managenai/\n",
      "│   ├── brainstorming.md\n",
      "│   ├── build_plan.md\n",
      "│   ├── code_of_conduct.md\n",
      "│   ├── contributing.md\n",
      "│   ├── explorations_blog.md\n",
      "│   ├── index.md\n",
      "│   ├── project_requirements.md\n",
      "│   └── site_graph.md\n",
      "├── shared/\n",
      "├── Understand/\n",
      "│   ├── agents/\n",
      "│   │   ├── actions_and_tools.md\n",
      "│   │   ├── cognitive_architecture.md\n",
      "│   │   ├── commercial.md\n",
      "│   │   ├── environments.md\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── examples.md\n",
      "│   │   ├── image.png\n",
      "│   │   ├── index.md\n",
      "│   │   ├── memory.md\n",
      "│   │   ├── rag.md\n",
      "│   │   └── systems.md\n",
      "│   ├── architectures/\n",
      "│   │   ├── evaluating_and_comparing.md\n",
      "│   │   ├── generation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── llm_systems.md\n",
      "│   │   ├── models/\n",
      "│   │   │   ├── components.md\n",
      "│   │   │   ├── developing_architectures.md\n",
      "│   │   │   ├── diffusers.md\n",
      "│   │   │   ├── gans.md\n",
      "│   │   │   ├── gpt.md\n",
      "│   │   │   ├── hybrid_models.md\n",
      "│   │   │   ├── index.md\n",
      "│   │   │   ├── multimodal.md\n",
      "│   │   │   ├── reinforcement_learning.md\n",
      "│   │   │   ├── transformers.md\n",
      "│   │   │   └── vision_language_transformers.md\n",
      "│   │   ├── optimization/index.md\n",
      "│   │   ├── pre_trained_models.md\n",
      "│   │   └── training/\n",
      "│   │       ├── feedback.md\n",
      "│   │       ├── finetuning.md\n",
      "│   │       ├── index.md\n",
      "│   │       ├── pre-training.md\n",
      "│   │       └── recurrent.md\n",
      "│   ├── background/\n",
      "│   │   └── tensor_maths.md\n",
      "│   ├── data/\n",
      "│   │   ├── augmentation.md\n",
      "│   │   ├── distillation.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── privacy.md\n",
      "│   │   ├── selection.md\n",
      "│   │   ├── synthetic.md\n",
      "│   │   ├── sources.md\n",
      "│   │   └── tokenizing.md\n",
      "│   ├── deploying/\n",
      "│   │   ├── back_end.md\n",
      "│   │   ├── commercial_products.md\n",
      "│   │   ├── computation.md\n",
      "│   │   ├── examples_and_tutorials.md\n",
      "│   │   ├── frameworks.md\n",
      "│   │   ├── front_end.md\n",
      "│   │   ├── index.md\n",
      "│   │   └── libraries_and_tools.md\n",
      "│   ├── index.md\n",
      "│   ├── overview/\n",
      "│   │   ├── ai_and_ml_basics/\n",
      "│   │   │   └── index.md\n",
      "│   │   ├── challenges.md\n",
      "│   │   ├── chronology.md\n",
      "│   │   ├── extra_resources.md\n",
      "│   │   ├── index.md\n",
      "│   │   ├── open_source.md\n",
      "│   │   └── use_cases.md\n",
      "│   ├── prompting/\n",
      "│   │   ├── hacking.md\n",
      "│   │   └── index.md\n",
      "│   └── studies/\n",
      "│       ├── behavior.md\n",
      "│       └── studies.md\n",
      "└── Use/\n",
      "    ├── building_and_buying.md\n",
      "    ├── business.md\n",
      "    ├── commercial_markets.md\n",
      "    ├── de-risking/\n",
      "    │   ├── red_teaming.md\n",
      "    │   └── security.md\n",
      "    ├── ethically/\n",
      "    │   ├── alignment.md\n",
      "    │   ├── alignment_and_exestential_concerns.md\n",
      "    │   ├── dual_use_concerns.md\n",
      "    │   ├── fairness.md\n",
      "    │   ├── index.md\n",
      "    │   └── transparency.md\n",
      "    ├── examples/\n",
      "    │   ├── by_field/\n",
      "    │   │   ├── business.md\n",
      "    │   │   ├── entertainment/\n",
      "    │   │   │   ├── dynamic.md\n",
      "    │   │   │   └── static.md\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── individuals_and_society/\n",
      "    │   │   │   ├── education.md\n",
      "    │   │   │   ├── law.md\n",
      "    │   │   │   └── socio_societal.md\n",
      "    │   │   ├── mathematics/\n",
      "    │   │   │   └── index.md\n",
      "    │   │   ├── science/\n",
      "    │   │   │   ├── biology.md\n",
      "    │   │   │   ├── chemistry.md\n",
      "    │   │   │   ├── healthcare.md\n",
      "    │   │   │   └── index.md\n",
      "    │   │   └── technology/\n",
      "    │   │       ├── coding.md\n",
      "    │   │       ├── finance.md\n",
      "    │   │       ├── healthcare.md\n",
      "    │   │       └── robotics.md\n",
      "    │   ├── by_modality/\n",
      "    │   │   ├── index.md\n",
      "    │   │   ├── knowledge_graphs.md\n",
      "    │   │   ├── language.md\n",
      "    │   │   ├── multimodal.md\n",
      "    │   │   ├── sound.md\n",
      "    │   │   ├── static_2d.md\n",
      "    │   │   ├── tabular.md\n",
      "    │   │   ├── text.md\n",
      "    │   │   ├── time_series.md\n",
      "    │   │   └── video.md\n",
      "    │   └── index.md\n",
      "    ├── index.md\n",
      "    ├── interfacing_layers/\n",
      "    │   └── web_plugins.md\n",
      "    ├── managing/\n",
      "    │   ├── governing.md\n",
      "    │   ├── index.md\n",
      "    │   ├── ml_ops.md\n",
      "    │   ├── observability.md\n",
      "    │   └── regulations_and_guidelines.md\n",
      "    └── marking_and_detecting.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DisplayablePath(object):\n",
    "    display_filename_prefix_middle = '├──'\n",
    "    display_filename_prefix_last = '└──'\n",
    "    display_parent_prefix_middle = '    '\n",
    "    display_parent_prefix_last = '│   '\n",
    "\n",
    "    def __init__(self, path, parent_path, is_last):\n",
    "        self.path = Path(str(path))\n",
    "        self.parent = parent_path\n",
    "        self.is_last = is_last\n",
    "        if self.parent:\n",
    "            self.depth = self.parent.depth + 1\n",
    "        else:\n",
    "            self.depth = 0\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    @classmethod\n",
    "    def make_tree(cls, root, parent=None, is_last=False, criteria=None):\n",
    "        root = Path(str(root))\n",
    "        criteria = criteria or cls._default_criteria\n",
    "\n",
    "        displayable_root = cls(root, parent, is_last)\n",
    "        yield displayable_root\n",
    "\n",
    "        children = sorted(list(path\n",
    "                               for path in root.iterdir()\n",
    "                               if criteria(path)),\n",
    "                          key=lambda s: str(s).lower())\n",
    "        count = 1\n",
    "        for path in children:\n",
    "            is_last = count == len(children)\n",
    "            if path.is_dir():\n",
    "                yield from cls.make_tree(path,\n",
    "                                         parent=displayable_root,\n",
    "                                         is_last=is_last,\n",
    "                                         criteria=criteria)\n",
    "            else:\n",
    "                yield cls(path, displayable_root, is_last)\n",
    "            count += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _default_criteria(cls, path):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def displayname(self):\n",
    "        if self.path.is_dir():\n",
    "            return self.path.name + '/'\n",
    "        return self.path.name\n",
    "\n",
    "    def displayable(self):\n",
    "        if self.parent is None:\n",
    "            return self.displayname\n",
    "\n",
    "        _filename_prefix = (self.display_filename_prefix_last\n",
    "                            if self.is_last\n",
    "                            else self.display_filename_prefix_middle)\n",
    "\n",
    "        parts = ['{!s} {!s}'.format(_filename_prefix,\n",
    "                                    self.displayname)]\n",
    "\n",
    "        parent = self.parent\n",
    "        while parent and parent.parent is not None:\n",
    "            parts.append(self.display_parent_prefix_middle\n",
    "                         if parent.is_last\n",
    "                         else self.display_parent_prefix_last)\n",
    "            parent = parent.parent\n",
    "\n",
    "        return ''.join(reversed(parts))\n",
    "\n",
    "# With a criteria (skip hidden files)\n",
    "def is_not_hidden(path):\n",
    "    return  not ( 'Icon' in path.name or '.DS_Store' in path.name or 'stylesheets'  in path.name or \\\n",
    "        'CNAME' in path.name or 'assets' in path.name or '.svg' in path.name or '.pages' in path.name)\n",
    "    \n",
    "# paths = DisplayablePath.make_tree(\n",
    "#     Path(base_docs_dir),\n",
    "#     criteria=is_not_hidden\n",
    "# )\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "\n",
    "\n",
    "# paths = DisplayablePath.make_tree(Path(base_docs_dir), criteria=is_not_hidden)\n",
    "# for path in paths:\n",
    "#     print(path.displayable())\n",
    "\n",
    "def get_tree_structure(path_base=BASE_DOCS_DIR):\n",
    "    \n",
    "    paths = DisplayablePath.make_tree(Path(path_base), criteria=is_not_hidden)\n",
    "    path_str = [p.displayable() for p in paths]\n",
    "    # for path in paths:\n",
    "    #     print(path.displayable())\n",
    "    # return ''.join([p for p in path.displayable()])\n",
    "    return '\\n'.join(path_str)\n",
    "    \n",
    "tree_structure = get_tree_structure()\n",
    "print(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fe1f87-8ecc-49c0-aa96-399775ac8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(file_path, base_dir=BASE_DOCS_DIR):\n",
    "    # iterator for getting filenames\n",
    "    return os.path.join(base_dir, file_path)\n",
    "\n",
    "def get_structure_pattern(file_class=None):\n",
    "    if file_class is None:\n",
    "        file_class = 'index.md'\n",
    "    patterns={}\n",
    "    patterns['index.md'] = \\\n",
    "    \"\"\"\n",
    "NOTE: '-' is used to denote a general topic, sentence, or consideration but not considered a 'list' item.\n",
    "## Executive Summary (TL;DR)\n",
    "- Concise summary highlighting the essence of the topic and its significance.\n",
    "- Designed for readability by a non-technical or executive-level audience.\n",
    "- Utilize emojis, images, and visual elements effectively to emphasize key points.\n",
    "- Include Mermaid diagrams where appropriate, or describe necessary images as `IMAGE: <image description>`.\n",
    "\n",
    "## Practical Application and Usage\n",
    "- Focus on providing immediately actionable guidance and high-priority examples.\n",
    "- Extract and condense key usage instructions from earlier content into concise, actionable steps.\n",
    "- Offer 'How-to' guides, quick-start tips, and links for direct application.\n",
    "\n",
    "## Introduction and Relevance\n",
    "- Thorough introduction to the topic, highlighting its relevance and importance.\n",
    "- Discuss core components and their interplay within the broader context of Generative AI.\n",
    "\n",
    "## Core Content and Results\n",
    "- Detailed exploration of specific aspects under clear subheadings.\n",
    "- Provide illustrations or diagrams (Mermaid or `IMAGE:<image description>`) for complex concepts.\n",
    "- For extensive topics, include brief summaries and links to dedicated markdown files. If markdown files are already created, link to them here. If markdown files are needed, suggest them. \n",
    "\n",
    "## Technological Aspects\n",
    "- Explore relevant tools, technologies, and methodologies.\n",
    "- Highlight current trends and future directions in technology related to the topic.\n",
    "\n",
    "## Background or Theoretical Foundation (if necessary)\n",
    "- Delve into historical context and foundational theories.\n",
    "- Clarify essential theoretical concepts and terminologies for comprehensive understanding.\n",
    "\n",
    "## Ethical Considerations and Challenges\n",
    "- Address ethical dilemmas, challenges, and potential risks.\n",
    "- Discuss strategies for ethical practice and risk mitigation.\n",
    "\n",
    "## Extended Examples (if applicable)\n",
    "- Link to practical examples, simulations, or code snippets for hands-on understanding.\n",
    "- Direct readers to external resources, tools, or demonstrations for further exploration.\n",
    "\n",
    "## Advanced Topics and Further Exploration (if applicable)\n",
    "- Present open challenges and future research directions.\n",
    "- Deep dive into complex aspects with links to advanced readings and resources.\n",
    "\n",
    "## FAQs and Common Queries\n",
    "- Tackle frequently asked questions and common queries related to the section.\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "- Recap the main points and emphasize the key messages from the section.\n",
    "\n",
    "## References and Additional Reading\n",
    "- List citations and provide links to source materials and further reading.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # patterns['glossary.md'] = \\\n",
    "    # \"\"\"\n",
    "    # \"\"\"\n",
    "    return patterns[file_class]\n",
    "\n",
    "\n",
    "def get_markdown_text(markdown_file):\n",
    "    with open(markdown_file, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    # print(markdown_text)\n",
    "    return markdown_text\n",
    "# Could potentially do this is in few-shot prompt templates\n",
    "# These should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426a3057-f92c-42dc-bba2-1555943a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role = \"expert AI technology creator and communicator\"\n",
    "\n",
    "# project_name = \"Managing Generative AI\"\n",
    "# project_goals = \"Overall: Create an automated repository that is able to explain in plain-English and in code, \"\\\n",
    "#                 \"Generative AI and how to improve upon it. \"\n",
    "# present_task_description=\"Improve the markdown based on best understandings.\\n\"\\\n",
    "#                          \"Be as honest and as accurate as possible. Be succinct in your responses. \"\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "# # Idea\n",
    "# # Select between prompt patterns\n",
    "# # Chain select them more effectively. \n",
    "# # The present tree-structure:\\n {tree_structure}\\n \n",
    "# # Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\n",
    "# template = \\\n",
    "# \"\"\" \n",
    "# You are a {role}\n",
    "# You are working on a project called: {project_name}\\n\n",
    "# You are part of a team working to: {project_goals}\\n\n",
    "# You are helping to: {present_task_description}\\n\n",
    "# You are helping to rewrite and expand a file called {file_name} \n",
    "# Here are some things we'd like you to be sure to do:\n",
    "# * Please present ALL html links without changing the link's text. \n",
    "# * Preserve any urls or relative links without changing them. \n",
    "# * Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "# * Please reformat any bulleted lists of links where github links have `!!! abstract`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "\n",
    "# After the markdown When the text is presented (after >>>), please improve upon it. \n",
    "# If text is sparse or missing create a reasonable outline following the pattern above and fill it in.\n",
    "\n",
    "# Markdown Input:\\n\n",
    "# >>>\\n\n",
    "# {markdown_text}\"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate(\n",
    "#     # input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\", \"tree_structure\", \"structure_pattern\", \"markdown_text\", ],\n",
    "#         input_variables=[\"role\", \"project_name\", \"project_goals\", \"present_task_description\", \"file_name\",  \"markdown_text\", ],\n",
    "#     template=template\n",
    "# )\n",
    "# file_from_base_dir = 'Use/deploying/index.md'\n",
    "# file_from_base_dir = 'Use/redteaming.md'\n",
    "# file_from_base_dir = 'Understand/agents/rag.md'\n",
    "# file_name=get_file_name(file_from_base_dir)\n",
    "# tree_structure=get_tree_structure()\n",
    "# markdown_text=get_markdown_text(file_name)\n",
    "# structure_pattern = get_structure_pattern()\n",
    "# prompt=prompt_template.format(role=role,\n",
    "#                               project_name=project_name,\n",
    "#                        project_goals=project_goals,\n",
    "#                        present_task_description=present_task_description,\n",
    "#                        file_name=file_name,\n",
    "#                        # tree_structure=tree_structure,\n",
    "#                         structure_pattern=structure_pattern,\n",
    "#                        markdown_text=markdown_text,)\n",
    "# # The above is very verboase especially as it requires a lot of repeated typing of the same variables.\n",
    "# # It also needs to work for variables that are only specified in the template. If they are not specified in the template, then they should be ignored.\n",
    "# # There will a list of template lines that are appended to create the final prompt.\n",
    "# # The template lines will be specified as a list of dictionaries.\n",
    "# #  \n",
    "# # Let's write this as a class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  write above but realizing template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0241f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartPromptTemplate:\n",
    "    def __init__(self, template_required, template_optional_dict, template_variable_independent):\n",
    "        self.template_required = template_required if template_required is not None else \"\"\n",
    "        self.template_optional_dict = template_optional_dict if template_optional_dict is not None else {}\n",
    "        self.template_variable_independent = template_variable_independent if template_variable_independent is not None else \"\"\n",
    "    \n",
    "    def get_prompt(self, **kwargs):\n",
    "        template_list = []\n",
    "        for k, v in kwargs.items():\n",
    "            if k in self.template_optional_dict.keys():\n",
    "                template_list.append(self.template_optional_dict[k])\n",
    "        begin_indicator = \"\\n What would you write given the requests above? \\n>>>\\n\"\n",
    "        \n",
    "        template =   '\\n'.join(template_list) + self.template_required  + \"\\n<<< end input \\n\" + \\\n",
    "                        self.template_variable_independent + begin_indicator\n",
    "        prompt = template.format(**kwargs)\n",
    "        return prompt\n",
    "\n",
    "template_optional_dict = {\n",
    "\n",
    "    'role': \"You are a {role}\",\n",
    "    # 'project_name': \"You are working on a project called: {project_name}\\n\",\n",
    "    # 'project_goals': \"You are part of a team working to: {project_goals}\\n\",\n",
    "    'present_task_description': \"You are helping to: {present_task_description}\\n\",\n",
    "    'file_name': \"You are helping to rewrite and expand a file called {file_name}\\n\",\n",
    "    'structure_pattern': \"Please use a heading/subheading structure that follows the general pattern : {structure_pattern}\\n\",\n",
    "    'tree_structure': \"The present tree-structure:\\n {tree_structure}\\n \",\n",
    "    'markdown_text': \"Markdown input \\n>>>\\n{markdown_text}\"\n",
    "}\n",
    "\n",
    "\n",
    "template_variable_independent = \\\n",
    "\"\"\"\n",
    "Things to keep in mind:\n",
    "* present ALL html links without changing the link's text.\n",
    "* Preserve any urls or relative links without changing them. \n",
    "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
    "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
    "* Keep all code blocks that are referenced in any manner.\n",
    "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
    "* Be as honest and as accurate as possible. \n",
    "* Be succinct in your responses. \n",
    "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
    "* If text is sparse or missing create a reasonable outline and follow it. \n",
    "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
    "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
    "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
    "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
    "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
    "* We'll get $1000 if we do this right, so let's do our best!\n",
    "\"\"\"\n",
    "# Please, do follow these instructions closely for it if we don't get this right, we might lose our job. \n",
    "# * reformat any bulleted lists of links where github links have `!!! abstract`, arxiv's have `!!! tip` and others have `!!! information`. \n",
    "# * Please be sure to keep any amonitions like `!!!` and `???`.\n",
    "template_required = \\\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "spt = SmartPromptTemplate(template_required=template_required, \n",
    "template_optional_dict=template_optional_dict, \n",
    "template_variable_independent=template_variable_independent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcaa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"expert AI technology creator and communicator\"\n",
    "\n",
    "project_name = \"Managing Generative AI\"\n",
    "project_goals = \"Create an automated repository that is able to explain Generative AI \"\\\n",
    "        \"and how to improve upon it in plain-English and how to enable it from idea to product, as well as new and interesting research. \"\\\n",
    "                \n",
    "present_task_description=\"Improve the markdown based on best understandings.\"\n",
    "                         \n",
    "# file_from_base_dir = 'Use/deploying/index.md'\n",
    "# file_from_base_dir = 'Use/deploying/libraries_and_tools.md'\n",
    "# file_from_base_dir = 'Understand/data/preparation/synthetic.md'\n",
    "# file_from_base_dir = 'Understand/prompting/index.md'\n",
    "# file_from_base_dir = 'Understand/architectures/optimization/index.md'\n",
    "# file_from_base_dir = 'Understand/architectures/rlhf.md'\n",
    "# file_from_base_dir = 'Understand/agents/rag.md'\n",
    "file_from_base_dir = 'Understand/Overview/challenges.md'\n",
    "# file_from_base_dir = 'Understand/architectures/feedback.md'\n",
    "file_name=get_file_name(file_from_base_dir)\n",
    "tree_structure=get_tree_structure()\n",
    "markdown_text=get_markdown_text(file_name)\n",
    "structure_pattern = get_structure_pattern()\n",
    "prompt = spt.get_prompt(role=role, \n",
    "    project_name=project_name,\n",
    "    project_goals=project_goals,\n",
    "    present_task_description=present_task_description,\n",
    "    file_name=file_name,\n",
    "#     tree_structure=tree_structure,\n",
    "    markdown_text=markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52c69f5-576b-49b6-b902-d6773140bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a expert AI technology creator and communicator\n",
      "You are helping to: Improve the markdown based on best understandings.\n",
      "\n",
      "You are helping to rewrite and expand a file called ../../docs/Understanding/Overview/challenges.md\n",
      "\n",
      "Markdown input \n",
      ">>>\n",
      "There are a host of challenges associated with the use of GenAI at multiple levels. These challenges may also be appropriately considered _risks_ to add weight to their importance.  While references herein may provide partial solutions to these challenges, we do not suggest they are 'solved'. \n",
      "\n",
      "At the base level [technical challenges](#technical-challenges) relate to enabling the use of GenAI.\n",
      "At a higher level [ethichal challenges](#ethical-challenges) relate to the risks that must be considered against benefits of using GenAI. \n",
      "\n",
      "## Technical Challenges\n",
      "\n",
      "???+ important \"Technical challenges with GenAI\" technical-challenges-with-genai\n",
      "    * Reducing [hallucinations](#hallucinations) and improving accuracy\n",
      "    * Make LLMs generate results more [quickly and cheaply](../architectures/generation.md)\n",
      "    * Optimize context length and context construction\n",
      "    * [Training](../architectures/training/index.md) LLMs more efficiently \n",
      "    * Improving the quality of [data](../data/index.md)\n",
      "    * Incorporating other [data modalities](../architectures/models/multimodal.md)\n",
      "    * Productionizing [new model architecture](../architectures/models/developing_architectures.md)\n",
      "    * Develop GPU alternatives\n",
      "    * Making [agents](../agents/index.md) usable\n",
      "    * Improve learning from human preferences\n",
      "    * Improving [interfaces](../deploying/front_end.md) with GenAI\n",
      "    \n",
      "### Hallucinations\n",
      "\n",
      "There are a number of issues related to modle accuracy that pose challenges for GenAI models. Most prominant among them are the effect of _Hallucinations_. Models hallucinate, by making up facts or sentences that have no reasoanble bearing to reality.\n",
      "\n",
      "[Some studies](https://arxiv.org/pdf/2311.14648.pdf) indicate that the halluciantion-rate is related to the frequency that a fact appears only once in a data set, and that calibrated models, like those that are pre-trained, are more likely to hallucinate than those that do not have calibrated next-token predictions.\n",
      "\n",
      "## Ethical Challenges\n",
      "\n",
      "???+ important \"Ethical challenges with GenAI\" ethical-challenges-with-genai\n",
      "    * [Job displacement](#job-displacement)\n",
      "    * [Copywrite and IP](#copywrite-and-ip)\n",
      "    * [Dual Use](#dual-use)\n",
      "\n",
      "In general [ethical use](../../Using/ethically/index.md) of GenAI will necessarily be considered to address all or most of these challenges.\n",
      "\n",
      "At a high level, the concerns for displacement and capture of people's jobs must be taken into consideration. With arguments both minimizing and amplifying the concern, estimates still have around 300 million jobs replaced by AI, [according to a Goldman Sachs report](https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html). AT the same time GDP could be increased by 7% and lift productivity. It is still apparent that [upskilling](#upskilling) to enable people to work with AI as an enabling tool is important to consider.\n",
      "\n",
      "At nearly the highest level of challenge is to have GenAI that is Aligned for the betterment of humanity and our planet and not to its detriment with [dual use](#dual-use). Because of the expansive and moral-philosophical nature of this, as in what is defining 'betterment' it is difficult. Concretely, however, minimizing potential risks associated with GenAI, especially Autonomous Agents, are necessary to address at a functional level, both at organizations and within governments and the regulatory bodies that coordinate the two.\n",
      "\n",
      "\n",
      "### Job displacement\n",
      "\n",
      "GenAI enables the automation of a large number of knowledge-based, and administrative tasks as well as creative efforts. Consequently, GenAI has already been found to enable job-displacement. In the next few years, up to [30% hours currently worked across the US economy could be automated with help of GenAI](https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america). \n",
      "\n",
      "#### Upskilling\n",
      "\n",
      "Upskilling will require training employees to use GenAI to enable their work, or to find other work that GenAI is not well-suited for.\n",
      "\n",
      "### Copywrite and IP\n",
      "\n",
      "Related to job-displacement, the content created with GenAI remains in a precarious state with regard's to copyright and IP. While there are indications that content generated purely from AI may not be copyrighted (in the US), it is generally accepted AI can provide the basis for content that may be copyrighted. The evolution of this may take years of debate and resolution of laws to settle before confusion is fully settled.\n",
      "\n",
      "??? note \"[Talkin’ ‘Bout AI Generation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4523551) A thorough discussion on copyright issues\"\n",
      "    <img width=\"529\" alt=\"image\" src=\"https://github.com/ianderrington/genai/assets/76016868/75a1b0e9-7d4b-4db2-a0ee-f18890cce403\">\n",
      "\n",
      "\n",
      "### Dual Use\n",
      "\n",
      "The technology may be found to have dual-use, or that which is harmful, _instead of helpful_ to end-recipients. \n",
      "\n",
      "## References\n",
      "\n",
      "!!! note \"[Open challenges in LLM research](LLMhttps://huyenchip.com/2023/08/16/llm-research-open-challenges.html#5_design_a_new_model_architecture)\"\n",
      "\n",
      "<<< end input \n",
      "\n",
      "Things to keep in mind:\n",
      "* present ALL html links without changing the link's text.\n",
      "* Preserve any urls or relative links without changing them. \n",
      "* Be sure to use `##` `###` subheadings and appropriately to reference sections and subsections.\n",
      "* keep ALL images `<img ...></img>` that are referenced in any manner.  \n",
      "* Keep all code blocks that are referenced in any manner.\n",
      "* Please be sure to keep any admonitions like `!!!` and `???`.\n",
      "* Be as honest and as accurate as possible. \n",
      "* Be succinct in your responses. \n",
      "* Keep the ORIGINAL VOICE of the author there, and avoid unecessary changes to headings and subheadings. \n",
      "* If text is sparse or missing create a reasonable outline and follow it. \n",
      "* If you see MANAGEN (<and execute requests in trailing parenthesis>) then please evolve and expand upon the text in that area. \n",
      "* If you see any MANAGEN requests to make a mermaid diagram, please do so using the information that was provided.\n",
      "* PRESERVE ALL STRUCTURED ADMONITIONS and following (that start with e.g. `!!!` and `???`) and DO NOT CHANGE THEM INTO BULLETS. Those need to be preserved.\n",
      "* PRESERVE ALL INFORMATION IN MAIN MARKDOWN TEXT\n",
      "* COPY ALL INFORMATON THAT IS IN ADMONITIONS!\n",
      "* We'll get $1000 if we do this right, so let's do our best!\n",
      "\n",
      " What would you write given the requests above? \n",
      ">>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b43d135-414a-4b9b-b109-7b69eb24b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = openai(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cefc40-c915-4369-a54a-38d18aad72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../docs/Understanding/Overview/challenges_temp0.md'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write the file to disk with a _temp suffix and then open it with a system call to tkdiff to visualize the two\n",
    "# files side by side.\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import webbrowser\n",
    "\n",
    "def write_to_file(file_name, text):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    return file_name\n",
    "\n",
    "# Please be sure to run `homebrew install tkdiff` or otherwise install tkdiff on your computer\n",
    "def open_with_tkdiff(file_name1, file_name2):\n",
    "    subprocess.run(['tkdiff', file_name1, file_name2])\n",
    "\n",
    "def make_name(file_name):\n",
    "    base, ext = os.path.splitext(file_name)\n",
    "    temp_name = base + '_temp0' + ext\n",
    "    #check to see if it exists and if so, make a new name with a _temp# where # is the next available number\n",
    "    count=0\n",
    "    while os.path.exists(temp_name):\n",
    "        count += 1\n",
    "        \n",
    "        temp_name = base + f'_temp{count}' + ext\n",
    "    return temp_name\n",
    "temp_name = make_name(file_name)\n",
    "write_to_file(temp_name, markdown_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mopen_with_tkdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mopen_with_tkdiff\u001b[0;34m(file_name1, file_name2)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_with_tkdiff\u001b[39m(file_name1, file_name2):\n\u001b[0;32m---> 15\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtkdiff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/miniconda3/envs/genai/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "open_with_tkdiff(file_name, temp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../docs/Using/index.md has been updated.\n"
     ]
    }
   ],
   "source": [
    "input_answer = input(\"Is the output correct Yes/no/deletefile? (y/n/d)\")\n",
    "## if the answer is y then move the temp-name to the original file name and delete the temp file\n",
    "if input_answer == 'y':\n",
    "    os.rename(temp_name, file_name)\n",
    "    print(f\"File {file_name} has been updated.\")\n",
    "else:\n",
    "    print(f\"File {file_name} has not been updated.\")\n",
    "    # if the answer is n then delete the temp file and do nothing\n",
    "    if input_answer == 'd':\n",
    "        os.remove(temp_name)\n",
    "        print(f\"File {temp_name} has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT THAT LOOKS AT DIFFERENCES CHHUNK BY CHUNK AND AMENDS THEM. \n",
    "CREATE DIFF, ITERATE ON DIFF AND UPDATE MODIFIED DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
