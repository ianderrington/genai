{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iqyV7VcsiXT"
      },
      "source": [
        "![read_agent_teaser](https://read-agent.github.io/img/teaser.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From https://github.com/read-agent/read-agent.github.io/blob/main/assets/read_agent_demo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/nyu-mll/quality/raw/main/data/v1.0.1/QuALITY.v1.0.1.htmlstripped.dev\n",
        "import re, time, datetime, json, string, copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "oz0kOxYJ4n3e"
      },
      "outputs": [],
      "source": [
        "# # @title Using OpenAI GPT model (DO NOT run the next cell if using GPT)\n",
        "# # !pip3 install openai\n",
        "# import openai\n",
        "\n",
        "# # key = 'YOUR API KEY'  #@param {type: \"string\"}\n",
        "# import os\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()\n",
        "# key = os.getenv('OPENAI_API_KEY')\n",
        "# gpt_client = openai.OpenAI(api_key=key)\n",
        "# model_type = 'gpt'\n",
        "\n",
        "# def query_gpt_model(\n",
        "#     prompt: str,\n",
        "#     lm: str = 'gpt-3.5-turbo-1106',\n",
        "#     temperature: float = 0.0,\n",
        "#     max_decode_steps: int = 512,\n",
        "#     seconds_to_reset_tokens: float = 30.0,\n",
        "# ) -> str:\n",
        "#   while True:\n",
        "#     try:\n",
        "#       raw_response = gpt_client.chat.completions.with_raw_response.create(\n",
        "#         model=lm,\n",
        "        \n",
        "#         max_tokens=max_decode_steps,\n",
        "#         temperature=temperature,\n",
        "#         messages=[\n",
        "#           {'role': 'user', 'content': prompt},\n",
        "#         ]\n",
        "#       )\n",
        "#       completion = raw_response.parse()\n",
        "#       return completion.choices[0].message.content\n",
        "#     except openai.RateLimitError as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: RateLimitError {e.message}: {e}')\n",
        "#       time.sleep(seconds_to_reset_tokens)\n",
        "#     except openai.APIError as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: APIError {e.message}: {e}')\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: Retrying after 5 seconds...')\n",
        "#       time.s\n",
        "# leep(5)\n",
        "from genai.llms.simple_query import ModelManager\n",
        "\n",
        "model_manager = ModelManager()\n",
        "\n",
        "\n",
        "model_type = 'gpt-3.5-turbo-1106'\n",
        "llm = model_manager.get_llm(model_type )\n",
        "llm_query = llm.query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "YcP_tIpZKNFY"
      },
      "outputs": [],
      "source": [
        "# # @title Using Google Gemini model (DO NOT run this if using GPT)\n",
        "# !pip3 install -q -U google-generativeai\n",
        "# import google.generativeai as genai\n",
        "\n",
        "# key = 'YOUR API KEY'  #@param {type: \"string\"}\n",
        "\n",
        "# genai.configure(api_key=key)\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# model_type = 'gemini'\n",
        "\n",
        "# def query_gemini_model(\n",
        "#     prompt: str,\n",
        "#     retries: int = 10,\n",
        "# ) -> str:\n",
        "#   while True and retries > 0:\n",
        "#     try:\n",
        "#       response = model.generate_content(prompt)\n",
        "#       text_response = response.text.replace(\"**\", \"\")\n",
        "#       return text_response\n",
        "#     except Exception as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gemini_model: Error: {e}')\n",
        "#       print(f'{datetime.datetime.now()}: query_gemini_model: Retrying after 5 seconds...')\n",
        "#       retries -= 1\n",
        "#       time.sleep(5)\n",
        "# from genai.llms.simple_query import query_gemini_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pYm2GsBGEvAI"
      },
      "outputs": [],
      "source": [
        "# def query_model(prompt):\n",
        "#   if model_type == \"gpt\":\n",
        "#     return query_gpt_model(prompt)\n",
        "#   elif model_type == \"gemini\":\n",
        "#     return query_gemini_model(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "1B70Rqg97aXu"
      },
      "outputs": [],
      "source": [
        "#@title Load a QuALITY example\n",
        "\n",
        "# Fields that are straight text copies from raw example to processed example.\n",
        "_ONE2ONE_FIELDS = (\n",
        "    'article',\n",
        "    'article_id',\n",
        "    'set_unique_id',\n",
        "    'writer_id',\n",
        "    'source',\n",
        "    'title',\n",
        "    'topic',\n",
        "    'url',\n",
        "    'writer_id',\n",
        "    'author',\n",
        ")\n",
        "\n",
        "quality_dev = []\n",
        "\n",
        "with open('QuALITY.v1.0.1.htmlstripped.dev', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    j = json.loads(line)\n",
        "    fields = {k: j[k] for k in _ONE2ONE_FIELDS}\n",
        "    fields.update({\n",
        "        'questions': [q['question'] for q in j['questions']],\n",
        "        'question_ids': [q['question_unique_id'] for q in j['questions']],\n",
        "        'difficults': [q['difficult'] for q in j['questions']],\n",
        "        'options': [q['options'] for q in j['questions']],\n",
        "    })\n",
        "\n",
        "    fields.update({\n",
        "        'gold_labels': [q['gold_label'] for q in j['questions']],\n",
        "        'writer_labels': [q['writer_label'] for q in j['questions']],\n",
        "      })\n",
        "\n",
        "    quality_dev.append(fields)\n",
        "\n",
        "example = quality_dev[13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "nQsb3n6pOlz2"
      },
      "outputs": [],
      "source": [
        "# #@title Helper functions\n",
        "\n",
        "# all_lowercase_letters = string.ascii_lowercase  # \"abcd...xyz\"\n",
        "# bracketed_lowercase_letters_set = set(\n",
        "#     [f\"({l})\" for l in all_lowercase_letters]\n",
        "# )  # {\"(a)\", ...}\n",
        "# bracketed_uppercase_letters_set = set(\n",
        "#     [f\"({l.upper()})\" for l in all_lowercase_letters]\n",
        "# )  # {\"(a)\", ...}\n",
        "\n",
        "# choices = ['(A)', '(B)', '(C)', '(D)']\n",
        "\n",
        "# def get_index_from_symbol(answer):\n",
        "#   \"\"\"Get the index from the letter symbols A, B, C, D, to extract answer texts.\n",
        "\n",
        "#   Args:\n",
        "#     answer (str): the string of answer like \"(B)\".\n",
        "\n",
        "#   Returns:\n",
        "#     index (int): how far the given choice is from \"a\", like 1 for answer \"(B)\".\n",
        "#   \"\"\"\n",
        "#   answer = str(answer).lower()\n",
        "#   # extract the choice letter from within bracket\n",
        "#   if answer in bracketed_lowercase_letters_set:\n",
        "#     answer = re.findall(r\"\\(.*?\\)\", answer)[0][1]\n",
        "#   index = ord(answer) - ord(\"a\")\n",
        "#   return index\n",
        "\n",
        "# def count_words(text):\n",
        "#   \"\"\"Simple word counting.\"\"\"\n",
        "#   return len(text.split())\n",
        "\n",
        "# def quality_gutenberg_parser(raw_article):\n",
        "#   \"\"\"Parse Gutenberg articles in the QuALITY dataset.\"\"\"\n",
        "#   lines = []\n",
        "#   previous_line = None\n",
        "#   for i, line in enumerate(raw_article.split('\\n')):\n",
        "#     line = line.strip()\n",
        "#     original_line = line\n",
        "#     if line == '':\n",
        "#       if previous_line == '':\n",
        "#         line = '\\n'\n",
        "#       else:\n",
        "#         previous_line = original_line\n",
        "#         continue\n",
        "#     previous_line = original_line\n",
        "#     lines.append(line)\n",
        "#   return ' '.join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BfFkEQKx0u9U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "# #@title ReadAgent (1) Episode Pagination\n",
        "\n",
        "# prompt_pagination_template = \"\"\"\n",
        "# You are given a passage that is taken from a larger text (article, book, ...) and some numbered labels between the paragraphs in the passage.\n",
        "# Numbered label are in angeled brackets. For example, if the label number is 19, it shows as <19> in text.\n",
        "# Please choose one label that it is natural to break reading.\n",
        "# Such point can be scene transition, end of a dialogue, end of an argument, narrative transition, etc.\n",
        "# Please answer the break point label and explain.\n",
        "# For example, if <57> is a good point to break, answer with \\\"Break point: <57>\\n Because ...\\\"\n",
        "\n",
        "# Passage:\n",
        "\n",
        "# {0}\n",
        "# {1}\n",
        "# {2}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def parse_pause_point(text):\n",
        "#   text = text.strip(\"Break point: \")\n",
        "#   if text[0] != '<':\n",
        "#     return None\n",
        "#   for i, c in enumerate(text):\n",
        "#     if c == '>':\n",
        "#       if text[1:i].isnumeric():\n",
        "#         return int(text[1:i])\n",
        "#       else:\n",
        "#         return None\n",
        "#   return None\n",
        "\n",
        "\n",
        "# def quality_pagination(example,\n",
        "#                        word_limit=600,\n",
        "#                        start_threshold=280,\n",
        "#                        max_retires=10,\n",
        "#                        verbose=True,\n",
        "#                        allow_fallback_to_last=True):\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   print(f\"[Pagination][Article {title}]\")\n",
        "#   paragraphs = quality_gutenberg_parser(article).split('\\n')\n",
        "\n",
        "#   i = 0\n",
        "#   pages = []\n",
        "#   while i < len(paragraphs):\n",
        "#     preceding = \"\" if i == 0 else \"...\\n\" + '\\n'.join(pages[-1])\n",
        "#     passage = [paragraphs[i]]\n",
        "#     wcount = count_words(paragraphs[i])\n",
        "#     j = i + 1\n",
        "#     while wcount < word_limit and j < len(paragraphs):\n",
        "#       wcount += count_words(paragraphs[j])\n",
        "#       if wcount >= start_threshold:\n",
        "#         passage.append(f\"<{j}>\")\n",
        "#       passage.append(paragraphs[j])\n",
        "#       j += 1\n",
        "#     passage.append(f\"<{j}>\")\n",
        "#     end_tag = \"\" if j == len(paragraphs) else paragraphs[j] + \"\\n...\"\n",
        "\n",
        "#     pause_point = None\n",
        "#     if wcount < 350:\n",
        "#       pause_point = len(paragraphs)\n",
        "#     else:\n",
        "#       prompt = prompt_pagination_template.format(preceding, '\\n'.join(passage), end_tag)\n",
        "#       response = query_model(prompt=prompt).strip()\n",
        "#       pause_point = parse_pause_point(response)\n",
        "#       if pause_point and (pause_point <= i or pause_point > j):\n",
        "#         print(f\"prompt:\\n{prompt},\\nresponse:\\n{response}\\n\")\n",
        "#         print(f\"i:{i} j:{j} pause_point:{pause_point}\")\n",
        "#         pause_point = None\n",
        "#       if pause_point is None:\n",
        "#         if allow_fallback_to_last:\n",
        "#           pause_point = j\n",
        "#         else:\n",
        "#           raise ValueError(f\"prompt:\\n{prompt},\\nresponse:\\n{response}\\n\")\n",
        "\n",
        "#     page = paragraphs[i:pause_point]\n",
        "#     pages.append(page)\n",
        "#     if verbose:\n",
        "#       print(f\"Paragraph {i}-{pause_point-1}\", page)\n",
        "#     i = pause_point\n",
        "#   print(f\"[Pagination] Done with {len(pages)} pages\")\n",
        "#   return pages\n",
        "\n",
        "# pages = quality_pagination(example, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DLBolKnkS_9y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Gisting][Article Off Course], 2712 words\n",
            "[gist] page 0: The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "[gist] page 1: Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "[gist] page 2: Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "[gist] page 3: Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "[gist] page 4: The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "[gist] page 5: Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            "[gist] page 6: Andersen, Sir Alfred, and McCord reassured Dameri Tass and made him comfortable in a chair. Andersen faced the audience and announced that they would now hear from the first being to come to Earth from another world. Dameri Tass, the alien, was surprised and worried, realizing he had made a mistake. He confessed that he was not an envoy from another planet, but a collector of specimens for a zoo. He quickly left the speaker's stand, saying he needed to leave immediately. President McCord signaled to the bodyguard to accompany the alien.\n",
            "[gist] page 7: Viljalmar Andersen tried to speak to Dameri Tass, but Tass refused and left with his pets.\n",
            "[gist] page 8: The alien said he almost forgot about taking creatures from Earth, but decided not to take the dog, cat, or horse. He expressed a desire to take a horse back to his planet, but ultimately left Earth in his spacecraft.\n",
            "Shortened article:\n",
            " The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            "Andersen, Sir Alfred, and McCord reassured Dameri Tass and made him comfortable in a chair. Andersen faced the audience and announced that they would now hear from the first being to come to Earth from another world. Dameri Tass, the alien, was surprised and worried, realizing he had made a mistake. He confessed that he was not an envoy from another planet, but a collector of specimens for a zoo. He quickly left the speaker's stand, saying he needed to leave immediately. President McCord signaled to the bodyguard to accompany the alien.\n",
            "Viljalmar Andersen tried to speak to Dameri Tass, but Tass refused and left with his pets.\n",
            "The alien said he almost forgot about taking creatures from Earth, but decided not to take the dog, cat, or horse. He expressed a desire to take a horse back to his planet, but ultimately left Earth in his spacecraft.\n",
            "compression rate 86.5% (366/2712)\n"
          ]
        }
      ],
      "source": [
        "# #@title ReadAgent (2) Memory Gisting\n",
        "\n",
        "# prompt_shorten_template = \"\"\"\n",
        "# Please shorten the following passage.\n",
        "# Just give me a shortened version. DO NOT explain your reason.\n",
        "\n",
        "# Passage:\n",
        "# {}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def quality_gisting(example, pages, word_limit=600, start_threshold=280, verbose=True):\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   word_count = count_words(article)\n",
        "#   print(f\"[Gisting][Article {title}], {word_count} words\")\n",
        "\n",
        "#   shortened_pages = []\n",
        "#   for i, page in enumerate(pages):\n",
        "#     prompt = prompt_shorten_template.format('\\n'.join(page))\n",
        "#     response = query_model(prompt)\n",
        "#     shortened_text = response.strip()\n",
        "#     shortened_pages.append(shortened_text)\n",
        "#     if verbose:\n",
        "#       print(\"[gist] page {}:\".format(i), shortened_text, flush=True)\n",
        "#   shortened_article = '\\n'.join(shortened_pages)\n",
        "#   gist_word_count = count_words(shortened_article)\n",
        "#   if verbose:\n",
        "#     print(\"Shortened article:\\n\", shortened_article, flush=True)\n",
        "#   output = copy.deepcopy(example)\n",
        "#   output.update({'title': title, 'word_count': word_count, 'gist_word_count': gist_word_count, 'shortened_pages': shortened_pages, 'pages': pages})\n",
        "#   if verbose:\n",
        "#     print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "#   return output\n",
        "from genai.text_processes.gisting import quality_gisting\n",
        "example_with_gists = quality_gisting(example, pages, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genai.text_processes.utils import count_words \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Look-Up][Article Off Course] 2712 words\n",
            "question:  What happened to Dameri while he was in custody of the government?\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'choices' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mquality_parallel_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_with_gists\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 66\u001b[0m, in \u001b[0;36mquality_parallel_lookup\u001b[0;34m(example, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m q \u001b[38;5;241m=\u001b[39m questions[i]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m, q)\n\u001b[0;32m---> 66\u001b[0m options_i \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ol, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mchoices\u001b[49m, options[i])]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(options_i))\n\u001b[1;32m     68\u001b[0m prompt_lookup \u001b[38;5;241m=\u001b[39m prompt_lookup_template\u001b[38;5;241m.\u001b[39mformat(shortened_article, q, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(options_i))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'choices' is not defined"
          ]
        }
      ],
      "source": [
        "#@title ReadAgent (3) Look-Up\n",
        "\n",
        "prompt_lookup_template = \"\"\"\n",
        "The following text is what you remembered from reading an article and a multiple choice question related to it.\n",
        "You may read 1 to 6 page(s) of the article again to refresh your memory to prepare yourselve for the question.\n",
        "Please respond with which page(s) you would like to read.\n",
        "For example, if your only need to read Page 8, respond with \\\"I want to look up Page [8] to ...\\\";\n",
        "if your would like to read Page 7 and 12, respond with \\\"I want to look up Page [7, 12] to ...\\\";\n",
        "if your would like to read Page 2, 3, 7, 15 and 18, respond with \\\"I want to look up Page [2, 3, 7, 15, 18] to ...\\\".\n",
        "if your would like to read Page 3, 4, 5, 12, 13 and 16, respond with \\\"I want to look up Page [3, 3, 4, 12, 13, 16] to ...\\\".\n",
        "DO NOT select more pages if you don't need to.\n",
        "DO NOT answer the question yet.\n",
        "\n",
        "Text:\n",
        "{}\n",
        "\n",
        "Question:\n",
        "{}\n",
        "{}\n",
        "\n",
        "Take a deep breath and tell me: Which page(s) would you like to read again?\n",
        "\"\"\"\n",
        "\n",
        "prompt_answer_template = \"\"\"\n",
        "Read the following article and answer a multiple choice question.\n",
        "For example, if (C) is correct, answer with \\\"Answer: (C) ...\\\"\n",
        "\n",
        "Article:\n",
        "{}\n",
        "\n",
        "Question:\n",
        "{}\n",
        "{}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def quality_parallel_lookup(example, verbose=True):\n",
        "  preprocessed_pages = example['pages']\n",
        "  article = example['article']\n",
        "  title = example['title']\n",
        "  word_count = example['word_count']\n",
        "  gist_word_count = example['gist_word_count']\n",
        "  pages = example['pages']\n",
        "  shortened_pages = example['shortened_pages']\n",
        "  questions = example['questions']\n",
        "  options = example['options']\n",
        "  gold_labels = example['gold_labels']  # numerical [1, 2, 3, 4]\n",
        "\n",
        "  print(f\"[Look-Up][Article {title}] {word_count} words\")\n",
        "\n",
        "  model_choices = []\n",
        "  lookup_page_ids = []\n",
        "\n",
        "  shortened_pages_pidx = []\n",
        "  for i, shortened_text in enumerate(shortened_pages):\n",
        "    shortened_pages_pidx.append(\"<Page {}>\\n\".format(i) + shortened_text)\n",
        "  shortened_article = '\\n'.join(shortened_pages_pidx)\n",
        "\n",
        "  expanded_gist_word_counts = []\n",
        "  for i, label in enumerate(gold_labels):\n",
        "    # only test the first question for demo\n",
        "    if i != 1:\n",
        "      continue\n",
        "    q = questions[i]\n",
        "    print(\"question: \", q)\n",
        "    options_i = [f\"{ol} {o}\" for ol, o in zip(choices, options[i])]\n",
        "    print(\"options: \", \"\\n\".join(options_i))\n",
        "    prompt_lookup = prompt_lookup_template.format(shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "    page_ids = []\n",
        "\n",
        "    response = query_model(prompt=prompt_lookup).strip()\n",
        "\n",
        "    try: start = response.index('[')\n",
        "    except ValueError: start = len(response)\n",
        "    try: end = response.index(']')\n",
        "    except ValueError: end = 0\n",
        "    if start < end:\n",
        "      page_ids_str = response[start+1:end].split(',')\n",
        "      page_ids = []\n",
        "      for p in page_ids_str:\n",
        "        if p.strip().isnumeric():\n",
        "          page_id = int(p)\n",
        "          if page_id < 0 or page_id >= len(pages):\n",
        "            print(\"Skip invalid page number: \", page_id, flush=True)\n",
        "          else:\n",
        "            page_ids.append(page_id)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Model chose to look up page {}\".format(page_ids))\n",
        "\n",
        "    # Memory expansion after look-up, replacing the target shortened page with the original page\n",
        "    expanded_shortened_pages = shortened_pages[:]\n",
        "    if len(page_ids) > 0:\n",
        "      for page_id in page_ids:\n",
        "        expanded_shortened_pages[page_id] = '\\n'.join(pages[page_id])\n",
        "\n",
        "    expanded_shortened_article = '\\n'.join(expanded_shortened_pages)\n",
        "    expanded_gist_word_count = count_words(expanded_shortened_article)\n",
        "    if verbose:\n",
        "      print(\"Expanded shortened article:\\n\", expanded_shortened_article, flush=True)\n",
        "    prompt_answer = prompt_answer_template.format(expanded_shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "    model_choice = None\n",
        "    response = llm_query(prompt=prompt_answer)\n",
        "    response = response.strip()\n",
        "    for j, choice in enumerate(choices):\n",
        "      if response.startswith(f\"Answer: {choice}\") or response.startswith(f\"Answer: {choice[1]}\"):\n",
        "        model_choice = j+1\n",
        "        break\n",
        "    is_correct = 1 if model_choice == label else 0\n",
        "    print(f\"question: {q}\")\n",
        "    print(f\"reference answer: {choices[label]}, model prediction: {choices[model_choice]}, is_correct: {is_correct}\")\n",
        "    print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "    print(f\"compression rate after look-up {round(100.0 - expanded_gist_word_count/word_count*100, 2)}% ({expanded_gist_word_count}/{word_count})\")\n",
        "\n",
        "quality_parallel_lookup(example_with_gists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8YKNTyDsXNIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Look-Up][Article Off Course] 2712 words\n",
            "question:  What happened to Dameri while he was in custody of the government?\n",
            "options:  \n",
            "Model chose to look up page [6, 7, 8]\n",
            "Expanded shortened article:\n",
            " The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            " \"I wish we knew what he was going to say,\" Andersen worried. \n",
            " \"Here he comes,\" said Sir Alfred. \n",
            " Surrounded by F.B.I. men, Dameri Tass was ushered to the speaker's stand. He had a kitten in his arms; a Scotty followed him. \n",
            " The alien frowned worriedly. \"Sure,\" he said, \"and what kin all this be? Is it some ordinance I've been after breakin'?\" \n",
            " McCord, Sir Alfred and Andersen hastened to reassure him and made him comfortable in a chair. \n",
            " Viljalmar Andersen faced the thousands in the audience and held up his hands, but it was ten minutes before he was able to quiet the cheering, stamping delegates from all Earth. \n",
            " Finally: \"Fellow Terrans, I shall not take your time for a lengthy introduction of the envoy from the stars. I will only say that, without doubt, this is the most important moment in the history of the human race. We will now hear from the first being to come to Earth from another world.\" \n",
            " He turned and gestured to Dameri Tass who hadn't been paying overmuch attention to the chairman in view of some dog and cat hostilities that had been developing about his feet. \n",
            " But now the alien's purplish face faded to a light blue. He stood and said hoarsely. \"Faith, an' what was that last you said?\" \n",
            " Viljalmar Andersen repeated, \"We will now hear from the first being ever to come to Earth from another world.\" \n",
            " The face of the alien went a lighter blue. \"Sure, an' ye wouldn't jist be frightenin' a body, would ye? You don't mean to tell me this planet isn't after bein' a member of the Galactic League?\" \n",
            " Andersen's face was blank. \"Galactic League?\" \n",
            " \"Cushlamachree,\" Dameri Tass moaned. \"I've gone and put me foot in it again. I'll be after getting kert for this.\" \n",
            " Sir Alfred was on his feet. \"I don't understand! Do you mean you aren't an envoy from another planet?\" \n",
            " Dameri Tass held his head in his hands and groaned. \"An envoy, he's sayin', and meself only a second-rate collector of specimens for the Carthis zoo.\" \n",
            " He straightened and started off the speaker's stand. \"Sure, an' I must blast off immediately.\" \n",
            " Things were moving fast for President McCord but already an edge of relief was manifesting itself. Taking the initiative, he said, \"Of course, of course, if that is your desire.\" He signaled to the bodyguard who had accompanied the alien to the assemblage. \n",
            " A dull roar was beginning to emanate from the thousands gathered in the tremendous hall, murmuring, questioning, disbelieving. Viljalmar Andersen felt that he must say something. He extended a detaining hand. \"Now you are here,\" he said urgently, \"even though by mistake, before you go can't you give us some brief word? Our world is in chaos. Many of us have lost faith. Perhaps ...\" \n",
            " Dameri Tass shook off the restraining hand. \"Do I look daft? Begorry, I should have been a-knowin' something was queer. All your weapons and your strange ideas. Faith, I wouldn't be surprised if ye hadn't yet established a planet-wide government. Sure, an' I'll go still further. Ye probably still have wars on this benighted world. No wonder it is ye haven't been invited to join the Galactic League an' take your place among the civilized planets.\" \n",
            " He hustled from the rostrum and made his way, still surrounded by guards, to the door by which he had entered. The dog and the cat trotted after, undismayed by the furor about them. \n",
            " They arrived about four hours later at the field on which he'd landed, and the alien from space hurried toward his craft, still muttering. He'd been accompanied by a general and by the President, but all the way he had refrained from speaking. \n",
            " He scurried from the car and toward the spacecraft. \n",
            " President McCord said, \"You've forgotten your pets. We would be glad if you would accept them as—\" \n",
            " The alien's face faded a light blue again. \"Faith, an' I'd almost forgotten,\" he said. \"If I'd taken a crature from this quarantined planet, my name'd be nork . Keep your dog and your kitty.\" He shook his head sadly and extracted a mouse from a pocket. \"An' this amazin' little crature as well.\" \n",
            " They followed him to the spacecraft. Just before entering, he spotted the bedraggled horse that had been present on his landing. \n",
            " A longing expression came over his highly colored face. \"Jist one thing,\" he said. \"Faith now, were they pullin' my leg when they said you were after ridin' on the back of those things?\" \n",
            " The President looked at the woebegone nag. \"It's a horse,\" he said, surprised. \"Man has been riding them for centuries.\" \n",
            " Dameri Tass shook his head. \"Sure, an' 'twould've been my makin' if I could've taken one back to Carthis.\" He entered his vessel. \n",
            " The others drew back, out of range of the expected blast, and watched, each with his own thoughts, as the first visitor from space hurriedly left Earth. ... THE END Transcriber's Note: This etext was produced from If Worlds of Science Fiction January 1954. Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed. Minor spelling and typographical errors have been corrected without note.\n",
            "question: What happened to Dameri while he was in custody of the government?\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mquality_parallel_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_with_gists\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 114\u001b[0m, in \u001b[0;36mquality_parallel_lookup\u001b[0;34m(example, verbose)\u001b[0m\n\u001b[1;32m    112\u001b[0m is_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m label \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_choices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, model prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_choices[model_choice]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is_correct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# #@title ReadAgent (3) Look-Up\n",
        "\n",
        "# prompt_lookup_template = \"\"\"\n",
        "# The following text is what you remembered from reading an article and a multiple choice question related to it.\n",
        "# You may read 1 to 6 page(s) of the article again to refresh your memory to prepare yourselve for the question.\n",
        "# Please respond with which page(s) you would like to read.\n",
        "# For example, if your only need to read Page 8, respond with \\\"I want to look up Page [8] to ...\\\";\n",
        "# if your would like to read Page 7 and 12, respond with \\\"I want to look up Page [7, 12] to ...\\\";\n",
        "# if your would like to read Page 2, 3, 7, 15 and 18, respond with \\\"I want to look up Page [2, 3, 7, 15, 18] to ...\\\".\n",
        "# if your would like to read Page 3, 4, 5, 12, 13 and 16, respond with \\\"I want to look up Page [3, 3, 4, 12, 13, 16] to ...\\\".\n",
        "# DO NOT select more pages if you don't need to.\n",
        "# DO NOT answer the question yet.\n",
        "\n",
        "# Text:\n",
        "# {}\n",
        "\n",
        "# Question:\n",
        "# {}\n",
        "# {}\n",
        "\n",
        "# Take a deep breath and tell me: Which page(s) would you like to read again?\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_answer_template = \"\"\"\n",
        "# Read the following article and answer a multiple choice question.\n",
        "# For example, if (C) is correct, answer with \\\"Answer: (C) ...\\\"\n",
        "\n",
        "# Article:\n",
        "# {}\n",
        "\n",
        "# Question:\n",
        "# {}\n",
        "# {}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def quality_parallel_lookup(example, verbose=True):\n",
        "#   preprocessed_pages = example['pages']\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   word_count = example['word_count']\n",
        "#   gist_word_count = example['gist_word_count']\n",
        "#   pages = example['pages']\n",
        "#   shortened_pages = example['shortened_pages']\n",
        "#   questions = example['questions']\n",
        "#   options = example['options']\n",
        "#   gold_labels = example['gold_labels']  # numerical [1, 2, 3, 4]\n",
        "\n",
        "#   print(f\"[Look-Up][Article {title}] {word_count} words\")\n",
        "\n",
        "#   model_choices = []\n",
        "#   lookup_page_ids = []\n",
        "\n",
        "#   shortened_pages_pidx = []\n",
        "#   for i, shortened_text in enumerate(shortened_pages):\n",
        "#     shortened_pages_pidx.append(\"<Page {}>\\n\".format(i) + shortened_text)\n",
        "#   shortened_article = '\\n'.join(shortened_pages_pidx)\n",
        "\n",
        "#   expanded_gist_word_counts = []\n",
        "#   for i, label in enumerate(gold_labels):\n",
        "#     # only test the first question for demo\n",
        "#     if i != 1:\n",
        "#       continue\n",
        "#     q = questions[i]\n",
        "#     print(\"question: \", q)\n",
        "#     options_i = [f\"{ol} {o}\" for ol, o in zip(model_choices, options[i])]\n",
        "#     print(\"options: \", \"\\n\".join(options_i))\n",
        "#     prompt_lookup = prompt_lookup_template.format(shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "#     page_ids = []\n",
        "\n",
        "#     response = (prompt=prompt_lookup).strip()\n",
        "\n",
        "\n",
        "#     try: start = response.index('[')\n",
        "#     except ValueError: start = len(response)\n",
        "#     try: end = response.index(']')\n",
        "#     except ValueError: end = 0\n",
        "#     if start < end:\n",
        "#       page_ids_str = response[start+1:end].split(',')\n",
        "#       page_ids = []\n",
        "#       for p in page_ids_str:\n",
        "#         if p.strip().isnumeric():\n",
        "#           page_id = int(p)\n",
        "#           if page_id < 0 or page_id >= len(pages):\n",
        "#             print(\"Skip invalid page number: \", page_id, flush=True)\n",
        "#           else:\n",
        "#             page_ids.append(page_id)\n",
        "\n",
        "#     if verbose:\n",
        "#       print(\"Model chose to look up page {}\".format(page_ids))\n",
        "\n",
        "#     # Memory expansion after look-up, replacing the target shortened page with the original page\n",
        "#     expanded_shortened_pages = shortened_pages[:]\n",
        "#     if len(page_ids) > 0:\n",
        "#       for page_id in page_ids:\n",
        "#         expanded_shortened_pages[page_id] = '\\n'.join(pages[page_id])\n",
        "\n",
        "#     expanded_shortened_article = '\\n'.join(expanded_shortened_pages)\n",
        "#     expanded_gist_word_count = count_words(expanded_shortened_article)\n",
        "#     if verbose:\n",
        "#       print(\"Expanded shortened article:\\n\", expanded_shortened_article, flush=True)\n",
        "#     prompt_answer = prompt_answer_template.format(expanded_shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "#     model_choice = None\n",
        "#     response = llm_query(prompt=prompt_answer)\n",
        "#     response = response.strip()\n",
        "#     for j, choice in enumerate(model_choices):\n",
        "#       if response.startswith(f\"Answer: {choice}\") or response.startswith(f\"Answer: {choice[1]}\"):\n",
        "#         model_choice = j+1\n",
        "#         break\n",
        "#     is_correct = 1 if model_choice == label else 0\n",
        "#     print(f\"question: {q}\")\n",
        "#     print(f\"reference answer: {model_choices[label]}, model prediction: {model_choices[model_choice]}, is_correct: {is_correct}\")\n",
        "#     print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "#     print(f\"compression rate after look-up {round(100.0 - expanded_gist_word_count/word_count*100, 2)}% ({expanded_gist_word_count}/{word_count})\")\n",
        "\n",
        "# quality_parallel_lookup(example_with_gists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEMPLATE_FOR_MACHINE_LEARNING = {\"Developments\": \n",
        "    \"This briefly describes the notable and important developments due to the paper. They should be high level and \"\n",
        "    \"not too detailed. If there are multiple developments, they should be separated by bullet points.\",\n",
        "    \"Data\": \n",
        "    \" This should be a brief description of the data used in the paper. It might include the data-source names, \"\n",
        "    \" any cleaning they did, and any new data they collected.\",\n",
        "    \"Models\":\n",
        "    \"This should be a brief description of the model used in the paper. It should include the type of models used, \"\n",
        "    \"if any new architectures were introduced, and any new techniques used in the model.\",\n",
        "    \"Training\": \n",
        "    \"This should be a brief description of the training process used in the paper. It should include the optimizer used, \"\n",
        "    \"the learning rate, the batch sizes, or other things that are important to the training process, \"\n",
        "    \" such as training from scratch or transfer learning.\",\n",
        "    \"Results\":\n",
        "    \"This should be a brief description of the results of the paper. It should include the evaluation metrics used, \"\n",
        "    \"the performance of the model, and any comparisons to other models or baselines.\",\n",
        "    \n",
        "    }\n",
        "\n",
        "\n",
        "def gisting_with_template(smart_pages, query_model, \n",
        "    template=TEMPLATE_FOR_MACHINE_LEARNING, verebose=True):\n",
        "    \"\"\" This takes in a function template of items that it wants to be\n",
        "     sketched into separate focused 'gist' summaries about athat particular topic. \n",
        "     If a chunk has nothing to add to a a particular focus of the template, nothing is added to any subject-focused gist.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_shorten_template = \"\"\"\n",
        "    Please shorten the following passage, focusing on the topic of {topic}, with some considerations such as the following\n",
        "    \n",
        "    {considerations}\n",
        "    \n",
        "    Just give me a shortened version if there is relevant information by saying\n",
        "    \n",
        "    {topic}: \n",
        "\n",
        "    and following it with the shortened text.\n",
        "    \n",
        "    DO NOT explain your reasoning or add unecessary information.\n",
        "\n",
        "    Passage:\n",
        "    {passage}\n",
        "\n",
        "    \"\"\"\n",
        "    # article = example['article']\n",
        "    # title = example['title']\n",
        "    # word_count = count_words(article)\n",
        "    # print(f\"[Gisting][Article {title}], {word_count} words\")\n",
        "    # pages = example['pages']\n",
        "    gist = {key: [] for key in template.keys()}\n",
        "    for i, page in enumerate(smart_pages):\n",
        "        for key, template_text in template.items():\n",
        "            prompt = prompt_shorten_template.format(topic=key, considerations = template_text, passage='\\n'.join(page))\n",
        "            response = query_model(prompt)\n",
        "            shortened_text = response.strip()\n",
        "            if key in shortened_text:\n",
        "                gist[key].append(shortened_text)\n",
        "            # prompt = prompt_shorten_template.format(, '\\n'.join(page))\n",
        "            # response = query_model(prompt)\n",
        "            # shortened_text = response.strip()\n",
        "            # for key, template_text in template.items():\n",
        "            key_marker = key + ':'\n",
        "\n",
        "            if shortened_text.startswith(key_marker):\n",
        "                gist[key].append(shortened_text[len(key_marker):].strip())\n",
        "        # if verbose:\n",
        "        #     print(\"[gist] page {}:\".format(i), shortened_text, flush=True)\n",
        "    # gist_word_count = {key: count_words(' '.join(value)) for key, value in gist.items()}\n",
        "    # if verbose:\n",
        "    #     print(\"Shortened article:\\n\", gist, flush=True)\n",
        "    # output = copy.deepcopy(example)\n",
        "    # output.update({'title': title, 'word_count': word_count, 'gist_word_count': gist_word_count, 'gist': gist, 'pages': pages})\n",
        "    # if verbose:\n",
        "    #     print(f\"compression rate {round(100.0 - sum(gist_word_count.values())/word_count*100, 2)}% ({sum(gist_word_count.values())}/{word_count})\")\n",
        "    return gist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genai.text_processes.pagination import quality_gutenberg_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "\n",
        "\n",
        "# file = 'test_text.txt'\n",
        "# with open(file, 'r') as f:\n",
        "#     text = f.read()\n",
        "# example = {'article': text, 'title': 'Test Article'}\n",
        "# pages = quality_pagination(example, query_model=llm_query)\n",
        "\n",
        "# We will need to create something that will be able to look at arxiv .tex folders/files and parse the pages in a way that \n",
        "# allow us to do a gist on each one of the different files in the order that they are presented in the paper.\n",
        "\n",
        "folder = '../text_processes/arXiv-2307.06435v7'\n",
        "\n",
        "def get_text_pages_from_tex_folder(tex_folder):\n",
        "    # look for the main.tex\n",
        "    # Get the /sections/ in order, \n",
        "    # get the text from each section and return those as separate pages.\n",
        "\n",
        "    main_tex = os.path.join(tex_folder, 'main.tex')\n",
        "    with open(main_tex, 'r') as f:\n",
        "        main_text = f.read()\n",
        "    # get the sections for example:\n",
        "    # \\input{sections/introduction} \n",
        "    sections = re.findall(r'\\\\input{sections/(.*?)}', main_text)\n",
        "    section_texts = []\n",
        "    for section in sections:\n",
        "        if section.endswith('.tex'):\n",
        "            section = section[:-4]\n",
        "        section_file = os.path.join(tex_folder, 'sections', section + '.tex')\n",
        "        with open(section_file, 'r') as f:\n",
        "            section_text = f.read()\n",
        "        section_texts.append(section_text)\n",
        "    return section_texts\n",
        "\n",
        "section_texts = get_text_pages_from_tex_folder(folder)\n",
        "\n",
        "smart_pages = []\n",
        "for i,  section_text in enumerate(section_texts):\n",
        "    # print(i)\n",
        "    # smart_pages = section_text.split('\\n')\n",
        "    smart_pages = quality_gutenberg_parser(section_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\\\section{Summary and Discussion}\\n\\\\subsection{Architecture}\\nDue to the gigantic scale of LLMs, minor changes in architecture and training strategies have a big impact on performance and stability. Here, we summarize key architectural modules used in various LLMs, leading to better performance, reduced training time and memory, and better training stability. \\\\\\\\\\n\\\\emph{\\\\textbf{Layer Normalization}} is found to have a significant effect on the performance and training stability of LLMs. Pre-norm, that is normalizing inputs rather than outputs, is more common among LLMs stabilizing the training~\\\\cite{GPT-3, touvron2023llama, PanGU_alpha}. BLOOM~\\\\cite{BLOOM} and AlexaTM~\\\\cite{soltan2022alexatm} utilize an additional layer normalization before embedding layer to stabilize the training of large-scale models, while the model's zero-shot generalization ability can be negatively impacted~\\\\cite{BLOOM}. However, another study~\\\\cite{GLM-130B} finds that pre-norm degrades fine-tuned model performance as compared to post-norm, and there are no stability benefits of pre-norm beyond the 100B scale. Therefore, GLM-130B~\\\\cite{GLM-130B} used deep-norm which is a variant of post-norm for better downstream task performance after fine-tuning.   \\\\\\\\\\n\\\\emph{\\\\textbf{Positional Encoding}} effect performance and training stability of LLMs like other building blocks of a model. BLOOM~\\\\cite{BLOOM} finds ALiBi outperforming learned and rotary positional encodings. Contrary to this, GLM-130B~\\\\cite{GLM-130B} identifies rotary positional encoding better than ALiBi. So, there is no conclusion in literature about the positional encodings yet. \\\\\\\\\\n\\\\emph{\\\\textbf{Parallel Attention}} where attention and feed-forward layers are parallel to each other rather than sequential in transformer block has shown to reduce training time by 15\\\\%. There is no evidence of performance drop due to this change in literature and used by the models PaLM~\\\\cite{PaLM}, GPT-NeoX~\\\\cite{GPT_NeoX}, and CodeGen~\\\\cite{CodeGen}.  \\\\\\\\\\n\\\\emph{\\\\textbf{Multi-Query Attention}} has shared key and value attention heads in a transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degradation has been observed with this change and makes the training efficient allowing larger batch sizes. Multi-query attention is used in~\\\\cite{PaLM, li2022competition}.    \\\\\\\\\\n\\\\emph{\\\\textbf{Mixture of Experts}} allows scaling model to trillion of parameters easily~\\\\cite{PanGu_sigma, du2022glam}. Only a few experts are activated during the computation making them compute-efficient. The performance of MoE models is better than the dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to the dense models as discussed in~\\\\cite{du2022glam}. MoE architectures are less prone to catastrophic forgetting, therefore are more suited for continual learning~\\\\cite{PanGu_sigma}. Extracting smaller sub-models for downstream tasks is possible without losing any performance, making MoE architecture hardware-friendly~\\\\cite {PanGu_sigma}.    \\\\\\\\\\n\\\\emph{\\\\textbf{Sparse vs Dense Activated}}\\nGPT-3~\\\\cite{GPT-3} uses sparse transformers~\\\\cite{sparse_transformer} whereas GLaM~\\\\cite{du2022glam} and PanGu-$\\\\sum$~\\\\cite{PanGu_sigma} use MoE~\\\\cite{shazeer2017outrageously} architecture to lower computational costs and increase the model size and capacity. According to the literature, sparse modules do not degrade the model's performance~\\\\cite{sparse_transformer}. However, more experiments are required to verify this statement.  \\\\\\\\\\n\\n\\\\subsection{Training Strategies}\\nTraining models at a huge scale require some tricks to reduce training costs, avoid loss divergence and achieve better performance. We summarize and discuss some of these key tricks used in different LLMs. \\\\\\\\\\n\\\\emph{\\\\textbf{Mixed Precision}} is a famous method for LLMs to reduce memory usage and improve training efficiency. In mixed precision, forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format~\\\\cite{Mixed_Precision}. A drawback associated with this format change is training instability due to a smaller value range resulting in loss spikes~\\\\cite{GLM-130B}. An alternative to FP16 is BF16 which has a comparatively larger range and performs some precision-sensitive operations like gradient accumulation and softmax in FP32~\\\\cite{BLOOM}. BF16 has better performance and training stability but uses more memory and is supported on specific hardware, for example, A100 GPUs. Therefore, its adoption in LLMs is limited. \\\\\\\\ %\\\\textcolor{blue}{for instance, the GLaM~\\\\cite{du2022glam} model adopts this method for training, where FP32 is used for model weights and BF16 is used for activations.}  \\\\\\\\\\n\\\\emph{\\\\textbf{Training Instability}} is a common issue in LLMs where loss divergence or spiking is observed multiple times during training. This happens in the presence of gradient clipping~\\\\cite{PaLM}. To mitigate this problem, many approaches suggest restarting training from an earlier checkpoint~\\\\cite{PaLM, GLM-130B, du2022glam}, skipping 200-500 earlier data batches at the point of divergence in~\\\\cite{PaLM} and re-shuffling batches in~\\\\cite{du2022glam}. The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers~\\\\cite{GLM-130B}. Another suggestion to improve training stability for larger models is not to use \\\\textbf{biases} in dense and norm layers as in~\\\\cite{PaLM}.    \\\\\\\\\\n\\\\emph{\\\\textbf{Training Parallelism}} 3D parallelism, a combination of data, pipeline and tensor parallelism, is the most utilized training parallelism approach in LLMs~\\\\cite{GLM-130B, PaLM, OPT, BLOOM, mtnlg, wu2021yuan,lieber2021jurassic}. In addition to the 3D parallelism, BLOOM~\\\\cite{BLOOM} uses zero optimizer~\\\\cite{ZeroOpt} to shard optimizer states. PanGu-$\\\\alpha$~\\\\cite{PanGU_alpha} and PanGu-$\\\\Sigma$~\\\\cite{PanGu_sigma} go beyond the 3D parallelism and apply 5D parallelism which additionally contains optimizer parallelism and rematerialization.     \\\\\\\\\\n\\\\emph{\\\\textbf{Mode Switching}} adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve the downstream task performance in~\\\\cite{UL2, U-PaLM, soltan2022alexatm}. During fine-tuning and inference, tokens are appended based on the downstream tasks.  \\\\\\\\\\n\\\\subsection{Pre-Training vs \\nInstruction Tuning}\\nWhile pre-training is important for the generalization of LLMs, instruction-tuning improves the performance of LLMs further and makes them useable. Therefore, it is suggested to perform instruction fine-tuning of pre-trained LLMs to use them effectively~\\\\cite{Flan, Tk-INSTRUCT, instructgpt, OPT_IML, nakano2021webgpt}. \\n\\\\subsection{Supervised Models vs Generalized Models}\\nAlthough generalized models are capable of performing diverse tasks with good performance they have not yet outperformed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by a large margin as shown in~\\\\cite{GPT-3, PaLM, Tk-INSTRUCT}.   \\n\\\\subsection{Zero-Shot vs Few-Shot}\\nLLMs perform well in zero-shot and few-shot settings. But the performance difference between zero-shot and few-shot is large for pre-trained models~\\\\cite{GPT-3, PaLM}, naming LLMs as meta-learners~\\\\cite{GPT-3}. LLMs zero-shot evaluations underperform unsupervised methods in neural machine translation~\\\\cite{GPT-3}. The literature shows pre-training is not enough for good zero-shot performance~\\\\cite{PaLM, Flan}. To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms baselines. Instruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model Flan-PaLM~\\\\cite{Flan} unlocks zero-shot reasoning with CoT training. \\n\\n\""
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "section_texts[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "\n",
        "\n",
        "# file = 'test_text.txt'\n",
        "# with open(file, 'r') as f:\n",
        "#     text = f.read()\n",
        "# example = {'article': text, 'title': 'Test Article'}\n",
        "# pages = quality_pagination(example, query_model=llm_query)\n",
        "\n",
        "# We will need to create something that will be able to look at arxiv .tex folders/files and parse the pages in a way that \n",
        "# allow us to do a gist on each one of the different files in the order that they are presented in the paper.\n",
        "\n",
        "folder = '../text_processes/arXiv-2307.06435v7'\n",
        "\n",
        "def get_text_pages_from_tex_folder(tex_folder):\n",
        "    # look for the main.tex\n",
        "    # Get the /sections/ in order, \n",
        "    # get the text from each section and return those as separate pages.\n",
        "\n",
        "    main_tex = os.path.join(tex_folder, 'main.tex')\n",
        "    with open(main_tex, 'r') as f:\n",
        "        main_text = f.read()\n",
        "    # get the sections for example:\n",
        "    # \\input{sections/introduction} \n",
        "    sections = re.findall(r'\\\\input{sections/(.*?)}', main_text)\n",
        "    section_texts = []\n",
        "    for section in sections:\n",
        "        if section.endswith('.tex'):\n",
        "            section = section[:-4]\n",
        "        section_file = os.path.join(tex_folder, 'sections', section + '.tex')\n",
        "        with open(section_file, 'r') as f:\n",
        "            section_text = f.read()\n",
        "        section_texts.append(section_text)\n",
        "    return section_texts\n",
        "\n",
        "section_texts = get_text_pages_from_tex_folder(folder)\n",
        "\n",
        "smart_pages = []\n",
        "for i,  section_text in enumerate(section_texts):\n",
        "    # print(i)\n",
        "    lines =  section_text.split('\\n')\n",
        "    # filter lines ignoring comments like % and empty lines\n",
        "    lines = [line for line in lines if line and not line.startswith('%')]\n",
        "    smart_pages += lines\n",
        "# smart_pages = quality_gutenberg_parser(smart_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "929"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(smart_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'paragraphs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparagraphs\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'paragraphs' is not defined"
          ]
        }
      ],
      "source": [
        "quality_pagination\n",
        "gists = gisting_with_template(smart_pages, llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "gisting_with_template() missing 1 required positional argument: 'template'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_gists \u001b[38;5;241m=\u001b[39m \u001b[43mgisting_with_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_query\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: gisting_with_template() missing 1 required positional argument: 'template'"
          ]
        }
      ],
      "source": [
        "result_gists = gisting_with_template(pages, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
