{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iqyV7VcsiXT"
      },
      "source": [
        "![read_agent_teaser](https://read-agent.github.io/img/teaser.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From https://github.com/read-agent/read-agent.github.io/blob/main/assets/read_agent_demo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/nyu-mll/quality/raw/main/data/v1.0.1/QuALITY.v1.0.1.htmlstripped.dev\n",
        "import re, time, datetime, json, string, copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "oz0kOxYJ4n3e"
      },
      "outputs": [],
      "source": [
        "# # @title Using OpenAI GPT model (DO NOT run the next cell if using GPT)\n",
        "# # !pip3 install openai\n",
        "# import openai\n",
        "\n",
        "# # key = 'YOUR API KEY'  #@param {type: \"string\"}\n",
        "# import os\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()\n",
        "# key = os.getenv('OPENAI_API_KEY')\n",
        "# gpt_client = openai.OpenAI(api_key=key)\n",
        "# model_type = 'gpt'\n",
        "\n",
        "# def query_gpt_model(\n",
        "#     prompt: str,\n",
        "#     lm: str = 'gpt-3.5-turbo-1106',\n",
        "#     temperature: float = 0.0,\n",
        "#     max_decode_steps: int = 512,\n",
        "#     seconds_to_reset_tokens: float = 30.0,\n",
        "# ) -> str:\n",
        "#   while True:\n",
        "#     try:\n",
        "#       raw_response = gpt_client.chat.completions.with_raw_response.create(\n",
        "#         model=lm,\n",
        "        \n",
        "#         max_tokens=max_decode_steps,\n",
        "#         temperature=temperature,\n",
        "#         messages=[\n",
        "#           {'role': 'user', 'content': prompt},\n",
        "#         ]\n",
        "#       )\n",
        "#       completion = raw_response.parse()\n",
        "#       return completion.choices[0].message.content\n",
        "#     except openai.RateLimitError as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: RateLimitError {e.message}: {e}')\n",
        "#       time.sleep(seconds_to_reset_tokens)\n",
        "#     except openai.APIError as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: APIError {e.message}: {e}')\n",
        "#       print(f'{datetime.datetime.now()}: query_gpt_model: Retrying after 5 seconds...')\n",
        "#       time.s\n",
        "# leep(5)\n",
        "from genai.llms.simple_query import ModelManager\n",
        "\n",
        "model_manager = ModelManager()\n",
        "\n",
        "\n",
        "model_type = 'gpt-3.5-turbo-1106'\n",
        "llm = model_manager.get_llm(model_type )\n",
        "llm_query = llm.query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "YcP_tIpZKNFY"
      },
      "outputs": [],
      "source": [
        "# # @title Using Google Gemini model (DO NOT run this if using GPT)\n",
        "# !pip3 install -q -U google-generativeai\n",
        "# import google.generativeai as genai\n",
        "\n",
        "# key = 'YOUR API KEY'  #@param {type: \"string\"}\n",
        "\n",
        "# genai.configure(api_key=key)\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# model_type = 'gemini'\n",
        "\n",
        "# def query_gemini_model(\n",
        "#     prompt: str,\n",
        "#     retries: int = 10,\n",
        "# ) -> str:\n",
        "#   while True and retries > 0:\n",
        "#     try:\n",
        "#       response = model.generate_content(prompt)\n",
        "#       text_response = response.text.replace(\"**\", \"\")\n",
        "#       return text_response\n",
        "#     except Exception as e:\n",
        "#       print(f'{datetime.datetime.now()}: query_gemini_model: Error: {e}')\n",
        "#       print(f'{datetime.datetime.now()}: query_gemini_model: Retrying after 5 seconds...')\n",
        "#       retries -= 1\n",
        "#       time.sleep(5)\n",
        "# from genai.llms.simple_query import query_gemini_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pYm2GsBGEvAI"
      },
      "outputs": [],
      "source": [
        "# def query_model(prompt):\n",
        "#   if model_type == \"gpt\":\n",
        "#     return query_gpt_model(prompt)\n",
        "#   elif model_type == \"gemini\":\n",
        "#     return query_gemini_model(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "1B70Rqg97aXu"
      },
      "outputs": [],
      "source": [
        "#@title Load a QuALITY example\n",
        "\n",
        "# Fields that are straight text copies from raw example to processed example.\n",
        "_ONE2ONE_FIELDS = (\n",
        "    'article',\n",
        "    'article_id',\n",
        "    'set_unique_id',\n",
        "    'writer_id',\n",
        "    'source',\n",
        "    'title',\n",
        "    'topic',\n",
        "    'url',\n",
        "    'writer_id',\n",
        "    'author',\n",
        ")\n",
        "\n",
        "quality_dev = []\n",
        "\n",
        "with open('QuALITY.v1.0.1.htmlstripped.dev', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    j = json.loads(line)\n",
        "    fields = {k: j[k] for k in _ONE2ONE_FIELDS}\n",
        "    fields.update({\n",
        "        'questions': [q['question'] for q in j['questions']],\n",
        "        'question_ids': [q['question_unique_id'] for q in j['questions']],\n",
        "        'difficults': [q['difficult'] for q in j['questions']],\n",
        "        'options': [q['options'] for q in j['questions']],\n",
        "    })\n",
        "\n",
        "    fields.update({\n",
        "        'gold_labels': [q['gold_label'] for q in j['questions']],\n",
        "        'writer_labels': [q['writer_label'] for q in j['questions']],\n",
        "      })\n",
        "\n",
        "    quality_dev.append(fields)\n",
        "\n",
        "example = quality_dev[13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "nQsb3n6pOlz2"
      },
      "outputs": [],
      "source": [
        "# #@title Helper functions\n",
        "\n",
        "# all_lowercase_letters = string.ascii_lowercase  # \"abcd...xyz\"\n",
        "# bracketed_lowercase_letters_set = set(\n",
        "#     [f\"({l})\" for l in all_lowercase_letters]\n",
        "# )  # {\"(a)\", ...}\n",
        "# bracketed_uppercase_letters_set = set(\n",
        "#     [f\"({l.upper()})\" for l in all_lowercase_letters]\n",
        "# )  # {\"(a)\", ...}\n",
        "\n",
        "# choices = ['(A)', '(B)', '(C)', '(D)']\n",
        "\n",
        "# def get_index_from_symbol(answer):\n",
        "#   \"\"\"Get the index from the letter symbols A, B, C, D, to extract answer texts.\n",
        "\n",
        "#   Args:\n",
        "#     answer (str): the string of answer like \"(B)\".\n",
        "\n",
        "#   Returns:\n",
        "#     index (int): how far the given choice is from \"a\", like 1 for answer \"(B)\".\n",
        "#   \"\"\"\n",
        "#   answer = str(answer).lower()\n",
        "#   # extract the choice letter from within bracket\n",
        "#   if answer in bracketed_lowercase_letters_set:\n",
        "#     answer = re.findall(r\"\\(.*?\\)\", answer)[0][1]\n",
        "#   index = ord(answer) - ord(\"a\")\n",
        "#   return index\n",
        "\n",
        "# def count_words(text):\n",
        "#   \"\"\"Simple word counting.\"\"\"\n",
        "#   return len(text.split())\n",
        "\n",
        "# def quality_gutenberg_parser(raw_article):\n",
        "#   \"\"\"Parse Gutenberg articles in the QuALITY dataset.\"\"\"\n",
        "#   lines = []\n",
        "#   previous_line = None\n",
        "#   for i, line in enumerate(raw_article.split('\\n')):\n",
        "#     line = line.strip()\n",
        "#     original_line = line\n",
        "#     if line == '':\n",
        "#       if previous_line == '':\n",
        "#         line = '\\n'\n",
        "#       else:\n",
        "#         previous_line = original_line\n",
        "#         continue\n",
        "#     previous_line = original_line\n",
        "#     lines.append(line)\n",
        "#   return ' '.join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BfFkEQKx0u9U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "# #@title ReadAgent (1) Episode Pagination\n",
        "\n",
        "# prompt_pagination_template = \"\"\"\n",
        "# You are given a passage that is taken from a larger text (article, book, ...) and some numbered labels between the paragraphs in the passage.\n",
        "# Numbered label are in angeled brackets. For example, if the label number is 19, it shows as <19> in text.\n",
        "# Please choose one label that it is natural to break reading.\n",
        "# Such point can be scene transition, end of a dialogue, end of an argument, narrative transition, etc.\n",
        "# Please answer the break point label and explain.\n",
        "# For example, if <57> is a good point to break, answer with \\\"Break point: <57>\\n Because ...\\\"\n",
        "\n",
        "# Passage:\n",
        "\n",
        "# {0}\n",
        "# {1}\n",
        "# {2}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def parse_pause_point(text):\n",
        "#   text = text.strip(\"Break point: \")\n",
        "#   if text[0] != '<':\n",
        "#     return None\n",
        "#   for i, c in enumerate(text):\n",
        "#     if c == '>':\n",
        "#       if text[1:i].isnumeric():\n",
        "#         return int(text[1:i])\n",
        "#       else:\n",
        "#         return None\n",
        "#   return None\n",
        "\n",
        "\n",
        "# def quality_pagination(example,\n",
        "#                        word_limit=600,\n",
        "#                        start_threshold=280,\n",
        "#                        max_retires=10,\n",
        "#                        verbose=True,\n",
        "#                        allow_fallback_to_last=True):\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   print(f\"[Pagination][Article {title}]\")\n",
        "#   paragraphs = quality_gutenberg_parser(article).split('\\n')\n",
        "\n",
        "#   i = 0\n",
        "#   pages = []\n",
        "#   while i < len(paragraphs):\n",
        "#     preceding = \"\" if i == 0 else \"...\\n\" + '\\n'.join(pages[-1])\n",
        "#     passage = [paragraphs[i]]\n",
        "#     wcount = count_words(paragraphs[i])\n",
        "#     j = i + 1\n",
        "#     while wcount < word_limit and j < len(paragraphs):\n",
        "#       wcount += count_words(paragraphs[j])\n",
        "#       if wcount >= start_threshold:\n",
        "#         passage.append(f\"<{j}>\")\n",
        "#       passage.append(paragraphs[j])\n",
        "#       j += 1\n",
        "#     passage.append(f\"<{j}>\")\n",
        "#     end_tag = \"\" if j == len(paragraphs) else paragraphs[j] + \"\\n...\"\n",
        "\n",
        "#     pause_point = None\n",
        "#     if wcount < 350:\n",
        "#       pause_point = len(paragraphs)\n",
        "#     else:\n",
        "#       prompt = prompt_pagination_template.format(preceding, '\\n'.join(passage), end_tag)\n",
        "#       response = query_model(prompt=prompt).strip()\n",
        "#       pause_point = parse_pause_point(response)\n",
        "#       if pause_point and (pause_point <= i or pause_point > j):\n",
        "#         print(f\"prompt:\\n{prompt},\\nresponse:\\n{response}\\n\")\n",
        "#         print(f\"i:{i} j:{j} pause_point:{pause_point}\")\n",
        "#         pause_point = None\n",
        "#       if pause_point is None:\n",
        "#         if allow_fallback_to_last:\n",
        "#           pause_point = j\n",
        "#         else:\n",
        "#           raise ValueError(f\"prompt:\\n{prompt},\\nresponse:\\n{response}\\n\")\n",
        "\n",
        "#     page = paragraphs[i:pause_point]\n",
        "#     pages.append(page)\n",
        "#     if verbose:\n",
        "#       print(f\"Paragraph {i}-{pause_point-1}\", page)\n",
        "#     i = pause_point\n",
        "#   print(f\"[Pagination] Done with {len(pages)} pages\")\n",
        "#   return pages\n",
        "\n",
        "# pages = quality_pagination(example, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DLBolKnkS_9y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Gisting][Article Off Course], 2712 words\n",
            "[gist] page 0: The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "[gist] page 1: Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "[gist] page 2: Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "[gist] page 3: Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "[gist] page 4: The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "[gist] page 5: Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            "[gist] page 6: Andersen, Sir Alfred, and McCord reassured Dameri Tass and made him comfortable in a chair. Andersen faced the audience and announced that they would now hear from the first being to come to Earth from another world. Dameri Tass, the alien, was surprised and worried, realizing he had made a mistake. He confessed that he was not an envoy from another planet, but a collector of specimens for a zoo. He quickly left the speaker's stand, saying he needed to leave immediately. President McCord signaled to the bodyguard to accompany the alien.\n",
            "[gist] page 7: Viljalmar Andersen tried to speak to Dameri Tass, but Tass refused and left with his pets.\n",
            "[gist] page 8: The alien said he almost forgot about taking creatures from Earth, but decided not to take the dog, cat, or horse. He expressed a desire to take a horse back to his planet, but ultimately left Earth in his spacecraft.\n",
            "Shortened article:\n",
            " The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            "Andersen, Sir Alfred, and McCord reassured Dameri Tass and made him comfortable in a chair. Andersen faced the audience and announced that they would now hear from the first being to come to Earth from another world. Dameri Tass, the alien, was surprised and worried, realizing he had made a mistake. He confessed that he was not an envoy from another planet, but a collector of specimens for a zoo. He quickly left the speaker's stand, saying he needed to leave immediately. President McCord signaled to the bodyguard to accompany the alien.\n",
            "Viljalmar Andersen tried to speak to Dameri Tass, but Tass refused and left with his pets.\n",
            "The alien said he almost forgot about taking creatures from Earth, but decided not to take the dog, cat, or horse. He expressed a desire to take a horse back to his planet, but ultimately left Earth in his spacecraft.\n",
            "compression rate 86.5% (366/2712)\n"
          ]
        }
      ],
      "source": [
        "# #@title ReadAgent (2) Memory Gisting\n",
        "\n",
        "# prompt_shorten_template = \"\"\"\n",
        "# Please shorten the following passage.\n",
        "# Just give me a shortened version. DO NOT explain your reason.\n",
        "\n",
        "# Passage:\n",
        "# {}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def quality_gisting(example, pages, word_limit=600, start_threshold=280, verbose=True):\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   word_count = count_words(article)\n",
        "#   print(f\"[Gisting][Article {title}], {word_count} words\")\n",
        "\n",
        "#   shortened_pages = []\n",
        "#   for i, page in enumerate(pages):\n",
        "#     prompt = prompt_shorten_template.format('\\n'.join(page))\n",
        "#     response = query_model(prompt)\n",
        "#     shortened_text = response.strip()\n",
        "#     shortened_pages.append(shortened_text)\n",
        "#     if verbose:\n",
        "#       print(\"[gist] page {}:\".format(i), shortened_text, flush=True)\n",
        "#   shortened_article = '\\n'.join(shortened_pages)\n",
        "#   gist_word_count = count_words(shortened_article)\n",
        "#   if verbose:\n",
        "#     print(\"Shortened article:\\n\", shortened_article, flush=True)\n",
        "#   output = copy.deepcopy(example)\n",
        "#   output.update({'title': title, 'word_count': word_count, 'gist_word_count': gist_word_count, 'shortened_pages': shortened_pages, 'pages': pages})\n",
        "#   if verbose:\n",
        "#     print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "#   return output\n",
        "from genai.text_processes.gisting import quality_gisting\n",
        "example_with_gists = quality_gisting(example, pages, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genai.text_processes.utils import count_words \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Look-Up][Article Off Course] 2712 words\n",
            "question:  What happened to Dameri while he was in custody of the government?\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'choices' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mquality_parallel_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_with_gists\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 66\u001b[0m, in \u001b[0;36mquality_parallel_lookup\u001b[0;34m(example, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m q \u001b[38;5;241m=\u001b[39m questions[i]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m, q)\n\u001b[0;32m---> 66\u001b[0m options_i \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ol, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mchoices\u001b[49m, options[i])]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(options_i))\n\u001b[1;32m     68\u001b[0m prompt_lookup \u001b[38;5;241m=\u001b[39m prompt_lookup_template\u001b[38;5;241m.\u001b[39mformat(shortened_article, q, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(options_i))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'choices' is not defined"
          ]
        }
      ],
      "source": [
        "#@title ReadAgent (3) Look-Up\n",
        "\n",
        "prompt_lookup_template = \"\"\"\n",
        "The following text is what you remembered from reading an article and a multiple choice question related to it.\n",
        "You may read 1 to 6 page(s) of the article again to refresh your memory to prepare yourselve for the question.\n",
        "Please respond with which page(s) you would like to read.\n",
        "For example, if your only need to read Page 8, respond with \\\"I want to look up Page [8] to ...\\\";\n",
        "if your would like to read Page 7 and 12, respond with \\\"I want to look up Page [7, 12] to ...\\\";\n",
        "if your would like to read Page 2, 3, 7, 15 and 18, respond with \\\"I want to look up Page [2, 3, 7, 15, 18] to ...\\\".\n",
        "if your would like to read Page 3, 4, 5, 12, 13 and 16, respond with \\\"I want to look up Page [3, 3, 4, 12, 13, 16] to ...\\\".\n",
        "DO NOT select more pages if you don't need to.\n",
        "DO NOT answer the question yet.\n",
        "\n",
        "Text:\n",
        "{}\n",
        "\n",
        "Question:\n",
        "{}\n",
        "{}\n",
        "\n",
        "Take a deep breath and tell me: Which page(s) would you like to read again?\n",
        "\"\"\"\n",
        "\n",
        "prompt_answer_template = \"\"\"\n",
        "Read the following article and answer a multiple choice question.\n",
        "For example, if (C) is correct, answer with \\\"Answer: (C) ...\\\"\n",
        "\n",
        "Article:\n",
        "{}\n",
        "\n",
        "Question:\n",
        "{}\n",
        "{}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def quality_parallel_lookup(example, verbose=True):\n",
        "  preprocessed_pages = example['pages']\n",
        "  article = example['article']\n",
        "  title = example['title']\n",
        "  word_count = example['word_count']\n",
        "  gist_word_count = example['gist_word_count']\n",
        "  pages = example['pages']\n",
        "  shortened_pages = example['shortened_pages']\n",
        "  questions = example['questions']\n",
        "  options = example['options']\n",
        "  gold_labels = example['gold_labels']  # numerical [1, 2, 3, 4]\n",
        "\n",
        "  print(f\"[Look-Up][Article {title}] {word_count} words\")\n",
        "\n",
        "  model_choices = []\n",
        "  lookup_page_ids = []\n",
        "\n",
        "  shortened_pages_pidx = []\n",
        "  for i, shortened_text in enumerate(shortened_pages):\n",
        "    shortened_pages_pidx.append(\"<Page {}>\\n\".format(i) + shortened_text)\n",
        "  shortened_article = '\\n'.join(shortened_pages_pidx)\n",
        "\n",
        "  expanded_gist_word_counts = []\n",
        "  for i, label in enumerate(gold_labels):\n",
        "    # only test the first question for demo\n",
        "    if i != 1:\n",
        "      continue\n",
        "    q = questions[i]\n",
        "    print(\"question: \", q)\n",
        "    options_i = [f\"{ol} {o}\" for ol, o in zip(choices, options[i])]\n",
        "    print(\"options: \", \"\\n\".join(options_i))\n",
        "    prompt_lookup = prompt_lookup_template.format(shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "    page_ids = []\n",
        "\n",
        "    response = query_model(prompt=prompt_lookup).strip()\n",
        "\n",
        "    try: start = response.index('[')\n",
        "    except ValueError: start = len(response)\n",
        "    try: end = response.index(']')\n",
        "    except ValueError: end = 0\n",
        "    if start < end:\n",
        "      page_ids_str = response[start+1:end].split(',')\n",
        "      page_ids = []\n",
        "      for p in page_ids_str:\n",
        "        if p.strip().isnumeric():\n",
        "          page_id = int(p)\n",
        "          if page_id < 0 or page_id >= len(pages):\n",
        "            print(\"Skip invalid page number: \", page_id, flush=True)\n",
        "          else:\n",
        "            page_ids.append(page_id)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Model chose to look up page {}\".format(page_ids))\n",
        "\n",
        "    # Memory expansion after look-up, replacing the target shortened page with the original page\n",
        "    expanded_shortened_pages = shortened_pages[:]\n",
        "    if len(page_ids) > 0:\n",
        "      for page_id in page_ids:\n",
        "        expanded_shortened_pages[page_id] = '\\n'.join(pages[page_id])\n",
        "\n",
        "    expanded_shortened_article = '\\n'.join(expanded_shortened_pages)\n",
        "    expanded_gist_word_count = count_words(expanded_shortened_article)\n",
        "    if verbose:\n",
        "      print(\"Expanded shortened article:\\n\", expanded_shortened_article, flush=True)\n",
        "    prompt_answer = prompt_answer_template.format(expanded_shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "    model_choice = None\n",
        "    response = llm_query(prompt=prompt_answer)\n",
        "    response = response.strip()\n",
        "    for j, choice in enumerate(choices):\n",
        "      if response.startswith(f\"Answer: {choice}\") or response.startswith(f\"Answer: {choice[1]}\"):\n",
        "        model_choice = j+1\n",
        "        break\n",
        "    is_correct = 1 if model_choice == label else 0\n",
        "    print(f\"question: {q}\")\n",
        "    print(f\"reference answer: {choices[label]}, model prediction: {choices[model_choice]}, is_correct: {is_correct}\")\n",
        "    print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "    print(f\"compression rate after look-up {round(100.0 - expanded_gist_word_count/word_count*100, 2)}% ({expanded_gist_word_count}/{word_count})\")\n",
        "\n",
        "quality_parallel_lookup(example_with_gists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8YKNTyDsXNIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Look-Up][Article Off Course] 2712 words\n",
            "question:  What happened to Dameri while he was in custody of the government?\n",
            "options:  \n",
            "Model chose to look up page [6, 7, 8]\n",
            "Expanded shortened article:\n",
            " The first envoy from another world was about to speak, but he seemed more interested in a horse. Patrolmen Dermott and Casey approached the strange craft and encountered Dameri Tass, who spoke in a language they didn't understand.\n",
            "Dameri Tass emerged from his spacecraft with a strange contraption and tried to get the patrolmen to put on a metal cap. Despite their reluctance, they were instructed to humor him until officials arrived.\n",
            "Casey put on the cap and felt a shock, thinking he had been murdered. His friend explained that the cap was harmless and allowed him to understand their language. The patrolman made a comment about their Irish accents.\n",
            "Dameri Tass encounters a horse and then is taken to Washington, causing a major uproar.\n",
            "The United Nations demanded the alien be heard, and the White House yielded. The world was excited and eagerly awaited the visitor's message. It was the most universally awaited event of the ages.\n",
            "Nine-tenths of Earth's population was ready to be guided by the space envoy, while the other tenth was resistant. President McCord and Secretary-General Andersen were nervous about introducing the space emissary, who had been asleep for almost two weeks upon arrival. The President revealed that the envoy spent his first day whistling and playing with a dog, cat, and mouse.\n",
            " \"I wish we knew what he was going to say,\" Andersen worried. \n",
            " \"Here he comes,\" said Sir Alfred. \n",
            " Surrounded by F.B.I. men, Dameri Tass was ushered to the speaker's stand. He had a kitten in his arms; a Scotty followed him. \n",
            " The alien frowned worriedly. \"Sure,\" he said, \"and what kin all this be? Is it some ordinance I've been after breakin'?\" \n",
            " McCord, Sir Alfred and Andersen hastened to reassure him and made him comfortable in a chair. \n",
            " Viljalmar Andersen faced the thousands in the audience and held up his hands, but it was ten minutes before he was able to quiet the cheering, stamping delegates from all Earth. \n",
            " Finally: \"Fellow Terrans, I shall not take your time for a lengthy introduction of the envoy from the stars. I will only say that, without doubt, this is the most important moment in the history of the human race. We will now hear from the first being to come to Earth from another world.\" \n",
            " He turned and gestured to Dameri Tass who hadn't been paying overmuch attention to the chairman in view of some dog and cat hostilities that had been developing about his feet. \n",
            " But now the alien's purplish face faded to a light blue. He stood and said hoarsely. \"Faith, an' what was that last you said?\" \n",
            " Viljalmar Andersen repeated, \"We will now hear from the first being ever to come to Earth from another world.\" \n",
            " The face of the alien went a lighter blue. \"Sure, an' ye wouldn't jist be frightenin' a body, would ye? You don't mean to tell me this planet isn't after bein' a member of the Galactic League?\" \n",
            " Andersen's face was blank. \"Galactic League?\" \n",
            " \"Cushlamachree,\" Dameri Tass moaned. \"I've gone and put me foot in it again. I'll be after getting kert for this.\" \n",
            " Sir Alfred was on his feet. \"I don't understand! Do you mean you aren't an envoy from another planet?\" \n",
            " Dameri Tass held his head in his hands and groaned. \"An envoy, he's sayin', and meself only a second-rate collector of specimens for the Carthis zoo.\" \n",
            " He straightened and started off the speaker's stand. \"Sure, an' I must blast off immediately.\" \n",
            " Things were moving fast for President McCord but already an edge of relief was manifesting itself. Taking the initiative, he said, \"Of course, of course, if that is your desire.\" He signaled to the bodyguard who had accompanied the alien to the assemblage. \n",
            " A dull roar was beginning to emanate from the thousands gathered in the tremendous hall, murmuring, questioning, disbelieving. Viljalmar Andersen felt that he must say something. He extended a detaining hand. \"Now you are here,\" he said urgently, \"even though by mistake, before you go can't you give us some brief word? Our world is in chaos. Many of us have lost faith. Perhaps ...\" \n",
            " Dameri Tass shook off the restraining hand. \"Do I look daft? Begorry, I should have been a-knowin' something was queer. All your weapons and your strange ideas. Faith, I wouldn't be surprised if ye hadn't yet established a planet-wide government. Sure, an' I'll go still further. Ye probably still have wars on this benighted world. No wonder it is ye haven't been invited to join the Galactic League an' take your place among the civilized planets.\" \n",
            " He hustled from the rostrum and made his way, still surrounded by guards, to the door by which he had entered. The dog and the cat trotted after, undismayed by the furor about them. \n",
            " They arrived about four hours later at the field on which he'd landed, and the alien from space hurried toward his craft, still muttering. He'd been accompanied by a general and by the President, but all the way he had refrained from speaking. \n",
            " He scurried from the car and toward the spacecraft. \n",
            " President McCord said, \"You've forgotten your pets. We would be glad if you would accept them asâ€”\" \n",
            " The alien's face faded a light blue again. \"Faith, an' I'd almost forgotten,\" he said. \"If I'd taken a crature from this quarantined planet, my name'd be nork . Keep your dog and your kitty.\" He shook his head sadly and extracted a mouse from a pocket. \"An' this amazin' little crature as well.\" \n",
            " They followed him to the spacecraft. Just before entering, he spotted the bedraggled horse that had been present on his landing. \n",
            " A longing expression came over his highly colored face. \"Jist one thing,\" he said. \"Faith now, were they pullin' my leg when they said you were after ridin' on the back of those things?\" \n",
            " The President looked at the woebegone nag. \"It's a horse,\" he said, surprised. \"Man has been riding them for centuries.\" \n",
            " Dameri Tass shook his head. \"Sure, an' 'twould've been my makin' if I could've taken one back to Carthis.\" He entered his vessel. \n",
            " The others drew back, out of range of the expected blast, and watched, each with his own thoughts, as the first visitor from space hurriedly left Earth. ... THE END Transcriber's Note: This etext was produced from If Worlds of Science Fiction January 1954. Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed. Minor spelling and typographical errors have been corrected without note.\n",
            "question: What happened to Dameri while he was in custody of the government?\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mquality_parallel_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_with_gists\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 114\u001b[0m, in \u001b[0;36mquality_parallel_lookup\u001b[0;34m(example, verbose)\u001b[0m\n\u001b[1;32m    112\u001b[0m is_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m label \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_choices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, model prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_choices[model_choice]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is_correct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression rate after look-up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mexpanded_gist_word_count\u001b[38;5;241m/\u001b[39mword_count\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_gist_word_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# #@title ReadAgent (3) Look-Up\n",
        "\n",
        "# prompt_lookup_template = \"\"\"\n",
        "# The following text is what you remembered from reading an article and a multiple choice question related to it.\n",
        "# You may read 1 to 6 page(s) of the article again to refresh your memory to prepare yourselve for the question.\n",
        "# Please respond with which page(s) you would like to read.\n",
        "# For example, if your only need to read Page 8, respond with \\\"I want to look up Page [8] to ...\\\";\n",
        "# if your would like to read Page 7 and 12, respond with \\\"I want to look up Page [7, 12] to ...\\\";\n",
        "# if your would like to read Page 2, 3, 7, 15 and 18, respond with \\\"I want to look up Page [2, 3, 7, 15, 18] to ...\\\".\n",
        "# if your would like to read Page 3, 4, 5, 12, 13 and 16, respond with \\\"I want to look up Page [3, 3, 4, 12, 13, 16] to ...\\\".\n",
        "# DO NOT select more pages if you don't need to.\n",
        "# DO NOT answer the question yet.\n",
        "\n",
        "# Text:\n",
        "# {}\n",
        "\n",
        "# Question:\n",
        "# {}\n",
        "# {}\n",
        "\n",
        "# Take a deep breath and tell me: Which page(s) would you like to read again?\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_answer_template = \"\"\"\n",
        "# Read the following article and answer a multiple choice question.\n",
        "# For example, if (C) is correct, answer with \\\"Answer: (C) ...\\\"\n",
        "\n",
        "# Article:\n",
        "# {}\n",
        "\n",
        "# Question:\n",
        "# {}\n",
        "# {}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# def quality_parallel_lookup(example, verbose=True):\n",
        "#   preprocessed_pages = example['pages']\n",
        "#   article = example['article']\n",
        "#   title = example['title']\n",
        "#   word_count = example['word_count']\n",
        "#   gist_word_count = example['gist_word_count']\n",
        "#   pages = example['pages']\n",
        "#   shortened_pages = example['shortened_pages']\n",
        "#   questions = example['questions']\n",
        "#   options = example['options']\n",
        "#   gold_labels = example['gold_labels']  # numerical [1, 2, 3, 4]\n",
        "\n",
        "#   print(f\"[Look-Up][Article {title}] {word_count} words\")\n",
        "\n",
        "#   model_choices = []\n",
        "#   lookup_page_ids = []\n",
        "\n",
        "#   shortened_pages_pidx = []\n",
        "#   for i, shortened_text in enumerate(shortened_pages):\n",
        "#     shortened_pages_pidx.append(\"<Page {}>\\n\".format(i) + shortened_text)\n",
        "#   shortened_article = '\\n'.join(shortened_pages_pidx)\n",
        "\n",
        "#   expanded_gist_word_counts = []\n",
        "#   for i, label in enumerate(gold_labels):\n",
        "#     # only test the first question for demo\n",
        "#     if i != 1:\n",
        "#       continue\n",
        "#     q = questions[i]\n",
        "#     print(\"question: \", q)\n",
        "#     options_i = [f\"{ol} {o}\" for ol, o in zip(model_choices, options[i])]\n",
        "#     print(\"options: \", \"\\n\".join(options_i))\n",
        "#     prompt_lookup = prompt_lookup_template.format(shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "#     page_ids = []\n",
        "\n",
        "#     response = (prompt=prompt_lookup).strip()\n",
        "\n",
        "\n",
        "#     try: start = response.index('[')\n",
        "#     except ValueError: start = len(response)\n",
        "#     try: end = response.index(']')\n",
        "#     except ValueError: end = 0\n",
        "#     if start < end:\n",
        "#       page_ids_str = response[start+1:end].split(',')\n",
        "#       page_ids = []\n",
        "#       for p in page_ids_str:\n",
        "#         if p.strip().isnumeric():\n",
        "#           page_id = int(p)\n",
        "#           if page_id < 0 or page_id >= len(pages):\n",
        "#             print(\"Skip invalid page number: \", page_id, flush=True)\n",
        "#           else:\n",
        "#             page_ids.append(page_id)\n",
        "\n",
        "#     if verbose:\n",
        "#       print(\"Model chose to look up page {}\".format(page_ids))\n",
        "\n",
        "#     # Memory expansion after look-up, replacing the target shortened page with the original page\n",
        "#     expanded_shortened_pages = shortened_pages[:]\n",
        "#     if len(page_ids) > 0:\n",
        "#       for page_id in page_ids:\n",
        "#         expanded_shortened_pages[page_id] = '\\n'.join(pages[page_id])\n",
        "\n",
        "#     expanded_shortened_article = '\\n'.join(expanded_shortened_pages)\n",
        "#     expanded_gist_word_count = count_words(expanded_shortened_article)\n",
        "#     if verbose:\n",
        "#       print(\"Expanded shortened article:\\n\", expanded_shortened_article, flush=True)\n",
        "#     prompt_answer = prompt_answer_template.format(expanded_shortened_article, q, '\\n'.join(options_i))\n",
        "\n",
        "#     model_choice = None\n",
        "#     response = llm_query(prompt=prompt_answer)\n",
        "#     response = response.strip()\n",
        "#     for j, choice in enumerate(model_choices):\n",
        "#       if response.startswith(f\"Answer: {choice}\") or response.startswith(f\"Answer: {choice[1]}\"):\n",
        "#         model_choice = j+1\n",
        "#         break\n",
        "#     is_correct = 1 if model_choice == label else 0\n",
        "#     print(f\"question: {q}\")\n",
        "#     print(f\"reference answer: {model_choices[label]}, model prediction: {model_choices[model_choice]}, is_correct: {is_correct}\")\n",
        "#     print(f\"compression rate {round(100.0 - gist_word_count/word_count*100, 2)}% ({gist_word_count}/{word_count})\")\n",
        "#     print(f\"compression rate after look-up {round(100.0 - expanded_gist_word_count/word_count*100, 2)}% ({expanded_gist_word_count}/{word_count})\")\n",
        "\n",
        "# quality_parallel_lookup(example_with_gists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEMPLATE_FOR_MACHINE_LEARNING = {\"Developments\": \n",
        "    \"This briefly describes the notable and important developments due to the paper. They should be high level and \"\n",
        "    \"not too detailed. If there are multiple developments, they should be separated by bullet points.\",\n",
        "    \"Data\": \n",
        "    \" This should be a brief description of the data used in the paper. It might include the data-source names, \"\n",
        "    \" any cleaning they did, and any new data they collected.\",\n",
        "    \"Models\":\n",
        "    \"This should be a brief description of the model used in the paper. It should include the type of models used, \"\n",
        "    \"if any new architectures were introduced, and any new techniques used in the model.\",\n",
        "    \"Training\": \n",
        "    \"This should be a brief description of the training process used in the paper. It should include the optimizer used, \"\n",
        "    \"the learning rate, the batch sizes, or other things that are important to the training process, \"\n",
        "    \" such as training from scratch or transfer learning.\",\n",
        "    \"Results\":\n",
        "    \"This should be a brief description of the results of the paper. It should include the evaluation metrics used, \"\n",
        "    \"the performance of the model, and any comparisons to other models or baselines.\",\n",
        "    \n",
        "    }\n",
        "\n",
        "\n",
        "def gisting_with_template(smart_pages, query_model, \n",
        "    template=TEMPLATE_FOR_MACHINE_LEARNING, verebose=True):\n",
        "    \"\"\" This takes in a function template of items that it wants to be\n",
        "     sketched into separate focused 'gist' summaries about athat particular topic. \n",
        "     If a chunk has nothing to add to a a particular focus of the template, nothing is added to any subject-focused gist.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_shorten_template = \"\"\"\n",
        "    Please shorten the following passage, focusing on the topic of {topic}, with some considerations such as the following\n",
        "    \n",
        "    {considerations}\n",
        "    \n",
        "    Just give me a shortened version if there is relevant information by saying\n",
        "    \n",
        "    {topic}: \n",
        "\n",
        "    and following it with the shortened text.\n",
        "    \n",
        "    DO NOT explain your reasoning or add unecessary information.\n",
        "\n",
        "    Passage:\n",
        "    {passage}\n",
        "\n",
        "    \"\"\"\n",
        "    # article = example['article']\n",
        "    # title = example['title']\n",
        "    # word_count = count_words(article)\n",
        "    # print(f\"[Gisting][Article {title}], {word_count} words\")\n",
        "    # pages = example['pages']\n",
        "    gist = {key: [] for key in template.keys()}\n",
        "    for i, page in enumerate(smart_pages):\n",
        "        for key, template_text in template.items():\n",
        "            prompt = prompt_shorten_template.format(topic=key, considerations = template_text, passage='\\n'.join(page))\n",
        "            response = query_model(prompt)\n",
        "            shortened_text = response.strip()\n",
        "            if key in shortened_text:\n",
        "                gist[key].append(shortened_text)\n",
        "            # prompt = prompt_shorten_template.format(, '\\n'.join(page))\n",
        "            # response = query_model(prompt)\n",
        "            # shortened_text = response.strip()\n",
        "            # for key, template_text in template.items():\n",
        "            key_marker = key + ':'\n",
        "\n",
        "            if shortened_text.startswith(key_marker):\n",
        "                gist[key].append(shortened_text[len(key_marker):].strip())\n",
        "        # if verbose:\n",
        "        #     print(\"[gist] page {}:\".format(i), shortened_text, flush=True)\n",
        "    # gist_word_count = {key: count_words(' '.join(value)) for key, value in gist.items()}\n",
        "    # if verbose:\n",
        "    #     print(\"Shortened article:\\n\", gist, flush=True)\n",
        "    # output = copy.deepcopy(example)\n",
        "    # output.update({'title': title, 'word_count': word_count, 'gist_word_count': gist_word_count, 'gist': gist, 'pages': pages})\n",
        "    # if verbose:\n",
        "    #     print(f\"compression rate {round(100.0 - sum(gist_word_count.values())/word_count*100, 2)}% ({sum(gist_word_count.values())}/{word_count})\")\n",
        "    return gist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from genai.text_processes.pagination import quality_gutenberg_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "\n",
        "\n",
        "# file = 'test_text.txt'\n",
        "# with open(file, 'r') as f:\n",
        "#     text = f.read()\n",
        "# example = {'article': text, 'title': 'Test Article'}\n",
        "# pages = quality_pagination(example, query_model=llm_query)\n",
        "\n",
        "# We will need to create something that will be able to look at arxiv .tex folders/files and parse the pages in a way that \n",
        "# allow us to do a gist on each one of the different files in the order that they are presented in the paper.\n",
        "\n",
        "folder = '../text_processes/arXiv-2307.06435v7'\n",
        "\n",
        "def get_text_pages_from_tex_folder(tex_folder):\n",
        "    # look for the main.tex\n",
        "    # Get the /sections/ in order, \n",
        "    # get the text from each section and return those as separate pages.\n",
        "\n",
        "    main_tex = os.path.join(tex_folder, 'main.tex')\n",
        "    with open(main_tex, 'r') as f:\n",
        "        main_text = f.read()\n",
        "    # get the sections for example:\n",
        "    # \\input{sections/introduction} \n",
        "    sections = re.findall(r'\\\\input{sections/(.*?)}', main_text)\n",
        "    section_texts = []\n",
        "    for section in sections:\n",
        "        if section.endswith('.tex'):\n",
        "            section = section[:-4]\n",
        "        section_file = os.path.join(tex_folder, 'sections', section + '.tex')\n",
        "        with open(section_file, 'r') as f:\n",
        "            section_text = f.read()\n",
        "        section_texts.append(section_text)\n",
        "    return section_texts\n",
        "\n",
        "section_texts = get_text_pages_from_tex_folder(folder)\n",
        "\n",
        "smart_pages = []\n",
        "for i,  section_text in enumerate(section_texts):\n",
        "    # print(i)\n",
        "    # smart_pages = section_text.split('\\n')\n",
        "    smart_pages += quality_gutenberg_parser(section_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\\\begin{IEEEbiography}{Humza Naveed}',\n",
              " 'Biography text here.',\n",
              " '\\\\end{IEEEbiography}',\n",
              " '',\n",
              " '']"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "section_text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['% Large Language Models (LLMs) have shown excellent generalization abilities that lead to the development of numerous models. These models suggest different approaches, for example, increasing model parameters, training data, tweaking architecture, training strategies, and pipelines to outperform baselines. Analyzing these aspects is important to identify changes that bring stability to the training and better generalization for the LLMs. In this paper, we summarize these fine-grained details of LLMs. Initially, we provide a background for the LLMs to discuss basic building blocks followed by a comprehensive overview of LLMs. The end of the paper discusses important findings by the LLMs and summarizes important architectural and training strategies to develop better LLMs. Because of the continuous development in LLMs, we will update this paper regularly with new sections and LLMs.  ',\n",
              " '',\n",
              " 'Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strategies for developing advanced LLMs. Given the continuous advancements in LLMs, we intend to regularly update this paper by incorporating new sections and featuring the latest LLM models.',\n",
              " 'Large Language Models, LLMs, auto-regressive models, encoder-decoder, training pipeline, architecture, survey, review',\n",
              " '\\\\section{Introduction}',\n",
              " '',\n",
              " 'Language plays a fundamental role in facilitating communication and self-expression for humans, and likewise, communication holds paramount importance for machines in their interactions with humans and other systems. Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently~\\\\cite{y2022large}. The need for LLMs stems from the growing demand for machines to handle complex language tasks, including translation, summarization, information retrieval, and conversational interactions. ',\n",
              " 'Recently, significant breakthroughs have been witnessed in language models, primarily attributed to deep learning techniques, advancements in neural architectures like transformers, increased computational capabilities, and the accessibility of training data extracted from the internet~\\\\cite{chernyavskiy2021transformers}. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks~\\\\cite{wang2019superglue,adiwardana2020towards}. ',\n",
              " '',\n",
              " '',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/Column_Chart.png}',\n",
              " '\\\\caption{The trends in the number of LLM models introduced over the years.}',\n",
              " '%\\\\caption{Number of LLMs introduced over the years.}',\n",
              " '\\\\label{fig:num_LLMs_barchart}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '\\\\begin{figure*}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=2\\\\columnwidth]{Figure/Bubble_Chart.png}',\n",
              " '\\\\caption{The progressive introduction of LLM models demonstrates advances in natural language processing explicitly adapted to various fields and provides increased research,  analysis, and application capabilities.}',\n",
              " '%\\\\caption{LLMs introduced over the years.}',\n",
              " '\\\\label{fig:LLMs_bubblechart}',\n",
              " '\\\\end{figure*}',\n",
              " '',\n",
              " 'LLMs, particularly pre-trained language models (PLM), have shown tremendous generalization abilities for text understanding and generation tasks while trained in a self-supervised setting on a large corpus of text~\\\\cite{Bert, ELMO, BART}. The performance of pre-trained language models (PLMs) improves significantly when fine-tuned for downstream tasks, surpassing the performance of models trained from scratch. These characteristics of language models motivated researchers to train larger PLMs on even bigger datasets and found that scaling model and dataset size further improve the generalization abilities. ',\n",
              " '',\n",
              " 'Now modern LLMs are capable of performing various tasks like code generation, text generation, tool manipulation, reasoning, and understanding in zero-shot and few-shot settings in diverse domains, even without requiring any fine-tuning on downstream tasks~\\\\cite{GPT-3, BLOOM, OPT}. Such generalization was previously unattainable with smaller models, marking a significant advancement in language modeling. This development has sparked enthusiasm and excitement within the research community for the enhancement of LLM architectures and training strategies, leading to the development of numerous LLMs~\\\\cite{T5, mT5, CPM-2, GPT-3, BLOOM, OPT, PaLM}. ',\n",
              " '',\n",
              " '% The graph~\\\\ref{fig:num_LLMs_barchart} illustrates a growing trend in the number of released LLMs, including open-source and closed-source models, over the years.',\n",
              " '',\n",
              " 'The graph presented in Fig~\\\\ref{fig:num_LLMs_barchart} depicts an increasing trend in the number of released LLMs, including open-source and closed-source models, over the years. Furthermore, Fig~\\\\ref{fig:LLMs_bubblechart} highlights the names of significant releases of various LLMs.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '% In recent years, pre-trained language models (PLM) have shown tremendous generalization abilities for text understanding and generation tasks while being trained in a self-supervised setting on a large corpus~\\\\cite{Bert, ELMO, BART}. PLMs perform better when fine-tuned for downstream tasks as compared to the models trained from scratch. These characteristics of language models motivated researchers to train larger PLMs on even bigger datasets and found scaling model and dataset size further improve the generalization abilities. These huge models are given the name Large Language Models (LLMs).  \\\\\\\\',\n",
              " '% LLMs are capable to perform diverse tasks like code generation, text generation, tool manipulation, reasoning, and understanding in zero-shot and few-shot settings in diverse domains, without requiring any fine-tuning on downstream tasks~\\\\cite{GPT-3, BLOOM, OPT}. Such a generalized capacity was not possible with smaller models. This created a spark in the research community to propose better LLMs architectures and training strategies leading to the development of numerous LLMs~\\\\cite{T5, mT5, CPM-2, GPT-3, BLOOM, OPT, PaLM}.   \\\\\\\\',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'During the early days of Large Language Models (LLMs), many research efforts focused on developing models for transfer learning to downstream tasks ~\\\\cite{T5, mT5, UL2} until the emergence of models like GPT-3~\\\\cite{GPT-3}, which demonstrated impressive performance even without fine-tuning. Due to the closed-source nature of GPT-3, there was a demand for open-source alternatives, leading to the development of various models ~\\\\cite{BLOOM, OPT} operating at the scale of GPT-3 and trained on extensive web-based datasets~\\\\cite{Common_Crawl,Wikipedia,Openwebtext_Dataset,BQ_Dataset}.   Subsequently, researchers proposed several architectural designs and training strategies that showed superior performance compared to GPT-3 across various tasks ~\\\\cite{UL2, PaLM, U-PaLM, mtnlg}. ',\n",
              " '',\n",
              " \"The performance of LLMs improves further with instruction fine-tuning, outperforming pre-trained LLMs on various benchmarks~\\\\cite{T0, mT0andBLOOMZ}. Instruction fine-tuning of LLMs refers to a specific training approach by incorporating additional prompts or instructions during the fine-tuning phase to guide the output and thus enable the users to have more fine-grained control over the outputs of LLMs. These prompts can be natural language instructions or example demonstrations based on the task's requirement. In the literature, different datasets have been curated for instruction fine-tuning. These datasets include more instances and tasks that further improve the performance over baselines~\\\\cite{OPT_IML,mT0andBLOOMZ,Flan,Tk-INSTRUCT}. When performing instruction fine-tuning, all the model parameters need to be updated. However, parameter-efficient fine-tuning takes a different approach by updating only a small number of parameters while still maintaining good performance. This method keeps the original model frozen and adds a few extra parameters at different locations within the model~\\\\cite{LMAdapted, LMAdapter_2, LMAdapter_3, Prompt_Tuning, Prefix_Tuning}. This approach helps achieve efficient fine-tuning while minimizing the impact on the model's overall performance.\",\n",
              " '% Instruction-tuning requires updating all the parameters. Compared to this, parameter-efficient fine-tuning updates only a small number of parameters without dropping the performance significantly. In this method, the original model is kept frozen and fewer additional parameters are added to the model at different positions as given in~\\\\cite{LMAdapted, LMAdapter_2, LMAdapter_3, Prompt_Tuning, Prefix_Tuning}.  ',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'The literature presents numerous pre-trained and fine-tuned models for LLMs with diverse approaches. Some survey papers provide an overview of augmented techniques in LLMs~\\\\cite{survey_1}. Additionally, a comprehensive review is available that covers architectures, fine-tuning, emergent abilities, and the usability of LLMs~\\\\cite{Survey_LLM}. Another survey provides a historical account of foundation models~\\\\cite{survey_2}. However, these review papers do not delve into the specific details of individual models, offering only a surface-level understanding of architectures and training methods. In contrast, our paper aims to provide a more in-depth analysis of individual LLMs by discussing fine-grained details.',\n",
              " '',\n",
              " '',\n",
              " 'The absence of comprehensive and detailed discussions, particularly from a historical standpoint, regarding the architecture, training datasets, and other granular aspects of Large Language Models (LLMs), has motivated us to undertake an exhaustive survey. This survey aims to provide an in-depth and comprehensive analysis of LLMs, delving into the details surrounding their development, architecture, training datasets, and related components.',\n",
              " '\\\\begin{itemize}',\n",
              " '\\\\item To the best of our knowledge, this is the first comprehensive survey paper that discusses fine-grained details of LLMs. ',\n",
              " '%Write about our survey and the contributions etc',\n",
              " '\\\\item We provide a thorough analysis of the existing literature regarding various LLMs architectures',\n",
              " 'and their categorization. Moreover, we also discussed the basics of the LLMs to make the paper self-sufficient and productive for the reader not familiar with LLMs.',\n",
              " '',\n",
              " '% \\\\item We have also identified future research directions that might help researchers to further enhance the methods and achieve new state-of-the-art.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\\\item Our paper focuses on providing comprehensive details for each LLM model and covers aspects such as architecture modifications, training objectives, datasets used, strategies for stable training, key findings, suggestions, and challenges encountered during training.',\n",
              " '',\n",
              " '\\\\item We aim to summarize these crucial details in our paper to assist researchers in identifying better architectures and training approaches for their work.',\n",
              " '\\\\end{itemize}',\n",
              " '',\n",
              " '',\n",
              " '% Our paper focuses on providing comprehensive details for each LLM model and covers aspects such as architecture modifications, training objectives, datasets used, strategies for stable training, key findings, suggestions, and challenges encountered during training.',\n",
              " 'Our paper complements a recent survey paper~\\\\cite{Survey_LLM} on LLMs that covers topics like data preprocessing, data cleaning, scaling laws, emergent abilities, adaptation tuning, and utilization. While the survey paper provides information on architectures, it does not delve into the fine-grained details of architectural changes, training objectives, and specific findings from proposed LLMs. ',\n",
              " '% For every LLM, we provide details on changes in architecture, training objective, datasets, and strategies to stable training, findings and suggestions by every LLM, and other issues faced during training. Our paper complements a recent survey paper on LLMs that explains various topics on data preprocessing, data cleaning, scaling laws, emergent abilities, adaptation tuning, and utilization. It also provides information on architectures but does not discuss fine details on architectural changes, training objectives and findings of proposed LLMs. We summarize these details in this paper with the aim to help researchers in identifying better architectures and training approaches in their research work.    \\\\\\\\',\n",
              " 'We discuss LLMs model with a minimum of 10 billion parameters or more similar to the paper~\\\\cite{Survey_LLM}. Models smaller than this scale are not discussed in our paper. One can refer to review papers such as~\\\\cite{survey_smaller_LLMs_1, survey_smaller_LLMs_2, survey_1} for exploring smaller models.',\n",
              " '',\n",
              " '',\n",
              " '% Similar to the paper~\\\\cite{Survey_LLM}, our definition of LLM is a model with at-least 10B or more parameters. We do not discuss models smaller than the 10B scale. One can see a review of these smaller models in~\\\\cite{survey_smaller_LLMs_1, survey_smaller_LLMs_2, survey_1}. \\\\\\\\',\n",
              " '     ',\n",
              " '',\n",
              " 'The organization of this paper is as follows. Section~\\\\ref{sec:Background} discusses the background of LLMs, offering a concise overview of the essential building blocks that constitute these models. We discuss architectural styles, fine-tuning strategies, libraries, and distributed training methods. This section serves as a foundation for understanding the subsequent discussions on LLMs. Section~\\\\ref{sec_review} focuses on LLMs overview, architectures, and training pipelines and strategies. Section~\\\\ref{sec:Findings} presents the key findings derived from each LLM. Section~\\\\ref{Model_Configurations_} highlights the configuration and parameters that play a crucial role in the functioning of these models. The LLM training and evaluation benchmarks are discussed in section~\\\\ref{Datasets_and_Evaluation_}, followed by concluding remarks and future direction in the conclusion section.',\n",
              " '\\\\section{Background}',\n",
              " '\\\\label{sec:Background}',\n",
              " '\\\\subsection{Tokenization}',\n",
              " '\\\\label{ss:tokenization}',\n",
              " 'LLMs are trained on text to predict text, and similar to other natural language processing systems, they use tokenization~\\\\cite{webster1992tokenization} as the essential preprocessing step. It aims to parse the text into non-decomposing units called tokens. Tokens can be characters, subwords~\\\\cite{unigramLM}, symbols~\\\\cite{bpe}, or words, depending on the size and type of the model. Some of the commonly used tokenization schemes in LLMs are briefed here. Readers are encouraged to refer to~\\\\cite{tokenizationsurvey} for a detailed survey.',\n",
              " '',\n",
              " '\\\\subsubsection{WordPiece~\\\\cite{wordpiece}}',\n",
              " '\\\\label{ss:wordpiece}',\n",
              " 'It was introduced in~\\\\cite{wordpiece} as a novel text segmentation technique for Japanese and Korean languages to improve the language model for voice search systems. WordPiece selects tokens that increase the likelihood of an n-gram-based language model trained on the vocabulary composed of tokens.',\n",
              " '',\n",
              " '\\\\subsubsection{BPE~\\\\cite{bpe}}',\n",
              " '\\\\label{ss:bpe}',\n",
              " 'Byte Pair Encoding (BPE) has its origin in compression algorithms. It is an iterative process of generating tokens where pairs of adjacent \\\\textit{symbols} are replaced by a new symbol, and the occurrences of the most occurring symbols in the input text are merged together.',\n",
              " '',\n",
              " '\\\\subsubsection{UnigramLM~\\\\cite{unigramLM}}',\n",
              " '\\\\label{ss:unigramLM}',\n",
              " 'In this tokenization, a simple unigram language model (LM) is trained using an initial vocabulary of \\\\textit{subword} units. The vocabulary is pruned iteratively by removing the lowest-probability items from the list, which are the worst performing on the unigram LM.',\n",
              " '',\n",
              " '\\\\subsection{Attention}',\n",
              " '\\\\label{ss:attention}',\n",
              " 'Attention, particularly \\\\textit{selective attention}, has been widely studied under perception, psychophysics and psychology. Selective attention can be conceived as \\\\enquote{the programming by the O of which stimuli will be processed or encoded and in what order this will occur}~\\\\cite{selectiveattention}. While this definition has its roots in visual perception, it has uncanny similarities with the recently formulated \\\\textit{attention}~\\\\cite{attention, Transformers} (which stimuli will be processed) and \\\\textit{positional encoding} (in what order this will occur)~\\\\cite{Transformers} in LLMs. We discuss both in sections~\\\\ref{ss:llmattention} and~\\\\ref{ss:encodingposition}, respectively. ',\n",
              " '',\n",
              " '\\\\subsection{Attention in LLMs}',\n",
              " '\\\\label{ss:llmattention}',\n",
              " 'The attention mechanism computes a representation of the input sequences by relating different positions (\\\\textit{tokens}) of these sequences. There are various approaches to calculating and implementing attention, out of which some famous types are given below.',\n",
              " '',\n",
              " '\\\\subsubsection{Self-Attention~\\\\cite{Transformers}}',\n",
              " '\\\\label{ss:selfattention}',\n",
              " 'The self-attention is also known as intra-attention since all the queries, keys and values come from the same block (encoder or decoder). The self-attention layer connects all the sequence positions to each other with $O(1)$ space complexity which is highly desirable for learning long-range dependencies in the input. ',\n",
              " '',\n",
              " '\\\\subsubsection{Cross Attention}',\n",
              " '\\\\label{ss:crossattention}',\n",
              " 'In encoder-decoder architectures, the outputs of the encoder blocks act as the queries to the intermediate representation of the decoder, which provides the keys and values to calculate a representation of the decoder conditioned on the encoder. This attention is called cross-attention.',\n",
              " '',\n",
              " '\\\\subsubsection{Full Attention}',\n",
              " '\\\\label{ss:fullattention}',\n",
              " 'The naive implementation of calculating self-attention is known as full attention.',\n",
              " '',\n",
              " '\\\\subsubsection{Sparse Attention~\\\\cite{sparse_transformer}}',\n",
              " '\\\\label{ss:sparseattention}',\n",
              " 'The self-attention has a time complexity of $O(n^2)$, which becomes prohibitive when scaling the LLMs to large context windows. An approximation to the self-attention was proposed in~\\\\cite{sparse_transformer}, which greatly enhanced the capacity of GPT series LLMs to process a greater number of input tokens in a reasonable time.',\n",
              " '',\n",
              " '\\\\subsubsection{Flash Attention~\\\\cite{flashattention}}',\n",
              " '\\\\label{ss:flashattention}',\n",
              " 'The bottleneck for calculating the attention using GPUs lies in the memory access rather than the computational speed. Flash Attention uses the classical input tiling approach in order to process the blocks of the input in GPU on-chip SRAM rather than doing IO for every token from the High Bandwith Memory (HBM). An extension of this approach to sparse attention follows the speed gains of the full attention implementation. This trick allows even greater context-length windows in the LLMs as compared to those LLMs with sparse attention.',\n",
              " '',\n",
              " '\\\\subsection{Encoding Positions}',\n",
              " '\\\\label{ss:encodingposition}',\n",
              " 'The \\\\textit{attention} modules do not consider the order of processing by design. Transformer~\\\\cite{Transformers} introduced \\\\enquote{positional encodings} to feed information about the position of the tokens in input sequences. Several variants of positional encoding have been proposed~\\\\cite{alibi, su2021roformer}. Interestingly, a recent study~\\\\cite{NoPE} suggests that adding this information may not matter for the state-of-the-art decoder-only Transformers.',\n",
              " '',\n",
              " '\\\\subsubsection{Absolute}',\n",
              " 'This is the most straightforward approach to adding the sequence order information by assigning a unique identifier to each position of the sequence before passing it to the attention module.',\n",
              " '',\n",
              " '\\\\subsubsection{Relative}',\n",
              " 'In order to pass the information of the relative dependencies of different tokens appearing at different locations in the sequence, a relative positional encoding is calculated by some kind of learning. Two famous types of relative encodings are: ',\n",
              " '',\n",
              " '\\\\noindent',\n",
              " '\\\\emph{\\\\textbf{Alibi}~\\\\cite{alibi}} In this approach, a scalar bias is subtracted from the attention score calculated using two tokens which increases with the distance between the positions of the tokens. This learned approach effectively favors using recent tokens for attention. ',\n",
              " '',\n",
              " '\\\\noindent',\n",
              " '\\\\emph{\\\\textbf{RoPE}} Keys, queries and values are all vectors in the LLMs. RoPE~\\\\cite{su2021roformer} involves the rotation of the query and key representations at an angle proportional to their absolute positions of the tokens in the input sequence. This step results in a relative positional encoding scheme which decays with the distance between the tokens.',\n",
              " '',\n",
              " '\\\\subsection{Activation Functions}',\n",
              " '\\\\label{sec:activation functions}',\n",
              " 'The activation functions serve a crucial role in the curve-fitting abilities of the neural networks, as proved in~\\\\cite{activationfunction}. The modern activation functions used in LLMs are different from the earlier squashing functions but are critical to the success of LLMs. We discuss these activation functions in this section.  ',\n",
              " '',\n",
              " '\\\\subsubsection{ReLU~\\\\cite{relu}}',\n",
              " '\\\\label{ss:relu}',\n",
              " 'Rectified linear unit (ReLU) is defined as',\n",
              " '\\\\begin{equation}',\n",
              " 'ReLU(x) = max(0,x)    ',\n",
              " '\\\\label{eq:relu}',\n",
              " '\\\\end{equation}',\n",
              " '',\n",
              " '\\\\subsubsection{GeLU~\\\\cite{gelu}}',\n",
              " '\\\\label{ss:gelu}',\n",
              " 'Gaussian Error Linear Unit (GeLU) is the combination of ReLU,  dropout~\\\\cite{srivastava2014dropout} and zoneout~\\\\cite{krueger2016zoneout}. It is the most widely used activation function in contemporary LLM literature.',\n",
              " '',\n",
              " '\\\\subsubsection{GLU variants~\\\\cite{shazeer2020glu}}',\n",
              " '\\\\label{ss:gluvariants}',\n",
              " 'Gated Linear Unit~\\\\cite{glu} is a neural network layer that is an element-wise product ($\\\\otimes$) of a linear transformation and a sigmoid transformed ($\\\\sigma$) linear projection of the input given as',\n",
              " '\\\\begin{equation}',\n",
              " 'GLU(x, W, V, b, c) = (xW + b) \\\\otimes \\\\sigma (xV + c),',\n",
              " '\\\\end{equation}',\n",
              " 'where $X$ is the input of layer and $l$, $W, b, V \\\\textnormal{ and }c$ are learned parameters.',\n",
              " '',\n",
              " 'GLU was modified in~\\\\cite{shazeer2020glu} to evaluate the effect of different variations in the training and testing of transformers, resulting in better empirical results. Here are the different GLU variations introduced in~\\\\cite{shazeer2020glu} and used in LLMs. ',\n",
              " '',\n",
              " '\\\\begin{align*}',\n",
              " 'ReGLU(x, W, V, b, c) &= max(0, xW + b) \\\\otimes , \\\\\\\\',\n",
              " 'GEGLU(x, W, V, b, c) &= GELU(xW + b) \\\\otimes (xV + c), \\\\\\\\',\n",
              " 'SwiGLU(x, W, V, b, c, \\\\beta) &= Swish\\\\beta (xW + b) \\\\otimes (xV + c).          ',\n",
              " '\\\\end{align*}',\n",
              " '',\n",
              " '\\\\subsection{Layer Normalization}',\n",
              " '\\\\label{sec:layernormalization}',\n",
              " 'Layer normalization leads to faster convergence and is a widely used component in transformers. In this section, we provide different normalization techniques widely used in LLM literature.',\n",
              " '',\n",
              " '\\\\subsubsection{LayerNorm}',\n",
              " '\\\\label{ss:layernorm}',\n",
              " 'Layer norm computes statistics over all the hidden units in a layer $(l)$ as follows: ',\n",
              " '\\\\begin{equation}',\n",
              " 'u^l = \\\\frac{1}{n} \\\\sum_{i}^{n} a_i^l \\\\hspace{2em} \\\\sigma^l = \\\\sqrt{\\\\frac{1}{n} \\\\sum_{i}^{n} (a_i^l - u^l)^2} ,',\n",
              " '\\\\end{equation}',\n",
              " 'where $n$ is the number of neurons in the layer $l$ and $a_i^l$ is the summed input of the $i$ neuron in layer $l$. LayerNorm provides invariance to rescaling of the weights and re-centering of the distribution.',\n",
              " '',\n",
              " '\\\\subsubsection{RMSNorm}',\n",
              " '~\\\\cite{rmsnorm} proposed that the invariance properties of LayerNorm are spurious, and we can achieve the same performance benefits as we get from LayerNorm by using a computationally efficient normalization technique that trades off re-centering invariance with speed. LayerNorm gives the normalized summed input to layer $l$ as follows',\n",
              " '\\\\begin{equation}',\n",
              " '\\\\overline{a_i^l} = \\\\frac{a_i^l - u^l}{\\\\sigma}g_i^l ',\n",
              " '\\\\end{equation}',\n",
              " 'where $g_i^l$ is the gain parameter. RMSNorm~\\\\cite{rmsnorm} modifies $\\\\overline{a_i^l}$ as',\n",
              " '',\n",
              " '\\\\begin{equation}',\n",
              " '\\\\overline{a_i^l} = \\\\frac{a_i^l}{\\\\textnormal{RMS}(\\\\mathbf{a}^l)} g_i^l, \\\\hspace{0.3em} \\\\textnormal{where} \\\\hspace{0.3em} \\\\textnormal{RMS}(\\\\mathbf{a}^l) = \\\\sqrt{\\\\frac{1}{n}\\\\sum_{i}^{n}(a_i^l)^2}.',\n",
              " '\\\\end{equation}',\n",
              " '',\n",
              " '\\\\subsubsection{Pre-Norm and Post-Norm}',\n",
              " 'LLMs use transformer~\\\\cite{Transformers} architecture with some variations. The original implementation~\\\\cite{Transformers} used layer normalization after the residual connection, commonly called post-LN, concerning the order of \\\\textit{Multihead attention â€“ Residual â€“ LN}. There is another order of the normalization, referred to as pre-LN~\\\\cite{preLN} due to the position of the normalization step before the self-attention layer as in \\\\textit{LN â€“ Multihead attention â€“ Residual}. Pre-LN is known to provide more stability in the training~\\\\cite{shleifer2021normformer}. ',\n",
              " '',\n",
              " '\\\\subsubsection{DeepNorm}',\n",
              " 'While pre-LN has certain benefits over post-LN training, pre-LN training has an unwanted effect on the gradients~\\\\cite{shleifer2021normformer}. The earlier layers have larger gradients than those at the bottom. DeepNorm~\\\\cite{deepnorm} mitigates these adverse effects on the gradients. It is given as',\n",
              " '\\\\begin{equation}',\n",
              " '\\\\mathbf{x}^{l_f} = LN(\\\\alpha \\\\mathbf{x}^{l_p} + G^{l_p}(\\\\mathbf{x}^{l_p}, {\\\\boldmath\\\\theta}{^{l_p}}), ',\n",
              " '\\\\end{equation}',\n",
              " 'where $\\\\alpha$ is a constant and $\\\\theta^{l_p}$ represents the parameters of layer $l_p$. These parameters are scaled by another constant $\\\\beta$. Both of these constants depend only on the architecture. ',\n",
              " '',\n",
              " '\\\\subsection{Distributed LLM Training}',\n",
              " 'This section describes distributed LLM training approaches briefly. A more detailed discussion is available in~\\\\cite{Survey_LLM}. ',\n",
              " '',\n",
              " '\\\\subsubsection{Data Parallelism}',\n",
              " 'Data parallelism replicates the model on multiple devices where data in a batch gets divided across devices. At the end of each training iteration weights are synchronized across all devices.    ',\n",
              " '',\n",
              " '\\\\subsubsection{Tensor Parallelism}',\n",
              " 'Tensor parallelism shards a tensor computation across devices. It is also known as horizontal parallelism or intra-layer model parallelism.',\n",
              " '',\n",
              " '\\\\subsubsection{Pipeline Parallelism}',\n",
              " 'Pipeline parallelism shards model layers across different devices. This is also known as vertical parallelism.',\n",
              " '',\n",
              " '\\\\subsubsection{Model Parallelism}',\n",
              " 'A combination of tensor and pipeline parallelism is known as model parallelism.',\n",
              " '\\\\subsubsection{3D Parallelism}',\n",
              " 'A combination of data, tensor, and model parallelism is known as 3D parallelism.',\n",
              " '',\n",
              " '\\\\subsubsection{Optimizer Parallelism}',\n",
              " 'Optimizer parallelism also known as zero redundancy optimizer~\\\\cite{ZeroOpt} implements optimizer state partitioning, gradient partitioning, and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible. ',\n",
              " '',\n",
              " '\\\\subsubsection{Rematerialization}',\n",
              " '',\n",
              " '\\\\subsection{Libraries}',\n",
              " 'Some commonly used libraries for LLM training are: 1) Transformer~\\\\cite{Lib_Transformers}, 2) DeepSpeed~\\\\cite{Lib_DeepSpeed}, 3) Megatraon-LM~\\\\cite{Lib_Megatron}, 4) JAX~\\\\cite{Lib_Jax}, 5) Colossal-AI~\\\\cite{Lib_Colossal}, 6) BMTrain~\\\\cite{Lib_Bmtrain}, 7) FastMoE~\\\\cite{Lib_Fastmoe}, and frameworks are 1) MindSpore~\\\\cite{Lib_Mindspore}, 2) PyTorch~\\\\cite{Lib_Pytorch}, 3) Tensorflow~\\\\cite{Lib_Tensorflow}, 4) MXNet~\\\\cite{Lib_Mxnet}.  ',\n",
              " '',\n",
              " '\\\\subsection{Data PreProcessing}',\n",
              " 'This section briefly summarizes data preprocessing techniques used in LLMs training. More details on this are available in~\\\\cite{Survey_LLM}.',\n",
              " '',\n",
              " '\\\\subsubsection{Quality Filtering}',\n",
              " 'For better results, training data quality is essential. Some approaches to filtering data are: 1) classifier-based and 2) heuristics-based. Classifier-based approaches train a classifier on high-quality data and predict the quality of text for filtering, whereas heuristics-based employ some rules for filtering like language, metrics, statistics, and keywords. ',\n",
              " '',\n",
              " '\\\\subsubsection{Data Deduplication}',\n",
              " 'Duplicated data can affect model performance and increase data memorization; therefore, to train LLMs, data deduplication is one of the preprocessing steps. This can be performed at multiple levels, like sentences, documents, and datasets.',\n",
              " '',\n",
              " '\\\\subsubsection{Privacy Reduction}',\n",
              " 'Most of the training data for LLMs is collected through web sources. This data contains private information; therefore, many LLMs employ heuristics-based methods to filter information such as names, addresses, and phone numbers to avoid learning the mentioned information.',\n",
              " '',\n",
              " '\\\\subsection{Architectures}',\n",
              " 'Here we discuss the variants of the transformer architectures at a higher level which arise due to the difference in the application of the attention and the connection of transformer blocks. An illustration of attention patterns of these architectures is shown in Figure~\\\\ref{architectures}.',\n",
              " '',\n",
              " '\\\\subsubsection{Encoder Decoder}',\n",
              " 'Transformers were originally designed as sequence transduction models and followed other prevalent model architectures for machine translation systems. They selected encoder-decoder architecture to train human language translation tasks. This architecture is adopted by~\\\\cite{T5, UL2}. In this architectural scheme, an encoder encodes the input sequences to variable length context vectors, which are then passed to the decoder to maximize a joint objective of minimizing the gap between predicted token labels and the actual target token labels.',\n",
              " '',\n",
              " '\\\\subsubsection{Causal Decoder}',\n",
              " \"The underlying objective of an LLM is to predict the next token based on the input sequence. While additional information from the encoder binds the prediction strongly to the context, it is found in practice that the LLMs can learn as well absent this encoder~\\\\cite{decoderonly} and adding the context in the decoder. Similar to the original encoder-decoder architecture's decoder block, this decoder restricts the flow of information backward, i.e., the predicted token $t_k$ only depends on the tokens preceded by and up to $t_{k-1}$. This is the most widely used variant in the state-of-the-art LLMs.\",\n",
              " '',\n",
              " '\\\\subsubsection{Prefix Decoder}',\n",
              " 'The causal masked attention is reasonable in the encoder-decoder architectures where the encoder can attend to all the tokens in the sentence from every position using self-attention. This means that the encoder can also attend to tokens $t_{k+1}$ to $t_n$ in addition to the tokens from $t_1$ to $t_{k-1}$ while calculating the representation for $t_k$. But when we drop the encoder and only keep the decoder, we also lose this flexibility in attention. A variation in the decoder-only architectures is by changing the mask from strictly causal to fully visible on a portion of the input sequence, as shown in Figure~\\\\ref{architectures}. The Prefix decoder is also known as non-causal decoder architecture.',\n",
              " '',\n",
              " '%\\\\subsubsection{Mixture of Experts}',\n",
              " '%\\\\SA{Check this?}',\n",
              " '% We will discuss this in next version',\n",
              " '',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/architectures.png}',\n",
              " '\\\\caption{An example of attention patterns in language models, image is taken from~\\\\cite{LLM_Objectives}.}',\n",
              " '\\\\label{architectures}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '\\\\subsection{Pre-Training Objectives}',\n",
              " '\\\\label{sec:pretrainobjectives}',\n",
              " 'This section describes LLMs pre-training objectives. For more details see the paper~\\\\cite{LLM_Objectives}. ',\n",
              " '',\n",
              " '\\\\subsubsection{Full Language Modeling}',\n",
              " 'An autoregressive language modeling objective where the model is asked to predict future tokens given the previous tokens, an example is shown in Figure~\\\\ref{t_objectives}. ',\n",
              " '',\n",
              " '\\\\subsubsection{Prefix Language Modeling}',\n",
              " 'A non-causal training objective, where a prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in Figure~\\\\ref{t_objectives}.',\n",
              " '',\n",
              " '\\\\subsubsection{Masked Language Modeling}',\n",
              " 'In this training objective, tokens or spans (a sequence of tokens) are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure~\\\\ref{t_objectives}. ',\n",
              " '',\n",
              " '\\\\subsubsection{Unified Language Modeling}',\n",
              " 'Unified language modeling~\\\\cite{Unified_LM} is a combination of causal, non-causal, and masked language training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either left-to-right or right-to-left context.',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/training_objectives.png}',\n",
              " '\\\\caption{An example of language model training objectives, image from~\\\\cite{LLM_Objectives}.}',\n",
              " '\\\\label{t_objectives}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '\\\\subsection{Model Adaptation}',\n",
              " 'This section discusses various model adaptation techniques, where a model is pre-trained on large data and then adapted for downstream tasks. ',\n",
              " '',\n",
              " '\\\\subsubsection{Transfer Learning}',\n",
              " 'Fine-tuning a pre-trained model with data for the downstream task is known as transfer learning. In this type of model adaptation, the model is initialized with pre-trained weights and updated according to the new data. Some of the LLMs employing this technique are~\\\\cite{T5, mT5, UL2, U-PaLM}. ',\n",
              " '',\n",
              " '\\\\subsubsection{Parameter Efficient Learning}',\n",
              " 'The parameter efficient learning fine-tunes a few parameters either by adding new parameters to the model or the existing ones. ',\n",
              " '',\n",
              " '\\\\vspace{1mm}',\n",
              " '\\\\noindent',\n",
              " '\\\\emph{\\\\textbf{Prompt Tuning:}}~\\\\cite{Prompt_Tuning, Prompt_Tuning_2} adds trainable prompt token embeddings as prefixes or free-style to the input token embeddings. During fine-tuning only these embeddings parameters are trained for the downstream task while keeping the rest of the weights frozen.',\n",
              " '',\n",
              " '\\\\vspace{1mm}',\n",
              " '\\\\noindent',\n",
              " '\\\\emph{\\\\textbf{Prefix Tuning:}}~\\\\cite{Prefix_Tuning} adds task-specific trainable prefix vectors to the transformer layers, where only prefix parameters are fine-tuned, and the rest of the model stays frozen. The input sequence tokens can attend prefixes acting as virtual tokens.    ',\n",
              " '',\n",
              " '\\\\vspace{1mm}',\n",
              " '\\\\noindent',\n",
              " '\\\\emph{\\\\textbf{Adapter Tuning:}} module is an encoder-decoder architecture that is placed either sequential or parallel to the attention and feed-forward layers in the transformer block~\\\\cite{LMAdapter, LMAdapter_2, LMAdapter_3}. Only these layers are fine-tuned, and the rest of the model is kept frozen.  ',\n",
              " '',\n",
              " '\\\\subsubsection{Instruction Finetuning}',\n",
              " 'Instruction tuning is an approach to fine-tuning pre-trained models on instruction formatted data. Instructions generally comprise multiple tasks in plain natural language, guiding the model to respond according to the prompt and the input. The training data consists of an instruction and an input-output pair. More details on formatting instruction data and its various styles are available in~\\\\cite{Survey_LLM}.     ',\n",
              " '',\n",
              " '\\\\subsubsection{Alignment Tuning}',\n",
              " 'LLMs are prone to generate false, biased, and harmful text. Therefore, models are aligned using human feedback to make them helpful, honest, and harmless. Alignment involves asking LLMs to generate unexpected responses and then updating their parameters to avoid such responses~\\\\cite{Survey_LLM}.',\n",
              " '',\n",
              " '\\\\subsubsection{In-context Learning} ',\n",
              " 'No fine-tuning is involved in this type of model adaptation. The model is shown multiple input-output demonstration pairs to generate a desired input response. This adaptation is similar to a few-shot learning but without requiring any parameter update. More details on formatting demonstrations are available in~\\\\cite{Survey_LLM}. ',\n",
              " ' ',\n",
              " '\\\\subsubsection{Chain-of-thought Prompting}',\n",
              " \"Chain-of-thought prompting (CoT) is a special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with reasonings. Some examples in literature train LLMs with CoT reasoning, whereas other utilizes LLMs' CoT abilities without fine-tuning. More details on designing prompts are available in~\\\\cite{Survey_LLM}.\",\n",
              " '',\n",
              " '',\n",
              " '\\\\section{Large Language Models}',\n",
              " '\\\\label{sec_review}',\n",
              " 'This section reviews LLMs, briefly describing their architectures, training objectives, pipelines, datasets, and fine-tuning details.      ',\n",
              " '\\\\subsection{Pre-Trained Models}',\n",
              " '\\\\subsubsection{T5~\\\\cite{T5}}',\n",
              " 'An encoder-decoder model trained on the Colossal Clean Crwal Corpus (C4) dataset with a unified text-to-text training for all NLP problems, shown in Figure~\\\\ref{t5_image}. The model differs from the traditional transformer model~\\\\cite{Transformers}. These changes include no bias in layer normalization, using relative positional embedding, and placing layer normalization outside the residual path. The masked language modeling is used as a pre-training objective where spans (consecutive tokens) were replaced with a single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training, the model is fine-tuned using adapter layers~\\\\cite{LMAdapter} for downstream tasks.',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/T5.png}',\n",
              " '\\\\caption{Unified text-to-text training example, source image from~\\\\cite{T5}.}',\n",
              " '\\\\label{t5_image}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '\\\\subsubsection{mT5~\\\\cite{mT5}}',\n",
              " 'A multilingual T5 model~\\\\cite{T5} trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses GeGLU activation and trains with a vocab size of 250,000 to cover multiple languages. To avoid over-fitting or under-fitting for a language, mT5 employs a data sampling procedure to select samples from all languages. The paper suggests using a small amount of pre-training datasets, including all languages when fine-tuning for a task using English language data. This allows the model to generate non-English outputs.  ',\n",
              " '%$p(L) \\\\propto \\\\mid L \\\\mid^\\\\alpha$, where $p(L)$ is sampling probability, $L$ is language sampling count, and $\\\\alpha$ controls the sampling probability',\n",
              " '',\n",
              " '\\\\subsubsection{PanGu-$\\\\alpha$~\\\\cite{PanGU_alpha}}',\n",
              " 'An autoregressive model trained on 1.1TB Chinese data collected from Common Crawl, e-Books, encyclopedia, etc. Additional to the standard transformer model, it has a query layer after stacked transformer layers, example shown in Figure~\\\\ref{pangu_alpha_image}. The purpose of the query layer is to predict the next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism, given in Eq.~\\\\ref{PanGu_alpha_eq}. The model is trained using MindSpore with five-dimensional parallelism, i.e., data parallelism, op-level model parallelism, pipeline model parallelism, optimizer parallelism, and rematerialization. ',\n",
              " '\\\\begin{equation}',\n",
              " 'a = p_nW_h^qW_h^kTH_L^T',\n",
              " '\\\\label{PanGu_alpha_eq}',\n",
              " '\\\\end{equation}',\n",
              " '',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/PanGU_alpha.png}',\n",
              " '\\\\caption{The image is the article of~\\\\cite{PanGU_alpha}, showing an example of PanGu-$\\\\alpha$ architecture.}',\n",
              " '\\\\label{pangu_alpha_image}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '',\n",
              " '\\\\subsubsection{CPM-2~\\\\cite{CPM-2}}',\n",
              " 'Cost-efficient Pre-trained language Models (CPM-2) pre-trains bilingual (English and Chinese) 11B and 198B mixture-of-experts (MoE) models on the WuDaoCorpus~\\\\cite{WuDaoCorpus} dataset. It has an encoder-decoder architecture with a bidirectional encoder and a unidirectional decoder. The tokenization process removes \\\\enquote{\\\\_} white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance, starting with only the Chinese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover, to use the model for downstream tasks, CPM-2 experimented with both complete fine-tuning and prompt fine-tuning as in~\\\\cite{LMAdapted} where only prompt-related parameters are updated by inserting prompts at various positions, front, middle, and back. CPM-2 also proposes INFMOE, a memory-efficient framework with a strategy to dynamically offload parameters to the CPU for inference at a 100B scale. It overlaps data movement with inference computation for lower inference time.   ',\n",
              " '',\n",
              " '\\\\subsubsection{CodeGen~\\\\cite{CodeGen}}',\n",
              " 'CodeGen has similar architecture to the PaLM~\\\\cite{PaLM}, i.e., parallel attention, MLP layers, and RoPE embeddings. The model is trained on both natural language and programming language data sequentially (trained on the first dataset, then the second and so on) on the following datasets 1) PILE, 2) BIGQUERY and 3) BIGPYTHON. CodeGen proposed a multi-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen opensource a Multi-Turn Programming Benchmark (MTPB) to evaluate multi-step program synthesis.   ',\n",
              " '',\n",
              " '\\\\subsubsection{GPT-NeoX-20B~\\\\cite{GPT_NeoX}}',\n",
              " 'An auto-regressive model that largely follows GPT-3 with a few deviations in architecture design, trained on the Pile dataset without any data deduplication. GPT-NeoX has parallel attention and feed-forward layers in a transformer block, given in Eq.~\\\\ref{GPT-NeoX-20B_eq}, that increases throughput by 15\\\\%. It uses rotary positional embedding~\\\\cite{su2021roformer}, applying it to only 25\\\\% of embedding vector dimension as in~\\\\cite{GPT_J_6B}. This reduces the computation without performance degradation. Opposite to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult; therefore, the model chooses hyperparameters from the method~\\\\cite{GPT-3} and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism.   ',\n",
              " '\\\\begin{equation}',\n",
              " 'x + Attn(LN_1(x)) + FF(LN_2(x))',\n",
              " '\\\\label{GPT-NeoX-20B_eq}',\n",
              " '\\\\end{equation}',\n",
              " '',\n",
              " '\\\\subsubsection{UL2~\\\\cite{UL2}}',\n",
              " 'An encoder-decoder architecture trained using a mixture of denoisers (MoD) objectives. Denoisers include 1) R-Denoiser: a regular span masking, 2) S-Denoiser: which corrupts consecutive tokens of a large sequence and 3) X-Denoiser: which corrupts a large number of tokens randomly. During pre-training, UL2 includes a denoiser token from ${R, S, X}$ to represent a denoising setup. It helps improve fine-tuning performance for downstream tasks that bind the task to one of the upstream training modes. This MoD style of training outperforms the T5 model on many benchmarks.   ',\n",
              " '',\n",
              " '\\\\subsubsection{OPT~\\\\cite{OPT}}',\n",
              " 'It is a clone of GPT-3, developed with the intention to open-source a model that replicates GPT-3 performance. The model was trained using RoBERTa, The Pile, and PushShift.io Reddit datasets. Training of OPT employs dynamic loss scaling ~\\\\cite{S_mixed_precision} and restarts from an earlier checkpoint with a lower learning rate whenever loss divergence is observed. Overall, the performance of OPT-175B models is comparable to the GPT3-175B model.',\n",
              " '',\n",
              " '\\\\subsubsection{GLM-130B~\\\\cite{GLM-130B}}',\n",
              " 'GLM-130B is a bilingual (English and Chinese) model trained using an auto-regressive mask infilling pre-training objective similar to the GLM~\\\\cite{GLM}. This training style makes the model bidirectional as compared to GPT-3, which is unidirectional. Opposite to the GLM, the training of GLM-130B includes a small amount of multi-task instruction pre-training data (5\\\\% of the total data) along with the self-supervised mask infilling. To stabilize the training, it applies embedding layer gradient shrink. ',\n",
              " '',\n",
              " '\\\\subsubsection{BLOOM~\\\\cite{BLOOM}}',\n",
              " 'A causal decoder model trained on ROOTS corpus with the aim of open-sourcing an LLM. The architecture of BLOOM is shown in Figure~\\\\ref{bloom_image}, with differences like ALiBi positional embedding, an additional normalization layer after the embedding layer as suggested by the bitsandbytes\\\\footnote{https://github.com/TimDettmers/bitsandbytes} library. These changes stabilize training with improved downstream performance. ',\n",
              " '',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/BLOOM.png}',\n",
              " '\\\\caption{The BLOOM architecture example sourced from~\\\\cite{BLOOM}.}',\n",
              " '\\\\label{bloom_image}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '\\\\subsubsection{Galactica~\\\\cite{galactica}}',\n",
              " 'A large curated corpus of human scientific knowledge with 48 million papers, textbooks, lecture notes, millions of compounds and proteins, scientific websites, encyclopedias, and more are trained using metaseq library3, which is built on PyTorch and fairscale~\\\\cite{fairscale}. The model wraps reasoning datasets with $<work>$ token to provide step-by-step reasoning context to the model, which has been shown to improve the performance on reasoning tasks.',\n",
              " '',\n",
              " '\\\\subsubsection{GPT-3}',\n",
              " 'The architecture of GPT-3 is mostly the same as GPT-2~\\\\cite{GPT-2} but with dense and sparse attention in transformer layers similar to the Sparse Transformer~\\\\cite{sparse_transformer}. The model is trained on data taken from CommonCrawl, Webtext dataset, books corpora, and English-language Wikipedia. Large models can train on larger batch sizes with a lower learning rate; in order to decide the batch size during training, GPT-3 uses the gradient noise scale as in ~\\\\cite{batch_size_selec}. Overall, GPT-3 increases model parameters to 175B showing that the performance of large language models improves with the scale and is competitive with the fine-tuned models.  ',\n",
              " '',\n",
              " '\\\\subsubsection{Codex~\\\\cite{codex}}',\n",
              " 'This LLM is trained on a subset of public Python Github repositories to generate code from docstrings. Computer programming is an iterative process where the programs are often debugged and updated before fulfilling the requirements. Similarly to this, Codex generates 100 versions of a program by repetitive sampling for a given description, which produces a working solution for 77.5\\\\% of the problems passing unit tests.  Its powerful version powers Github Copilot\\\\footnote{https://github.com/features/copilot}.  ',\n",
              " '',\n",
              " '\\\\subsubsection{ERNIE 3.0~\\\\cite{ernie3}}',\n",
              " 'ERNIE 3.0 takes inspiration from multi-task learning to build a modular architecture using Transformer-XL~\\\\cite{dai2019transformer} as the backbone. The universal representation module is shared by all the tasks, which serve as the basic block for task-specific representation modules, which are all trained jointly for natural language understanding, natural language generation, and knowledge extraction. This LLM is primarily focused on the Chinese language, claims to train on the largest Chinese text corpora for LLM training, and achieved state-of-the-art in 54 Chinese NLP tasks.',\n",
              " '',\n",
              " '\\\\subsubsection{Jurassic-1~\\\\cite{lieber2021jurassic}}',\n",
              " \"A pair of auto-regressive language models, including a 7B-parameter J1-Large model and a 178B-parameter J1-Jumbo model. The Jurassic-1 models are mainly structured on the Transformer decoder module~\\\\cite{Transformers}, while the architecture modifications proposed by GPT-2~\\\\cite{GPT-2} are also incorporated. In particular, the training vocabulary items of Jurassic-1 comprise word pieces, complete words, and multi-word expressions without any word boundaries, where possible out-of-vocabulary instances are interpreted as Unicode bytes. In practice, data collected from publicly available resources are formed in the GPT-3's data structure to train the Jurassic-1 models following the conventional self-supervised auto-regressive training objective. Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more balanced depth-to-width self-attention architecture~\\\\cite{levine2020limits} and an improved tokenizer for a faster prediction based on broader resources, achieving a comparable performance in zero-shot learning tasks and a superior performance in few-shot learning tasks given the ability to feed more examples as a prompt.\",\n",
              " '',\n",
              " '\\\\subsubsection{HyperCLOVA~\\\\cite{hyperclova}}',\n",
              " 'The architecture is the same as that of GPT3~\\\\cite{GPT-3} with morphene aware byte level encoding tokenization step. A large Korean-centric corpus gathered from various sources (see table for details) is trained using Megatron LM. Prompt-based tuning is also applied to enhance performance on downstream tasks. The main objective of training this model is to see how the non-English language model fares compared to universally found English-based LMs.',\n",
              " '',\n",
              " '\\\\subsubsection{Yuan 1.0~\\\\cite{wu2021yuan}}',\n",
              " 'A large singleton language model with 245B parameters. The Yuan 1.0 is structured as a Transformer~\\\\cite{Transformers}. A Chinese corpus with 5TB of high-quality text is created to train Yuan 1.0 model, where the raw data is collected from Internet resources. A Massive Data Filtering System (MDFS) built on Spark is developed to process the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 with the aim of saving energy expenses and carbon emissions, a collaborative design of model architecture and large-scale distributed training is introduced. In practice, the Yuan 1.0 model performs well on text classification, Winograd Schema, natural language inference, and reading comprehension tasks.',\n",
              " '',\n",
              " '\\\\subsubsection{Gopher~\\\\cite{gopher}}',\n",
              " 'It is the largest of six causal decoder LLMs trained on the subsets of MassiveWeb, Books, C4, News, GitHub, and Wikipedia samples from high-quality curated MassiveText. The model is a modified version of Transformer architecture used in~\\\\cite{GPT-2}. The Gopher family of models ranges from 44M to 280B parameters in size to study the effect of \\\\textit{scale} on the LLMs performance. The 280B model beats GPT-3~\\\\cite{GPT-3}, Jurrasic-1~\\\\cite{lieber2021jurassic}, MT-NLG~\\\\cite{mtnlg}, and others on 81\\\\% of the evaluated tasks.',\n",
              " '',\n",
              " '\\\\subsubsection{ERNIE 3.0 TITAN~\\\\cite{ernie3titan}}',\n",
              " 'ERNIE 3.0 Titan extends ERNIE 3.0 by training a larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the-art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with factual consistency, ERNIE 3.0 Titan adds another task, \\\\textit{Credible and Controllable Generations}, to its multi-task learning setup. It introduces additional self-supervised adversarial and controllable language modeling losses to the pre-training step, which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations.',\n",
              " '',\n",
              " '\\\\subsubsection{GLaM~\\\\cite{du2022glam}}',\n",
              " \"Generalist Language Model (GLaM) represents a family of language models using a sparsely activated mixture-of-experts (MoE) structure~\\\\cite{shazeer2017outrageously,fedus2022switch}. Specifically, the architecture of GLaM is derived from a Decoder-only Transformer~\\\\cite{Transformers}. To gain more model capacity while reducing computation, the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLaM model, GLaM (64B/64E), is about 7$\\\\times$ larger than GPT-3~\\\\cite{GPT-3}, while only a part of the parameters is activated per input token. To effectively compare with GPT-3, the evaluation of GLaM follows the similar zero, one, and few-shot learning protocols as in GPT-3. Specifically, the largest GLaM (64B/64E) model achieves better overall results while consuming only one-third of GPT-3's training energy.\",\n",
              " '',\n",
              " '\\\\subsubsection{LaMDA~\\\\cite{thoppilan2022lamda}}',\n",
              " \"A family of Transformer-based neural language models for dialog ranging from 2B to 137B parameters. The model architecture of LaMDA follows a decoder-only Transformer~\\\\cite{Transformers} language model. LaMDA is pre-trained on public dialog data, public dialog utterances, and public web documents. Particularly, more than 90\\\\% of the pre-training data is in English. Particularly, LaMDA aims to produce responses that exhibit high levels of quality, safety, and groundedness. To achieve this, discriminative and generative fine-tuning techniques are incorporated to enhance the model's safety and quality aspects. As a result, the LaMDA models can be utilized as a general language model performing various tasks.\",\n",
              " '',\n",
              " '\\\\subsubsection{MT-NLG~\\\\cite{mtnlg}}',\n",
              " 'A causal decoder transformer trained on two snapshots of Common Crawl along with some other datasets given in table \\\\ref{datasets}. MT-NLG uses 8-way tensor slicing by Megatron for memory efficiency and 35-way pipeline parallelism using DeepSpeed for compute efficiency to train a 530B model, roughly 3$\\\\times$ GPT-3 model parameters. This model beats GPT-3 on a number of evaluations.',\n",
              " '',\n",
              " '\\\\subsubsection{AlphaCode~\\\\cite{li2022competition}}',\n",
              " 'A set of large language models designed for competition-level code generation tasks. Basically, the AlphaCode models follow an encoder-decoder transformer architecture~\\\\cite{Transformers} ranging from 300M to 41B parameters. Moreover, the multi-query attention~\\\\cite{shazeer2019fast} is applied to reduce memory and cache costs. Since competitive programming problems highly require deep reasoning and an understanding of complex natural language algorithms, the AlphaCode models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on a new competitive programming dataset named CodeContests. Particularly, the CodeContests dataset mainly contains problems, solutions, and test cases collected from the Codeforces platform\\\\footnote{https://codeforces.com/}. In practice, standard language modeling objectives are used for the pre-training on GitHub code data, while GOLD~\\\\cite{pang2020text} with tempering~\\\\cite{dabre2020softmax} serve as the training objective for the fine-tuning on CodeContests data. To evaluate the performance of AlphaCode, simulated programming competitions are hosted on the Codeforces platform: overall, AlphaCode ranks at the top 54.3\\\\% among over 5000 competitors, where its Codeforces rating is within the top 28\\\\% of recently participated users.',\n",
              " '',\n",
              " '\\\\subsubsection{Chinchilla~\\\\cite{chinchilla}}',\n",
              " 'A causal decoder trained on the same dataset as the Gopher~\\\\cite{gopher} but with a little different data sampling distribution (sampled from MassiveText). The model architecture is similar to the one used for Gopher, with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the relationship that model size should be doubled for every doubling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens are trained to get the estimates for compute-optimal training under a given budget. The authors train a 70B model with the same compute budget as Gopher (280B) but with 4 times more data. It outperforms Gopher~\\\\cite{gopher}, GPT-3~\\\\cite{GPT-3}, and others on various downstream tasks, after fine-tuning. ',\n",
              " '',\n",
              " '',\n",
              " '\\\\subsubsection{PaLM}',\n",
              " 'A causal decoder model trained on a dataset of 780B tokens collected from webpages, books, Wikipedia, news, and others, given in Table~\\\\ref{datasets}. The PaLM has parallel attention and feed-forward layers similar to Eq.~\\\\ref{GPT-NeoX-20B_eq}, speeding up training 15 times faster. Additional changes to the conventional transformer model include SwiGLU activation, RoPE embeddings, multi-query attention that saves computation cost during decoding, and shared input-output embeddings. During training, loss spiking was observed, and to fix it, model training was restarted from a 100 steps earlier checkpoint by skipping 200-500 batches around the spike. Moreover, the model was found to memorize around 2.4\\\\% of the training data at the 540B model scale, whereas this number was lower for smaller models.   ',\n",
              " '',\n",
              " '\\\\subsubsection{AlexaTM~\\\\cite{soltan2022alexatm}}',\n",
              " 'The first multilingual sequence-to-sequence model (20B parameter) is capable of in-context learning. The pre-training data is collected from Wikipedia and mC4 dataset~\\\\cite{mT5} covering 12 programming languages. To enable the AlexaTM 20B model to perform on both spoken and written cases, all data is converted into spoken format via a written-to-spoken formatter. In addition to pre-training on the denoising task, an extra Causal Language Modeling (CLM) task is performed for 20\\\\% of the time to help the model with efficient in-context learning. In practice, the model is asked to continue the input instead of denoising the input once a special CLM token is attached to the beginning of the input.',\n",
              " '',\n",
              " '\\\\subsubsection{Sparrow~\\\\cite{glaese2022improving}}',\n",
              " \"An information-seeking dialogue agent is trained to gain more helpfulness and correctness with less harm. Two additions are proposed to help human raters judge agent behavior: the first is the specific natural language rules that need raters to rate separately, and the second is to make the agent show proof from sources that support factual claims when collecting opinions about the model's statements. The architecture of the Sparrow models is based on Dialogue Prompted Chinchila 70B~\\\\cite{chinchilla}. Human data is collected for rule violations and per-turn response preferences, which mainly aims to train preference reward models (preference RMs) and a rule reward model (rule RM). In practice, reinforcement learning with advantage actor-critic (A2C)~\\\\cite{mnih2016asynchronous} is used to train the initialized Chinchilla model; the rule RM estimated rule violation rate and the preference RMs estimated per-turn response preferences are jointly optimized. Given experimental data, Sparrow's evidence can support the sampled response for factual questions 78\\\\% of the time. Moreover, Sparrow is highly resistant to human adversarial probing since it only violates the defined rules 8\\\\% of the time when probed.\",\n",
              " '',\n",
              " '\\\\subsubsection{U-PaLM~\\\\cite{U-PaLM}}',\n",
              " 'This method trains PaLM for 0.1\\\\% additional compute with UL2 (also named as UL2Restore) objective~\\\\cite{UL2} using the same dataset and outperforms baseline significantly on various NLP tasks, including zero-shot, few-shot, commonsense reasoning, CoT, etc. Training with UL2R involves converting a causal decoder PaLM to a non-causal decoder PaLM and employing 50\\\\% sequential denoising, 25\\\\% regular denoising, and 25\\\\% extreme denoising loss functions.',\n",
              " '',\n",
              " '\\\\subsubsection{LLaMA~\\\\cite{touvron2023llama}}',\n",
              " \"A set of foundation language models varying from 7B to 65B parameters. The overall architecture of LLaMA follows the Transformer~\\\\cite{Transformers}, while a few subsequently proposed improvements of normalization~\\\\cite{rmsnorm}, activation~\\\\cite{shazeer2020glu}, and positional embedding~\\\\cite{su2021roformer} operations are incorporated for better performances. About 67\\\\% of LLaMA's pre-training data is collected from English CommonCrawl following the CCNet method~\\\\cite{wenzek2019ccnet}. LLaMA and the associated variants are widely used for parameter-efficient tuning, especially for instruction following tasks.\",\n",
              " '',\n",
              " '\\\\subsubsection{PanGu-$\\\\Sigma$~\\\\cite{PanGu_sigma}}',\n",
              " 'An autoregressive model with parameters copied from PanGu-$\\\\alpha$ and extended to a trillion scale with Random Routed Experts (RRE), the architectural diagram is shown in Figure~\\\\ref{pangu_sigma_image}. RRE is similar to the MoE architecture, with distinctions at the second level, where tokens are randomly routed to experts in a domain instead of using a learnable gating method. The model has bottom layers densely activated and shared across all domains, whereas top layers are sparsely activated according to the domain. This training style allows extracting task-specific models and reduces catastrophic forgetting effects in case of continual learning. ',\n",
              " '',\n",
              " '\\\\begin{figure}[tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/pangu_sigma.png}',\n",
              " '\\\\caption{This example illustrates the PanGu-$\\\\sum$ architecture, as depicted in the image sourced from~\\\\cite{PanGu_sigma}.}',\n",
              " '\\\\label{pangu_sigma_image}',\n",
              " '\\\\end{figure}',\n",
              " '',\n",
              " '',\n",
              " '\\\\begin{table*}[!tbhp]',\n",
              " '',\n",
              " '%\\\\renewcommand\\\\tabcolsep{1pt}',\n",
              " '\\\\caption{Some of the critical findings and crucial discoveries of each \\\\emph{pre-trained} Large Language Model.}',\n",
              " '%\\\\caption{Important findings of pre-trained LLMs}',\n",
              " '\\\\begin{tabular}{lc}',\n",
              " '\\\\hline \\\\hline',\n",
              " '\\\\rowcolor{gray!50} Models & Findings \\\\& Insights\\\\\\\\ \\\\hline \\\\hline',\n",
              " 'T5 & \\\\begin{tabular}{c} ',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Encoder and decoder with shared parameters perform equivalently when parameters are not shared',\n",
              " '\\\\item Fine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}  \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'mT5 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Large multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi-lingual models perform worse',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'CPM-2 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Prompt fine-tuning requires updating very few parameters while achieving performance comparable to full model fine-tuning ',\n",
              " '\\\\item Prompt fine-tuning takes more time to converge as compared to full model fine-tuning ',\n",
              " '\\\\item Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences',\n",
              " '\\\\item In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator (aggregate information with the input text) for the model',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}  \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'PanGu-$\\\\alpha$ & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item LLMs are good at a few shot capabilities',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular} \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'CodeGen & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Multi-step prompting for code synthesis leads to a better user intent understanding and code generation',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'GPT-NeoX-20B & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Parallel attention + FF layers speed-up training 15\\\\% with the same performance as with cascaded layers',\n",
              " '\\\\item Initializing feed-forward output layers before residuals with scheme in~\\\\cite{Mesh_Transformer_JAX} avoids activations from growing with increasing depth and width',\n",
              " '\\\\item Training on Pile outperforms GPT-3 on five-shot ',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'UL2 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Mode switching training enables better performance on downstream tasks',\n",
              " '\\\\item CoT prompting outperforms standard prompting for UL2  ',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'OPT & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Restart training from an earlier checkpoint with a lower learning rate if loss diverges',\n",
              " '\\\\item Model is prone to generate repetitive text and stuck in a loop',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'GLM-130B &  \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Pre-training data with a small proportion of multi-task instruction data improves the overall model performance',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'BLOOM & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item None',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}  \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Galactica & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " \"\\\\item Galactica's performance has continued to improve across validation set, in-domain, and out-of-domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing research on LLMs.\",\n",
              " '\\\\item A working memory token approach can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream tasks such as PubMedQA (77.6\\\\%) and MedMCQA dev (52.9\\\\%).',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'GPT-3 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-learners',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Codex & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item This LLM focuses on code evaluations and introduces a novel way of selecting the best code samples.',\n",
              " '\\\\item The results indicate it is possible to accurately select code samples using heuristic ranking in lieu of a detailed evaluation of each sample, which may not be feasible or feasible in some situations.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'ERNIE 3.0 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item ERNIE 3.0 shows that a modular LLM architecture with a universal representation module and task-specific representation module helps in finetuning phase.',\n",
              " '\\\\item Optimizing the parameters of a task-specific representation network during the fine-tuning phase is an efficient way to take advantage of the powerful pretrained model.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Jurassic-1 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item The performance of an LLM is highly related to the network size.',\n",
              " '\\\\item To improve runtime performance, more operations can be performed',\n",
              " 'in parallel (width) rather than sequentially (depth).',\n",
              " '\\\\item To efficiently represent and fit more text in the same context length, the model uses a larger vocabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This tokenizer improvement can further benefit few-shot learning tasks.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    %\\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " '\\\\\\\\ \\\\hline',\n",
              " '\\\\end{tabular}%',\n",
              " '\\\\vspace{2mm}',\n",
              " '\\\\begin{flushright}',\n",
              " 'Table Continued on Next Page ',\n",
              " '\\\\end{flushright}',\n",
              " ' \\\\label{tab:pre_trained_findings}',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '%\\\\clearpage',\n",
              " '%\\\\pagebreak',\n",
              " '',\n",
              " '\\\\begin{table*}[!tbhp]',\n",
              " '%\\\\ContinuedFloat',\n",
              " '%\\\\caption{(Continued on next page)}',\n",
              " '\\\\begin{tabular}{lc}',\n",
              " '\\\\hline \\\\hline',\n",
              " '\\\\rowcolor{gray!50}Models & Findings  \\\\& Insights\\\\\\\\ \\\\hline \\\\hline',\n",
              " '',\n",
              " 'HyperCLOVA & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item By employing prompt-based tuning, the performances of models can be improved, often surpassing those of state-of-the-art models when the backward gradients of inputs are accessible.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Yuan 1.0 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Gopher & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Relative encodings enable models to be evaluated for longer sequences than those on which it was trained.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'ERNIE 3.0 Titan & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item This LLM builds on top of ERNIE 3.0 and add a self-supervised adversarial loss to distinguish whether a text is generated or the original one. ',\n",
              " \"\\\\item This distinction ability between real and generate text improves the LLM's performance as compared to ERNIE 3.0.\",\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'GLaM & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " \"\\\\item The feed-forward component of each Transformer layer can be replaced with a mixture-of-experts (MoE) module consisting of a set of independent feed-forward networks (\\\\emph{i.e.}, the `experts'). By sparsely activating these experts, the model capacity can be maintained while much computation is saved. \",\n",
              " '\\\\item By leveraging sparsity, we can make significant strides towards developing high-quality NLP models while simultaneously reducing energy consumption. Consequently, MoE emerges as a robust candidate for future scaling endeavors.',\n",
              " '\\\\item The model trained on filtered data shows consistent better performances on both NLG and NLU tasks, where the effect of filtering is more significant on the former tasks.',\n",
              " '\\\\item Filtered pretraining corpora plays a crucial role in the generation capability of LLMs, especially for the downstream tasks.',\n",
              " '\\\\item The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given a fixed budget of computation, more experts contribute to better predictions.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}% \\\\hline',\n",
              " '',\n",
              " 'LaMDA & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item The model can be fine-tuned to learn to call different external information resources and tools.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'MT-NLG & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item None.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular} \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'AlphaCode & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed with a shallower encoder and a deeper decoder.',\n",
              " '\\\\item To achieve better performances, it is necessary to employ strategies such as massively scaling up sampling, followed by the filtering and clustering of samples into a compact set. ',\n",
              " '\\\\item The utilization of novel sampling-efficient transformer architectures designed to facilitate large-scale sampling is crucial.',\n",
              " '\\\\item Simplifying problem descriptions can effectively improve the modelâ€™s performance.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Chinchilla & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item The experiments that culminated in the development of Chinchilla determined that for optimal computation during training, the model size and the number of training tokens should be scaled proportionately: for each doubling of the model size, the number of training tokens should be doubled as well.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\  \\\\cline{2-2} %\\\\hline',\n",
              " 'PaLM &  \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item English-centric models produce better translations when translating to English as compared to non-English',\n",
              " '\\\\item Generalized models can have equivalent performance for language translation to specialized small models',\n",
              " '\\\\item Larger models have a higher percentage of training data memorization',\n",
              " '\\\\item Performance has not yet saturated even at 540B scale, which means larger models are likely to perform better',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}   \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'AlexaTM & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Compared to commonly used Decoder-only Transformer models, seq2seq architecture is more suitable for training generative LLMs given stronger bidirectional attention to the context. ',\n",
              " '\\\\item An extra Causal Language Modeling (CLM) task can be added to benefit the model with a more efficient in-context learning, especially for few-shot learning tasks. ',\n",
              " '\\\\item The key to training powerful seq2seq-based LLMs lies in mixed pre-training, rather than additional multitask training.',\n",
              " '\\\\item Placing layernorms at the beginning of each transformer layer can improve the training stability of large models.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Sparrow & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Reinforcement learning from multi-objective human feedback can be leveraged to train the models, in order to maximize preference rates and minimize rule violations.',\n",
              " '\\\\item The judgments of labelers and the alignments with defined rules can help the model generate better responses.',\n",
              " '\\\\item Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.',\n",
              " '\\\\item Constructing useful and reliable agents from generative models requires a combination of width and depth: the width aspect enables addressing the complexities of goals and topics; while the depth aspect ensures accurate handling of them.',\n",
              " '\\\\item The combination of reinforcement learning (RL) with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\hline %\\\\cline{2-2}',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '%InstructGPT & \\\\begin{tabular}{c}',\n",
              " '%\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '%\\\\item InstructGPT is trained using a novel RLHF technique which produces outputs that are preferred to the outputs from its base model of GPT-3 (175B), despite having 100x fewer parameters.',\n",
              " '%\\\\item The human feedback step-trained models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets',\n",
              " '%\\\\end{itemize}}',\n",
              " '%\\\\end{tabular}    \\\\\\\\ \\\\hline %\\\\hline',\n",
              " '',\n",
              " '\\\\end{tabular}%',\n",
              " '',\n",
              " '\\\\vspace{2mm}',\n",
              " '\\\\begin{flushright}',\n",
              " 'Table Continued on Next Page ',\n",
              " '\\\\end{flushright}',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '\\\\begin{table*}[!tbhp]',\n",
              " '%\\\\ContinuedFloat',\n",
              " '%\\\\caption{}',\n",
              " '\\\\begin{tabular}{lc}',\n",
              " '\\\\hline \\\\hline',\n",
              " '\\\\rowcolor{gray!50}Model & Findings \\\\& Insights \\\\\\\\ \\\\hline \\\\hline',\n",
              " '',\n",
              " 'U-PaLM & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs ',\n",
              " '\\\\item Training with a mixture of denoisers improves the infilling ability and open-ended text generation diversity',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}\\\\\\\\\\\\cline{2-2}% \\\\hline',\n",
              " '',\n",
              " 'LLaMA & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item LLaMA is open-source and can be fine-tuned or continually pre-trained to develop new models or instruction-based tools.',\n",
              " '\\\\item A few optimizations are proposed to improve the training efficiency of LLaMA, such as efficient implementation of multi-head self-attention and a reduced amount of activations during back-propagation.',\n",
              " '\\\\item Training exclusively on public data can also achieve state-of-the-art performance.',\n",
              " '\\\\item A constant performance improvement is gained when scaling the model.',\n",
              " '\\\\item Smaller models can also realize good performances using more training data and time.  ',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " '',\n",
              " 'PanGu-$\\\\Sigma$ & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Sparse models provide the benefits of large models at a lower computation cost',\n",
              " '\\\\item Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for continual learning ',\n",
              " '\\\\item Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while maintaining a performance similar to the original',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\hline',\n",
              " '',\n",
              " '\\\\end{tabular}%',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '',\n",
              " '\\\\begin{table*}[!tbhp]',\n",
              " '\\\\caption{Our several key insights and findings from the study of \\\\emph{instruction-tuned} Large Language Models. }',\n",
              " '%\\\\caption{Important findings of instruction-tuned LLMs}',\n",
              " '',\n",
              " '\\\\begin{tabular}{lccl}',\n",
              " '\\\\hline \\\\hline',\n",
              " '\\\\rowcolor{gray!50}Models & Findings \\\\& Insights \\\\\\\\ \\\\hline \\\\hline',\n",
              " '',\n",
              " 'T0 & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Multi-task prompting enables zero-shot generalization and outperforms baselines    ',\n",
              " '\\\\item Even a single prompt per dataset task is enough to improve performance',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}\\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'WebGPT & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item The answer quality of LLMs can be further improved with human feedback. ',\n",
              " '\\\\item To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering questions regarding the usefulness of the retrieved documents.',\n",
              " '\\\\item Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning.',\n",
              " '\\\\item Generating answers with references can make labelers easily judge the factual accuracy of answers.%\\\\textbf{}',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " 'Tk-INSTRUCT & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Instruction tuning leads to a stronger generalization of unseen tasks ',\n",
              " '\\\\item More tasks improve generalization whereas only increasing task instances does not help ',\n",
              " '\\\\item Supervised trained models are better than generalized models ',\n",
              " '\\\\item Models pre-trained with instructions and examples perform well for different types of inputs ',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " '',\n",
              " 'mT0 and BLOOMZ & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Instruction tuning enables zero-shot generalization to the tasks never seen before    ',\n",
              " '\\\\item Multi-lingual training leads to even better zero-shot generalization for both English and non-English',\n",
              " '\\\\item Training on machine-translated prompts improves performance for held-out tasks with non-English prompts  ',\n",
              " '\\\\item English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2} ',\n",
              " '',\n",
              " 'OPT-IML & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Task size sampling to create a batch with most of the task examples is important for better performance ',\n",
              " '\\\\item Only example proportional sampling is not enough, training datasets/benchmarks should also be proportional for better generalization/performance',\n",
              " '\\\\item Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no effect',\n",
              " '',\n",
              " '\\\\item Including small amounts i.e. 5\\\\% of pretraining data during fine-tuning is effective ',\n",
              " '',\n",
              " '\\\\item Only 1\\\\% reasoning data improves the performance, adding more deteriorates performance',\n",
              " '',\n",
              " '\\\\item Adding dialogue data makes the performance worse',\n",
              " '',\n",
              " '%\\\\item Meta training for in-context learning degrades performance??? It improves performance for other LLMs though.',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}    \\\\\\\\ \\\\cline{2-2}%\\\\hline',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Flan & \\\\begin{tabular}{c}',\n",
              " '\\\\multicolumn{1}{p{15cm}}{\\\\begin{itemize}',\n",
              " '\\\\item Finetuning with CoT improves performance on held-out tasks ',\n",
              " '\\\\item Fine-tuning along with CoT data improves reasoning abilities ',\n",
              " '\\\\item CoT tuning improves zero-shot reasoning ',\n",
              " '\\\\item Performance improves with more tasks ',\n",
              " '\\\\item Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models',\n",
              " \"\\\\item Improving the model's performance with instruction tuning is compute efficient \",\n",
              " '\\\\item Multitask prompting enables zero-shot generalization abilities in LM',\n",
              " '\\\\end{itemize}}',\n",
              " '\\\\end{tabular}\\\\\\\\ \\\\hline %\\\\cline{2-2}',\n",
              " '\\\\end{tabular}%',\n",
              " '\\\\label{tab:instruction_tuned_findings}',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '\\\\begin{table*}[htbp]',\n",
              " '\\\\rowcolors{2}{gray!25}{white}',\n",
              " '\\\\begin{center}',\n",
              " '\\\\caption{Statistics  of  pre-trained LLMs greater than 10B parameters. Pre-training data scale (either in the number of tokens or storage size), and hardware resource costs. In this table, we only include LLMs with a public paper about the technical details. Here, \\\\enquote{Release Time} indicates the date when the corresponding paper was officially released. \\\\enquote{Open Source} means  that the model checkpoints can be publicly accessible. \\\\enquote{Data Cleaning} indicates whether the data cleaning is performed or not. This includes heuristics (Heur), deduplication (Dedup), quality filtering (QF) and privacy filtering (PF).',\n",
              " '\\\\enquote{Training Parallelism} indicates distributed training using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), model parallelism (M), optimizer parallelism (OP), and materialization (R). ',\n",
              " '}',\n",
              " '\\\\label{tab:statistics_pt}',\n",
              " '\\\\footnotesize',\n",
              " '%\\\\renewcommand\\\\tabcolsep{1pt}',\n",
              " '%\\\\hspace*{-5em}',\n",
              " '\\\\resizebox{\\\\linewidth}{!}{',\n",
              " '\\\\begin{tabular}{llcrccccccccccc}',\n",
              " '\\\\toprule',\n",
              " '\\\\rowcolor{gray!50}',\n",
              " '  &  &   &  & & &   &    & &  &  & &   \\\\\\\\',\n",
              " '\\\\rowcolor{gray!50}',\n",
              " '\\\\multirow{-2}{*}{\\\\textbf{Models}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Release\\\\\\\\ Time\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Open\\\\\\\\ Source\\\\end{tabular}}} & \\\\multicolumn{1}{c}{\\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Size\\\\\\\\ (B)\\\\end{tabular}}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Base\\\\\\\\ Model\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Steps\\\\\\\\ Trained\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Data/\\\\\\\\ Tokens\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Data\\\\\\\\ Cleaning\\\\end{tabular}}} & \\\\textbf{Hardware} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Training\\\\\\\\ Time\\\\end{tabular}}} &  \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Training\\\\\\\\ Parallelism\\\\end{tabular}}} & \\\\textbf{Library}  \\\\\\\\',\n",
              " '\\\\midrule',\n",
              " 'T5~\\\\cite{T5}   & Oct-2019 &$\\\\checkmark$ & 11   & -   & 1M & 1T & Heur+Dedup & 1024 TPU v3 &  & D+M & Mesh TensorFlow  \\\\\\\\',\n",
              " '',\n",
              " 'mT5~\\\\cite{mT5}  & Oct-2020  &$\\\\checkmark$  & 13    & T5  & 1M & 1T & - & -   & -  & - & - \\\\\\\\',\n",
              " '',\n",
              " '{PanGu-$\\\\alpha$}~\\\\cite{PanGU_alpha}   & Apr-2021    &  $\\\\checkmark$  & 200  &  -  &  260k  & 1.1TB  & Heur+Dedup &  2048 Ascend 910  & - & D+OP+P+O+R & MindSpore\\\\\\\\',\n",
              " '',\n",
              " 'CPM-2~\\\\cite{CPM-2}    & Jun-2021  & $\\\\checkmark$  & 198   & - & 1M & 2.6TB  & Dedup & - & - & D+M  & JAXFormer   \\\\\\\\',\n",
              " '',\n",
              " 'CodeGen~\\\\cite{CodeGen}  & Mar-2022    &  $\\\\checkmark$  & 16   & -  & 650k & 577B  & Heur+Dedup   & TPU v4 & - & D+M & JAXFormer  \\\\\\\\',\n",
              " '',\n",
              " 'GPT-NeoX-20B~\\\\cite{black2022gpt} & Apr-2022    & $\\\\checkmark$    & 20   & - & 150k & 825GB & None & 96 40G A100 & - & M & Megatron+DS+PyTorch     \\\\\\\\',\n",
              " '',\n",
              " 'UL2~\\\\cite{UL2}  & May-2022    &  $\\\\checkmark$   & 20   & T5  & 2M & 1T & - & 512 TPU v4  & -  & M & JAX+T5X    \\\\\\\\',\n",
              " '',\n",
              " 'OPT~\\\\cite{OPT}  & May-2022    & $\\\\checkmark$   & 175   & - &  150k  & 180B  &  Dedup   & 992 80G A100  & -  & D+T & Megatron   \\\\\\\\',\n",
              " '',\n",
              " 'GLM~\\\\cite{GLM-130B}  & Oct-2022 & $\\\\checkmark$   & 130   & - & - & 400B   & -  & 768 40G A100    & 60d  & M & -    \\\\\\\\',\n",
              " '',\n",
              " 'BLOOM~\\\\cite{BLOOM}    & Nov-2022  & $\\\\checkmark$  & 176   & -   & - & 366B & Dedup+PR  & 384 80G A100    & 105d  &  D+T+P  & Megatron+DS    \\\\\\\\',\n",
              " '',\n",
              " 'Galactica~\\\\cite{galactica} & Nov-2022    & $\\\\checkmark$ & 120   & - & 225k & 106B & Dedup  &  128 80GB A100   & -  & - & Metaseq    \\\\\\\\',\n",
              " '',\n",
              " 'GPT-3~\\\\cite{GPT-3}    & May-2020   & $\\\\times$ & 175   & - & - & 300B & -  & -   & -  & - & -       \\\\\\\\',\n",
              " '',\n",
              " 'Codex~\\\\cite{codex}    & Jul-2021 & $\\\\times$   & 12    & GPT-3 & - & 100B & Heur & -   & -  & -    & - \\\\\\\\',\n",
              " '',\n",
              " 'ERNIE 3.0~\\\\cite{ernie3}    & Jul-2021  & $\\\\times$  & 10  & - & 120k$^*$ & 375B & Heur+Dedup & 384 V100   & -  & M$^*$ & PaddlePaddle    \\\\\\\\',\n",
              " '',\n",
              " 'Jurassic-1~\\\\cite{lieber2021jurassic}   & Aug-2021 & $\\\\times$  & 178      & - & - & 300B   & -  & 800 GPU & -  & D+M+P    & Megatron+DS       \\\\\\\\',\n",
              " '',\n",
              " 'HyperCLOVA~\\\\cite{hyperclova}   & Sep-2021 & $\\\\times$   & 82 & -  & - &  300B  & Clf+Dedup+PF & 1024 A100   & 13.4d  & M  & Megatron      \\\\\\\\',\n",
              " '',\n",
              " 'Yuan 1.0~\\\\cite{wu2021yuan}   & Oct-2021  & $\\\\times$  & 245   & -  & 26k$^*$ &  180B  &  Heur+Clf+Dedup  & 2128 GPU  &  -  & D+T+P & - \\\\\\\\',\n",
              " '',\n",
              " '% WebGPT~\\\\cite{nakano2021webgpt}   & Dec-2021  &$\\\\times$  & 175   & GPT-3   & - & -  & - & -  & -   & -  & -   \\\\\\\\',\n",
              " '',\n",
              " 'Gopher~\\\\cite{gopher}   & Dec-2021 & $\\\\times$   & 280 & ERNIE-3.0 & - & 300B   & QF+Dedup  & 4096 TPU v3 & 920h  & D+M    & JAX+Haiku        \\\\\\\\',\n",
              " '',\n",
              " 'ERNIE 3.0 Titan~\\\\cite{ernie3titan}   & Dec-2021 & $\\\\times$   & 260  & -  & - &  300B  & Heur+Dedup &  V100+Ascend 910  & -  & D+M+P+D* & PaddlePaddle  \\\\\\\\',\n",
              " '',\n",
              " 'GLaM~\\\\cite{du2022glam} &Dec-2021  &$\\\\times$    & 1200  & -   & 600k$^*$  & 600B   & Clf  & 1024 TPU v4 & -  & M & GSPMD       \\\\\\\\',\n",
              " '',\n",
              " 'LaMDA~\\\\cite{thoppilan2022lamda}    & Jan-2022   & $\\\\times$ & 137      & - & 3M & 2.81T  & Filtered  & 1024 TPU v3 & 57.7d & D+M    & Lingvo    \\\\\\\\',\n",
              " '',\n",
              " 'MT-NLG~\\\\cite{mtnlg}   & Jan-2022    & $\\\\times$   & 530   & - & - & 270B & - & 4480 80G A100   & -  & D+T+P    & Megatron+DS       \\\\\\\\',\n",
              " '',\n",
              " 'AlphaCode~\\\\cite{li2022competition}    & Feb-2022  &$\\\\times$  & 41    & -   & 205k & 967B & Heur+Dedup & TPU v4  & -  &M   & JAX+Haiku       \\\\\\\\',\n",
              " '',\n",
              " 'Chinchilla~\\\\cite{chinchilla}   & Mar-2022    & $\\\\times$    & 70   & - & - & 1.4T  & QF+Dedup  & TPUv3/TPUv4   & -  & -    & JAX+Haiku        \\\\\\\\',\n",
              " '',\n",
              " 'PaLM~\\\\cite{PaLM} & Apr-2022    & $\\\\times$ & 540 & - &  255k  & 780B  & Heur & 6144 TPU v4  & - & D+M & JAX+T5X\\\\\\\\',\n",
              " '',\n",
              " 'AlexaTM~\\\\cite{soltan2022alexatm}  & Aug-2022 &$\\\\times$   & 20  & -  & 500k  & 1.1T   & Filtered  & 128 A100    & 120d  & M & DS      \\\\\\\\',\n",
              " '',\n",
              " 'Sparrow~\\\\cite{glaese2022improving}  & Sep-2022 &$\\\\times$    & 70    & Chinchilla   & - & -  & -  & 64 TPU v3   & -  & M    & -       \\\\\\\\',\n",
              " '',\n",
              " 'U-PaLM~\\\\cite{U-PaLM}   & Oct-2022    & $\\\\times$ & 540   & PaLM & 20k & - & -   & 512 TPU v4  & 5d    & - & - \\\\\\\\',\n",
              " '',\n",
              " 'LLaMA~\\\\cite{touvron2023llama}    & Feb-2023 & $\\\\checkmark$   & 65      & - & 350k & 1.4T & Heur+Dedup   & 2048 80G A100 & 21d   & D+M & xFormers     \\\\\\\\',\n",
              " '',\n",
              " '{PanGu-$\\\\Sigma$}~\\\\cite{PanGu_sigma}    & Mar-2023  & $\\\\times$  & 1085 & {PanGu-$\\\\alpha$} & -  & 329B & -  & 512 Ascend 910   & 100d  & D+OP+P+O+R & MindSpore  \\\\\\\\',\n",
              " '%\\\\multirow{-25}{*}{\\\\begin{tabular}[c]{@{}c@{}}Closed\\\\\\\\ Source\\\\end{tabular}} & \\\\multirow{-18}{*}{\\\\begin{tabular}[c]{@{}c@{}}Publicly\\\\\\\\ Available\\\\end{tabular}} &',\n",
              " '\\\\bottomrule',\n",
              " '\\\\end{tabular}',\n",
              " '} ',\n",
              " '\\\\end{center}',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\\\begin{table*}[tbp]',\n",
              " '\\\\rowcolors{2}{gray!25}{white}',\n",
              " '\\\\centering',\n",
              " '\\\\caption{Statistics of instruction tuned LLMs greater than 10B parameters. All abbreviations are the same as Table~\\\\ref{tab:statistics_pt}}',\n",
              " '\\\\footnotesize',\n",
              " '\\\\resizebox{\\\\linewidth}{!}{',\n",
              " '\\\\begin{tabular}{llcrccccccccccc}',\n",
              " '\\\\toprule',\n",
              " '\\\\rowcolor{gray!50}',\n",
              " '  &  &   &  & & &   &    & &  &  & &   \\\\\\\\',\n",
              " '\\\\rowcolor{gray!50}',\n",
              " '\\\\multirow{-2}{*}{\\\\textbf{Models}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Release\\\\\\\\ Time\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Open\\\\\\\\ Source\\\\end{tabular}}} & \\\\multicolumn{1}{c}{\\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Size\\\\\\\\ (B)\\\\end{tabular}}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Base\\\\\\\\ Model\\\\end{tabular}}}  & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Steps\\\\\\\\ Trained\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Data/\\\\\\\\ Tokens\\\\end{tabular}}} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Data\\\\\\\\ Cleaning\\\\end{tabular}}} & \\\\textbf{Hardware} & \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Training\\\\\\\\ Time\\\\end{tabular}}} &  \\\\multirow{-2}{*}{\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Training\\\\\\\\ Parallelism\\\\end{tabular}}} & \\\\textbf{Library}  \\\\\\\\',\n",
              " '\\\\midrule',\n",
              " '',\n",
              " 'T0~\\\\cite{T0}   & Oct-2021  & $\\\\checkmark$ & 11    & T5  & -  & 250B &  - & 512 TPU v3  & 270h   & - & - \\\\\\\\',\n",
              " '',\n",
              " 'WebGPT~\\\\cite{nakano2021webgpt}   & Dec-2021  &$\\\\times$  & 175   & GPT-3   & -  & - & -  & -   & -  &  - &  -  \\\\\\\\',\n",
              " '',\n",
              " 'Tk-Instruct~\\\\cite{Tk-INSTRUCT}  & Apr-2022   & $\\\\checkmark$ & 11    & T5  & 1000  & - & - & 256 TPU v3   & 4h  & -  & Google T5   \\\\\\\\',\n",
              " '',\n",
              " 'mT0~\\\\cite{mT0}  & Nov-2022  & $\\\\checkmark$  & 13    & mT5 & -  & - & - & -  & -   & -  & - \\\\\\\\',\n",
              " '',\n",
              " 'OPT-IML~\\\\cite{OPT_IML}  & Dec-2022  & $\\\\checkmark$ & 175   & OPT & 8k  & 2B & Dedup & 128 40G A100   & - & D+T & Megatron    \\\\\\\\',\n",
              " '',\n",
              " 'Flan-U-PaLM~\\\\cite{Flan}  & Oct-2022  & $\\\\times$ & 540   & U-PaLM  & 30k  & - & -  & 512 TPU v4 & - & - & JAX+T5X  \\\\\\\\',\n",
              " '',\n",
              " '',\n",
              " '%\\\\multirow{-25}{*}{\\\\begin{tabular}[c]{@{}c@{}}Closed\\\\\\\\ Source\\\\end{tabular}} & \\\\multirow{-18}{*}{\\\\begin{tabular}[c]{@{}c@{}}Publicly\\\\\\\\ Available\\\\end{tabular}} &',\n",
              " '\\\\bottomrule',\n",
              " '\\\\end{tabular}}',\n",
              " '\\\\label{tab:statistics_it}',\n",
              " '\\\\end{table*}',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '\\\\subsection{Instruction-Tuned Models}',\n",
              " '',\n",
              " '\\\\subsubsection{T0}',\n",
              " 'Similar to Flan~\\\\cite{Flan}, T0 fine-tunes the LM-adapted T5 model~\\\\cite{LMAdapted} with multi-task instruction prompts. To train the model, T0 designs various prompt templates to convert different datasets into prompts. The model trained with explicit multi-task prompting improves zero-shot performance, outperforming models 6 to 16 times larger in size.  ',\n",
              " '',\n",
              " '\\\\subsubsection{WebGPT~\\\\cite{nakano2021webgpt}}',\n",
              " \"A set of fine-tuned GPT-3~\\\\cite{GPT-3} models that can answer long-form questions in a text-based web-browsing environment. During browsing, the models must collect references to support their answers in order to exploit easier human evaluation. In addition to the Q\\\\&A data in the ELI5 dataset~\\\\cite{fan2019eli5}, two more types of data, \\\\emph{demonstrations} of humans and \\\\emph{comparisons} between two model-generated answers, are collected for training. Using human labelers' feedback on whether the retrieved information is useful to answer the given inputs, the data is tested with four usages: behavior cloning, reward modeling, reinforcement learning, and rejection sampling. Practically, the combination of behavior cloning and rejection sampling contributes to the best-performing model containing 175B parameters. To evaluate the performance of the WebGPT model, the model's answers are compared with the human demonstrators' written ones, the highest-voted answers in ELI5~\\\\cite{fan2019eli5}, and the adversarial short-form questions and answers in the TruthfulQA~\\\\cite{lin2021truthfulqa} dataset.\",\n",
              " '',\n",
              " '\\\\subsubsection{Tk-INSTRUCT~\\\\cite{Tk-INSTRUCT}}',\n",
              " 'Tk-INSTRUCT instruction fine-tunes T5 model on self-proposed SUPER-NATURALINSTRUCTIONS dataset of 1616 different NLP tasks. To analyze the generalization capabilities of Tk-INSTRUCT, the model is evaluated on unseen tasks with a few examples as ground-truth in-context instructions. The model outperforms GPT-3 and Instruct-GPT by large margins, although it is smaller in size 11B, opposite to 175B.   ',\n",
              " '',\n",
              " '\\\\subsubsection{mT0 and BLOOMZ~\\\\cite{mT0andBLOOMZ}}',\n",
              " 'This method fine-tunes BLOOM and T0 using a multilingual multi-task prompt dataset. It creates a new dataset named xP3, adding 46 different language datasets with new tasks not present previously in P3. Training with this dataset improves zero-shot generalization for both English and non-English. This also leads to better generalization on held-out tasks.    ',\n",
              " '',\n",
              " '\\\\begin{figure}[!tbp]',\n",
              " '\\\\centering',\n",
              " '\\\\includegraphics[width=1\\\\columnwidth]{Figure/Flan.png}',\n",
              " '\\\\caption{An example image shows an instance of the Flan training paradigm, taken from~\\\\cite{Flan}.}',\n",
              " '\\\\label{flan_image}',\n",
              " '\\\\end{figure}',\n",
              " '\\\\subsubsection{OPT-IML~\\\\cite{OPT_IML}}',\n",
              " 'An instruction-tuned OPT model, trained on instruction meta-learning benchmark of 2000 NLP tasks that is a combination of 8 meta-datasets including, Super-NaturalInstructions, PromptSource, FLAN, and others as given in Table~\\\\ref{datasets}. For computational efficiency, OPT-IML utilizes the maximum sequence length of 2048 tokens by packing multiple instances together, separated by the $<eos>$ token during training. It employs a masking mechanism to separate instances in a sequence to avoid attending tokens from different instances. Overall, OPT-IML outperforms baseline model OPT with instruction-finetuning on zero and few-shot generalization abilities.',\n",
              " '',\n",
              " '\\\\subsubsection{Flan~\\\\cite{Flan}}',\n",
              " 'Fine-tuning language models (Flan), fine-tunes T5, PaLM, and UPaLM with 1836 instruction tasks taken from Muffin (80 tasks), T0-SF (193 tasks), NIV2 (1554 tasks), and CoT (taken from nine datasets), as shown in Figure~\\\\ref{flan_image}. Instruction fine-tuning improves the model performance significantly with minimal computing, only 0.2\\\\% of the total pre-training compute in the case of PaLM 540B. Flan also suggests that adding more instruction fine-tuning tasks with CoT reasoning data will likely improve the performance further.   ',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '% % We will externalize the figures',\n",
              " '% \\\\usepgfplotslibrary{external}',\n",
              " '% \\\\tikzexternalize',\n",
              " '',\n",
              " '% \\\\pgfplotsset{width=18cm,compat=1.9}',\n",
              " '% \\\\begin{figure*}[!tbp]',\n",
              " '% \\\\centering',\n",
              " '% \\\\pgfplotstableread{',\n",
              " '% Group  Posttest LLMs Val color',\n",
              " '% 2019 1 {T5} 1 .1',\n",
              " '% 2020  2  {GPT-3, GShard, mT5} 3 .1',\n",
              " '% 2021  1 {PanGu-$\\\\alpha$, PLUG, Codex, ERNIE 3.0, Jurassic-1, CPM-2, FLAN, Yuan 1.0, T0, HyperCLOVA, Anthropic, WebGPT, ERNIE 3.0 TITAN, Gopher, GLaM} 7 .1',\n",
              " '% 2022  2  {InstructGPT, CodeGen, MT-NLG, LaMBDA, AlphaCode, Chinchilla, UL2, PaLM, YaLM, OPT, GPT-NeoX-20B, Tk-Instruct, Cohere, GLM, AlexaTM, WeLM, Sparrow, Flan-T5, Flan-PaLM, Luminous, NLLB, BLOOM, mT0, BLOOMZ, Galacitca, OPT-IML, ChatGPT} 10.5 .1',\n",
              " '% 2023 1 {CodeGeeX, Pythia, Vicuna, PanGu-$\\\\sum$, Bard, LLaMA, GPT-4} 4 .1',\n",
              " '% }\\\\datatable',\n",
              " '',\n",
              " '% \\\\begin{tikzpicture}',\n",
              " '% \\\\begin{axis}[',\n",
              " '%     xtick=data,',\n",
              " '%     enlargelimits=.1,',\n",
              " '%     symbolic x coords = {2019,2020,2021,2022,2023},',\n",
              " '%     ytick distance=1,',\n",
              " '%     ymajorticks=false,',\n",
              " '%     scale only axis,',\n",
              " '%     xticklabel style={anchor=north,align=center},',\n",
              " '%     xticklabels={2019,2020,2021,2022,2023},',\n",
              " '%     enlarge y limits=0.75, % Adjust this value as needed',\n",
              " '% ]',\n",
              " '% \\\\addplot[%',\n",
              " '%     scatter=true,',\n",
              " '%     only marks,',\n",
              " '%     point meta=\\\\thisrow{color},',\n",
              " '%     fill opacity=0.1,',\n",
              " '%     text opacity=1,',\n",
              " '%     visualization depends on={10*\\\\thisrow{Val} \\\\as \\\\perpointmarksize},',\n",
              " '%     visualization depends on={\\\\thisrow{Val} \\\\as \\\\Val},',\n",
              " ...]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# smart_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[58], line 45\u001b[0m\n",
            "\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
            "\u001b[1;32m     44\u001b[0m     smart_pages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m section_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;32m---> 45\u001b[0m smart_pages \u001b[38;5;241m=\u001b[39m \u001b[43mquality_gutenberg_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmart_pages\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/git/genai/genai/text_processes/pagination.py:39\u001b[0m, in \u001b[0;36mquality_gutenberg_parser\u001b[0;34m(raw_article)\u001b[0m\n",
            "\u001b[1;32m     37\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;32m     38\u001b[0m previous_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mraw_article\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)):\n",
            "\u001b[1;32m     40\u001b[0m   line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
            "\u001b[1;32m     41\u001b[0m   original_line \u001b[38;5;241m=\u001b[39m line\n",
            "\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "from genai.text_processes.pagination import quality_pagination\n",
        "\n",
        "\n",
        "# file = 'test_text.txt'\n",
        "# with open(file, 'r') as f:\n",
        "#     text = f.read()\n",
        "# example = {'article': text, 'title': 'Test Article'}\n",
        "# pages = quality_pagination(example, query_model=llm_query)\n",
        "\n",
        "# We will need to create something that will be able to look at arxiv .tex folders/files and parse the pages in a way that \n",
        "# allow us to do a gist on each one of the different files in the order that they are presented in the paper.\n",
        "\n",
        "folder = '../text_processes/arXiv-2307.06435v7'\n",
        "\n",
        "def get_text_pages_from_tex_folder(tex_folder):\n",
        "    # look for the main.tex\n",
        "    # Get the /sections/ in order, \n",
        "    # get the text from each section and return those as separate pages.\n",
        "\n",
        "    main_tex = os.path.join(tex_folder, 'main.tex')\n",
        "    with open(main_tex, 'r') as f:\n",
        "        main_text = f.read()\n",
        "    # get the sections for example:\n",
        "    # \\input{sections/introduction} \n",
        "    sections = re.findall(r'\\\\input{sections/(.*?)}', main_text)\n",
        "    section_texts = []\n",
        "    for section in sections:\n",
        "        if section.endswith('.tex'):\n",
        "            section = section[:-4]\n",
        "        section_file = os.path.join(tex_folder, 'sections', section + '.tex')\n",
        "        with open(section_file, 'r') as f:\n",
        "            section_text = f.read()\n",
        "        section_texts.append(section_text)\n",
        "    return section_texts\n",
        "\n",
        "section_texts = get_text_pages_from_tex_folder(folder)\n",
        "\n",
        "smart_pages = []\n",
        "for i,  section_text in enumerate(section_texts):\n",
        "    print(i)\n",
        "    smart_pages += section_text.split('\\n')\n",
        "smart_pages = quality_gutenberg_parser(smart_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'paragraphs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparagraphs\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'paragraphs' is not defined"
          ]
        }
      ],
      "source": [
        "quality_pagination\n",
        "gists = gisting_with_template(smart_pages, llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "gisting_with_template() missing 1 required positional argument: 'template'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_gists \u001b[38;5;241m=\u001b[39m \u001b[43mgisting_with_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_query\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: gisting_with_template() missing 1 required positional argument: 'template'"
          ]
        }
      ],
      "source": [
        "result_gists = gisting_with_template(pages, query_model=llm_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
