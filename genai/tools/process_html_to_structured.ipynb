{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will help to process websites to provide structured represenations of the listed content within them. \n",
    "# It will first take take an list of string urls. \n",
    "# For each url, it will download the content of the website\n",
    "# If it is a github, it will use the README.md (downloading the github repo if it is not downloaded, and pulling it if it is already present in the saved directory)\n",
    "# It will then use gpt3 to identify the main columns to be extracted for that website. \n",
    "# It will suggest these columns to the user. \n",
    "# The user may use these columns or not. The columns will then be used in a prompt template that we will use to extract information. \n",
    "\n",
    "# The html content will then be 'scanned through' in blocks.   \n",
    "# For each block, we will run the prompt with gpt4 extract the information into the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "local_path=../../downloads/github/a16z-infra/llm-app-stack\n",
      "Document ../../downloads/github/a16z-infra/llm-app-stack already exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from genai.memory.downloader import url_downloader\n",
    "output_path = '../../downloads/'\n",
    "dry_run = False\n",
    "verbose = True\n",
    "overwrite = False\n",
    "\n",
    "link = 'https://github.com/a16z-infra/llm-app-stack'\n",
    "\n",
    "local_path = url_downloader(link, output_path, overwrite=overwrite, dry_run=dry_run, verbose=verbose)\n",
    "\n",
    "if 'github' in local_path:\n",
    "    # add readme.md\n",
    "    local_path = local_path + '/README.md'\n",
    "\n",
    "# open content\n",
    "with open(local_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# print(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the information from the input. \n",
    "template = \"\"\"\n",
    "    Please survey the following information and extract the column names that you would like to extract from the content.\n",
    "    First focus on columns that are already present in any tables. USE THOSE AS STARTING COLUMNS.\n",
    "    Next, determine if tables are separated by headings subheadings. \n",
    "    Add any necessary column names to describe those headings/subheadings for better classification.\n",
    "    These could include 'type', 'category', 'tags', etc... \n",
    "    For instance for the the following Content would yield the Results\n",
    "    \n",
    "    === Content\n",
    "    ## section 1\n",
    "    ...\n",
    "    ### subjectA\n",
    "    ...\n",
    "    <table with column col1, col2, col3>\n",
    "    \n",
    "    === Result\n",
    "    col1, col2, col3, DescriptorOf(subjectA)\n",
    "\n",
    "    === \n",
    "    An example of DescriptorOf(subjectA) if subjectA was 'Data Pipelines' would be 'category'\n",
    "    {subject}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template+\".\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.9)\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "columns = chain.invoke({\"subject\": content})\n",
    "print(columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.append('Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the content in blocks, as separated by the headings.\n",
    "# For each block, we will run the prompt with gpt4 extract the information into the columns.\n",
    "\n",
    "# for block in content.split('#'):\n",
    "#     print(block)\n",
    "    # print('\\n\n",
    "\n",
    "import re\n",
    "# re.split(\"^#.*$\", content, flags=re.MULTILINE)\n",
    "# This splits wrong. It should append the the heading as part of the block.\n",
    "headings = re.findall(\"^#.*$\", content, flags=re.MULTILINE)\n",
    "# get information between the headings\n",
    "\n",
    "blocks = []\n",
    "for i in range(len(headings)-1):\n",
    "    block = content.split(headings[i])[1].split(headings[i+1])[0]\n",
    "    blocks.append(headings[i] + block)\n",
    "    # print(block)\n",
    "    # print('\\n\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each block, we will run the prompt with gpt4 extract the information into the columns.\n",
    "template = \"\"\"\n",
    " \n",
    "    Use any of the headers to help fill in any columns that are not explicitly mentioned in tables. \n",
    "    {block}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You will be presented with content, that may or may not have relevant information to the following columns: {columns}\\n. If there is no relevant content, do not output anything. If there is multiple elemnts of relevant content, output a table with the columns specified.\" + template ,\n",
    "    input_variables=[\"block\"],\n",
    "    partial_variables={\"columns\": columns},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.0)\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "column_results = []\n",
    "for block in blocks:\n",
    "    if len(block) < 20:\n",
    "        continue\n",
    "    results = chain.invoke({\"block\": block})\n",
    "    column_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "18\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "1\n",
      "18\n",
      "5\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for c in column_results:\n",
    "    print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blocks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
