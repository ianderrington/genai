<!DOCTYPE html>
<!-- saved from url=(0035)https://arxiv.org/html/2307.06435v7 -->
<html lang="en" data-theme="dark"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>A Comprehensive Overview of Large Language Models</title>
<!--Generated on Wed Dec 27 10:08:30 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./A Comprehensive Overview of Large Language Models_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./A Comprehensive Overview of Large Language Models_files/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="./A Comprehensive Overview of Large Language Models_files/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="./A Comprehensive Overview of Large Language Models_files/bootstrap.bundle.min.js"></script>
<script src="./A Comprehensive Overview of Large Language Models_files/html2canvas.min.js"></script>
<script src="./A Comprehensive Overview of Large Language Models_files/addons.js"></script>
<script src="./A Comprehensive Overview of Large Language Models_files/feedbackOverlay.js"></script>
<meta content="
" lang="en" name="keywords">
<!--<base href="/html/2307.06435v7/">--><base href="."><link rel="stylesheet" href="./A Comprehensive Overview of Large Language Models_files/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body class="vsc-initialized"><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./A Comprehensive Overview of Large Language Models_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2307.06435v7">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Dark mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./A Comprehensive Overview of Large Language Models_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2307.06435v7/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2307.06435v7">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2307.06435v7" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S1" title="I Introduction ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2" title="II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS1" title="II-A Tokenization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Tokenization</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS1.SSS1" title="II-A1 WordPiece [59] ‣ II-A Tokenization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>WordPiece&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">59</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS1.SSS2" title="II-A2 BPE [57] ‣ II-A Tokenization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>BPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">57</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS1.SSS3" title="II-A3 UnigramLM [56] ‣ II-A Tokenization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>UnigramLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">56</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS2" title="II-B Attention ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Attention</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3" title="II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Attention in LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3.SSS1" title="II-C1 Self-Attention [62] ‣ II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Self-Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">62</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3.SSS2" title="II-C2 Cross Attention ‣ II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Cross Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3.SSS3" title="II-C3 Full Attention ‣ II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>3 </span>Full Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3.SSS4" title="II-C4 Sparse Attention [63] ‣ II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>4 </span>Sparse Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">63</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3.SSS5" title="II-C5 Flash Attention [64] ‣ II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>5 </span>Flash Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">64</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS4" title="II-D Encoding Positions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Encoding Positions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS4.SSS1" title="II-D1 Absolute ‣ II-D Encoding Positions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>1 </span>Absolute</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS4.SSS2" title="II-D2 Relative ‣ II-D Encoding Positions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>2 </span>Relative</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS5" title="II-E Activation Functions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span> </span><span class="ltx_text ltx_font_italic">Activation Functions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS5.SSS1" title="II-E1 ReLU [69] ‣ II-E Activation Functions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span>1 </span>ReLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">69</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS5.SSS2" title="II-E2 GeLU [70] ‣ II-E Activation Functions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span>2 </span>GeLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">70</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS5.SSS3" title="II-E3 GLU variants [73] ‣ II-E Activation Functions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span>3 </span>GLU variants&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">73</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6" title="II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span> </span><span class="ltx_text ltx_font_italic">Layer Normalization</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6.SSS1" title="II-F1 LayerNorm ‣ II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span>1 </span>LayerNorm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6.SSS2" title="II-F2 RMSNorm ‣ II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span>2 </span>RMSNorm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6.SSS3" title="II-F3 Pre-Norm and Post-Norm ‣ II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span>3 </span>Pre-Norm and Post-Norm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6.SSS4" title="II-F4 DeepNorm ‣ II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-F</span>4 </span>DeepNorm</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7" title="II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span> </span><span class="ltx_text ltx_font_italic">Distributed LLM Training</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS1" title="II-G1 Data Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>1 </span>Data Parallelism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS2" title="II-G2 Tensor Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>2 </span>Tensor Parallelism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS3" title="II-G3 Pipeline Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>3 </span>Pipeline Parallelism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS4" title="II-G4 Model Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>4 </span>Model Parallelism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS5" title="II-G5 3D Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>5 </span>3D Parallelism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS7.SSS6" title="II-G6 Optimizer Parallelism ‣ II-G Distributed LLM Training ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-G</span>6 </span>Optimizer Parallelism</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS8" title="II-H Libraries ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-H</span> </span><span class="ltx_text ltx_font_italic">Libraries</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS9" title="II-I Data PreProcessing ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-I</span> </span><span class="ltx_text ltx_font_italic">Data PreProcessing</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS9.SSS1" title="II-I1 Quality Filtering ‣ II-I Data PreProcessing ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-I</span>1 </span>Quality Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS9.SSS2" title="II-I2 Data Deduplication ‣ II-I Data PreProcessing ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-I</span>2 </span>Data Deduplication</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS9.SSS3" title="II-I3 Privacy Reduction ‣ II-I Data PreProcessing ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-I</span>3 </span>Privacy Reduction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS10" title="II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-J</span> </span><span class="ltx_text ltx_font_italic">Architectures</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS10.SSS1" title="II-J1 Encoder Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-J</span>1 </span>Encoder Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS10.SSS2" title="II-J2 Causal Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-J</span>2 </span>Causal Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS10.SSS3" title="II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-J</span>3 </span>Prefix Decoder</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11" title="II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-K</span> </span><span class="ltx_text ltx_font_italic">Pre-Training Objectives</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11.SSS1" title="II-K1 Full Language Modeling ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-K</span>1 </span>Full Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11.SSS2" title="II-K2 Prefix Language Modeling ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-K</span>2 </span>Prefix Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11.SSS3" title="II-K3 Masked Language Modeling ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-K</span>3 </span>Masked Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11.SSS4" title="II-K4 Unified Language Modeling ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-K</span>4 </span>Unified Language Modeling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS12" title="II-L Model Adaptation ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-L</span> </span><span class="ltx_text ltx_font_italic">Model Adaptation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS12.SSS1" title="II-L1 Pre-Training ‣ II-L Model Adaptation ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-L</span>1 </span>Pre-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS12.SSS2" title="II-L2 Fine-Tuning ‣ II-L Model Adaptation ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-L</span>2 </span>Fine-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS12.SSS3" title="II-L3 Prompting/Utilization ‣ II-L Model Adaptation ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-L</span>3 </span>Prompting/Utilization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3" title="III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Large Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1" title="III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Pre-Trained LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1" title="III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>General Purpose</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px1" title="T5 [10] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">10</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px2" title="GPT-3 [6] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">6</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px3" title="mT5 [11] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">11</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px4" title="PanGu-𝛼 [104] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline"><semantics><mi>α</mi><annotation-xml encoding="MathML-Content"><ci>𝛼</ci></annotation-xml><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">104</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px5" title="CPM-2 [12] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CPM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">12</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px6" title="ERNIE 3.0 [107] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ERNIE 3.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">107</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px7" title="Jurassic-1 [109] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Jurassic-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">109</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px8" title="HyperCLOVA [111] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">HyperCLOVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">111</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px9" title="Yuan 1.0 [112] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Yuan 1.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">112</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px10" title="Gopher [113] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">113</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px11" title="ERNIE 3.0 TITAN [35] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ERNIE 3.0 TITAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">35</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px12" title="GPT-NeoX-20B [115] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GPT-NeoX-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">115</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px13" title="OPT [14] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">14</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px14" title="BLOOM [13] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">13</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px15" title="GLaM [118] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">118</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px16" title="MT-NLG [114] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">114</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px17" title="Chinchilla [121] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">121</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px18" title="AlexaTM [122] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">AlexaTM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">122</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px19" title="PaLM [15] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">15</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px20" title="U-PaLM [124] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">124</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px21" title="UL2 [89] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">89</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px22" title="GLM-130B [33] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">33</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px23" title="LLaMA [126, 21] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">126</span>, <span class="ltx_ref">21</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS1.Px24" title="PanGu-Σ [129] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline"><semantics><mi mathvariant="normal">Σ</mi><annotation-xml encoding="MathML-Content"><ci>Σ</ci></annotation-xml><annotation encoding="application/x-tex">\Sigma</annotation><annotation encoding="application/x-llamapun">roman_Σ</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">129</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2" title="III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span>Coding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2.Px1" title="CodeGen [130] ‣ III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">130</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2.Px2" title="Codex [131] ‣ III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">131</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2.Px3" title="AlphaCode [132] ‣ III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">AlphaCode&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">132</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2.Px4" title="CodeT5+ [34] ‣ III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">34</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2.Px5" title="StarCoder [137] ‣ III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">137</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS3" title="III-A3 Scientific Knowledge ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span>Scientific Knowledge</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS3.Px1" title="Galactica [138] ‣ III-A3 Scientific Knowledge ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">138</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS4" title="III-A4 Dialog ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span>Dialog</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS4.Px1" title="LaMDA [140] ‣ III-A4 Dialog ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">140</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS5" title="III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>5 </span>Finance</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS5.Px1" title="BloombergGPT [141] ‣ III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">141</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS5.Px2" title="Xuan Yuan 2.0 [143] ‣ III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Xuan Yuan 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">143</span>]</cite></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2" title="III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Fine-Tuned LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2.SSS1" title="III-B1 Instruction-Tuning with Manually Created Datasets ‣ III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Instruction-Tuning with Manually Created Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2.SSS2" title="III-B2 Instruction-Tuning with LLMs Generated Datasets ‣ III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Instruction-Tuning with LLMs Generated Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2.SSS3" title="III-B3 Aligning with Human Preferences ‣ III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>3 </span>Aligning with Human Preferences</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2.SSS4" title="III-B4 Continue Pre-Training ‣ III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>4 </span>Continue Pre-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS2.SSS5" title="III-B5 Sample Efficiency ‣ III-B Fine-Tuned LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>5 </span>Sample Efficiency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS3" title="III-C Increasing Context Window ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Increasing Context Window</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS4" title="III-D Robotics ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Robotics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS5" title="III-E Multimodal LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Multimodal LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS6" title="III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">Augmented LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS6.SSS1" title="III-F1 Retrieval Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span>1 </span>Retrieval Augmented LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS6.SSS2" title="III-F2 Tool Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span>2 </span>Tool Augmented LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS7" title="III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span> </span><span class="ltx_text ltx_font_italic">Efficient LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS7.SSS1" title="III-G1 Parameter Efficient Fine-Tuning ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span>1 </span>Parameter Efficient Fine-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS7.SSS2" title="III-G2 Quantization ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span>2 </span>Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS7.SSS3" title="III-G3 Pruning ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span>3 </span>Pruning</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S4" title="IV Findings &amp; Insights ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Findings &amp; Insights</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5" title="V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Model Configurations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6" title="VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Datasets and Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS1" title="VI-A Training Datasets ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Training Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2" title="VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Datasets and Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS1" title="VI-B1 Multi-task ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>1 </span>Multi-task</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS1.Px1" title="MMLU [307] ‣ VI-B1 Multi-task ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">307</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS1.Px2" title="SuperGLUE [2] ‣ VI-B1 Multi-task ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">SuperGLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">2</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS1.Px3" title="BIG-bench [308] ‣ VI-B1 Multi-task ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">BIG-bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">308</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS1.Px4" title="GLUE [309] ‣ VI-B1 Multi-task ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">309</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2" title="VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>2 </span>Language Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px1" title="WinoGrande [354] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">354</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px2" title="CoQA [316] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CoQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">316</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px3" title="WiC [317] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">WiC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">317</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px4" title="Wikitext103 [318] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Wikitext103&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">318</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px5" title="PG19 [319] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PG19&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">319</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px6" title="C4 [10] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">10</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS2.Px7" title="LCQMC [320] ‣ VI-B2 Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">LCQMC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">320</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS3" title="VI-B3 Story Cloze and Sentence Completion ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>3 </span>Story Cloze and Sentence Completion</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS3.Px1" title="StoryCloze [334] ‣ VI-B3 Story Cloze and Sentence Completion ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">StoryCloze&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">334</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS3.Px2" title="LAMBADA [335] ‣ VI-B3 Story Cloze and Sentence Completion ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">335</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4" title="VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>4 </span>Physical Knowledge and World Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4.Px1" title="PIQA [340] ‣ VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">340</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4.Px2" title="TriviaQA [341] ‣ VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">341</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4.Px3" title="ARC [342] ‣ VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ARC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">342</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4.Px4" title="ARC-Easy [342] ‣ VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ARC-Easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">342</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS4.Px5" title="ARC-Challenge [342] ‣ VI-B4 Physical Knowledge and World Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">342</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS5" title="VI-B5 Contextual Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>5 </span>Contextual Language Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS5.Px1" title="RACE [347] ‣ VI-B5 Contextual Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">347</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS5.Px2" title="RACE-Middle [347] ‣ VI-B5 Contextual Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">RACE-Middle&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">347</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS5.Px3" title="RACE-High [347] ‣ VI-B5 Contextual Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">RACE-High&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">347</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS5.Px4" title="QuAC [348] ‣ VI-B5 Contextual Language Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">QuAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">348</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS6" title="VI-B6 Commonsense Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>6 </span>Commonsense Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS6.Px1" title="HellaSwag [355] ‣ VI-B6 Commonsense Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">355</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS6.Px2" title="COPA [402] ‣ VI-B6 Commonsense Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">COPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">402</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS6.Px3" title="WSC [357] ‣ VI-B6 Commonsense Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">WSC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">357</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS6.Px4" title="CSQA [358] ‣ VI-B6 Commonsense Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CSQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">358</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7" title="VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>7 </span>Reading Comprehension</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px1" title="BoolQ [363] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">363</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px2" title="SQUADv2 [364] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">SQUADv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">364</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px3" title="DROP [365] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">DROP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">365</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px4" title="RTE [366] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">RTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">366</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px5" title="WebQA [367] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">WebQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">367</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS7.Px6" title="CMRC2018 [369] ‣ VI-B7 Reading Comprehension ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">CMRC2018&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">369</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS8" title="VI-B8 Mathematical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>8 </span>Mathematical Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS8.Px1" title="MATH [382] ‣ VI-B8 Mathematical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">382</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS8.Px2" title="Math23k [383] ‣ VI-B8 Mathematical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Math23k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">383</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS8.Px3" title="GSM8K [384] ‣ VI-B8 Mathematical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">384</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS9" title="VI-B9 Problem Solving and Logical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>9 </span>Problem Solving and Logical Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS9.Px1" title="ANLI [394] ‣ VI-B9 Problem Solving and Logical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ANLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">394</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS9.Px2" title="HumanEval [391] ‣ VI-B9 Problem Solving and Logical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">391</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS9.Px3" title="StrategyQA [349] ‣ VI-B9 Problem Solving and Logical Reasoning ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">StrategyQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">349</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS10" title="VI-B10 Cross-Lingual Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>10 </span>Cross-Lingual Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS10.Px1" title="XNLI [399] ‣ VI-B10 Cross-Lingual Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">XNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">399</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS10.Px2" title="PAWS-X [400] ‣ VI-B10 Cross-Lingual Understanding ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">PAWS-X&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">400</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS11" title="VI-B11 Truthfulness ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>11 </span>Truthfulness</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS11.Px1" title="Truthful-QA [406] ‣ VI-B11 Truthfulness ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">Truthful-QA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">406</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS12" title="VI-B12 Biases and Ethics in AI ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span>12 </span>Biases and Ethics in AI</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS12.Px1" title="ETHOS [409] ‣ VI-B12 Biases and Ethics in AI ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">ETHOS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">409</span>]</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.SS2.SSS12.Px2" title="StereoSet [410] ‣ VI-B12 Biases and Ethics in AI ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title">StereoSet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">410</span>]</cite></span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S7" title="VII Applications ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Applications</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8" title="VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Summary and Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS1" title="VIII-A Architecture ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-A</span> </span><span class="ltx_text ltx_font_italic">Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS2" title="VIII-B Training Strategies ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-B</span> </span><span class="ltx_text ltx_font_italic">Training Strategies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS3" title="VIII-C Pre-Training vs Instruction Tuning ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-C</span> </span><span class="ltx_text ltx_font_italic">Pre-Training vs
Instruction Tuning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS4" title="VIII-D Supervised Models vs Generalized Models ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-D</span> </span><span class="ltx_text ltx_font_italic">Supervised Models vs Generalized Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS5" title="VIII-E Zero-Shot vs Few-Shot ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-E</span> </span><span class="ltx_text ltx_font_italic">Zero-Shot vs Few-Shot</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8.SS6" title="VIII-F Encoder vs Decoder vs Encoder-Decoder ‣ VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-F</span> </span><span class="ltx_text ltx_font_italic">Encoder vs Decoder vs Encoder-Decoder</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S9" title="IX Challenges and Future Directions ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Challenges and Future Directions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S10" title="X Conclusion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">X </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S11" title="XI Versioning ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">XI </span><span class="ltx_text ltx_font_smallcaps">Versioning</span></span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2307.06435v7 [cs.CL] 27 Dec 2023</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Comprehensive Overview of
<br class="ltx_break">Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Humza Naveed<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mn id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><cn id="id1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Asad Ullah Khan<math alttext="{}^{1,*}" class="ltx_Math" display="inline" id="id2.2.m2.2"><semantics id="id2.2.m2.2a"><msup id="id2.2.m2.2.2" xref="id2.2.m2.2.2.cmml"><mi id="id2.2.m2.2.2a" xref="id2.2.m2.2.2.cmml"></mi><mrow id="id2.2.m2.2.2.2.4" xref="id2.2.m2.2.2.2.3.cmml"><mn id="id2.2.m2.1.1.1.1" xref="id2.2.m2.1.1.1.1.cmml">1</mn><mo id="id2.2.m2.2.2.2.4.1" rspace="0em" xref="id2.2.m2.2.2.2.3.cmml">,</mo><mo id="id2.2.m2.2.2.2.2" lspace="0em" xref="id2.2.m2.2.2.2.2.cmml">*</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.2b"><apply id="id2.2.m2.2.2.cmml" xref="id2.2.m2.2.2"><list id="id2.2.m2.2.2.2.3.cmml" xref="id2.2.m2.2.2.2.4"><cn id="id2.2.m2.1.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1.1">1</cn><times id="id2.2.m2.2.2.2.2.cmml" xref="id2.2.m2.2.2.2.2"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.2c">{}^{1,*}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.2d">start_FLOATSUPERSCRIPT 1 , * end_FLOATSUPERSCRIPT</annotation></semantics></math>, Shi Qiu<math alttext="{}^{2,*}" class="ltx_Math" display="inline" id="id3.3.m3.2"><semantics id="id3.3.m3.2a"><msup id="id3.3.m3.2.2" xref="id3.3.m3.2.2.cmml"><mi id="id3.3.m3.2.2a" xref="id3.3.m3.2.2.cmml"></mi><mrow id="id3.3.m3.2.2.2.4" xref="id3.3.m3.2.2.2.3.cmml"><mn id="id3.3.m3.1.1.1.1" xref="id3.3.m3.1.1.1.1.cmml">2</mn><mo id="id3.3.m3.2.2.2.4.1" rspace="0em" xref="id3.3.m3.2.2.2.3.cmml">,</mo><mo id="id3.3.m3.2.2.2.2" lspace="0em" xref="id3.3.m3.2.2.2.2.cmml">*</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.2b"><apply id="id3.3.m3.2.2.cmml" xref="id3.3.m3.2.2"><list id="id3.3.m3.2.2.2.3.cmml" xref="id3.3.m3.2.2.2.4"><cn id="id3.3.m3.1.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1.1">2</cn><times id="id3.3.m3.2.2.2.2.cmml" xref="id3.3.m3.2.2.2.2"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.2c">{}^{2,*}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.2d">start_FLOATSUPERSCRIPT 2 , * end_FLOATSUPERSCRIPT</annotation></semantics></math>, Muhammad Saqib<math alttext="{}^{3,4,*}" class="ltx_Math" display="inline" id="id4.4.m4.3"><semantics id="id4.4.m4.3a"><msup id="id4.4.m4.3.3" xref="id4.4.m4.3.3.cmml"><mi id="id4.4.m4.3.3a" xref="id4.4.m4.3.3.cmml"></mi><mrow id="id4.4.m4.3.3.3.5" xref="id4.4.m4.3.3.3.4.cmml"><mn id="id4.4.m4.1.1.1.1" xref="id4.4.m4.1.1.1.1.cmml">3</mn><mo id="id4.4.m4.3.3.3.5.1" xref="id4.4.m4.3.3.3.4.cmml">,</mo><mn id="id4.4.m4.2.2.2.2" xref="id4.4.m4.2.2.2.2.cmml">4</mn><mo id="id4.4.m4.3.3.3.5.2" rspace="0em" xref="id4.4.m4.3.3.3.4.cmml">,</mo><mo id="id4.4.m4.3.3.3.3" lspace="0em" xref="id4.4.m4.3.3.3.3.cmml">*</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.3b"><apply id="id4.4.m4.3.3.cmml" xref="id4.4.m4.3.3"><list id="id4.4.m4.3.3.3.4.cmml" xref="id4.4.m4.3.3.3.5"><cn id="id4.4.m4.1.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1.1">3</cn><cn id="id4.4.m4.2.2.2.2.cmml" type="integer" xref="id4.4.m4.2.2.2.2">4</cn><times id="id4.4.m4.3.3.3.3.cmml" xref="id4.4.m4.3.3.3.3"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.3c">{}^{3,4,*}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.3d">start_FLOATSUPERSCRIPT 3 , 4 , * end_FLOATSUPERSCRIPT</annotation></semantics></math>, 
<br class="ltx_break">Saeed Anwar<math alttext="{}^{5,6}" class="ltx_Math" display="inline" id="id5.5.m5.2"><semantics id="id5.5.m5.2a"><msup id="id5.5.m5.2.2" xref="id5.5.m5.2.2.cmml"><mi id="id5.5.m5.2.2a" xref="id5.5.m5.2.2.cmml"></mi><mrow id="id5.5.m5.2.2.2.4" xref="id5.5.m5.2.2.2.3.cmml"><mn id="id5.5.m5.1.1.1.1" xref="id5.5.m5.1.1.1.1.cmml">5</mn><mo id="id5.5.m5.2.2.2.4.1" xref="id5.5.m5.2.2.2.3.cmml">,</mo><mn id="id5.5.m5.2.2.2.2" xref="id5.5.m5.2.2.2.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.2b"><apply id="id5.5.m5.2.2.cmml" xref="id5.5.m5.2.2"><list id="id5.5.m5.2.2.2.3.cmml" xref="id5.5.m5.2.2.2.4"><cn id="id5.5.m5.1.1.1.1.cmml" type="integer" xref="id5.5.m5.1.1.1.1">5</cn><cn id="id5.5.m5.2.2.2.2.cmml" type="integer" xref="id5.5.m5.2.2.2.2">6</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.2c">{}^{5,6}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.2d">start_FLOATSUPERSCRIPT 5 , 6 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Muhammad Usman<math alttext="{}^{5,6}" class="ltx_Math" display="inline" id="id6.6.m6.2"><semantics id="id6.6.m6.2a"><msup id="id6.6.m6.2.2" xref="id6.6.m6.2.2.cmml"><mi id="id6.6.m6.2.2a" xref="id6.6.m6.2.2.cmml"></mi><mrow id="id6.6.m6.2.2.2.4" xref="id6.6.m6.2.2.2.3.cmml"><mn id="id6.6.m6.1.1.1.1" xref="id6.6.m6.1.1.1.1.cmml">5</mn><mo id="id6.6.m6.2.2.2.4.1" xref="id6.6.m6.2.2.2.3.cmml">,</mo><mn id="id6.6.m6.2.2.2.2" xref="id6.6.m6.2.2.2.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id6.6.m6.2b"><apply id="id6.6.m6.2.2.cmml" xref="id6.6.m6.2.2"><list id="id6.6.m6.2.2.2.3.cmml" xref="id6.6.m6.2.2.2.4"><cn id="id6.6.m6.1.1.1.1.cmml" type="integer" xref="id6.6.m6.1.1.1.1">5</cn><cn id="id6.6.m6.2.2.2.2.cmml" type="integer" xref="id6.6.m6.2.2.2.2">6</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.2c">{}^{5,6}</annotation><annotation encoding="application/x-llamapun" id="id6.6.m6.2d">start_FLOATSUPERSCRIPT 5 , 6 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Naveed Akhtar<math alttext="{}^{7}" class="ltx_Math" display="inline" id="id7.7.m7.1"><semantics id="id7.7.m7.1a"><msup id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml"><mi id="id7.7.m7.1.1a" xref="id7.7.m7.1.1.cmml"></mi><mn id="id7.7.m7.1.1.1" xref="id7.7.m7.1.1.1.cmml">7</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><apply id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1"><cn id="id7.7.m7.1.1.1.cmml" type="integer" xref="id7.7.m7.1.1.1">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">{}^{7}</annotation><annotation encoding="application/x-llamapun" id="id7.7.m7.1d">start_FLOATSUPERSCRIPT 7 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Nick Barnes<math alttext="{}^{8}" class="ltx_Math" display="inline" id="id8.8.m8.1"><semantics id="id8.8.m8.1a"><msup id="id8.8.m8.1.1" xref="id8.8.m8.1.1.cmml"><mi id="id8.8.m8.1.1a" xref="id8.8.m8.1.1.cmml"></mi><mn id="id8.8.m8.1.1.1" xref="id8.8.m8.1.1.1.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.m8.1b"><apply id="id8.8.m8.1.1.cmml" xref="id8.8.m8.1.1"><cn id="id8.8.m8.1.1.1.cmml" type="integer" xref="id8.8.m8.1.1.1">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.1c">{}^{8}</annotation><annotation encoding="application/x-llamapun" id="id8.8.m8.1d">start_FLOATSUPERSCRIPT 8 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Ajmal Mian<math alttext="{}^{9}" class="ltx_Math" display="inline" id="id9.9.m9.1"><semantics id="id9.9.m9.1a"><msup id="id9.9.m9.1.1" xref="id9.9.m9.1.1.cmml"><mi id="id9.9.m9.1.1a" xref="id9.9.m9.1.1.cmml"></mi><mn id="id9.9.m9.1.1.1" xref="id9.9.m9.1.1.1.cmml">9</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.m9.1b"><apply id="id9.9.m9.1.1.cmml" xref="id9.9.m9.1.1"><cn id="id9.9.m9.1.1.1.cmml" type="integer" xref="id9.9.m9.1.1.1">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m9.1c">{}^{9}</annotation><annotation encoding="application/x-llamapun" id="id9.9.m9.1d">start_FLOATSUPERSCRIPT 9 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">
</span><span class="ltx_author_notes">
* is for equal contribution
Contact e-mail: humza_naveed@yahoo.com
Email: humza_naveed@yahoo.com, aukhanee@gmail.com,
shiqiu@cse.cuhk.edu.hk, muhammad.saqib@data61.csiro.au,
saeed.anwar@kfupm.edu.sa, muhammad.usman@kfupm.edu.sa,
naveed.akhtar1@unimelb.edu.au, nick.barnes@anu.edu.au,
ajmal.mian@uwa.edu.au
Repo: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/humza909/LLM_Survey.git" title="">https://github.com/humza909/LLM_Survey.git</a>
<span class="ltx_contact ltx_role_affiliation"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id10.10.m1.1"><semantics id="id10.10.m1.1a"><msup id="id10.10.m1.1.1" xref="id10.10.m1.1.1.cmml"><mi id="id10.10.m1.1.1a" xref="id10.10.m1.1.1.cmml"></mi><mn id="id10.10.m1.1.1.1" xref="id10.10.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.m1.1b"><apply id="id10.10.m1.1.1.cmml" xref="id10.10.m1.1.1"><cn id="id10.10.m1.1.1.1.cmml" type="integer" xref="id10.10.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id10.10.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Engineering and Technology (UET), Lahore, Pakistan
<br class="ltx_break"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id11.11.m2.1"><semantics id="id11.11.m2.1a"><msup id="id11.11.m2.1.1" xref="id11.11.m2.1.1.cmml"><mi id="id11.11.m2.1.1a" xref="id11.11.m2.1.1.cmml"></mi><mn id="id11.11.m2.1.1.1" xref="id11.11.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.m2.1b"><apply id="id11.11.m2.1.1.cmml" xref="id11.11.m2.1.1"><cn id="id11.11.m2.1.1.1.cmml" type="integer" xref="id11.11.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id11.11.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>The Chinese University of Hong Kong (CUHK), HKSAR, China
<br class="ltx_break"><math alttext="{}^{3}" class="ltx_Math" display="inline" id="id12.12.m3.1"><semantics id="id12.12.m3.1a"><msup id="id12.12.m3.1.1" xref="id12.12.m3.1.1.cmml"><mi id="id12.12.m3.1.1a" xref="id12.12.m3.1.1.cmml"></mi><mn id="id12.12.m3.1.1.1" xref="id12.12.m3.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="id12.12.m3.1b"><apply id="id12.12.m3.1.1.cmml" xref="id12.12.m3.1.1"><cn id="id12.12.m3.1.1.1.cmml" type="integer" xref="id12.12.m3.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m3.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m3.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Technology Sydney (UTS), Sydney, Australia
<br class="ltx_break"><math alttext="{}^{4}" class="ltx_Math" display="inline" id="id13.13.m4.1"><semantics id="id13.13.m4.1a"><msup id="id13.13.m4.1.1" xref="id13.13.m4.1.1.cmml"><mi id="id13.13.m4.1.1a" xref="id13.13.m4.1.1.cmml"></mi><mn id="id13.13.m4.1.1.1" xref="id13.13.m4.1.1.1.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="id13.13.m4.1b"><apply id="id13.13.m4.1.1.cmml" xref="id13.13.m4.1.1"><cn id="id13.13.m4.1.1.1.cmml" type="integer" xref="id13.13.m4.1.1.1">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.13.m4.1c">{}^{4}</annotation><annotation encoding="application/x-llamapun" id="id13.13.m4.1d">start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT</annotation></semantics></math>Commonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia
<br class="ltx_break"><math alttext="{}^{5}" class="ltx_Math" display="inline" id="id14.14.m5.1"><semantics id="id14.14.m5.1a"><msup id="id14.14.m5.1.1" xref="id14.14.m5.1.1.cmml"><mi id="id14.14.m5.1.1a" xref="id14.14.m5.1.1.cmml"></mi><mn id="id14.14.m5.1.1.1" xref="id14.14.m5.1.1.1.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="id14.14.m5.1b"><apply id="id14.14.m5.1.1.cmml" xref="id14.14.m5.1.1"><cn id="id14.14.m5.1.1.1.cmml" type="integer" xref="id14.14.m5.1.1.1">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m5.1c">{}^{5}</annotation><annotation encoding="application/x-llamapun" id="id14.14.m5.1d">start_FLOATSUPERSCRIPT 5 end_FLOATSUPERSCRIPT</annotation></semantics></math>King Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia
<br class="ltx_break"><math alttext="{}^{6}" class="ltx_Math" display="inline" id="id15.15.m6.1"><semantics id="id15.15.m6.1a"><msup id="id15.15.m6.1.1" xref="id15.15.m6.1.1.cmml"><mi id="id15.15.m6.1.1a" xref="id15.15.m6.1.1.cmml"></mi><mn id="id15.15.m6.1.1.1" xref="id15.15.m6.1.1.1.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="id15.15.m6.1b"><apply id="id15.15.m6.1.1.cmml" xref="id15.15.m6.1.1"><cn id="id15.15.m6.1.1.1.cmml" type="integer" xref="id15.15.m6.1.1.1">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id15.15.m6.1c">{}^{6}</annotation><annotation encoding="application/x-llamapun" id="id15.15.m6.1d">start_FLOATSUPERSCRIPT 6 end_FLOATSUPERSCRIPT</annotation></semantics></math>SDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia
<br class="ltx_break"><math alttext="{}^{7}" class="ltx_Math" display="inline" id="id16.16.m7.1"><semantics id="id16.16.m7.1a"><msup id="id16.16.m7.1.1" xref="id16.16.m7.1.1.cmml"><mi id="id16.16.m7.1.1a" xref="id16.16.m7.1.1.cmml"></mi><mn id="id16.16.m7.1.1.1" xref="id16.16.m7.1.1.1.cmml">7</mn></msup><annotation-xml encoding="MathML-Content" id="id16.16.m7.1b"><apply id="id16.16.m7.1.1.cmml" xref="id16.16.m7.1.1"><cn id="id16.16.m7.1.1.1.cmml" type="integer" xref="id16.16.m7.1.1.1">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id16.16.m7.1c">{}^{7}</annotation><annotation encoding="application/x-llamapun" id="id16.16.m7.1d">start_FLOATSUPERSCRIPT 7 end_FLOATSUPERSCRIPT</annotation></semantics></math>The University of Melbourne (UoM), Melbourne, Australia
<br class="ltx_break"><math alttext="{}^{8}" class="ltx_Math" display="inline" id="id17.17.m8.1"><semantics id="id17.17.m8.1a"><msup id="id17.17.m8.1.1" xref="id17.17.m8.1.1.cmml"><mi id="id17.17.m8.1.1a" xref="id17.17.m8.1.1.cmml"></mi><mn id="id17.17.m8.1.1.1" xref="id17.17.m8.1.1.1.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="id17.17.m8.1b"><apply id="id17.17.m8.1.1.cmml" xref="id17.17.m8.1.1"><cn id="id17.17.m8.1.1.1.cmml" type="integer" xref="id17.17.m8.1.1.1">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id17.17.m8.1c">{}^{8}</annotation><annotation encoding="application/x-llamapun" id="id17.17.m8.1d">start_FLOATSUPERSCRIPT 8 end_FLOATSUPERSCRIPT</annotation></semantics></math>Australian National University (ANU), Canberra, Australia
<br class="ltx_break"><math alttext="{}^{9}" class="ltx_Math" display="inline" id="id18.18.m9.1"><semantics id="id18.18.m9.1a"><msup id="id18.18.m9.1.1" xref="id18.18.m9.1.1.cmml"><mi id="id18.18.m9.1.1a" xref="id18.18.m9.1.1.cmml"></mi><mn id="id18.18.m9.1.1.1" xref="id18.18.m9.1.1.1.cmml">9</mn></msup><annotation-xml encoding="MathML-Content" id="id18.18.m9.1b"><apply id="id18.18.m9.1.1.cmml" xref="id18.18.m9.1.1"><cn id="id18.18.m9.1.1.1.cmml" type="integer" xref="id18.18.m9.1.1.1">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id18.18.m9.1c">{}^{9}</annotation><annotation encoding="application/x-llamapun" id="id18.18.m9.1d">start_FLOATSUPERSCRIPT 9 end_FLOATSUPERSCRIPT</annotation></semantics></math>The University of Western Australia (UWA), Perth, Australia
<br class="ltx_break">
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id19.id1">Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking


</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="398" id="S0.F1.g1" src="./A Comprehensive Overview of Large Language Models_files/Column_Chart.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The trends in the number of LLM models introduced over the years.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="1175" id="S0.F2.g1" src="./A Comprehensive Overview of Large Language Models_files/x1.png" width="1660">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Chronological display of LLM releases: light blue rectangles represent ‘pre-trained’ models, while dark rectangles correspond to ‘instruction-tuned’ models. Models on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Language plays a fundamental role in facilitating communication and self-expression for humans, and their interaction with machines. The need for generalized models stems from the growing demand for machines to handle complex language tasks, including translation, summarization, information retrieval, conversational interactions, etc.
Recently, significant breakthroughs have been witnessed in language models, primarily attributed to transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib1" title="">1</a>]</cite>, increased computational capabilities, and the availability of large-scale training data. These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib3" title="">3</a>]</cite>. Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems that can process and generate text with coherent communication&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib4" title="">4</a>]</cite>, and generalize to multiple tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>. 
<br class="ltx_break">The historical progress in natural language processing (NLP) evolved from statistical to neural language modeling and then from pre-trained language models (PLMs) to LLMs. While conventional language modeling (LM) trains task-specific models in supervised settings, PLMs are trained in a self-supervised setting on a large corpus of text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib9" title="">9</a>]</cite> with the aim to learn generic representation shareable among various NLP tasks. After fine-tuning for downstream tasks, PLMs surpass the performance gains of traditional language modeling (LM). The larger PLMs bring more performance gains, which has led to the transitioning of PLMs to LLMs by significantly increasing model parameters (tens to hundreds of billions)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> and training dataset (many GBs and TBs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite>. Following this development, numerous LLMs have been proposed in the literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>. An increasing trend in the number of released LLMs and names of a few significant LLMs proposed over the years are shown in Fig&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S0.F1" title="Figure 1 ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S0.F2" title="Figure 2 ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively.
<br class="ltx_break">The early work on LLMs, such as T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> and mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite> employed transfer learning until GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> showing LLMs are zero-shot transferable to downstream tasks without fine-tuning. LLMs accurately respond to task queries when prompted with task descriptions and examples. However, pre-trained LLMs fail to follow user intent and perform worse in zero-shot settings than in few-shot. Fine-tuning them with task instructions data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib19" title="">19</a>]</cite> and aligning with human preferences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite> enhances generalization to unseen tasks, improving zero-shot performance significantly and reducing misaligned behavior. 
<br class="ltx_break">Additional to better generalization and domain adaptation, LLMs appear to have emergent abilities, such as reasoning, planning, decision-making, in-context learning, answering in zero-shot settings, etc. These abilities are known to be acquired by them due to their gigantic scale even when the pre-trained LLMs are not trained specifically to possess these attributes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib24" title="">24</a>]</cite>. Such abilities have led LLMs widely adopted in diverse settings including, multi-modal, robotics, tool manipulation, question answering, autonomous agents, etc. Various improvements have also been suggested in these areas either by task-specific training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib31" title="">31</a>]</cite> or better prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib32" title="">32</a>]</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1076" id="S1.F3.g1" src="./A Comprehensive Overview of Large Language Models_files/LLMs_Overview.png" width="1196">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A broader overview of LLMs, dividing LLMs into five branches: 1. Training 2. Inference 3. Evaluation 4. Applications 5. Challenges</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The LLMs abilities to solve diverse tasks with human-level performance come at a cost of slow training and inference, extensive hardware requirements, and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib35" title="">35</a>]</cite> and training strategies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib41" title="">41</a>]</cite>. Parameter efficient tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib40" title="">40</a>]</cite>, pruning, quantization, knowledge distillation, and context length interpolation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib45" title="">45</a>]</cite> among others are some of the methods widely studied for efficient LLM utilization. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Due to the success of LLMs on a wide variety of tasks, the research literature has recently experienced a large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib49" title="">49</a>]</cite>, and topic-specific surveys in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib54" title="">54</a>]</cite>.
In contrast to these surveys, our contribution focuses on providing a comprehensive yet concise overview of the general direction of LLM research. This article summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine-tuning, multi-modal LLMs, robotics, augmented LLMs, datasets, evaluation, and others to provide a self-contained comprehensive overview. Our key contributions are summarized as follows.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a survey on the
developments in LLM research with the specific aim of providing a concise yet comprehensive overview of the direction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We present extensive summaries of pre-trained models that include fine-grained details of architecture and training details.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Besides paying special attention to the chronological order of LLMs throughout the article, we also summarize major findings of the popular contributions and provide detailed discussion on the key design and development aspects of LLMs to help practitioners to effectively leverage this technology.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">In this self-contained article, we cover a range of concepts to comprehend the general direction of LLMs comprehensively, including background, pre-training, fine-tuning, robotics, multi-modal LLMs, augmented LLMs, datasets, evaluation, etc.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We loosely follow the existing terminologies to ensure providing a more standardized outlook of this research direction. For instance, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib46" title="">46</a>]</cite>, our survey discusses pre-trained LLMs with 10B parameters or more. We refer the readers interested in smaller pre-trained models to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib49" title="">49</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The organization of this paper is as follows. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2" title="II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">II</span></a> discusses the background of LLMs. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3" title="III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a> focuses on LLMs overview, architectures, training pipelines and strategies, and utilization in different aspects. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S4" title="IV Findings &amp; Insights ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the key findings derived from each LLM. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5" title="V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">V</span></a> highlights the configuration and parameters that play a crucial role in the functioning of these models. Summary and discussions are presented in section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8" title="VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VIII</span></a>. The LLM training and evaluation, datasets and benchmarks are discussed in section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6" title="VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VI</span></a>, followed by challenges and future directions and conclusion in sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S9" title="IX Challenges and Future Directions ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">IX</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S10" title="X Conclusion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">X</span></a>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We provide the relevant background to understand the fundamentals related to LLMs in this section. Aligned with our objective of providing a comprehensive overview of this direction, this section offers a comprehensive yet concise outline of the basic concepts. We focus more on the intuitive aspects and refer the readers interested in details to the original works.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Tokenization</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">LLMs are trained on text to predict text, and similar to other natural language processing systems, they use tokenization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib55" title="">55</a>]</cite> as the essential preprocessing step. It aims to parse the text into non-decomposing units called tokens. Tokens can be characters, subwords&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib56" title="">56</a>]</cite>, symbols&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib57" title="">57</a>]</cite>, or words, depending on the size and type of the model. Some of the commonly used tokenization schemes in LLMs are briefed here. Readers are encouraged to refer to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib58" title="">58</a>]</cite> for a detailed survey.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.5.1.1">II-A</span>1 </span>WordPiece&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib59" title="">59</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">It was introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib59" title="">59</a>]</cite> as a novel text segmentation technique for Japanese and Korean languages to improve the language model for voice search systems. WordPiece selects tokens that increase the likelihood of an n-gram-based language model trained on the vocabulary composed of tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.5.1.1">II-A</span>2 </span>BPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib57" title="">57</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Byte Pair Encoding (BPE) has its origin in compression algorithms. It is an iterative process of generating tokens where pairs of adjacent <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p1.1.1">symbols</span> are replaced by a new symbol, and the occurrences of the most occurring symbols in the input text are merged.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.5.1.1">II-A</span>3 </span>UnigramLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib56" title="">56</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">In this tokenization, a simple unigram language model (LM) is trained using an initial vocabulary of <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS3.p1.1.1">subword</span> units. The vocabulary is pruned iteratively by removing the lowest probability items from the list, which are the worst performing on the unigram LM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Attention</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Attention, particularly <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">selective attention</span>, has been widely studied under perception, psychophysics, and psychology. Selective attention can be conceived as <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S2.SS2.p1.1.2">“the programming by the O of which stimuli will be processed or encoded and in what order this will occur”</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib60" title="">60</a>]</cite>. While this definition has its roots in visual perception, it has uncanny similarities with the recently formulated <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">attention</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite> (which stimuli will be processed) and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.4">positional encoding</span> (in what order this will occur)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite> in LLMs. We discuss both in sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS3" title="II-C Attention in LLMs ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS4" title="II-D Encoding Positions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Attention in LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The attention mechanism computes a representation of the input sequences by relating different positions (<span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">tokens</span>) of these sequences. There are various approaches to calculating and implementing attention, out of which some famous types are given below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.5.1.1">II-C</span>1 </span>Self-Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The self-attention is also known as intra-attention since all the queries, keys, and values come from the same block (encoder or decoder). The self-attention layer connects all the sequence positions with <math alttext="O(1)" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.1.m1.1"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mrow id="S2.SS3.SSS1.p1.1.m1.1.2" xref="S2.SS3.SSS1.p1.1.m1.1.2.cmml"><mi id="S2.SS3.SSS1.p1.1.m1.1.2.2" xref="S2.SS3.SSS1.p1.1.m1.1.2.2.cmml">O</mi><mo id="S2.SS3.SSS1.p1.1.m1.1.2.1" xref="S2.SS3.SSS1.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS3.SSS1.p1.1.m1.1.2.3.2" xref="S2.SS3.SSS1.p1.1.m1.1.2.cmml"><mo id="S2.SS3.SSS1.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS3.SSS1.p1.1.m1.1.2.cmml">(</mo><mn id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">1</mn><mo id="S2.SS3.SSS1.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS3.SSS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><apply id="S2.SS3.SSS1.p1.1.m1.1.2.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.2"><times id="S2.SS3.SSS1.p1.1.m1.1.2.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.2.1"></times><ci id="S2.SS3.SSS1.p1.1.m1.1.2.2.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.2.2">𝑂</ci><cn id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS3.SSS1.p1.1.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">O(1)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS1.p1.1.m1.1d">italic_O ( 1 )</annotation></semantics></math> space complexity which is highly desirable for learning long-range dependencies in the input.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.5.1.1">II-C</span>2 </span>Cross Attention</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">In encoder-decoder architectures, the outputs of the encoder blocks act as the queries to the intermediate representation of the decoder, which provides the keys and values to calculate a representation of the decoder conditioned on the encoder. This attention is called cross-attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS3.5.1.1">II-C</span>3 </span>Full Attention</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">The naive implementation of calculating self-attention is known as full attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS4.5.1.1">II-C</span>4 </span>Sparse Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib63" title="">63</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS4.p1">
<p class="ltx_p" id="S2.SS3.SSS4.p1.1">The self-attention has a time complexity of <math alttext="O(n^{2})" class="ltx_Math" display="inline" id="S2.SS3.SSS4.p1.1.m1.1"><semantics id="S2.SS3.SSS4.p1.1.m1.1a"><mrow id="S2.SS3.SSS4.p1.1.m1.1.1" xref="S2.SS3.SSS4.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS4.p1.1.m1.1.1.3" xref="S2.SS3.SSS4.p1.1.m1.1.1.3.cmml">O</mi><mo id="S2.SS3.SSS4.p1.1.m1.1.1.2" xref="S2.SS3.SSS4.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.SS3.SSS4.p1.1.m1.1.1.1.1" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml"><mo id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.2" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.3" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p1.1.m1.1b"><apply id="S2.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1"><times id="S2.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.2"></times><ci id="S2.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.3">𝑂</ci><apply id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.2">𝑛</ci><cn id="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS3.SSS4.p1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p1.1.m1.1c">O(n^{2})</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS4.p1.1.m1.1d">italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, which becomes prohibitive when scaling the LLMs to large context windows. An approximation to the self-attention was proposed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib63" title="">63</a>]</cite>, which greatly enhanced the capacity of GPT series LLMs to process a greater number of input tokens in a reasonable time.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS5.5.1.1">II-C</span>5 </span>Flash Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib64" title="">64</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS5.p1">
<p class="ltx_p" id="S2.SS3.SSS5.p1.1">The bottleneck for calculating the attention using GPUs lies in the memory access rather than the computational speed. Flash Attention uses the classical input tiling approach to process the blocks of the input in GPU on-chip SRAM rather than doing IO for every token from the High Bandwith Memory (HBM). An extension of this approach to sparse attention follows the speed gains of the full attention implementation. This trick allows even greater context-length windows in the LLMs as compared to those LLMs with sparse attention.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Encoding Positions</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">attention</span> modules do not consider the order of processing by design. Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite> introduced <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S2.SS4.p1.1.2">“positional encodings”</span> to feed information about the position of the tokens in input sequences. Several variants of positional encoding have been proposed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib66" title="">66</a>]</cite>. Interestingly, a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib67" title="">67</a>]</cite> suggests that adding this information may not matter for the state-of-the-art decoder-only Transformers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS1.5.1.1">II-D</span>1 </span>Absolute</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">This is the most straightforward approach to adding the sequence order information by assigning a unique identifier to each position of the sequence before passing it to the attention module.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS2.5.1.1">II-D</span>2 </span>Relative</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">To pass the information on the relative dependencies of different tokens appearing at different locations in the sequence, a relative positional encoding is calculated by some kind of learning. Two famous types of relative encodings are:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1"><em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS2.p2.1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p2.1.1.1">Alibi:</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib65" title="">65</a>]</cite></em> In this approach, a scalar bias is subtracted from the attention score calculated using two tokens which increases with the distance between the positions of the tokens. This learned approach effectively favors using recent tokens for attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS2.p3">
<p class="ltx_p" id="S2.SS4.SSS2.p3.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS4.SSS2.p3.1.1">RoPE:</em> Keys, queries, and values are all vectors in the LLMs. RoPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib66" title="">66</a>]</cite> involves the rotation of the query and key representations at an angle proportional to their absolute positions of the tokens in the input sequence. This step results in a relative positional encoding scheme which decays with the distance between the tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS5.5.1.1">II-E</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS5.6.2">Activation Functions</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">The activation functions serve a crucial role in the curve-fitting abilities of the neural networks, as proved in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib68" title="">68</a>]</cite>. The modern activation functions used in LLMs are different from the earlier squashing functions but are critical to the success of LLMs. We discuss these activation functions in this section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS5.SSS1.5.1.1">II-E</span>1 </span>ReLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib69" title="">69</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS1.p1">
<p class="ltx_p" id="S2.SS5.SSS1.p1.1">Rectified linear unit (ReLU) is defined as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="ReLU(x)=max(0,x)" class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.4" xref="S2.E1.m1.3.4.cmml"><mrow id="S2.E1.m1.3.4.2" xref="S2.E1.m1.3.4.2.cmml"><mi id="S2.E1.m1.3.4.2.2" xref="S2.E1.m1.3.4.2.2.cmml">R</mi><mo id="S2.E1.m1.3.4.2.1" xref="S2.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S2.E1.m1.3.4.2.3" xref="S2.E1.m1.3.4.2.3.cmml">e</mi><mo id="S2.E1.m1.3.4.2.1a" xref="S2.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S2.E1.m1.3.4.2.4" xref="S2.E1.m1.3.4.2.4.cmml">L</mi><mo id="S2.E1.m1.3.4.2.1b" xref="S2.E1.m1.3.4.2.1.cmml">⁢</mo><mi id="S2.E1.m1.3.4.2.5" xref="S2.E1.m1.3.4.2.5.cmml">U</mi><mo id="S2.E1.m1.3.4.2.1c" xref="S2.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.3.4.2.6.2" xref="S2.E1.m1.3.4.2.cmml"><mo id="S2.E1.m1.3.4.2.6.2.1" stretchy="false" xref="S2.E1.m1.3.4.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo id="S2.E1.m1.3.4.2.6.2.2" stretchy="false" xref="S2.E1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.4.1" xref="S2.E1.m1.3.4.1.cmml">=</mo><mrow id="S2.E1.m1.3.4.3" xref="S2.E1.m1.3.4.3.cmml"><mi id="S2.E1.m1.3.4.3.2" xref="S2.E1.m1.3.4.3.2.cmml">m</mi><mo id="S2.E1.m1.3.4.3.1" xref="S2.E1.m1.3.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.3.4.3.3" xref="S2.E1.m1.3.4.3.3.cmml">a</mi><mo id="S2.E1.m1.3.4.3.1a" xref="S2.E1.m1.3.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.3.4.3.4" xref="S2.E1.m1.3.4.3.4.cmml">x</mi><mo id="S2.E1.m1.3.4.3.1b" xref="S2.E1.m1.3.4.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.3.4.3.5.2" xref="S2.E1.m1.3.4.3.5.1.cmml"><mo id="S2.E1.m1.3.4.3.5.2.1" stretchy="false" xref="S2.E1.m1.3.4.3.5.1.cmml">(</mo><mn id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">0</mn><mo id="S2.E1.m1.3.4.3.5.2.2" xref="S2.E1.m1.3.4.3.5.1.cmml">,</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">x</mi><mo id="S2.E1.m1.3.4.3.5.2.3" stretchy="false" xref="S2.E1.m1.3.4.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.4.cmml" xref="S2.E1.m1.3.4"><eq id="S2.E1.m1.3.4.1.cmml" xref="S2.E1.m1.3.4.1"></eq><apply id="S2.E1.m1.3.4.2.cmml" xref="S2.E1.m1.3.4.2"><times id="S2.E1.m1.3.4.2.1.cmml" xref="S2.E1.m1.3.4.2.1"></times><ci id="S2.E1.m1.3.4.2.2.cmml" xref="S2.E1.m1.3.4.2.2">𝑅</ci><ci id="S2.E1.m1.3.4.2.3.cmml" xref="S2.E1.m1.3.4.2.3">𝑒</ci><ci id="S2.E1.m1.3.4.2.4.cmml" xref="S2.E1.m1.3.4.2.4">𝐿</ci><ci id="S2.E1.m1.3.4.2.5.cmml" xref="S2.E1.m1.3.4.2.5">𝑈</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑥</ci></apply><apply id="S2.E1.m1.3.4.3.cmml" xref="S2.E1.m1.3.4.3"><times id="S2.E1.m1.3.4.3.1.cmml" xref="S2.E1.m1.3.4.3.1"></times><ci id="S2.E1.m1.3.4.3.2.cmml" xref="S2.E1.m1.3.4.3.2">𝑚</ci><ci id="S2.E1.m1.3.4.3.3.cmml" xref="S2.E1.m1.3.4.3.3">𝑎</ci><ci id="S2.E1.m1.3.4.3.4.cmml" xref="S2.E1.m1.3.4.3.4">𝑥</ci><interval closure="open" id="S2.E1.m1.3.4.3.5.1.cmml" xref="S2.E1.m1.3.4.3.5.2"><cn id="S2.E1.m1.2.2.cmml" type="integer" xref="S2.E1.m1.2.2">0</cn><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑥</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">ReLU(x)=max(0,x)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_R italic_e italic_L italic_U ( italic_x ) = italic_m italic_a italic_x ( 0 , italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS5.SSS2.5.1.1">II-E</span>2 </span>GeLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib70" title="">70</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS2.p1">
<p class="ltx_p" id="S2.SS5.SSS2.p1.1">Gaussian Error Linear Unit (GeLU) is the combination of ReLU, dropout&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib71" title="">71</a>]</cite> and zoneout&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib72" title="">72</a>]</cite>. It is the most widely used activation function in contemporary LLM literature.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS5.SSS3.5.1.1">II-E</span>3 </span>GLU variants&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib73" title="">73</a>]</cite>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS3.p1">
<p class="ltx_p" id="S2.SS5.SSS3.p1.2">Gated Linear Unit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib74" title="">74</a>]</cite> is a neural network layer that is an element-wise product (<math alttext="\otimes" class="ltx_Math" display="inline" id="S2.SS5.SSS3.p1.1.m1.1"><semantics id="S2.SS5.SSS3.p1.1.m1.1a"><mo id="S2.SS5.SSS3.p1.1.m1.1.1" xref="S2.SS5.SSS3.p1.1.m1.1.1.cmml">⊗</mo><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS3.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS5.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS5.SSS3.p1.1.m1.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS3.p1.1.m1.1c">\otimes</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.SSS3.p1.1.m1.1d">⊗</annotation></semantics></math>) of a linear transformation and a sigmoid transformed (<math alttext="\sigma" class="ltx_Math" display="inline" id="S2.SS5.SSS3.p1.2.m2.1"><semantics id="S2.SS5.SSS3.p1.2.m2.1a"><mi id="S2.SS5.SSS3.p1.2.m2.1.1" xref="S2.SS5.SSS3.p1.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS3.p1.2.m2.1b"><ci id="S2.SS5.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS5.SSS3.p1.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS3.p1.2.m2.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.SSS3.p1.2.m2.1d">italic_σ</annotation></semantics></math>) linear projection of the input given as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="GLU(x,W,V,b,c)=(xW+b)\otimes\sigma(xV+c)," class="ltx_Math" display="block" id="S2.E2.m1.6"><semantics id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.6.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.4" xref="S2.E2.m1.6.6.1.1.4.cmml"><mi id="S2.E2.m1.6.6.1.1.4.2" xref="S2.E2.m1.6.6.1.1.4.2.cmml">G</mi><mo id="S2.E2.m1.6.6.1.1.4.1" xref="S2.E2.m1.6.6.1.1.4.1.cmml">⁢</mo><mi id="S2.E2.m1.6.6.1.1.4.3" xref="S2.E2.m1.6.6.1.1.4.3.cmml">L</mi><mo id="S2.E2.m1.6.6.1.1.4.1a" xref="S2.E2.m1.6.6.1.1.4.1.cmml">⁢</mo><mi id="S2.E2.m1.6.6.1.1.4.4" xref="S2.E2.m1.6.6.1.1.4.4.cmml">U</mi><mo id="S2.E2.m1.6.6.1.1.4.1b" xref="S2.E2.m1.6.6.1.1.4.1.cmml">⁢</mo><mrow id="S2.E2.m1.6.6.1.1.4.5.2" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml"><mo id="S2.E2.m1.6.6.1.1.4.5.2.1" stretchy="false" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo id="S2.E2.m1.6.6.1.1.4.5.2.2" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">W</mi><mo id="S2.E2.m1.6.6.1.1.4.5.2.3" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">V</mi><mo id="S2.E2.m1.6.6.1.1.4.5.2.4" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">,</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">b</mi><mo id="S2.E2.m1.6.6.1.1.4.5.2.5" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">,</mo><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">c</mi><mo id="S2.E2.m1.6.6.1.1.4.5.2.6" stretchy="false" xref="S2.E2.m1.6.6.1.1.4.5.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.6.6.1.1.3" xref="S2.E2.m1.6.6.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.6.6.1.1.2" xref="S2.E2.m1.6.6.1.1.2.cmml"><mrow id="S2.E2.m1.6.6.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.6.6.1.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">W</mi></mrow><mo id="S2.E2.m1.6.6.1.1.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="S2.E2.m1.6.6.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E2.m1.6.6.1.1.1.1.2" rspace="0.222em" xref="S2.E2.m1.6.6.1.1.1.1.2.cmml">⊗</mo><mi id="S2.E2.m1.6.6.1.1.1.1.3" xref="S2.E2.m1.6.6.1.1.1.1.3.cmml">σ</mi></mrow><mo id="S2.E2.m1.6.6.1.1.2.3" xref="S2.E2.m1.6.6.1.1.2.3.cmml">⁢</mo><mrow id="S2.E2.m1.6.6.1.1.2.2.1" xref="S2.E2.m1.6.6.1.1.2.2.1.1.cmml"><mo id="S2.E2.m1.6.6.1.1.2.2.1.2" stretchy="false" xref="S2.E2.m1.6.6.1.1.2.2.1.1.cmml">(</mo><mrow id="S2.E2.m1.6.6.1.1.2.2.1.1" xref="S2.E2.m1.6.6.1.1.2.2.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.2.2.1.1.2" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.cmml"><mi id="S2.E2.m1.6.6.1.1.2.2.1.1.2.2" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.2.cmml">x</mi><mo id="S2.E2.m1.6.6.1.1.2.2.1.1.2.1" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S2.E2.m1.6.6.1.1.2.2.1.1.2.3" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.3.cmml">V</mi></mrow><mo id="S2.E2.m1.6.6.1.1.2.2.1.1.1" xref="S2.E2.m1.6.6.1.1.2.2.1.1.1.cmml">+</mo><mi id="S2.E2.m1.6.6.1.1.2.2.1.1.3" xref="S2.E2.m1.6.6.1.1.2.2.1.1.3.cmml">c</mi></mrow><mo id="S2.E2.m1.6.6.1.1.2.2.1.3" stretchy="false" xref="S2.E2.m1.6.6.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.6.6.1.2" xref="S2.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.6.1.1.cmml" xref="S2.E2.m1.6.6.1"><eq id="S2.E2.m1.6.6.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.3"></eq><apply id="S2.E2.m1.6.6.1.1.4.cmml" xref="S2.E2.m1.6.6.1.1.4"><times id="S2.E2.m1.6.6.1.1.4.1.cmml" xref="S2.E2.m1.6.6.1.1.4.1"></times><ci id="S2.E2.m1.6.6.1.1.4.2.cmml" xref="S2.E2.m1.6.6.1.1.4.2">𝐺</ci><ci id="S2.E2.m1.6.6.1.1.4.3.cmml" xref="S2.E2.m1.6.6.1.1.4.3">𝐿</ci><ci id="S2.E2.m1.6.6.1.1.4.4.cmml" xref="S2.E2.m1.6.6.1.1.4.4">𝑈</ci><vector id="S2.E2.m1.6.6.1.1.4.5.1.cmml" xref="S2.E2.m1.6.6.1.1.4.5.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑊</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑉</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">𝑏</ci><ci id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5">𝑐</ci></vector></apply><apply id="S2.E2.m1.6.6.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.2"><times id="S2.E2.m1.6.6.1.1.2.3.cmml" xref="S2.E2.m1.6.6.1.1.2.3"></times><apply id="S2.E2.m1.6.6.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.6.6.1.1.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.2">tensor-product</csymbol><apply id="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1"><plus id="S2.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.1"></plus><apply id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2"><times id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.1"></times><ci id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.3">𝑊</ci></apply><ci id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3">𝑏</ci></apply><ci id="S2.E2.m1.6.6.1.1.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.3">𝜎</ci></apply><apply id="S2.E2.m1.6.6.1.1.2.2.1.1.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1"><plus id="S2.E2.m1.6.6.1.1.2.2.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.1"></plus><apply id="S2.E2.m1.6.6.1.1.2.2.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2"><times id="S2.E2.m1.6.6.1.1.2.2.1.1.2.1.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.1"></times><ci id="S2.E2.m1.6.6.1.1.2.2.1.1.2.2.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.2">𝑥</ci><ci id="S2.E2.m1.6.6.1.1.2.2.1.1.2.3.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.2.3">𝑉</ci></apply><ci id="S2.E2.m1.6.6.1.1.2.2.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.2.2.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.6c">GLU(x,W,V,b,c)=(xW+b)\otimes\sigma(xV+c),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.6d">italic_G italic_L italic_U ( italic_x , italic_W , italic_V , italic_b , italic_c ) = ( italic_x italic_W + italic_b ) ⊗ italic_σ ( italic_x italic_V + italic_c ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS5.SSS3.p1.5">where <math alttext="X" class="ltx_Math" display="inline" id="S2.SS5.SSS3.p1.3.m1.1"><semantics id="S2.SS5.SSS3.p1.3.m1.1a"><mi id="S2.SS5.SSS3.p1.3.m1.1.1" xref="S2.SS5.SSS3.p1.3.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS3.p1.3.m1.1b"><ci id="S2.SS5.SSS3.p1.3.m1.1.1.cmml" xref="S2.SS5.SSS3.p1.3.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS3.p1.3.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.SSS3.p1.3.m1.1d">italic_X</annotation></semantics></math> is the input of layer and <math alttext="l" class="ltx_Math" display="inline" id="S2.SS5.SSS3.p1.4.m2.1"><semantics id="S2.SS5.SSS3.p1.4.m2.1a"><mi id="S2.SS5.SSS3.p1.4.m2.1.1" xref="S2.SS5.SSS3.p1.4.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS3.p1.4.m2.1b"><ci id="S2.SS5.SSS3.p1.4.m2.1.1.cmml" xref="S2.SS5.SSS3.p1.4.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS3.p1.4.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.SSS3.p1.4.m2.1d">italic_l</annotation></semantics></math>, <math alttext="W,b,V\textnormal{ and }c" class="ltx_Math" display="inline" id="S2.SS5.SSS3.p1.5.m3.3"><semantics id="S2.SS5.SSS3.p1.5.m3.3a"><mrow id="S2.SS5.SSS3.p1.5.m3.3.3.1" xref="S2.SS5.SSS3.p1.5.m3.3.3.2.cmml"><mi id="S2.SS5.SSS3.p1.5.m3.1.1" xref="S2.SS5.SSS3.p1.5.m3.1.1.cmml">W</mi><mo id="S2.SS5.SSS3.p1.5.m3.3.3.1.2" xref="S2.SS5.SSS3.p1.5.m3.3.3.2.cmml">,</mo><mi id="S2.SS5.SSS3.p1.5.m3.2.2" xref="S2.SS5.SSS3.p1.5.m3.2.2.cmml">b</mi><mo id="S2.SS5.SSS3.p1.5.m3.3.3.1.3" xref="S2.SS5.SSS3.p1.5.m3.3.3.2.cmml">,</mo><mrow id="S2.SS5.SSS3.p1.5.m3.3.3.1.1" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.cmml"><mi id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.2" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.2.cmml">V</mi><mo id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1.cmml">⁢</mo><mtext id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3a.cmml">&nbsp;and&nbsp;</mtext><mo id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1a" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1.cmml">⁢</mo><mi id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.4" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.4.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS3.p1.5.m3.3b"><list id="S2.SS5.SSS3.p1.5.m3.3.3.2.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1"><ci id="S2.SS5.SSS3.p1.5.m3.1.1.cmml" xref="S2.SS5.SSS3.p1.5.m3.1.1">𝑊</ci><ci id="S2.SS5.SSS3.p1.5.m3.2.2.cmml" xref="S2.SS5.SSS3.p1.5.m3.2.2">𝑏</ci><apply id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1"><times id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.1"></times><ci id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.2.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.2">𝑉</ci><ci id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3a.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3"><mtext id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.3">&nbsp;and&nbsp;</mtext></ci><ci id="S2.SS5.SSS3.p1.5.m3.3.3.1.1.4.cmml" xref="S2.SS5.SSS3.p1.5.m3.3.3.1.1.4">𝑐</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS3.p1.5.m3.3c">W,b,V\textnormal{ and }c</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.SSS3.p1.5.m3.3d">italic_W , italic_b , italic_V and italic_c</annotation></semantics></math> are learned parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS3.p2">
<p class="ltx_p" id="S2.SS5.SSS3.p2.1">GLU was modified in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib73" title="">73</a>]</cite> to evaluate the effect of different variations in the training and testing of transformers, resulting in better empirical results. Here are the different GLU variations introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib73" title="">73</a>]</cite> and used in LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS3.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S11.EGx1">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle ReGLU(x,W,V,b,c)" class="ltx_Math" display="inline" id="S2.Ex1.m1.5"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5.6" xref="S2.Ex1.m1.5.6.cmml"><mi id="S2.Ex1.m1.5.6.2" xref="S2.Ex1.m1.5.6.2.cmml">R</mi><mo id="S2.Ex1.m1.5.6.1" xref="S2.Ex1.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex1.m1.5.6.3" xref="S2.Ex1.m1.5.6.3.cmml">e</mi><mo id="S2.Ex1.m1.5.6.1a" xref="S2.Ex1.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex1.m1.5.6.4" xref="S2.Ex1.m1.5.6.4.cmml">G</mi><mo id="S2.Ex1.m1.5.6.1b" xref="S2.Ex1.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex1.m1.5.6.5" xref="S2.Ex1.m1.5.6.5.cmml">L</mi><mo id="S2.Ex1.m1.5.6.1c" xref="S2.Ex1.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex1.m1.5.6.6" xref="S2.Ex1.m1.5.6.6.cmml">U</mi><mo id="S2.Ex1.m1.5.6.1d" xref="S2.Ex1.m1.5.6.1.cmml">⁢</mo><mrow id="S2.Ex1.m1.5.6.7.2" xref="S2.Ex1.m1.5.6.7.1.cmml"><mo id="S2.Ex1.m1.5.6.7.2.1" stretchy="false" xref="S2.Ex1.m1.5.6.7.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">x</mi><mo id="S2.Ex1.m1.5.6.7.2.2" xref="S2.Ex1.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">W</mi><mo id="S2.Ex1.m1.5.6.7.2.3" xref="S2.Ex1.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">V</mi><mo id="S2.Ex1.m1.5.6.7.2.4" xref="S2.Ex1.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml">b</mi><mo id="S2.Ex1.m1.5.6.7.2.5" xref="S2.Ex1.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex1.m1.5.5" xref="S2.Ex1.m1.5.5.cmml">c</mi><mo id="S2.Ex1.m1.5.6.7.2.6" stretchy="false" xref="S2.Ex1.m1.5.6.7.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.5b"><apply id="S2.Ex1.m1.5.6.cmml" xref="S2.Ex1.m1.5.6"><times id="S2.Ex1.m1.5.6.1.cmml" xref="S2.Ex1.m1.5.6.1"></times><ci id="S2.Ex1.m1.5.6.2.cmml" xref="S2.Ex1.m1.5.6.2">𝑅</ci><ci id="S2.Ex1.m1.5.6.3.cmml" xref="S2.Ex1.m1.5.6.3">𝑒</ci><ci id="S2.Ex1.m1.5.6.4.cmml" xref="S2.Ex1.m1.5.6.4">𝐺</ci><ci id="S2.Ex1.m1.5.6.5.cmml" xref="S2.Ex1.m1.5.6.5">𝐿</ci><ci id="S2.Ex1.m1.5.6.6.cmml" xref="S2.Ex1.m1.5.6.6">𝑈</ci><vector id="S2.Ex1.m1.5.6.7.1.cmml" xref="S2.Ex1.m1.5.6.7.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑥</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝑊</ci><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">𝑉</ci><ci id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4">𝑏</ci><ci id="S2.Ex1.m1.5.5.cmml" xref="S2.Ex1.m1.5.5">𝑐</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">\displaystyle ReGLU(x,W,V,b,c)</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">italic_R italic_e italic_G italic_L italic_U ( italic_x , italic_W , italic_V , italic_b , italic_c )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=max(0,xW+b)\otimes," class="ltx_math_unparsed" display="inline" id="S2.Ex1.m2.1"><semantics id="S2.Ex1.m2.1a"><mrow id="S2.Ex1.m2.1b"><mo id="S2.Ex1.m2.1.2">=</mo><mi id="S2.Ex1.m2.1.3">m</mi><mi id="S2.Ex1.m2.1.4">a</mi><mi id="S2.Ex1.m2.1.5">x</mi><mrow id="S2.Ex1.m2.1.6"><mo id="S2.Ex1.m2.1.6.1" stretchy="false">(</mo><mn id="S2.Ex1.m2.1.1">0</mn><mo id="S2.Ex1.m2.1.6.2">,</mo><mi id="S2.Ex1.m2.1.6.3">x</mi><mi id="S2.Ex1.m2.1.6.4">W</mi><mo id="S2.Ex1.m2.1.6.5">+</mo><mi id="S2.Ex1.m2.1.6.6">b</mi><mo id="S2.Ex1.m2.1.6.7" rspace="0.055em" stretchy="false">)</mo></mrow><mo id="S2.Ex1.m2.1.7" rspace="0em">⊗</mo><mo id="S2.Ex1.m2.1.8">,</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle=max(0,xW+b)\otimes,</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m2.1d">= italic_m italic_a italic_x ( 0 , italic_x italic_W + italic_b ) ⊗ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle GEGLU(x,W,V,b,c)" class="ltx_Math" display="inline" id="S2.Ex2.m1.5"><semantics id="S2.Ex2.m1.5a"><mrow id="S2.Ex2.m1.5.6" xref="S2.Ex2.m1.5.6.cmml"><mi id="S2.Ex2.m1.5.6.2" xref="S2.Ex2.m1.5.6.2.cmml">G</mi><mo id="S2.Ex2.m1.5.6.1" xref="S2.Ex2.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex2.m1.5.6.3" xref="S2.Ex2.m1.5.6.3.cmml">E</mi><mo id="S2.Ex2.m1.5.6.1a" xref="S2.Ex2.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex2.m1.5.6.4" xref="S2.Ex2.m1.5.6.4.cmml">G</mi><mo id="S2.Ex2.m1.5.6.1b" xref="S2.Ex2.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex2.m1.5.6.5" xref="S2.Ex2.m1.5.6.5.cmml">L</mi><mo id="S2.Ex2.m1.5.6.1c" xref="S2.Ex2.m1.5.6.1.cmml">⁢</mo><mi id="S2.Ex2.m1.5.6.6" xref="S2.Ex2.m1.5.6.6.cmml">U</mi><mo id="S2.Ex2.m1.5.6.1d" xref="S2.Ex2.m1.5.6.1.cmml">⁢</mo><mrow id="S2.Ex2.m1.5.6.7.2" xref="S2.Ex2.m1.5.6.7.1.cmml"><mo id="S2.Ex2.m1.5.6.7.2.1" stretchy="false" xref="S2.Ex2.m1.5.6.7.1.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">x</mi><mo id="S2.Ex2.m1.5.6.7.2.2" xref="S2.Ex2.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">W</mi><mo id="S2.Ex2.m1.5.6.7.2.3" xref="S2.Ex2.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml">V</mi><mo id="S2.Ex2.m1.5.6.7.2.4" xref="S2.Ex2.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex2.m1.4.4" xref="S2.Ex2.m1.4.4.cmml">b</mi><mo id="S2.Ex2.m1.5.6.7.2.5" xref="S2.Ex2.m1.5.6.7.1.cmml">,</mo><mi id="S2.Ex2.m1.5.5" xref="S2.Ex2.m1.5.5.cmml">c</mi><mo id="S2.Ex2.m1.5.6.7.2.6" stretchy="false" xref="S2.Ex2.m1.5.6.7.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.5b"><apply id="S2.Ex2.m1.5.6.cmml" xref="S2.Ex2.m1.5.6"><times id="S2.Ex2.m1.5.6.1.cmml" xref="S2.Ex2.m1.5.6.1"></times><ci id="S2.Ex2.m1.5.6.2.cmml" xref="S2.Ex2.m1.5.6.2">𝐺</ci><ci id="S2.Ex2.m1.5.6.3.cmml" xref="S2.Ex2.m1.5.6.3">𝐸</ci><ci id="S2.Ex2.m1.5.6.4.cmml" xref="S2.Ex2.m1.5.6.4">𝐺</ci><ci id="S2.Ex2.m1.5.6.5.cmml" xref="S2.Ex2.m1.5.6.5">𝐿</ci><ci id="S2.Ex2.m1.5.6.6.cmml" xref="S2.Ex2.m1.5.6.6">𝑈</ci><vector id="S2.Ex2.m1.5.6.7.1.cmml" xref="S2.Ex2.m1.5.6.7.2"><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝑥</ci><ci id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2">𝑊</ci><ci id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3">𝑉</ci><ci id="S2.Ex2.m1.4.4.cmml" xref="S2.Ex2.m1.4.4">𝑏</ci><ci id="S2.Ex2.m1.5.5.cmml" xref="S2.Ex2.m1.5.5">𝑐</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.5c">\displaystyle GEGLU(x,W,V,b,c)</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.5d">italic_G italic_E italic_G italic_L italic_U ( italic_x , italic_W , italic_V , italic_b , italic_c )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=GELU(xW+b)\otimes(xV+c)," class="ltx_Math" display="inline" id="S2.Ex2.m2.1"><semantics id="S2.Ex2.m2.1a"><mrow id="S2.Ex2.m2.1.1.1" xref="S2.Ex2.m2.1.1.1.1.cmml"><mrow id="S2.Ex2.m2.1.1.1.1" xref="S2.Ex2.m2.1.1.1.1.cmml"><mi id="S2.Ex2.m2.1.1.1.1.4" xref="S2.Ex2.m2.1.1.1.1.4.cmml"></mi><mo id="S2.Ex2.m2.1.1.1.1.3" xref="S2.Ex2.m2.1.1.1.1.3.cmml">=</mo><mrow id="S2.Ex2.m2.1.1.1.1.2" xref="S2.Ex2.m2.1.1.1.1.2.cmml"><mrow id="S2.Ex2.m2.1.1.1.1.1.1" xref="S2.Ex2.m2.1.1.1.1.1.1.cmml"><mi id="S2.Ex2.m2.1.1.1.1.1.1.3" xref="S2.Ex2.m2.1.1.1.1.1.1.3.cmml">G</mi><mo id="S2.Ex2.m2.1.1.1.1.1.1.2" xref="S2.Ex2.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex2.m2.1.1.1.1.1.1.4" xref="S2.Ex2.m2.1.1.1.1.1.1.4.cmml">E</mi><mo id="S2.Ex2.m2.1.1.1.1.1.1.2a" xref="S2.Ex2.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex2.m2.1.1.1.1.1.1.5" xref="S2.Ex2.m2.1.1.1.1.1.1.5.cmml">L</mi><mo id="S2.Ex2.m2.1.1.1.1.1.1.2b" xref="S2.Ex2.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex2.m2.1.1.1.1.1.1.6" xref="S2.Ex2.m2.1.1.1.1.1.1.6.cmml">U</mi><mo id="S2.Ex2.m2.1.1.1.1.1.1.2c" xref="S2.Ex2.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex2.m2.1.1.1.1.1.1.1.1" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.Ex2.m2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.3.cmml">W</mi></mrow><mo id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="S2.Ex2.m2.1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m2.1.1.1.1.2.3" rspace="0.222em" xref="S2.Ex2.m2.1.1.1.1.2.3.cmml">⊗</mo><mrow id="S2.Ex2.m2.1.1.1.1.2.2.1" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.cmml"><mo id="S2.Ex2.m2.1.1.1.1.2.2.1.2" stretchy="false" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S2.Ex2.m2.1.1.1.1.2.2.1.1" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.cmml"><mrow id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.cmml"><mi id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.2" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.2.cmml">x</mi><mo id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.1" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.3" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.3.cmml">V</mi></mrow><mo id="S2.Ex2.m2.1.1.1.1.2.2.1.1.1" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.1.cmml">+</mo><mi id="S2.Ex2.m2.1.1.1.1.2.2.1.1.3" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.3.cmml">c</mi></mrow><mo id="S2.Ex2.m2.1.1.1.1.2.2.1.3" stretchy="false" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex2.m2.1.1.1.2" xref="S2.Ex2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m2.1b"><apply id="S2.Ex2.m2.1.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1"><eq id="S2.Ex2.m2.1.1.1.1.3.cmml" xref="S2.Ex2.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S2.Ex2.m2.1.1.1.1.4.cmml" xref="S2.Ex2.m2.1.1.1.1.4">absent</csymbol><apply id="S2.Ex2.m2.1.1.1.1.2.cmml" xref="S2.Ex2.m2.1.1.1.1.2"><csymbol cd="latexml" id="S2.Ex2.m2.1.1.1.1.2.3.cmml" xref="S2.Ex2.m2.1.1.1.1.2.3">tensor-product</csymbol><apply id="S2.Ex2.m2.1.1.1.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1"><times id="S2.Ex2.m2.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.2"></times><ci id="S2.Ex2.m2.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.3">𝐺</ci><ci id="S2.Ex2.m2.1.1.1.1.1.1.4.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.4">𝐸</ci><ci id="S2.Ex2.m2.1.1.1.1.1.1.5.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.5">𝐿</ci><ci id="S2.Ex2.m2.1.1.1.1.1.1.6.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.6">𝑈</ci><apply id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1"><plus id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2"><times id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.2.3">𝑊</ci></apply><ci id="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m2.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply><apply id="S2.Ex2.m2.1.1.1.1.2.2.1.1.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1"><plus id="S2.Ex2.m2.1.1.1.1.2.2.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.1"></plus><apply id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2"><times id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.1.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.1"></times><ci id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.2.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.2">𝑥</ci><ci id="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.3.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.2.3">𝑉</ci></apply><ci id="S2.Ex2.m2.1.1.1.1.2.2.1.1.3.cmml" xref="S2.Ex2.m2.1.1.1.1.2.2.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m2.1c">\displaystyle=GELU(xW+b)\otimes(xV+c),</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m2.1d">= italic_G italic_E italic_L italic_U ( italic_x italic_W + italic_b ) ⊗ ( italic_x italic_V + italic_c ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle SwiGLU(x,W,V,b,c,\beta)" class="ltx_Math" display="inline" id="S2.Ex3.m1.6"><semantics id="S2.Ex3.m1.6a"><mrow id="S2.Ex3.m1.6.7" xref="S2.Ex3.m1.6.7.cmml"><mi id="S2.Ex3.m1.6.7.2" xref="S2.Ex3.m1.6.7.2.cmml">S</mi><mo id="S2.Ex3.m1.6.7.1" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mi id="S2.Ex3.m1.6.7.3" xref="S2.Ex3.m1.6.7.3.cmml">w</mi><mo id="S2.Ex3.m1.6.7.1a" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mi id="S2.Ex3.m1.6.7.4" xref="S2.Ex3.m1.6.7.4.cmml">i</mi><mo id="S2.Ex3.m1.6.7.1b" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mi id="S2.Ex3.m1.6.7.5" xref="S2.Ex3.m1.6.7.5.cmml">G</mi><mo id="S2.Ex3.m1.6.7.1c" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mi id="S2.Ex3.m1.6.7.6" xref="S2.Ex3.m1.6.7.6.cmml">L</mi><mo id="S2.Ex3.m1.6.7.1d" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mi id="S2.Ex3.m1.6.7.7" xref="S2.Ex3.m1.6.7.7.cmml">U</mi><mo id="S2.Ex3.m1.6.7.1e" xref="S2.Ex3.m1.6.7.1.cmml">⁢</mo><mrow id="S2.Ex3.m1.6.7.8.2" xref="S2.Ex3.m1.6.7.8.1.cmml"><mo id="S2.Ex3.m1.6.7.8.2.1" stretchy="false" xref="S2.Ex3.m1.6.7.8.1.cmml">(</mo><mi id="S2.Ex3.m1.1.1" xref="S2.Ex3.m1.1.1.cmml">x</mi><mo id="S2.Ex3.m1.6.7.8.2.2" xref="S2.Ex3.m1.6.7.8.1.cmml">,</mo><mi id="S2.Ex3.m1.2.2" xref="S2.Ex3.m1.2.2.cmml">W</mi><mo id="S2.Ex3.m1.6.7.8.2.3" xref="S2.Ex3.m1.6.7.8.1.cmml">,</mo><mi id="S2.Ex3.m1.3.3" xref="S2.Ex3.m1.3.3.cmml">V</mi><mo id="S2.Ex3.m1.6.7.8.2.4" xref="S2.Ex3.m1.6.7.8.1.cmml">,</mo><mi id="S2.Ex3.m1.4.4" xref="S2.Ex3.m1.4.4.cmml">b</mi><mo id="S2.Ex3.m1.6.7.8.2.5" xref="S2.Ex3.m1.6.7.8.1.cmml">,</mo><mi id="S2.Ex3.m1.5.5" xref="S2.Ex3.m1.5.5.cmml">c</mi><mo id="S2.Ex3.m1.6.7.8.2.6" xref="S2.Ex3.m1.6.7.8.1.cmml">,</mo><mi id="S2.Ex3.m1.6.6" xref="S2.Ex3.m1.6.6.cmml">β</mi><mo id="S2.Ex3.m1.6.7.8.2.7" stretchy="false" xref="S2.Ex3.m1.6.7.8.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.6b"><apply id="S2.Ex3.m1.6.7.cmml" xref="S2.Ex3.m1.6.7"><times id="S2.Ex3.m1.6.7.1.cmml" xref="S2.Ex3.m1.6.7.1"></times><ci id="S2.Ex3.m1.6.7.2.cmml" xref="S2.Ex3.m1.6.7.2">𝑆</ci><ci id="S2.Ex3.m1.6.7.3.cmml" xref="S2.Ex3.m1.6.7.3">𝑤</ci><ci id="S2.Ex3.m1.6.7.4.cmml" xref="S2.Ex3.m1.6.7.4">𝑖</ci><ci id="S2.Ex3.m1.6.7.5.cmml" xref="S2.Ex3.m1.6.7.5">𝐺</ci><ci id="S2.Ex3.m1.6.7.6.cmml" xref="S2.Ex3.m1.6.7.6">𝐿</ci><ci id="S2.Ex3.m1.6.7.7.cmml" xref="S2.Ex3.m1.6.7.7">𝑈</ci><vector id="S2.Ex3.m1.6.7.8.1.cmml" xref="S2.Ex3.m1.6.7.8.2"><ci id="S2.Ex3.m1.1.1.cmml" xref="S2.Ex3.m1.1.1">𝑥</ci><ci id="S2.Ex3.m1.2.2.cmml" xref="S2.Ex3.m1.2.2">𝑊</ci><ci id="S2.Ex3.m1.3.3.cmml" xref="S2.Ex3.m1.3.3">𝑉</ci><ci id="S2.Ex3.m1.4.4.cmml" xref="S2.Ex3.m1.4.4">𝑏</ci><ci id="S2.Ex3.m1.5.5.cmml" xref="S2.Ex3.m1.5.5">𝑐</ci><ci id="S2.Ex3.m1.6.6.cmml" xref="S2.Ex3.m1.6.6">𝛽</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.6c">\displaystyle SwiGLU(x,W,V,b,c,\beta)</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m1.6d">italic_S italic_w italic_i italic_G italic_L italic_U ( italic_x , italic_W , italic_V , italic_b , italic_c , italic_β )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=Swish\beta(xW+b)\otimes(xV+c)." class="ltx_Math" display="inline" id="S2.Ex3.m2.1"><semantics id="S2.Ex3.m2.1a"><mrow id="S2.Ex3.m2.1.1.1" xref="S2.Ex3.m2.1.1.1.1.cmml"><mrow id="S2.Ex3.m2.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.cmml"><mi id="S2.Ex3.m2.1.1.1.1.4" xref="S2.Ex3.m2.1.1.1.1.4.cmml"></mi><mo id="S2.Ex3.m2.1.1.1.1.3" xref="S2.Ex3.m2.1.1.1.1.3.cmml">=</mo><mrow id="S2.Ex3.m2.1.1.1.1.2" xref="S2.Ex3.m2.1.1.1.1.2.cmml"><mrow id="S2.Ex3.m2.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.cmml"><mi id="S2.Ex3.m2.1.1.1.1.1.1.3" xref="S2.Ex3.m2.1.1.1.1.1.1.3.cmml">S</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.4" xref="S2.Ex3.m2.1.1.1.1.1.1.4.cmml">w</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2a" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.5" xref="S2.Ex3.m2.1.1.1.1.1.1.5.cmml">i</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2b" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.6" xref="S2.Ex3.m2.1.1.1.1.1.1.6.cmml">s</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2c" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.7" xref="S2.Ex3.m2.1.1.1.1.1.1.7.cmml">h</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2d" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.8" xref="S2.Ex3.m2.1.1.1.1.1.1.8.cmml">β</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.2e" xref="S2.Ex3.m2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex3.m2.1.1.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.Ex3.m2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.3.cmml">W</mi></mrow><mo id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="S2.Ex3.m2.1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex3.m2.1.1.1.1.2.3" rspace="0.222em" xref="S2.Ex3.m2.1.1.1.1.2.3.cmml">⊗</mo><mrow id="S2.Ex3.m2.1.1.1.1.2.2.1" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.cmml"><mo id="S2.Ex3.m2.1.1.1.1.2.2.1.2" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S2.Ex3.m2.1.1.1.1.2.2.1.1" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.cmml"><mrow id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.cmml"><mi id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.2" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.2.cmml">x</mi><mo id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.1" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.3" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.3.cmml">V</mi></mrow><mo id="S2.Ex3.m2.1.1.1.1.2.2.1.1.1" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.1.cmml">+</mo><mi id="S2.Ex3.m2.1.1.1.1.2.2.1.1.3" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.3.cmml">c</mi></mrow><mo id="S2.Ex3.m2.1.1.1.1.2.2.1.3" stretchy="false" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex3.m2.1.1.1.2" lspace="0em" xref="S2.Ex3.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m2.1b"><apply id="S2.Ex3.m2.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1"><eq id="S2.Ex3.m2.1.1.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S2.Ex3.m2.1.1.1.1.4.cmml" xref="S2.Ex3.m2.1.1.1.1.4">absent</csymbol><apply id="S2.Ex3.m2.1.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.2"><csymbol cd="latexml" id="S2.Ex3.m2.1.1.1.1.2.3.cmml" xref="S2.Ex3.m2.1.1.1.1.2.3">tensor-product</csymbol><apply id="S2.Ex3.m2.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1"><times id="S2.Ex3.m2.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.2"></times><ci id="S2.Ex3.m2.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.3">𝑆</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.4.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.4">𝑤</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.5.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.5">𝑖</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.6.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.6">𝑠</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.7.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.7">ℎ</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.8.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.8">𝛽</ci><apply id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1"><plus id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2"><times id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.2.3">𝑊</ci></apply><ci id="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply><apply id="S2.Ex3.m2.1.1.1.1.2.2.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1"><plus id="S2.Ex3.m2.1.1.1.1.2.2.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.1"></plus><apply id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2"><times id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.1.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.1"></times><ci id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.2.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.2">𝑥</ci><ci id="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.3.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.2.3">𝑉</ci></apply><ci id="S2.Ex3.m2.1.1.1.1.2.2.1.1.3.cmml" xref="S2.Ex3.m2.1.1.1.1.2.2.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m2.1c">\displaystyle=Swish\beta(xW+b)\otimes(xV+c).</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m2.1d">= italic_S italic_w italic_i italic_s italic_h italic_β ( italic_x italic_W + italic_b ) ⊗ ( italic_x italic_V + italic_c ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS6.5.1.1">II-F</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS6.6.2">Layer Normalization</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">Layer normalization leads to faster convergence and is a widely used component in transformers. In this section, we provide different normalization techniques widely used in LLM literature.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS6.SSS1.5.1.1">II-F</span>1 </span>LayerNorm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS6.SSS1.p1">
<p class="ltx_p" id="S2.SS6.SSS1.p1.1">Layer norm computes statistics over all the hidden units in a layer <math alttext="(l)" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.1.m1.1"><semantics id="S2.SS6.SSS1.p1.1.m1.1a"><mrow id="S2.SS6.SSS1.p1.1.m1.1.2.2"><mo id="S2.SS6.SSS1.p1.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S2.SS6.SSS1.p1.1.m1.1.1" xref="S2.SS6.SSS1.p1.1.m1.1.1.cmml">l</mi><mo id="S2.SS6.SSS1.p1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.1.m1.1b"><ci id="S2.SS6.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS6.SSS1.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.1.m1.1c">(l)</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.1.m1.1d">( italic_l )</annotation></semantics></math> as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="u^{l}=\frac{1}{n}\sum_{i}^{n}a_{i}^{l}\hskip 20.00003pt\sigma^{l}=\sqrt{\frac{%
1}{n}\sum_{i}^{n}(a_{i}^{l}-u^{l})^{2}}," class="ltx_Math" display="block" id="S2.E3.m1.2"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2.1"><mrow id="S2.E3.m1.2.2.1.1.2" xref="S2.E3.m1.2.2.1.1.3.cmml"><mrow id="S2.E3.m1.2.2.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.cmml"><msup id="S2.E3.m1.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.2.2" xref="S2.E3.m1.2.2.1.1.1.1.2.2.cmml">u</mi><mi id="S2.E3.m1.2.2.1.1.1.1.2.3" xref="S2.E3.m1.2.2.1.1.1.1.2.3.cmml">l</mi></msup><mo id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.3.cmml"><mfrac id="S2.E3.m1.2.2.1.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.2.cmml"><mn id="S2.E3.m1.2.2.1.1.1.1.3.2.2" xref="S2.E3.m1.2.2.1.1.1.1.3.2.2.cmml">1</mn><mi id="S2.E3.m1.2.2.1.1.1.1.3.2.3" xref="S2.E3.m1.2.2.1.1.1.1.3.2.3.cmml">n</mi></mfrac><mo id="S2.E3.m1.2.2.1.1.1.1.3.1" xref="S2.E3.m1.2.2.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.cmml"><munderover id="S2.E3.m1.2.2.1.1.1.1.3.3.1" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml"><mo id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.2" movablelimits="false" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.2.cmml">∑</mo><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.3.cmml">i</mi><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.1.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.3.cmml">n</mi></munderover><msubsup id="S2.E3.m1.2.2.1.1.1.1.3.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.2.cmml">a</mi><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.3.cmml">i</mi><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.2.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.3.cmml">l</mi></msubsup></mrow></mrow></mrow><mspace id="S2.E3.m1.2.2.1.1.2.3" width="2em" xref="S2.E3.m1.2.2.1.1.3a.cmml"></mspace><mrow id="S2.E3.m1.2.2.1.1.2.2" xref="S2.E3.m1.2.2.1.1.2.2.cmml"><msup id="S2.E3.m1.2.2.1.1.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.cmml"><mi id="S2.E3.m1.2.2.1.1.2.2.2.2" xref="S2.E3.m1.2.2.1.1.2.2.2.2.cmml">σ</mi><mi id="S2.E3.m1.2.2.1.1.2.2.2.3" xref="S2.E3.m1.2.2.1.1.2.2.2.3.cmml">l</mi></msup><mo id="S2.E3.m1.2.2.1.1.2.2.1" xref="S2.E3.m1.2.2.1.1.2.2.1.cmml">=</mo><msqrt id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><mfrac id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mn id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">1</mn><mi id="S2.E3.m1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><munderover id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S2.E3.m1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S2.E3.m1.1.1.1.1.2.2.3" xref="S2.E3.m1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S2.E3.m1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S2.E3.m1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">a</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">l</mi></msubsup><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">u</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></msup></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></msqrt></mrow></mrow><mo id="S2.E3.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.3a.cmml" xref="S2.E3.m1.2.2.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E3.m1.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1"><eq id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1"></eq><apply id="S2.E3.m1.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2">superscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2.2">𝑢</ci><ci id="S2.E3.m1.2.2.1.1.1.1.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2.3">𝑙</ci></apply><apply id="S2.E3.m1.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3"><times id="S2.E3.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.1"></times><apply id="S2.E3.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.2"><divide id="S2.E3.m1.2.2.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.2"></divide><cn id="S2.E3.m1.2.2.1.1.1.1.3.2.2.cmml" type="integer" xref="S2.E3.m1.2.2.1.1.1.1.3.2.2">1</cn><ci id="S2.E3.m1.2.2.1.1.1.1.3.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.2.3">𝑛</ci></apply><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3"><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.3.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1">superscript</csymbol><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1">subscript</csymbol><sum id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.2"></sum><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.2.3">𝑖</ci></apply><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.3">𝑛</ci></apply><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.3.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2">superscript</csymbol><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.2">𝑎</ci><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.2.3">𝑖</ci></apply><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.3">𝑙</ci></apply></apply></apply></apply><apply id="S2.E3.m1.2.2.1.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2"><eq id="S2.E3.m1.2.2.1.1.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.1"></eq><apply id="S2.E3.m1.2.2.1.1.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2">superscript</csymbol><ci id="S2.E3.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.2">𝜎</ci><ci id="S2.E3.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.2.2.2.3">𝑙</ci></apply><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><root id="S2.E3.m1.1.1a.cmml" xref="S2.E3.m1.1.1"></root><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><divide id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3"></divide><cn id="S2.E3.m1.1.1.1.3.2.cmml" type="integer" xref="S2.E3.m1.1.1.1.3.2">1</cn><ci id="S2.E3.m1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.3.3">𝑛</ci></apply><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1"><apply id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.1.1.1.1.2">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.1.1.1.1.2.2.2"></sum><ci id="S2.E3.m1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E3.m1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><minus id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.2">𝑎</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2">𝑢</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><cn id="S2.E3.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">u^{l}=\frac{1}{n}\sum_{i}^{n}a_{i}^{l}\hskip 20.00003pt\sigma^{l}=\sqrt{\frac{%
1}{n}\sum_{i}^{n}(a_{i}^{l}-u^{l})^{2}},</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.2d">italic_u start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT italic_σ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS6.SSS1.p1.6">where <math alttext="n" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.2.m1.1"><semantics id="S2.SS6.SSS1.p1.2.m1.1a"><mi id="S2.SS6.SSS1.p1.2.m1.1.1" xref="S2.SS6.SSS1.p1.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.2.m1.1b"><ci id="S2.SS6.SSS1.p1.2.m1.1.1.cmml" xref="S2.SS6.SSS1.p1.2.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.2.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.2.m1.1d">italic_n</annotation></semantics></math> is the number of neurons in the layer <math alttext="l" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.3.m2.1"><semantics id="S2.SS6.SSS1.p1.3.m2.1a"><mi id="S2.SS6.SSS1.p1.3.m2.1.1" xref="S2.SS6.SSS1.p1.3.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.3.m2.1b"><ci id="S2.SS6.SSS1.p1.3.m2.1.1.cmml" xref="S2.SS6.SSS1.p1.3.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.3.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.3.m2.1d">italic_l</annotation></semantics></math> and <math alttext="a_{i}^{l}" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.4.m3.1"><semantics id="S2.SS6.SSS1.p1.4.m3.1a"><msubsup id="S2.SS6.SSS1.p1.4.m3.1.1" xref="S2.SS6.SSS1.p1.4.m3.1.1.cmml"><mi id="S2.SS6.SSS1.p1.4.m3.1.1.2.2" xref="S2.SS6.SSS1.p1.4.m3.1.1.2.2.cmml">a</mi><mi id="S2.SS6.SSS1.p1.4.m3.1.1.2.3" xref="S2.SS6.SSS1.p1.4.m3.1.1.2.3.cmml">i</mi><mi id="S2.SS6.SSS1.p1.4.m3.1.1.3" xref="S2.SS6.SSS1.p1.4.m3.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.4.m3.1b"><apply id="S2.SS6.SSS1.p1.4.m3.1.1.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS1.p1.4.m3.1.1.1.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1">superscript</csymbol><apply id="S2.SS6.SSS1.p1.4.m3.1.1.2.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS1.p1.4.m3.1.1.2.1.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1">subscript</csymbol><ci id="S2.SS6.SSS1.p1.4.m3.1.1.2.2.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1.2.2">𝑎</ci><ci id="S2.SS6.SSS1.p1.4.m3.1.1.2.3.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1.2.3">𝑖</ci></apply><ci id="S2.SS6.SSS1.p1.4.m3.1.1.3.cmml" xref="S2.SS6.SSS1.p1.4.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.4.m3.1c">a_{i}^{l}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.4.m3.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> is the summed input of the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.5.m4.1"><semantics id="S2.SS6.SSS1.p1.5.m4.1a"><mi id="S2.SS6.SSS1.p1.5.m4.1.1" xref="S2.SS6.SSS1.p1.5.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.5.m4.1b"><ci id="S2.SS6.SSS1.p1.5.m4.1.1.cmml" xref="S2.SS6.SSS1.p1.5.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.5.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.5.m4.1d">italic_i</annotation></semantics></math> neuron in layer <math alttext="l" class="ltx_Math" display="inline" id="S2.SS6.SSS1.p1.6.m5.1"><semantics id="S2.SS6.SSS1.p1.6.m5.1a"><mi id="S2.SS6.SSS1.p1.6.m5.1.1" xref="S2.SS6.SSS1.p1.6.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS1.p1.6.m5.1b"><ci id="S2.SS6.SSS1.p1.6.m5.1.1.cmml" xref="S2.SS6.SSS1.p1.6.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS1.p1.6.m5.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS1.p1.6.m5.1d">italic_l</annotation></semantics></math>. LayerNorm provides invariance to rescaling of the weights and re-centering of the distribution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS6.SSS2.5.1.1">II-F</span>2 </span>RMSNorm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS6.SSS2.p1">
<p class="ltx_p" id="S2.SS6.SSS2.p1.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib75" title="">75</a>]</cite> proposed that the invariance properties of LayerNorm are spurious, and we can achieve the same performance benefits as we get from LayerNorm by using a computationally efficient normalization technique that trades off re-centering invariance with speed. LayerNorm gives the normalized summed input to layer <math alttext="l" class="ltx_Math" display="inline" id="S2.SS6.SSS2.p1.1.m1.1"><semantics id="S2.SS6.SSS2.p1.1.m1.1a"><mi id="S2.SS6.SSS2.p1.1.m1.1.1" xref="S2.SS6.SSS2.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS2.p1.1.m1.1b"><ci id="S2.SS6.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS6.SSS2.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS2.p1.1.m1.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS2.p1.1.m1.1d">italic_l</annotation></semantics></math> as follows</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\overline{a_{i}^{l}}=\frac{a_{i}^{l}-u^{l}}{\sigma}g_{i}^{l}" class="ltx_Math" display="block" id="S2.E4.m1.1"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mover accent="true" id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml"><msubsup id="S2.E4.m1.1.1.2.2" xref="S2.E4.m1.1.1.2.2.cmml"><mi id="S2.E4.m1.1.1.2.2.2.2" xref="S2.E4.m1.1.1.2.2.2.2.cmml">a</mi><mi id="S2.E4.m1.1.1.2.2.2.3" xref="S2.E4.m1.1.1.2.2.2.3.cmml">i</mi><mi id="S2.E4.m1.1.1.2.2.3" xref="S2.E4.m1.1.1.2.2.3.cmml">l</mi></msubsup><mo id="S2.E4.m1.1.1.2.1" xref="S2.E4.m1.1.1.2.1.cmml">¯</mo></mover><mo id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml">=</mo><mrow id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml"><mfrac id="S2.E4.m1.1.1.3.2" xref="S2.E4.m1.1.1.3.2.cmml"><mrow id="S2.E4.m1.1.1.3.2.2" xref="S2.E4.m1.1.1.3.2.2.cmml"><msubsup id="S2.E4.m1.1.1.3.2.2.2" xref="S2.E4.m1.1.1.3.2.2.2.cmml"><mi id="S2.E4.m1.1.1.3.2.2.2.2.2" xref="S2.E4.m1.1.1.3.2.2.2.2.2.cmml">a</mi><mi id="S2.E4.m1.1.1.3.2.2.2.2.3" xref="S2.E4.m1.1.1.3.2.2.2.2.3.cmml">i</mi><mi id="S2.E4.m1.1.1.3.2.2.2.3" xref="S2.E4.m1.1.1.3.2.2.2.3.cmml">l</mi></msubsup><mo id="S2.E4.m1.1.1.3.2.2.1" xref="S2.E4.m1.1.1.3.2.2.1.cmml">−</mo><msup id="S2.E4.m1.1.1.3.2.2.3" xref="S2.E4.m1.1.1.3.2.2.3.cmml"><mi id="S2.E4.m1.1.1.3.2.2.3.2" xref="S2.E4.m1.1.1.3.2.2.3.2.cmml">u</mi><mi id="S2.E4.m1.1.1.3.2.2.3.3" xref="S2.E4.m1.1.1.3.2.2.3.3.cmml">l</mi></msup></mrow><mi id="S2.E4.m1.1.1.3.2.3" xref="S2.E4.m1.1.1.3.2.3.cmml">σ</mi></mfrac><mo id="S2.E4.m1.1.1.3.1" xref="S2.E4.m1.1.1.3.1.cmml">⁢</mo><msubsup id="S2.E4.m1.1.1.3.3" xref="S2.E4.m1.1.1.3.3.cmml"><mi id="S2.E4.m1.1.1.3.3.2.2" xref="S2.E4.m1.1.1.3.3.2.2.cmml">g</mi><mi id="S2.E4.m1.1.1.3.3.2.3" xref="S2.E4.m1.1.1.3.3.2.3.cmml">i</mi><mi id="S2.E4.m1.1.1.3.3.3" xref="S2.E4.m1.1.1.3.3.3.cmml">l</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><eq id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"></eq><apply id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2"><ci id="S2.E4.m1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.2.1">¯</ci><apply id="S2.E4.m1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.2.2.1.cmml" xref="S2.E4.m1.1.1.2.2">superscript</csymbol><apply id="S2.E4.m1.1.1.2.2.2.cmml" xref="S2.E4.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.2.2.2.1.cmml" xref="S2.E4.m1.1.1.2.2">subscript</csymbol><ci id="S2.E4.m1.1.1.2.2.2.2.cmml" xref="S2.E4.m1.1.1.2.2.2.2">𝑎</ci><ci id="S2.E4.m1.1.1.2.2.2.3.cmml" xref="S2.E4.m1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S2.E4.m1.1.1.2.2.3.cmml" xref="S2.E4.m1.1.1.2.2.3">𝑙</ci></apply></apply><apply id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3"><times id="S2.E4.m1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.3.1"></times><apply id="S2.E4.m1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.3.2"><divide id="S2.E4.m1.1.1.3.2.1.cmml" xref="S2.E4.m1.1.1.3.2"></divide><apply id="S2.E4.m1.1.1.3.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2"><minus id="S2.E4.m1.1.1.3.2.2.1.cmml" xref="S2.E4.m1.1.1.3.2.2.1"></minus><apply id="S2.E4.m1.1.1.3.2.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.2.2.1.cmml" xref="S2.E4.m1.1.1.3.2.2.2">superscript</csymbol><apply id="S2.E4.m1.1.1.3.2.2.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.2.2.2.1.cmml" xref="S2.E4.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E4.m1.1.1.3.2.2.2.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2.2.2.2">𝑎</ci><ci id="S2.E4.m1.1.1.3.2.2.2.2.3.cmml" xref="S2.E4.m1.1.1.3.2.2.2.2.3">𝑖</ci></apply><ci id="S2.E4.m1.1.1.3.2.2.2.3.cmml" xref="S2.E4.m1.1.1.3.2.2.2.3">𝑙</ci></apply><apply id="S2.E4.m1.1.1.3.2.2.3.cmml" xref="S2.E4.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.2.2.3.1.cmml" xref="S2.E4.m1.1.1.3.2.2.3">superscript</csymbol><ci id="S2.E4.m1.1.1.3.2.2.3.2.cmml" xref="S2.E4.m1.1.1.3.2.2.3.2">𝑢</ci><ci id="S2.E4.m1.1.1.3.2.2.3.3.cmml" xref="S2.E4.m1.1.1.3.2.2.3.3">𝑙</ci></apply></apply><ci id="S2.E4.m1.1.1.3.2.3.cmml" xref="S2.E4.m1.1.1.3.2.3">𝜎</ci></apply><apply id="S2.E4.m1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.3.3">superscript</csymbol><apply id="S2.E4.m1.1.1.3.3.2.cmml" xref="S2.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.3.3.2.1.cmml" xref="S2.E4.m1.1.1.3.3">subscript</csymbol><ci id="S2.E4.m1.1.1.3.3.2.2.cmml" xref="S2.E4.m1.1.1.3.3.2.2">𝑔</ci><ci id="S2.E4.m1.1.1.3.3.2.3.cmml" xref="S2.E4.m1.1.1.3.3.2.3">𝑖</ci></apply><ci id="S2.E4.m1.1.1.3.3.3.cmml" xref="S2.E4.m1.1.1.3.3.3">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\overline{a_{i}^{l}}=\frac{a_{i}^{l}-u^{l}}{\sigma}g_{i}^{l}</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.1d">over¯ start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG = divide start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG start_ARG italic_σ end_ARG italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS6.SSS2.p1.3">where <math alttext="g_{i}^{l}" class="ltx_Math" display="inline" id="S2.SS6.SSS2.p1.2.m1.1"><semantics id="S2.SS6.SSS2.p1.2.m1.1a"><msubsup id="S2.SS6.SSS2.p1.2.m1.1.1" xref="S2.SS6.SSS2.p1.2.m1.1.1.cmml"><mi id="S2.SS6.SSS2.p1.2.m1.1.1.2.2" xref="S2.SS6.SSS2.p1.2.m1.1.1.2.2.cmml">g</mi><mi id="S2.SS6.SSS2.p1.2.m1.1.1.2.3" xref="S2.SS6.SSS2.p1.2.m1.1.1.2.3.cmml">i</mi><mi id="S2.SS6.SSS2.p1.2.m1.1.1.3" xref="S2.SS6.SSS2.p1.2.m1.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS2.p1.2.m1.1b"><apply id="S2.SS6.SSS2.p1.2.m1.1.1.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS2.p1.2.m1.1.1.1.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1">superscript</csymbol><apply id="S2.SS6.SSS2.p1.2.m1.1.1.2.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS2.p1.2.m1.1.1.2.1.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1">subscript</csymbol><ci id="S2.SS6.SSS2.p1.2.m1.1.1.2.2.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1.2.2">𝑔</ci><ci id="S2.SS6.SSS2.p1.2.m1.1.1.2.3.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1.2.3">𝑖</ci></apply><ci id="S2.SS6.SSS2.p1.2.m1.1.1.3.cmml" xref="S2.SS6.SSS2.p1.2.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS2.p1.2.m1.1c">g_{i}^{l}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS2.p1.2.m1.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> is the gain parameter. RMSNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib75" title="">75</a>]</cite> modifies <math alttext="\overline{a_{i}^{l}}" class="ltx_Math" display="inline" id="S2.SS6.SSS2.p1.3.m2.1"><semantics id="S2.SS6.SSS2.p1.3.m2.1a"><mover accent="true" id="S2.SS6.SSS2.p1.3.m2.1.1" xref="S2.SS6.SSS2.p1.3.m2.1.1.cmml"><msubsup id="S2.SS6.SSS2.p1.3.m2.1.1.2" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.cmml"><mi id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.2" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.2.2.cmml">a</mi><mi id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.3" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.2.3.cmml">i</mi><mi id="S2.SS6.SSS2.p1.3.m2.1.1.2.3" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.3.cmml">l</mi></msubsup><mo id="S2.SS6.SSS2.p1.3.m2.1.1.1" xref="S2.SS6.SSS2.p1.3.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS2.p1.3.m2.1b"><apply id="S2.SS6.SSS2.p1.3.m2.1.1.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1"><ci id="S2.SS6.SSS2.p1.3.m2.1.1.1.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.1">¯</ci><apply id="S2.SS6.SSS2.p1.3.m2.1.1.2.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS6.SSS2.p1.3.m2.1.1.2.1.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2">superscript</csymbol><apply id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.1.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2">subscript</csymbol><ci id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.2.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.2.2">𝑎</ci><ci id="S2.SS6.SSS2.p1.3.m2.1.1.2.2.3.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.2.3">𝑖</ci></apply><ci id="S2.SS6.SSS2.p1.3.m2.1.1.2.3.cmml" xref="S2.SS6.SSS2.p1.3.m2.1.1.2.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS2.p1.3.m2.1c">\overline{a_{i}^{l}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS2.p1.3.m2.1d">over¯ start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math> as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS6.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\overline{a_{i}^{l}}=\frac{a_{i}^{l}}{\textnormal{RMS}(\mathbf{a}^{l})}g_{i}^{%
l},\hskip 3.00003pt\textnormal{where}\hskip 3.00003pt\textnormal{RMS}(\mathbf{%
a}^{l})=\sqrt{\frac{1}{n}\sum_{i}^{n}(a_{i}^{l})^{2}}." class="ltx_Math" display="block" id="S2.E5.m1.3"><semantics id="S2.E5.m1.3a"><mrow id="S2.E5.m1.3.3.1"><mrow id="S2.E5.m1.3.3.1.1.2" xref="S2.E5.m1.3.3.1.1.3.cmml"><mrow id="S2.E5.m1.3.3.1.1.1.1" xref="S2.E5.m1.3.3.1.1.1.1.cmml"><mover accent="true" id="S2.E5.m1.3.3.1.1.1.1.2" xref="S2.E5.m1.3.3.1.1.1.1.2.cmml"><msubsup id="S2.E5.m1.3.3.1.1.1.1.2.2" xref="S2.E5.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S2.E5.m1.3.3.1.1.1.1.2.2.2.2" xref="S2.E5.m1.3.3.1.1.1.1.2.2.2.2.cmml">a</mi><mi id="S2.E5.m1.3.3.1.1.1.1.2.2.2.3" xref="S2.E5.m1.3.3.1.1.1.1.2.2.2.3.cmml">i</mi><mi id="S2.E5.m1.3.3.1.1.1.1.2.2.3" xref="S2.E5.m1.3.3.1.1.1.1.2.2.3.cmml">l</mi></msubsup><mo id="S2.E5.m1.3.3.1.1.1.1.2.1" xref="S2.E5.m1.3.3.1.1.1.1.2.1.cmml">¯</mo></mover><mo id="S2.E5.m1.3.3.1.1.1.1.1" xref="S2.E5.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S2.E5.m1.3.3.1.1.1.1.3" xref="S2.E5.m1.3.3.1.1.1.1.3.cmml"><mfrac id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><msubsup id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.3.2.2" xref="S2.E5.m1.1.1.3.2.2.cmml">a</mi><mi id="S2.E5.m1.1.1.3.2.3" xref="S2.E5.m1.1.1.3.2.3.cmml">i</mi><mi id="S2.E5.m1.1.1.3.3" xref="S2.E5.m1.1.1.3.3.cmml">l</mi></msubsup><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mtext id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3a.cmml">RMS</mtext><mo id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mo id="S2.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.cmml">(</mo><msup id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml">𝐚</mi><mi id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.3.cmml">l</mi></msup><mo id="S2.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S2.E5.m1.3.3.1.1.1.1.3.1" xref="S2.E5.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><msubsup id="S2.E5.m1.3.3.1.1.1.1.3.2" xref="S2.E5.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S2.E5.m1.3.3.1.1.1.1.3.2.2.2" xref="S2.E5.m1.3.3.1.1.1.1.3.2.2.2.cmml">g</mi><mi id="S2.E5.m1.3.3.1.1.1.1.3.2.2.3" xref="S2.E5.m1.3.3.1.1.1.1.3.2.2.3.cmml">i</mi><mi id="S2.E5.m1.3.3.1.1.1.1.3.2.3" xref="S2.E5.m1.3.3.1.1.1.1.3.2.3.cmml">l</mi></msubsup></mrow></mrow><mo id="S2.E5.m1.3.3.1.1.2.3" rspace="0.467em" xref="S2.E5.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S2.E5.m1.3.3.1.1.2.2" xref="S2.E5.m1.3.3.1.1.2.2.cmml"><mrow id="S2.E5.m1.3.3.1.1.2.2.1" xref="S2.E5.m1.3.3.1.1.2.2.1.cmml"><mtext id="S2.E5.m1.3.3.1.1.2.2.1.3" xref="S2.E5.m1.3.3.1.1.2.2.1.3a.cmml">where</mtext><mo id="S2.E5.m1.3.3.1.1.2.2.1.2" lspace="0.300em" xref="S2.E5.m1.3.3.1.1.2.2.1.2.cmml">⁢</mo><mtext id="S2.E5.m1.3.3.1.1.2.2.1.4" xref="S2.E5.m1.3.3.1.1.2.2.1.4a.cmml">RMS</mtext><mo id="S2.E5.m1.3.3.1.1.2.2.1.2a" xref="S2.E5.m1.3.3.1.1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.3.3.1.1.2.2.1.1.1" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml"><mo id="S2.E5.m1.3.3.1.1.2.2.1.1.1.2" stretchy="false" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml">(</mo><msup id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml"><mi id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.2" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.2.cmml">𝐚</mi><mi id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.3" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.3.cmml">l</mi></msup><mo id="S2.E5.m1.3.3.1.1.2.2.1.1.1.3" stretchy="false" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.3.3.1.1.2.2.2" xref="S2.E5.m1.3.3.1.1.2.2.2.cmml">=</mo><msqrt id="S2.E5.m1.2.2" xref="S2.E5.m1.2.2.cmml"><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.cmml"><mfrac id="S2.E5.m1.2.2.1.3" xref="S2.E5.m1.2.2.1.3.cmml"><mn id="S2.E5.m1.2.2.1.3.2" xref="S2.E5.m1.2.2.1.3.2.cmml">1</mn><mi id="S2.E5.m1.2.2.1.3.3" xref="S2.E5.m1.2.2.1.3.3.cmml">n</mi></mfrac><mo id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.2.2.1.1" xref="S2.E5.m1.2.2.1.1.cmml"><munderover id="S2.E5.m1.2.2.1.1.2" xref="S2.E5.m1.2.2.1.1.2.cmml"><mo id="S2.E5.m1.2.2.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S2.E5.m1.2.2.1.1.2.2.2.cmml">∑</mo><mi id="S2.E5.m1.2.2.1.1.2.2.3" xref="S2.E5.m1.2.2.1.1.2.2.3.cmml">i</mi><mi id="S2.E5.m1.2.2.1.1.2.3" xref="S2.E5.m1.2.2.1.1.2.3.cmml">n</mi></munderover><msup id="S2.E5.m1.2.2.1.1.1" xref="S2.E5.m1.2.2.1.1.1.cmml"><mrow id="S2.E5.m1.2.2.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S2.E5.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S2.E5.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E5.m1.2.2.1.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.2.2" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2.2.cmml">a</mi><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.2.3" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S2.E5.m1.2.2.1.1.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3.cmml">l</mi></msubsup><mo id="S2.E5.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S2.E5.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S2.E5.m1.2.2.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.3.cmml">2</mn></msup></mrow></mrow></msqrt></mrow></mrow><mo id="S2.E5.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.3b"><apply id="S2.E5.m1.3.3.1.1.3.cmml" xref="S2.E5.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.3a.cmml" xref="S2.E5.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E5.m1.3.3.1.1.1.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1"><eq id="S2.E5.m1.3.3.1.1.1.1.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.1"></eq><apply id="S2.E5.m1.3.3.1.1.1.1.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2"><ci id="S2.E5.m1.3.3.1.1.1.1.2.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.1">¯</ci><apply id="S2.E5.m1.3.3.1.1.1.1.2.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2">superscript</csymbol><apply id="S2.E5.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.1.1.2.2.2.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E5.m1.3.3.1.1.1.1.2.2.2.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2.2.2">𝑎</ci><ci id="S2.E5.m1.3.3.1.1.1.1.2.2.2.3.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S2.E5.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S2.E5.m1.3.3.1.1.1.1.2.2.3">𝑙</ci></apply></apply><apply id="S2.E5.m1.3.3.1.1.1.1.3.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3"><times id="S2.E5.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.1"></times><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><divide id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1"></divide><apply id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.3">superscript</csymbol><apply id="S2.E5.m1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.3.2.1.cmml" xref="S2.E5.m1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.3.2.2.cmml" xref="S2.E5.m1.1.1.3.2.2">𝑎</ci><ci id="S2.E5.m1.1.1.3.2.3.cmml" xref="S2.E5.m1.1.1.3.2.3">𝑖</ci></apply><ci id="S2.E5.m1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.3.3">𝑙</ci></apply><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><ci id="S2.E5.m1.1.1.1.3a.cmml" xref="S2.E5.m1.1.1.1.3"><mtext id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3">RMS</mtext></ci><apply id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1">superscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2">𝐚</ci><ci id="S2.E5.m1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.3">𝑙</ci></apply></apply></apply><apply id="S2.E5.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2">superscript</csymbol><apply id="S2.E5.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.1.1.3.2.2.1.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E5.m1.3.3.1.1.1.1.3.2.2.2.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2.2.2">𝑔</ci><ci id="S2.E5.m1.3.3.1.1.1.1.3.2.2.3.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2.2.3">𝑖</ci></apply><ci id="S2.E5.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S2.E5.m1.3.3.1.1.1.1.3.2.3">𝑙</ci></apply></apply></apply><apply id="S2.E5.m1.3.3.1.1.2.2.cmml" xref="S2.E5.m1.3.3.1.1.2.2"><eq id="S2.E5.m1.3.3.1.1.2.2.2.cmml" xref="S2.E5.m1.3.3.1.1.2.2.2"></eq><apply id="S2.E5.m1.3.3.1.1.2.2.1.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1"><times id="S2.E5.m1.3.3.1.1.2.2.1.2.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.2"></times><ci id="S2.E5.m1.3.3.1.1.2.2.1.3a.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.3"><mtext id="S2.E5.m1.3.3.1.1.2.2.1.3.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.3">where</mtext></ci><ci id="S2.E5.m1.3.3.1.1.2.2.1.4a.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.4"><mtext id="S2.E5.m1.3.3.1.1.2.2.1.4.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.4">RMS</mtext></ci><apply id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.1.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1">superscript</csymbol><ci id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.2">𝐚</ci><ci id="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.3.cmml" xref="S2.E5.m1.3.3.1.1.2.2.1.1.1.1.3">𝑙</ci></apply></apply><apply id="S2.E5.m1.2.2.cmml" xref="S2.E5.m1.2.2"><root id="S2.E5.m1.2.2a.cmml" xref="S2.E5.m1.2.2"></root><apply id="S2.E5.m1.2.2.1.cmml" xref="S2.E5.m1.2.2.1"><times id="S2.E5.m1.2.2.1.2.cmml" xref="S2.E5.m1.2.2.1.2"></times><apply id="S2.E5.m1.2.2.1.3.cmml" xref="S2.E5.m1.2.2.1.3"><divide id="S2.E5.m1.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.1.3"></divide><cn id="S2.E5.m1.2.2.1.3.2.cmml" type="integer" xref="S2.E5.m1.2.2.1.3.2">1</cn><ci id="S2.E5.m1.2.2.1.3.3.cmml" xref="S2.E5.m1.2.2.1.3.3">𝑛</ci></apply><apply id="S2.E5.m1.2.2.1.1.cmml" xref="S2.E5.m1.2.2.1.1"><apply id="S2.E5.m1.2.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2">superscript</csymbol><apply id="S2.E5.m1.2.2.1.1.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.2.2.1.cmml" xref="S2.E5.m1.2.2.1.1.2">subscript</csymbol><sum id="S2.E5.m1.2.2.1.1.2.2.2.cmml" xref="S2.E5.m1.2.2.1.1.2.2.2"></sum><ci id="S2.E5.m1.2.2.1.1.2.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.1.1.2.3.cmml" xref="S2.E5.m1.2.2.1.1.2.3">𝑛</ci></apply><apply id="S2.E5.m1.2.2.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1">superscript</csymbol><apply id="S2.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1">superscript</csymbol><apply id="S2.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2.2">𝑎</ci><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.1.3">𝑙</ci></apply><cn id="S2.E5.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.E5.m1.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.3c">\overline{a_{i}^{l}}=\frac{a_{i}^{l}}{\textnormal{RMS}(\mathbf{a}^{l})}g_{i}^{%
l},\hskip 3.00003pt\textnormal{where}\hskip 3.00003pt\textnormal{RMS}(\mathbf{%
a}^{l})=\sqrt{\frac{1}{n}\sum_{i}^{n}(a_{i}^{l})^{2}}.</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.3d">over¯ start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG = divide start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG start_ARG RMS ( bold_a start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) end_ARG italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , where RMS ( bold_a start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) = square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS6.SSS3.5.1.1">II-F</span>3 </span>Pre-Norm and Post-Norm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS6.SSS3.p1">
<p class="ltx_p" id="S2.SS6.SSS3.p1.1">LLMs use transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite> architecture with some variations. The original implementation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite> used layer normalization after the residual connection, commonly called post-LN, concerning the order of <span class="ltx_text ltx_font_italic" id="S2.SS6.SSS3.p1.1.1">Multihead attention – Residual – LN</span>. There is another order of the normalization, referred to as pre-LN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib76" title="">76</a>]</cite> due to the position of the normalization step before the self-attention layer as in <span class="ltx_text ltx_font_italic" id="S2.SS6.SSS3.p1.1.2">LN – Multihead attention – Residual</span>. Pre-LN is known to provide more stability in the training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib77" title="">77</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS6.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS6.SSS4.5.1.1">II-F</span>4 </span>DeepNorm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS6.SSS4.p1">
<p class="ltx_p" id="S2.SS6.SSS4.p1.5">While pre-LN has certain benefits over post-LN training, pre-LN training has an unwanted effect on the gradients&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib77" title="">77</a>]</cite>. The earlier layers have larger gradients than those at the bottom. DeepNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib78" title="">78</a>]</cite> mitigates these adverse effects on the gradients. It is given as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}^{l_{f}}=LN(\alpha\mathbf{x}^{l_{p}}+G^{l_{p}}(\mathbf{x}^{l_{p}},{%
\boldmath\theta}{{}^{l_{p}}})," class="ltx_math_unparsed" display="block" id="S2.E6.m1.1"><semantics id="S2.E6.m1.1a"><mrow id="S2.E6.m1.1b"><msup id="S2.E6.m1.1.2"><mi id="S2.E6.m1.1.2.2">𝐱</mi><msub id="S2.E6.m1.1.2.3"><mi id="S2.E6.m1.1.2.3.2">l</mi><mi id="S2.E6.m1.1.2.3.3">f</mi></msub></msup><mo id="S2.E6.m1.1.3">=</mo><mi id="S2.E6.m1.1.4">L</mi><mi id="S2.E6.m1.1.5">N</mi><mrow id="S2.E6.m1.1.6"><mo id="S2.E6.m1.1.6.1" stretchy="false">(</mo><mi id="S2.E6.m1.1.6.2">α</mi><msup id="S2.E6.m1.1.6.3"><mi id="S2.E6.m1.1.6.3.2">𝐱</mi><msub id="S2.E6.m1.1.6.3.3"><mi id="S2.E6.m1.1.6.3.3.2">l</mi><mi id="S2.E6.m1.1.6.3.3.3">p</mi></msub></msup><mo id="S2.E6.m1.1.6.4">+</mo><msup id="S2.E6.m1.1.6.5"><mi id="S2.E6.m1.1.6.5.2">G</mi><msub id="S2.E6.m1.1.6.5.3"><mi id="S2.E6.m1.1.6.5.3.2">l</mi><mi id="S2.E6.m1.1.6.5.3.3">p</mi></msub></msup><mrow id="S2.E6.m1.1.6.6"><mo id="S2.E6.m1.1.6.6.1" stretchy="false">(</mo><msup id="S2.E6.m1.1.6.6.2"><mi id="S2.E6.m1.1.6.6.2.2">𝐱</mi><msub id="S2.E6.m1.1.6.6.2.3"><mi id="S2.E6.m1.1.6.6.2.3.2">l</mi><mi id="S2.E6.m1.1.6.6.2.3.3">p</mi></msub></msup><mo id="S2.E6.m1.1.6.6.3">,</mo><mi id="S2.E6.m1.1.1">θ</mi><mmultiscripts id="S2.E6.m1.1.6.6.4"><mo id="S2.E6.m1.1.6.6.4.2" stretchy="false">)</mo><mprescripts id="S2.E6.m1.1.6.6.4a"></mprescripts><mrow id="S2.E6.m1.1.6.6.4b"></mrow><msub id="S2.E6.m1.1.6.6.4.3"><mi id="S2.E6.m1.1.6.6.4.3.2">l</mi><mi id="S2.E6.m1.1.6.6.4.3.3">p</mi></msub></mmultiscripts></mrow><mo id="S2.E6.m1.1.6.7">,</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.E6.m1.1c">\mathbf{x}^{l_{f}}=LN(\alpha\mathbf{x}^{l_{p}}+G^{l_{p}}(\mathbf{x}^{l_{p}},{%
\boldmath\theta}{{}^{l_{p}}}),</annotation><annotation encoding="application/x-llamapun" id="S2.E6.m1.1d">bold_x start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUPERSCRIPT = italic_L italic_N ( italic_α bold_x start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT + italic_G start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( bold_x start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_θ start_FLOATSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_FLOATSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS6.SSS4.p1.4">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS6.SSS4.p1.1.m1.1"><semantics id="S2.SS6.SSS4.p1.1.m1.1a"><mi id="S2.SS6.SSS4.p1.1.m1.1.1" xref="S2.SS6.SSS4.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS4.p1.1.m1.1b"><ci id="S2.SS6.SSS4.p1.1.m1.1.1.cmml" xref="S2.SS6.SSS4.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS4.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS4.p1.1.m1.1d">italic_α</annotation></semantics></math> is a constant and <math alttext="\theta^{l_{p}}" class="ltx_Math" display="inline" id="S2.SS6.SSS4.p1.2.m2.1"><semantics id="S2.SS6.SSS4.p1.2.m2.1a"><msup id="S2.SS6.SSS4.p1.2.m2.1.1" xref="S2.SS6.SSS4.p1.2.m2.1.1.cmml"><mi id="S2.SS6.SSS4.p1.2.m2.1.1.2" xref="S2.SS6.SSS4.p1.2.m2.1.1.2.cmml">θ</mi><msub id="S2.SS6.SSS4.p1.2.m2.1.1.3" xref="S2.SS6.SSS4.p1.2.m2.1.1.3.cmml"><mi id="S2.SS6.SSS4.p1.2.m2.1.1.3.2" xref="S2.SS6.SSS4.p1.2.m2.1.1.3.2.cmml">l</mi><mi id="S2.SS6.SSS4.p1.2.m2.1.1.3.3" xref="S2.SS6.SSS4.p1.2.m2.1.1.3.3.cmml">p</mi></msub></msup><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS4.p1.2.m2.1b"><apply id="S2.SS6.SSS4.p1.2.m2.1.1.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS4.p1.2.m2.1.1.1.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS6.SSS4.p1.2.m2.1.1.2.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1.2">𝜃</ci><apply id="S2.SS6.SSS4.p1.2.m2.1.1.3.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS6.SSS4.p1.2.m2.1.1.3.1.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1.3">subscript</csymbol><ci id="S2.SS6.SSS4.p1.2.m2.1.1.3.2.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1.3.2">𝑙</ci><ci id="S2.SS6.SSS4.p1.2.m2.1.1.3.3.cmml" xref="S2.SS6.SSS4.p1.2.m2.1.1.3.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS4.p1.2.m2.1c">\theta^{l_{p}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS4.p1.2.m2.1d">italic_θ start_POSTSUPERSCRIPT italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> represents the parameters of layer <math alttext="l_{p}" class="ltx_Math" display="inline" id="S2.SS6.SSS4.p1.3.m3.1"><semantics id="S2.SS6.SSS4.p1.3.m3.1a"><msub id="S2.SS6.SSS4.p1.3.m3.1.1" xref="S2.SS6.SSS4.p1.3.m3.1.1.cmml"><mi id="S2.SS6.SSS4.p1.3.m3.1.1.2" xref="S2.SS6.SSS4.p1.3.m3.1.1.2.cmml">l</mi><mi id="S2.SS6.SSS4.p1.3.m3.1.1.3" xref="S2.SS6.SSS4.p1.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS4.p1.3.m3.1b"><apply id="S2.SS6.SSS4.p1.3.m3.1.1.cmml" xref="S2.SS6.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS6.SSS4.p1.3.m3.1.1.1.cmml" xref="S2.SS6.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS6.SSS4.p1.3.m3.1.1.2.cmml" xref="S2.SS6.SSS4.p1.3.m3.1.1.2">𝑙</ci><ci id="S2.SS6.SSS4.p1.3.m3.1.1.3.cmml" xref="S2.SS6.SSS4.p1.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS4.p1.3.m3.1c">l_{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS4.p1.3.m3.1d">italic_l start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. These parameters are scaled by another constant <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS6.SSS4.p1.4.m4.1"><semantics id="S2.SS6.SSS4.p1.4.m4.1a"><mi id="S2.SS6.SSS4.p1.4.m4.1.1" xref="S2.SS6.SSS4.p1.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS6.SSS4.p1.4.m4.1b"><ci id="S2.SS6.SSS4.p1.4.m4.1.1.cmml" xref="S2.SS6.SSS4.p1.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS6.SSS4.p1.4.m4.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS6.SSS4.p1.4.m4.1d">italic_β</annotation></semantics></math>. Both of these constants depend only on the architecture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS7.5.1.1">II-G</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS7.6.2">Distributed LLM Training</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.p1">
<p class="ltx_p" id="S2.SS7.p1.1">This section describes distributed LLM training approaches briefly. More details are available in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib80" title="">80</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS1.5.1.1">II-G</span>1 </span>Data Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS1.p1">
<p class="ltx_p" id="S2.SS7.SSS1.p1.1">Data parallelism replicates the model on multiple devices where data in a batch gets divided across devices. At the end of each training iteration weights are synchronized across all devices.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS2.5.1.1">II-G</span>2 </span>Tensor Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS2.p1">
<p class="ltx_p" id="S2.SS7.SSS2.p1.1">Tensor parallelism shards a tensor computation across devices. It is also known as horizontal parallelism or intra-layer model parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS7.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS3.5.1.1">II-G</span>3 </span>Pipeline Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS3.p1">
<p class="ltx_p" id="S2.SS7.SSS3.p1.1">Pipeline parallelism shards model layers across different devices. This is also known as vertical parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS7.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS4.5.1.1">II-G</span>4 </span>Model Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS4.p1">
<p class="ltx_p" id="S2.SS7.SSS4.p1.1">A combination of tensor and pipeline parallelism is known as model parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS7.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS5.5.1.1">II-G</span>5 </span>3D Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS5.p1">
<p class="ltx_p" id="S2.SS7.SSS5.p1.1">A combination of data, tensor, and model parallelism is known as 3D parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS7.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS7.SSS6.5.1.1">II-G</span>6 </span>Optimizer Parallelism</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS7.SSS6.p1">
<p class="ltx_p" id="S2.SS7.SSS6.p1.1">Optimizer parallelism also known as zero redundancy optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib37" title="">37</a>]</cite> implements optimizer state partitioning, gradient partitioning, and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS8.5.1.1">II-H</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS8.6.2">Libraries</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS8.p1">
<p class="ltx_p" id="S2.SS8.p1.1">Some commonly used libraries for LLMs training are: 1) Transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib81" title="">81</a>]</cite>, 2) DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib36" title="">36</a>]</cite>, 3) Megatron-LM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib79" title="">79</a>]</cite>, 4) JAX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib82" title="">82</a>]</cite>, 5) Colossal-AI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib83" title="">83</a>]</cite>, 6) BMTrain&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib80" title="">80</a>]</cite>, 7) FastMoE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib84" title="">84</a>]</cite>, and frameworks are 1) MindSpore&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib85" title="">85</a>]</cite>, 2) PyTorch&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib86" title="">86</a>]</cite>, 3) Tensorflow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib87" title="">87</a>]</cite>, 4) MXNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib88" title="">88</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS9.5.1.1">II-I</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS9.6.2">Data PreProcessing</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS9.p1">
<p class="ltx_p" id="S2.SS9.p1.1">This section briefly summarizes data preprocessing techniques used in LLMs training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS9.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS9.SSS1.5.1.1">II-I</span>1 </span>Quality Filtering</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS9.SSS1.p1">
<p class="ltx_p" id="S2.SS9.SSS1.p1.1">For better results, training data quality is essential. Some approaches to filtering data are: 1) classifier-based and 2) heuristics-based. Classifier-based approaches train a classifier on high-quality data and predict the quality of text for filtering, whereas heuristics-based employ some rules for filtering like language, metrics, statistics, and keywords.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS9.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS9.SSS2.5.1.1">II-I</span>2 </span>Data Deduplication</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS9.SSS2.p1">
<p class="ltx_p" id="S2.SS9.SSS2.p1.1">Duplicated data can affect model performance and increase data memorization; therefore, to train LLMs, data deduplication is one of the preprocessing steps. This can be performed at multiple levels, like sentences, documents, and datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS9.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS9.SSS3.5.1.1">II-I</span>3 </span>Privacy Reduction</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS9.SSS3.p1">
<p class="ltx_p" id="S2.SS9.SSS3.p1.1">Most of the training data for LLMs is collected through web sources. This data contains private information; therefore, many LLMs employ heuristics-based methods to filter information such as names, addresses, and phone numbers to avoid learning personal information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS10.5.1.1">II-J</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS10.6.2">Architectures</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS10.p1">
<p class="ltx_p" id="S2.SS10.p1.1">Here we discuss the variants of the transformer architectures at a higher level which arise due to the difference in the application of the attention and the connection of transformer blocks. An illustration of attention patterns of these architectures is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F4" title="Figure 4 ‣ II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS10.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS10.SSS1.5.1.1">II-J</span>1 </span>Encoder Decoder</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS10.SSS1.p1">
<p class="ltx_p" id="S2.SS10.SSS1.p1.1">Transformers were originally designed as sequence transduction models and followed other prevalent model architectures for machine translation systems. They selected encoder-decoder architecture to train human language translation tasks. This architecture is adopted by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>]</cite>. In this architectural scheme, an encoder encodes the input sequences to variable length context vectors, which are then passed to the decoder to maximize a joint objective of minimizing the gap between predicted token labels and the actual target token labels.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS10.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS10.SSS2.5.1.1">II-J</span>2 </span>Causal Decoder</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS10.SSS2.p1">
<p class="ltx_p" id="S2.SS10.SSS2.p1.2">The underlying objective of an LLM is to predict the next token based on the input sequence. While additional information from the encoder binds the prediction strongly to the context, it is found in practice that the LLMs can perform well in the absence of encoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib90" title="">90</a>]</cite>, relying only on the decoder. Similar to the original encoder-decoder architecture’s decoder block, this decoder restricts the flow of information backward, i.e., the predicted token <math alttext="t_{k}" class="ltx_Math" display="inline" id="S2.SS10.SSS2.p1.1.m1.1"><semantics id="S2.SS10.SSS2.p1.1.m1.1a"><msub id="S2.SS10.SSS2.p1.1.m1.1.1" xref="S2.SS10.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS10.SSS2.p1.1.m1.1.1.2" xref="S2.SS10.SSS2.p1.1.m1.1.1.2.cmml">t</mi><mi id="S2.SS10.SSS2.p1.1.m1.1.1.3" xref="S2.SS10.SSS2.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS2.p1.1.m1.1b"><apply id="S2.SS10.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS10.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS10.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS10.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS10.SSS2.p1.1.m1.1.1.2">𝑡</ci><ci id="S2.SS10.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS10.SSS2.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS2.p1.1.m1.1c">t_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS2.p1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> only depends on the tokens preceded by and up to <math alttext="t_{k-1}" class="ltx_Math" display="inline" id="S2.SS10.SSS2.p1.2.m2.1"><semantics id="S2.SS10.SSS2.p1.2.m2.1a"><msub id="S2.SS10.SSS2.p1.2.m2.1.1" xref="S2.SS10.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS10.SSS2.p1.2.m2.1.1.2" xref="S2.SS10.SSS2.p1.2.m2.1.1.2.cmml">t</mi><mrow id="S2.SS10.SSS2.p1.2.m2.1.1.3" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS10.SSS2.p1.2.m2.1.1.3.2" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.2.cmml">k</mi><mo id="S2.SS10.SSS2.p1.2.m2.1.1.3.1" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.1.cmml">−</mo><mn id="S2.SS10.SSS2.p1.2.m2.1.1.3.3" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS2.p1.2.m2.1b"><apply id="S2.SS10.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS10.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1.2">𝑡</ci><apply id="S2.SS10.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1.3"><minus id="S2.SS10.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.1"></minus><ci id="S2.SS10.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.2">𝑘</ci><cn id="S2.SS10.SSS2.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S2.SS10.SSS2.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS2.p1.2.m2.1c">t_{k-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS2.p1.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. This is the most widely used variant in the state-of-the-art LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS10.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS10.SSS3.5.1.1">II-J</span>3 </span>Prefix Decoder</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS10.SSS3.p1">
<p class="ltx_p" id="S2.SS10.SSS3.p1.5">The causal masked attention is reasonable in the encoder-decoder architectures where the encoder can attend to all the tokens in the sentence from every position using self-attention. This means that the encoder can also attend to tokens <math alttext="t_{k+1}" class="ltx_Math" display="inline" id="S2.SS10.SSS3.p1.1.m1.1"><semantics id="S2.SS10.SSS3.p1.1.m1.1a"><msub id="S2.SS10.SSS3.p1.1.m1.1.1" xref="S2.SS10.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS10.SSS3.p1.1.m1.1.1.2" xref="S2.SS10.SSS3.p1.1.m1.1.1.2.cmml">t</mi><mrow id="S2.SS10.SSS3.p1.1.m1.1.1.3" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS10.SSS3.p1.1.m1.1.1.3.2" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.2.cmml">k</mi><mo id="S2.SS10.SSS3.p1.1.m1.1.1.3.1" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.SS10.SSS3.p1.1.m1.1.1.3.3" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS3.p1.1.m1.1b"><apply id="S2.SS10.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS10.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1.2">𝑡</ci><apply id="S2.SS10.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1.3"><plus id="S2.SS10.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.1"></plus><ci id="S2.SS10.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.2">𝑘</ci><cn id="S2.SS10.SSS3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S2.SS10.SSS3.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS3.p1.1.m1.1c">t_{k+1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS3.p1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="t_{n}" class="ltx_Math" display="inline" id="S2.SS10.SSS3.p1.2.m2.1"><semantics id="S2.SS10.SSS3.p1.2.m2.1a"><msub id="S2.SS10.SSS3.p1.2.m2.1.1" xref="S2.SS10.SSS3.p1.2.m2.1.1.cmml"><mi id="S2.SS10.SSS3.p1.2.m2.1.1.2" xref="S2.SS10.SSS3.p1.2.m2.1.1.2.cmml">t</mi><mi id="S2.SS10.SSS3.p1.2.m2.1.1.3" xref="S2.SS10.SSS3.p1.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS3.p1.2.m2.1b"><apply id="S2.SS10.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS10.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS10.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS10.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS10.SSS3.p1.2.m2.1.1.2">𝑡</ci><ci id="S2.SS10.SSS3.p1.2.m2.1.1.3.cmml" xref="S2.SS10.SSS3.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS3.p1.2.m2.1c">t_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS3.p1.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> in addition to the tokens from <math alttext="t_{1}" class="ltx_Math" display="inline" id="S2.SS10.SSS3.p1.3.m3.1"><semantics id="S2.SS10.SSS3.p1.3.m3.1a"><msub id="S2.SS10.SSS3.p1.3.m3.1.1" xref="S2.SS10.SSS3.p1.3.m3.1.1.cmml"><mi id="S2.SS10.SSS3.p1.3.m3.1.1.2" xref="S2.SS10.SSS3.p1.3.m3.1.1.2.cmml">t</mi><mn id="S2.SS10.SSS3.p1.3.m3.1.1.3" xref="S2.SS10.SSS3.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS3.p1.3.m3.1b"><apply id="S2.SS10.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS10.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS10.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS10.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS10.SSS3.p1.3.m3.1.1.2">𝑡</ci><cn id="S2.SS10.SSS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.SS10.SSS3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS3.p1.3.m3.1c">t_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS3.p1.3.m3.1d">italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="t_{k-1}" class="ltx_Math" display="inline" id="S2.SS10.SSS3.p1.4.m4.1"><semantics id="S2.SS10.SSS3.p1.4.m4.1a"><msub id="S2.SS10.SSS3.p1.4.m4.1.1" xref="S2.SS10.SSS3.p1.4.m4.1.1.cmml"><mi id="S2.SS10.SSS3.p1.4.m4.1.1.2" xref="S2.SS10.SSS3.p1.4.m4.1.1.2.cmml">t</mi><mrow id="S2.SS10.SSS3.p1.4.m4.1.1.3" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.cmml"><mi id="S2.SS10.SSS3.p1.4.m4.1.1.3.2" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.2.cmml">k</mi><mo id="S2.SS10.SSS3.p1.4.m4.1.1.3.1" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.1.cmml">−</mo><mn id="S2.SS10.SSS3.p1.4.m4.1.1.3.3" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS3.p1.4.m4.1b"><apply id="S2.SS10.SSS3.p1.4.m4.1.1.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS3.p1.4.m4.1.1.1.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS10.SSS3.p1.4.m4.1.1.2.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1.2">𝑡</ci><apply id="S2.SS10.SSS3.p1.4.m4.1.1.3.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1.3"><minus id="S2.SS10.SSS3.p1.4.m4.1.1.3.1.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.1"></minus><ci id="S2.SS10.SSS3.p1.4.m4.1.1.3.2.cmml" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.2">𝑘</ci><cn id="S2.SS10.SSS3.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S2.SS10.SSS3.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS3.p1.4.m4.1c">t_{k-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS3.p1.4.m4.1d">italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT</annotation></semantics></math> while calculating the representation for <math alttext="t_{k}" class="ltx_Math" display="inline" id="S2.SS10.SSS3.p1.5.m5.1"><semantics id="S2.SS10.SSS3.p1.5.m5.1a"><msub id="S2.SS10.SSS3.p1.5.m5.1.1" xref="S2.SS10.SSS3.p1.5.m5.1.1.cmml"><mi id="S2.SS10.SSS3.p1.5.m5.1.1.2" xref="S2.SS10.SSS3.p1.5.m5.1.1.2.cmml">t</mi><mi id="S2.SS10.SSS3.p1.5.m5.1.1.3" xref="S2.SS10.SSS3.p1.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS10.SSS3.p1.5.m5.1b"><apply id="S2.SS10.SSS3.p1.5.m5.1.1.cmml" xref="S2.SS10.SSS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS10.SSS3.p1.5.m5.1.1.1.cmml" xref="S2.SS10.SSS3.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS10.SSS3.p1.5.m5.1.1.2.cmml" xref="S2.SS10.SSS3.p1.5.m5.1.1.2">𝑡</ci><ci id="S2.SS10.SSS3.p1.5.m5.1.1.3.cmml" xref="S2.SS10.SSS3.p1.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS10.SSS3.p1.5.m5.1c">t_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS10.SSS3.p1.5.m5.1d">italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. But when we drop the encoder and only keep the decoder, we also lose this flexibility in attention. A variation in the decoder-only architectures is by changing the mask from strictly causal to fully visible on a portion of the input sequence, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F4" title="Figure 4 ‣ II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>. The Prefix decoder is also known as non-causal decoder architecture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S2.F4.g1" src="./A Comprehensive Overview of Large Language Models_files/architectures.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An example of attention patterns in language models, image is taken from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib91" title="">91</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S2.F5.g1" src="./A Comprehensive Overview of Large Language Models_files/training_objectives.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An example of language model training objectives, image from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib91" title="">91</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S2.SS11">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS11.5.1.1">II-K</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS11.6.2">Pre-Training Objectives</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS11.p1">
<p class="ltx_p" id="S2.SS11.p1.1">This section describes LLMs pre-training objectives. For more details see the paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib91" title="">91</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="855" id="S2.F6.g1" src="./A Comprehensive Overview of Large Language Models_files/Flow_diagram_LLMs.png" width="1256">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at different training stages like pre-training, instruction-tuning, or alignment tuning.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S2.SS11.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS11.SSS1.5.1.1">II-K</span>1 </span>Full Language Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS11.SSS1.p1">
<p class="ltx_p" id="S2.SS11.SSS1.p1.1">An autoregressive language modeling objective where the model is asked to predict future tokens given the previous tokens, an example is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F5" title="Figure 5 ‣ II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS11.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS11.SSS2.5.1.1">II-K</span>2 </span>Prefix Language Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS11.SSS2.p1">
<p class="ltx_p" id="S2.SS11.SSS2.p1.1">A non-causal training objective, where a prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F5" title="Figure 5 ‣ II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS11.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS11.SSS3.5.1.1">II-K</span>3 </span>Masked Language Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS11.SSS3.p1">
<p class="ltx_p" id="S2.SS11.SSS3.p1.1">In this training objective, tokens or spans (a sequence of tokens) are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F5" title="Figure 5 ‣ II-J3 Prefix Decoder ‣ II-J Architectures ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS11.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS11.SSS4.5.1.1">II-K</span>4 </span>Unified Language Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS11.SSS4.p1">
<p class="ltx_p" id="S2.SS11.SSS4.p1.1">Unified language modeling&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib92" title="">92</a>]</cite> is a combination of causal, non-causal, and masked language training objectives. Here in masked language modeling, the attention is not bidirectional but unidirectional, attending either left-to-right or right-to-left context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS12">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS12.5.1.1">II-L</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS12.6.2">Model Adaptation</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS12.p1">
<p class="ltx_p" id="S2.SS12.p1.1">This section discusses the fundamentals of LLMs adaptation stages, from pre-training to fine-tuning for downstream tasks and utilization. An example of different training stages and inference in LLMs is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F6" title="Figure 6 ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>. In this paper, we refer alignment-tuning to aligning with human preferences, while occasionally the literature uses the term alignment for different purposes.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS12.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS12.SSS1.5.1.1">II-L</span>1 </span>Pre-Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS12.SSS1.p1">
<p class="ltx_p" id="S2.SS12.SSS1.p1.1">In the very first stage, the model is trained in a self-supervised manner on a large corpus to predict the next tokens given the input. The design choices of LLMs vary from encoder-decoder to decoder-only architectures with different building blocks and loss functions in sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS6" title="II-F Layer Normalization ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-F</span></span></a>,&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS5" title="II-E Activation Functions ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-E</span></span></a>, &nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.SS11" title="II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-K</span></span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS12.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS12.SSS2.5.1.1">II-L</span>2 </span>Fine-Tuning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS12.SSS2.p1">
<p class="ltx_p" id="S2.SS12.SSS2.p1.1">There are different styles to fine-tune an LLM. This section briefly discusses fine-tuning approaches. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S2.SS12.SSS2.p1.1.1">&nbsp;<span class="ltx_text ltx_font_bold" id="S2.SS12.SSS2.p1.1.1.1">Transfer Learning:</span></em>
The pre-trained LLMs perform well for various tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>. But to improve the performance for a downstream task, pre-trained models are fine-tuned with the task-specific data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite>, known as transfer learning.
&nbsp;<em class="ltx_emph ltx_font_italic" id="S2.SS12.SSS2.p1.1.2">&nbsp;<span class="ltx_text ltx_font_bold" id="S2.SS12.SSS2.p1.1.2.1">Instruction-tuning:</span></em>
To enable a model to respond to user queries effectively, the pre-trained model is fine-tuned on instruction formatted data i.e., instruction and an input-output pair. Instructions generally comprise multi-task data in plain natural language, guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero-shot generalization and downstream task performance. Details on formatting instruction data and its various styles are available in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite>. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S2.SS12.SSS2.p1.1.3">&nbsp;<span class="ltx_text ltx_font_bold" id="S2.SS12.SSS2.p1.1.3.1">Alignment-tuning:</span></em>
LLMs are prone to generate false, biased, and harmful text. To make them helpful, honest, and harmless models are aligned using human feedback. Alignment involves asking LLMs to generate unexpected responses and then updating their parameters to avoid such responses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib94" title="">94</a>]</cite>. 
<br class="ltx_break">It ensures LLMs operate according to human intentions and values. A model is defined to be an “aligned” model if the model fulfills three criteria of helpful, honest, and harmless or <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S2.SS12.SSS2.p1.1.4">“HHH”</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib95" title="">95</a>]</cite>. 
<br class="ltx_break">Researchers employ reinforcement learning with human feedback (RLHF)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib96" title="">96</a>]</cite> for model alignment. In RLHF, a fine-tuned model on demonstrations is further trained with reward modeling (RM) and reinforcement learning (RL), shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F6" title="Figure 6 ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>. Below we briefly discuss RM and RL pipelines in RLHF.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS12.SSS2.p2">
<p class="ltx_p" id="S2.SS12.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S2.SS12.SSS2.p2.1.1">Reward modeling:</span> trains a model to rank generated responses according to human preferences using a classification objective. To train the classifier humans annotate LLMs generated responses based on HHH criteria. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S2.SS12.SSS2.p2.1.2">Reinforcement learning:</span> in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred vs. dispreferred, which is used to align the model with proximal policy optimization (PPO). This process repeats iteratively until convergence.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS12.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS12.SSS3.5.1.1">II-L</span>3 </span>Prompting/Utilization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS12.SSS3.p1">
<p class="ltx_p" id="S2.SS12.SSS3.p1.1">Prompting is a method to query trained LLMs for generating responses, as illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S2.F6" title="Figure 6 ‣ II-K Pre-Training Objectives ‣ II Background ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>. LLMs can be prompted in various prompt setups, where they can be adapted to the instructions without fine-tuning and in other cases with fine-tuning on data containing different prompt styles&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib98" title="">98</a>]</cite>. A good guide on prompt engineering is available at&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib32" title="">32</a>]</cite>. Below, we will discuss various widely used prompt setups.
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS12.SSS3.p1.1.1">Zero-Shot Prompting:</em>
LLMs are zero-shot learners and capable of answering queries never seen before. This style of prompting requires LLMs to answer user questions without seeing any examples in the prompt. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS12.SSS3.p1.1.2">In-context Learning:</em>
Also known as few-shot learning, here, multiple input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style is also called few-shot learning. A discussion on formatting in-context learning (ICL) templates is available in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S2.SS12.SSS3.p1.1.3">&nbsp;<span class="ltx_text ltx_font_bold" id="S2.SS12.SSS3.p1.1.3.1">Reasoning in LLMs:</span></em>
LLMs are zero-shot reasoners and can be provoked to generate answers to logical problems, task planning, critical thinking, etc. with reasoning. Generating reasons is possible only by using different prompting styles, whereas to improve LLMs further on reasoning tasks many methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite> train them on reasoning datasets. We discuss various prompting techniques for reasoning below. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S2.SS12.SSS3.p1.1.4">Chain-of-Thought (CoT):</span>
A special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with step-by-step reasoning. More details on CoT prompts are available in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib97" title="">97</a>]</cite>. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S2.SS12.SSS3.p1.1.5">Self-Consistency:</span> Improves CoT performance by generating multiple responses and selecting the most frequent answer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib100" title="">100</a>]</cite>.
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S2.SS12.SSS3.p1.1.6">Tree-of-Thought (ToT):</span> Explores multiple reasoning paths with possibilities to look ahead and backtrack for problem-solving&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib101" title="">101</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS12.SSS3.p1.1.7">Single-Turn Instructions:</em>
In this prompting setup, LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by understanding the context either in a zero-shot or few-shot setting. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS12.SSS3.p1.1.8">Multi-Turn Instructions:</em>
Solving a complex task requires multiple interactions with LLMs, where feedback and responses from the other tools are given as input to the LLM for the next rounds. This style of using LLMs in the loop is common in autonomous agents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Large Language Models</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section reviews LLMs, briefly describing their architectures, training objectives, pipelines, datasets, and fine-tuning details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Pre-Trained LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Here, we provide summaries of various well-known pre-trained LLMs with significant discoveries, changing the course of research and development in NLP. These LLMs have considerably improved the performance in NLU and NLG domains, and are widely fine-tuned for downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span>General Purpose</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.1">An encoder-decoder model employing a unified text-to-text training for all NLP problems, shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F7" title="Figure 7 ‣ T5 [10] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>. T5 places layer normalization outside the residual path in a conventional transformer model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib62" title="">62</a>]</cite>. It uses masked language modeling as a pre-training objective where spans (consecutive tokens) are replaced with a single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training, the model is fine-tuned using adapter layers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib102" title="">102</a>]</cite> for downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S3.F7.g1" src="./A Comprehensive Overview of Large Language Models_files/T5.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Unified text-to-text training example, source image from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">The GPT-3 architecture is same as the GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib5" title="">5</a>]</cite> but with dense and sparse attention in transformer layers similar to the Sparse Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib63" title="">63</a>]</cite>. It shows that large models can train on larger batch sizes with a lower learning rate; in order to decide the batch size during training, GPT-3 uses the gradient noise scale as in &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib103" title="">103</a>]</cite>. Overall, GPT-3 increases model parameters to 175B showing that the performance of large language models improves with the scale and is competitive with the fine-tuned models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px3.p1.1">A multilingual T5 model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses a larger vocabulary size of 250,000 to cover multiple languages. To avoid over-fitting or under-fitting for a language, mT5 employs a data sampling procedure to select samples from all languages. The paper suggests using a small amount of pre-training datasets, including all languages when fine-tuning for a task using English language data. This allows the model to generate correct non-English outputs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px4.1.m1.1"><semantics id="S3.SS1.SSS1.Px4.1.m1.1b"><mi id="S3.SS1.SSS1.Px4.1.m1.1.1" xref="S3.SS1.SSS1.Px4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px4.1.m1.1c"><ci id="S3.SS1.SSS1.Px4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px4.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px4.1.m1.1e">italic_α</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib104" title="">104</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px4.p1.1">An autoregressive model that has a query layer at the end of standard transformer layers, example shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F8" title="Figure 8 ‣ PanGu-𝛼 [104] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a>, with aim to predict next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism, given in Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.E7" title="7 ‣ PanGu-𝛼 [104] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="a=p_{n}W_{h}^{q}W_{h}^{k}TH_{L}^{T}" class="ltx_Math" display="block" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mi id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">a</mi><mo id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><msub id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml">p</mi><mi id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml">n</mi></msub><mo id="S3.E7.m1.1.1.3.1" xref="S3.E7.m1.1.1.3.1.cmml">⁢</mo><msubsup id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml"><mi id="S3.E7.m1.1.1.3.3.2.2" xref="S3.E7.m1.1.1.3.3.2.2.cmml">W</mi><mi id="S3.E7.m1.1.1.3.3.2.3" xref="S3.E7.m1.1.1.3.3.2.3.cmml">h</mi><mi id="S3.E7.m1.1.1.3.3.3" xref="S3.E7.m1.1.1.3.3.3.cmml">q</mi></msubsup><mo id="S3.E7.m1.1.1.3.1a" xref="S3.E7.m1.1.1.3.1.cmml">⁢</mo><msubsup id="S3.E7.m1.1.1.3.4" xref="S3.E7.m1.1.1.3.4.cmml"><mi id="S3.E7.m1.1.1.3.4.2.2" xref="S3.E7.m1.1.1.3.4.2.2.cmml">W</mi><mi id="S3.E7.m1.1.1.3.4.2.3" xref="S3.E7.m1.1.1.3.4.2.3.cmml">h</mi><mi id="S3.E7.m1.1.1.3.4.3" xref="S3.E7.m1.1.1.3.4.3.cmml">k</mi></msubsup><mo id="S3.E7.m1.1.1.3.1b" xref="S3.E7.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E7.m1.1.1.3.5" xref="S3.E7.m1.1.1.3.5.cmml">T</mi><mo id="S3.E7.m1.1.1.3.1c" xref="S3.E7.m1.1.1.3.1.cmml">⁢</mo><msubsup id="S3.E7.m1.1.1.3.6" xref="S3.E7.m1.1.1.3.6.cmml"><mi id="S3.E7.m1.1.1.3.6.2.2" xref="S3.E7.m1.1.1.3.6.2.2.cmml">H</mi><mi id="S3.E7.m1.1.1.3.6.2.3" xref="S3.E7.m1.1.1.3.6.2.3.cmml">L</mi><mi id="S3.E7.m1.1.1.3.6.3" xref="S3.E7.m1.1.1.3.6.3.cmml">T</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><ci id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2">𝑎</ci><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><times id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3.1"></times><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2">𝑝</ci><ci id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3">𝑛</ci></apply><apply id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.3.3">superscript</csymbol><apply id="S3.E7.m1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.3.2.1.cmml" xref="S3.E7.m1.1.1.3.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.3.2.2.cmml" xref="S3.E7.m1.1.1.3.3.2.2">𝑊</ci><ci id="S3.E7.m1.1.1.3.3.2.3.cmml" xref="S3.E7.m1.1.1.3.3.2.3">ℎ</ci></apply><ci id="S3.E7.m1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.3.3.3">𝑞</ci></apply><apply id="S3.E7.m1.1.1.3.4.cmml" xref="S3.E7.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.4.1.cmml" xref="S3.E7.m1.1.1.3.4">superscript</csymbol><apply id="S3.E7.m1.1.1.3.4.2.cmml" xref="S3.E7.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.4.2.1.cmml" xref="S3.E7.m1.1.1.3.4">subscript</csymbol><ci id="S3.E7.m1.1.1.3.4.2.2.cmml" xref="S3.E7.m1.1.1.3.4.2.2">𝑊</ci><ci id="S3.E7.m1.1.1.3.4.2.3.cmml" xref="S3.E7.m1.1.1.3.4.2.3">ℎ</ci></apply><ci id="S3.E7.m1.1.1.3.4.3.cmml" xref="S3.E7.m1.1.1.3.4.3">𝑘</ci></apply><ci id="S3.E7.m1.1.1.3.5.cmml" xref="S3.E7.m1.1.1.3.5">𝑇</ci><apply id="S3.E7.m1.1.1.3.6.cmml" xref="S3.E7.m1.1.1.3.6"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.6.1.cmml" xref="S3.E7.m1.1.1.3.6">superscript</csymbol><apply id="S3.E7.m1.1.1.3.6.2.cmml" xref="S3.E7.m1.1.1.3.6"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.6.2.1.cmml" xref="S3.E7.m1.1.1.3.6">subscript</csymbol><ci id="S3.E7.m1.1.1.3.6.2.2.cmml" xref="S3.E7.m1.1.1.3.6.2.2">𝐻</ci><ci id="S3.E7.m1.1.1.3.6.2.3.cmml" xref="S3.E7.m1.1.1.3.6.2.3">𝐿</ci></apply><ci id="S3.E7.m1.1.1.3.6.3.cmml" xref="S3.E7.m1.1.1.3.6.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">a=p_{n}W_{h}^{q}W_{h}^{k}TH_{L}^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">italic_a = italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_T italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="372" id="S3.F8.g1" src="./A Comprehensive Overview of Large Language Models_files/PanGU_alpha.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The image is the article of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib104" title="">104</a>]</cite>, showing an example of PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F8.2.m1.1"><semantics id="S3.F8.2.m1.1b"><mi id="S3.F8.2.m1.1.1" xref="S3.F8.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F8.2.m1.1c"><ci id="S3.F8.2.m1.1.1.cmml" xref="S3.F8.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.2.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F8.2.m1.1e">italic_α</annotation></semantics></math> architecture.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px5">
<h5 class="ltx_title ltx_title_paragraph">CPM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib12" title="">12</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px5.p1.1">Cost-efficient Pre-trained language Models (CPM-2) pre-trains bilingual (English and Chinese) 11B and 198B mixture-of-experts (MoE) models on the WuDaoCorpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib105" title="">105</a>]</cite> dataset. The tokenization process removes <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.SS1.SSS1.Px5.p1.1.1">“_”</span> white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance, starting with only the Chinese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover, to use the model for downstream tasks, CPM-2 experimented with both complete fine-tuning and prompt fine-tuning as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib106" title="">106</a>]</cite> where only prompt-related parameters are updated by inserting prompts at various positions, front, middle, and back. CPM-2 also proposes INFMOE, a memory-efficient framework with a strategy to dynamically offload parameters to the CPU for inference at a 100B scale. It overlaps data movement with inference computation for lower inference time.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px6">
<h5 class="ltx_title ltx_title_paragraph">ERNIE 3.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib107" title="">107</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px6.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px6.p1.1">ERNIE 3.0 takes inspiration from multi-task learning to build a modular architecture using Transformer-XL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib108" title="">108</a>]</cite> as the backbone. The universal representation module is shared by all the tasks, which serve as the basic block for task-specific representation modules, which are all trained jointly for natural language understanding, natural language generation, and knowledge extraction. This LLM is primarily focused on the Chinese language, claims to train on the largest Chinese text corpora for LLM training, and achieved state-of-the-art in 54 Chinese NLP tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px7">
<h5 class="ltx_title ltx_title_paragraph">Jurassic-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib109" title="">109</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px7.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px7.p1.1">A pair of auto-regressive language models, including a 7B-parameter J1-Large model and a 178B-parameter J1-Jumbo model. The training vocabulary of Jurassic-1 comprise word pieces, complete words, and multi-word expressions without any word boundaries, where possible out-of-vocabulary instances are interpreted as Unicode bytes. Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more balanced depth-to-width self-attention architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib110" title="">110</a>]</cite> and an improved tokenizer for a faster prediction based on broader resources, achieving a comparable performance in zero-shot learning tasks and a superior performance in few-shot learning tasks given the ability to feed more examples as a prompt.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px8">
<h5 class="ltx_title ltx_title_paragraph">HyperCLOVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib111" title="">111</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px8.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px8.p1.1">A Korean language model with GPT-3 architecture.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px9">
<h5 class="ltx_title ltx_title_paragraph">Yuan 1.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib112" title="">112</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px9.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px9.p1.1">Trained on a Chinese corpus with 5TB of high-quality text collected from the Internet. A Massive Data Filtering System (MDFS) built on Spark is developed to process the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 with the aim of saving energy expenses and carbon emissions, various factors that improve the performance of distributed training are incorporated in architecture and training like increasing the number of hidden size improves pipeline and tensor parallelism performance, larger micro batches improve pipeline parallelism performance, and higher global batch size improve data parallelism performance. In practice, the Yuan 1.0 model performs well on text classification, Winograd Schema, natural language inference, and reading comprehension tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px10">
<h5 class="ltx_title ltx_title_paragraph">Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib113" title="">113</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px10.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px10.p1.1">The Gopher family of models ranges from 44M to 280B parameters in size to study the effect of <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px10.p1.1.1">scale</span> on the LLMs performance. The 280B model beats GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>, Jurrasic-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib109" title="">109</a>]</cite>, MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>]</cite>, and others on 81% of the evaluated tasks.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px11">
<h5 class="ltx_title ltx_title_paragraph">ERNIE 3.0 TITAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib35" title="">35</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px11.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px11.p1.1">ERNIE 3.0 Titan extends ERNIE 3.0 by training a larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the-art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with factual consistency, ERNIE 3.0 Titan adds another task, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px11.p1.1.1">Credible and Controllable Generations</span>, to its multi-task learning setup. It introduces additional self-supervised adversarial and controllable language modeling losses to the pre-training step, which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px12">
<h5 class="ltx_title ltx_title_paragraph">GPT-NeoX-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib115" title="">115</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px12.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px12.p1.1">An auto-regressive model that largely follows GPT-3 with a few deviations in architecture design, trained on the Pile dataset without any data deduplication. GPT-NeoX has parallel attention and feed-forward layers in a transformer block, given in Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.E8" title="8 ‣ GPT-NeoX-20B [115] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a>, that increases throughput by 15%. It uses rotary positional embedding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib66" title="">66</a>]</cite>, applying it to only 25% of embedding vector dimension as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib116" title="">116</a>]</cite>. This reduces the computation without performance degradation. Opposite to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult; therefore, the model chooses hyperparameters from the method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x+Attn(LN_{1}(x))+FF(LN_{2}(x))" class="ltx_Math" display="block" id="S3.E8.m1.4"><semantics id="S3.E8.m1.4a"><mrow id="S3.E8.m1.4.4" xref="S3.E8.m1.4.4.cmml"><mi id="S3.E8.m1.4.4.4" xref="S3.E8.m1.4.4.4.cmml">x</mi><mo id="S3.E8.m1.4.4.3" xref="S3.E8.m1.4.4.3.cmml">+</mo><mrow id="S3.E8.m1.3.3.1" xref="S3.E8.m1.3.3.1.cmml"><mi id="S3.E8.m1.3.3.1.3" xref="S3.E8.m1.3.3.1.3.cmml">A</mi><mo id="S3.E8.m1.3.3.1.2" xref="S3.E8.m1.3.3.1.2.cmml">⁢</mo><mi id="S3.E8.m1.3.3.1.4" xref="S3.E8.m1.3.3.1.4.cmml">t</mi><mo id="S3.E8.m1.3.3.1.2a" xref="S3.E8.m1.3.3.1.2.cmml">⁢</mo><mi id="S3.E8.m1.3.3.1.5" xref="S3.E8.m1.3.3.1.5.cmml">t</mi><mo id="S3.E8.m1.3.3.1.2b" xref="S3.E8.m1.3.3.1.2.cmml">⁢</mo><mi id="S3.E8.m1.3.3.1.6" xref="S3.E8.m1.3.3.1.6.cmml">n</mi><mo id="S3.E8.m1.3.3.1.2c" xref="S3.E8.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E8.m1.3.3.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.cmml"><mo id="S3.E8.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E8.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.3.3.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.cmml"><mi id="S3.E8.m1.3.3.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.1.2.cmml">L</mi><mo id="S3.E8.m1.3.3.1.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.1.cmml">⁢</mo><msub id="S3.E8.m1.3.3.1.1.1.1.3" xref="S3.E8.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.E8.m1.3.3.1.1.1.1.3.2" xref="S3.E8.m1.3.3.1.1.1.1.3.2.cmml">N</mi><mn id="S3.E8.m1.3.3.1.1.1.1.3.3" xref="S3.E8.m1.3.3.1.1.1.1.3.3.cmml">1</mn></msub><mo id="S3.E8.m1.3.3.1.1.1.1.1a" xref="S3.E8.m1.3.3.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E8.m1.3.3.1.1.1.1.4.2" xref="S3.E8.m1.3.3.1.1.1.1.cmml"><mo id="S3.E8.m1.3.3.1.1.1.1.4.2.1" stretchy="false" xref="S3.E8.m1.3.3.1.1.1.1.cmml">(</mo><mi id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">x</mi><mo id="S3.E8.m1.3.3.1.1.1.1.4.2.2" stretchy="false" xref="S3.E8.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E8.m1.3.3.1.1.1.3" stretchy="false" xref="S3.E8.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E8.m1.4.4.3a" xref="S3.E8.m1.4.4.3.cmml">+</mo><mrow id="S3.E8.m1.4.4.2" xref="S3.E8.m1.4.4.2.cmml"><mi id="S3.E8.m1.4.4.2.3" xref="S3.E8.m1.4.4.2.3.cmml">F</mi><mo id="S3.E8.m1.4.4.2.2" xref="S3.E8.m1.4.4.2.2.cmml">⁢</mo><mi id="S3.E8.m1.4.4.2.4" xref="S3.E8.m1.4.4.2.4.cmml">F</mi><mo id="S3.E8.m1.4.4.2.2a" xref="S3.E8.m1.4.4.2.2.cmml">⁢</mo><mrow id="S3.E8.m1.4.4.2.1.1" xref="S3.E8.m1.4.4.2.1.1.1.cmml"><mo id="S3.E8.m1.4.4.2.1.1.2" stretchy="false" xref="S3.E8.m1.4.4.2.1.1.1.cmml">(</mo><mrow id="S3.E8.m1.4.4.2.1.1.1" xref="S3.E8.m1.4.4.2.1.1.1.cmml"><mi id="S3.E8.m1.4.4.2.1.1.1.2" xref="S3.E8.m1.4.4.2.1.1.1.2.cmml">L</mi><mo id="S3.E8.m1.4.4.2.1.1.1.1" xref="S3.E8.m1.4.4.2.1.1.1.1.cmml">⁢</mo><msub id="S3.E8.m1.4.4.2.1.1.1.3" xref="S3.E8.m1.4.4.2.1.1.1.3.cmml"><mi id="S3.E8.m1.4.4.2.1.1.1.3.2" xref="S3.E8.m1.4.4.2.1.1.1.3.2.cmml">N</mi><mn id="S3.E8.m1.4.4.2.1.1.1.3.3" xref="S3.E8.m1.4.4.2.1.1.1.3.3.cmml">2</mn></msub><mo id="S3.E8.m1.4.4.2.1.1.1.1a" xref="S3.E8.m1.4.4.2.1.1.1.1.cmml">⁢</mo><mrow id="S3.E8.m1.4.4.2.1.1.1.4.2" xref="S3.E8.m1.4.4.2.1.1.1.cmml"><mo id="S3.E8.m1.4.4.2.1.1.1.4.2.1" stretchy="false" xref="S3.E8.m1.4.4.2.1.1.1.cmml">(</mo><mi id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml">x</mi><mo id="S3.E8.m1.4.4.2.1.1.1.4.2.2" stretchy="false" xref="S3.E8.m1.4.4.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E8.m1.4.4.2.1.1.3" stretchy="false" xref="S3.E8.m1.4.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.4b"><apply id="S3.E8.m1.4.4.cmml" xref="S3.E8.m1.4.4"><plus id="S3.E8.m1.4.4.3.cmml" xref="S3.E8.m1.4.4.3"></plus><ci id="S3.E8.m1.4.4.4.cmml" xref="S3.E8.m1.4.4.4">𝑥</ci><apply id="S3.E8.m1.3.3.1.cmml" xref="S3.E8.m1.3.3.1"><times id="S3.E8.m1.3.3.1.2.cmml" xref="S3.E8.m1.3.3.1.2"></times><ci id="S3.E8.m1.3.3.1.3.cmml" xref="S3.E8.m1.3.3.1.3">𝐴</ci><ci id="S3.E8.m1.3.3.1.4.cmml" xref="S3.E8.m1.3.3.1.4">𝑡</ci><ci id="S3.E8.m1.3.3.1.5.cmml" xref="S3.E8.m1.3.3.1.5">𝑡</ci><ci id="S3.E8.m1.3.3.1.6.cmml" xref="S3.E8.m1.3.3.1.6">𝑛</ci><apply id="S3.E8.m1.3.3.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1"><times id="S3.E8.m1.3.3.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1"></times><ci id="S3.E8.m1.3.3.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.2">𝐿</ci><apply id="S3.E8.m1.3.3.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.3.2">𝑁</ci><cn id="S3.E8.m1.3.3.1.1.1.1.3.3.cmml" type="integer" xref="S3.E8.m1.3.3.1.1.1.1.3.3">1</cn></apply><ci id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1">𝑥</ci></apply></apply><apply id="S3.E8.m1.4.4.2.cmml" xref="S3.E8.m1.4.4.2"><times id="S3.E8.m1.4.4.2.2.cmml" xref="S3.E8.m1.4.4.2.2"></times><ci id="S3.E8.m1.4.4.2.3.cmml" xref="S3.E8.m1.4.4.2.3">𝐹</ci><ci id="S3.E8.m1.4.4.2.4.cmml" xref="S3.E8.m1.4.4.2.4">𝐹</ci><apply id="S3.E8.m1.4.4.2.1.1.1.cmml" xref="S3.E8.m1.4.4.2.1.1"><times id="S3.E8.m1.4.4.2.1.1.1.1.cmml" xref="S3.E8.m1.4.4.2.1.1.1.1"></times><ci id="S3.E8.m1.4.4.2.1.1.1.2.cmml" xref="S3.E8.m1.4.4.2.1.1.1.2">𝐿</ci><apply id="S3.E8.m1.4.4.2.1.1.1.3.cmml" xref="S3.E8.m1.4.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.4.4.2.1.1.1.3.1.cmml" xref="S3.E8.m1.4.4.2.1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.4.4.2.1.1.1.3.2.cmml" xref="S3.E8.m1.4.4.2.1.1.1.3.2">𝑁</ci><cn id="S3.E8.m1.4.4.2.1.1.1.3.3.cmml" type="integer" xref="S3.E8.m1.4.4.2.1.1.1.3.3">2</cn></apply><ci id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.4c">x+Attn(LN_{1}(x))+FF(LN_{2}(x))</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.4d">italic_x + italic_A italic_t italic_t italic_n ( italic_L italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) ) + italic_F italic_F ( italic_L italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px13">
<h5 class="ltx_title ltx_title_paragraph">OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px13.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px13.p1.1">It is a clone of GPT-3, developed with the intention to open-source a model that replicates GPT-3 performance. Training of OPT employs dynamic loss scaling &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib117" title="">117</a>]</cite> and restarts from an earlier checkpoint with a lower learning rate whenever loss divergence is observed. Overall, the performance of OPT-175B models is comparable to the GPT3-175B model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px14">
<h5 class="ltx_title ltx_title_paragraph">BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px14.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px14.p1.1">A causal decoder model trained on ROOTS corpus with the aim of open-sourcing an LLM. The architecture of BLOOM is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F9" title="Figure 9 ‣ GLaM [118] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>, with differences like ALiBi positional embedding, an additional normalization layer after the embedding layer as suggested by the bitsandbytes<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/TimDettmers/bitsandbytes</span></span></span> library. These changes stabilize training with improved downstream performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px15">
<h5 class="ltx_title ltx_title_paragraph">GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px15.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px15.p1.1">Generalist Language Model (GLaM) represents a family of language models using a sparsely activated decoder-only mixture-of-experts (MoE) structure&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib119" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib120" title="">120</a>]</cite>. To gain more model capacity while reducing computation, the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLaM model, GLaM (64B/64E), is about 7<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px15.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px15.p1.1.m1.1a"><mo id="S3.SS1.SSS1.Px15.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px15.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px15.p1.1.m1.1b"><times id="S3.SS1.SSS1.Px15.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px15.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px15.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px15.p1.1.m1.1d">×</annotation></semantics></math> larger than GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>, while only a part of the parameters is activated per input token. The largest GLaM (64B/64E) model achieves better overall results as compared to GPT-3 while consuming only one-third of GPT-3’s training energy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="S3.F9.g1" src="./A Comprehensive Overview of Large Language Models_files/BLOOM.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The BLOOM architecture example sourced from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px16">
<h5 class="ltx_title ltx_title_paragraph">MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px16.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px16.p1.1">A 530B causal decoder based on GPT-2 architecture that is roughly 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px16.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px16.p1.1.m1.1a"><mo id="S3.SS1.SSS1.Px16.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px16.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px16.p1.1.m1.1b"><times id="S3.SS1.SSS1.Px16.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px16.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px16.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px16.p1.1.m1.1d">×</annotation></semantics></math> GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collected from various public datasets and blends various types of datasets in a single batch, which beats GPT-3 on a number of evaluations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px17">
<h5 class="ltx_title ltx_title_paragraph">Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib121" title="">121</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px17.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px17.p1.1">A causal decoder trained on the same dataset as the Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib113" title="">113</a>]</cite> but with a little different data sampling distribution (sampled from MassiveText). The model architecture is similar to the one used for Gopher, with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the relationship that model size should be doubled for every doubling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens are trained to get the estimates for compute-optimal training under a given budget. The authors train a 70B model with the same compute budget as Gopher (280B) but with 4 times more data. It outperforms Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib113" title="">113</a>]</cite>, GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>, and others on various downstream tasks, after fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px18">
<h5 class="ltx_title ltx_title_paragraph">AlexaTM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px18.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px18.p1.1">An encoder-decoder model, where encoder weights and decoder embeddings are initialized with a pre-trained encoder to speedup training. The encoder stays frozen for initial 100k steps and later unfreezed for end-to-end training. The model is trained on a combination of denoising and causal language modeling (CLM) objectives, concatenating <math alttext="[CLM]" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px18.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px18.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.2.cmml"><mo id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.3" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.3.cmml">L</mi><mo id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1a" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.4" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.4.cmml">M</mi></mrow><mo id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px18.p1.1.m1.1b"><apply id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1"><times id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.2">𝐶</ci><ci id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.3">𝐿</ci><ci id="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.SSS1.Px18.p1.1.m1.1.1.1.1.4">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px18.p1.1.m1.1c">[CLM]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px18.p1.1.m1.1d">[ italic_C italic_L italic_M ]</annotation></semantics></math> token at the beginning for mode switiching. During training, the CLM task is applied for 20% of the time, which improves the in-context learning performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px19">
<h5 class="ltx_title ltx_title_paragraph">PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px19.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px19.p1.1">A causal decoder with parallel attention and feed-forward layers similar to Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.E8" title="8 ‣ GPT-NeoX-20B [115] ‣ III-A1 General Purpose ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a>, speeding up training 15 times faster. Additional changes to the conventional transformer model include SwiGLU activation, RoPE embeddings, multi-query attention that saves computation cost during decoding, and shared input-output embeddings. During training, loss spiking was observed, and to fix it, model training was restarted from a 100 steps earlier checkpoint by skipping 200-500 batches around the spike. Moreover, the model was found to memorize around 2.4% of the training data at the 540B model scale, whereas this number was lower for smaller models. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px19.p1.1.1">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.Px19.p1.1.1.1">PaLM-2</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib123" title="">123</a>]</cite>:</em>
A smaller multi-lingual variant of PaLM, trained for larger iterations on a better quality dataset. The PaLM-2 shows significant improvements over PaLM, while reducing training and inference costs due to its smaller size. To lessen toxicity and memorization, it appends special tokens with a fraction of pre-training data, which shows reduction in generating harmful responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px20">
<h5 class="ltx_title ltx_title_paragraph">U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib124" title="">124</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px20.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px20.p1.1">This method trains PaLM for 0.1% additional compute with UL2 (also named as UL2Restore) objective&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>]</cite> using the same dataset and outperforms baseline significantly on various NLP tasks, including zero-shot, few-shot, commonsense reasoning, CoT, etc. Training with UL2R involves converting a causal decoder PaLM to a non-causal decoder PaLM and employing 50% sequential denoising, 25% regular denoising, and 25% extreme denoising loss functions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px21">
<h5 class="ltx_title ltx_title_paragraph">UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px21.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px21.p1.1">An encoder-decoder architecture trained using a mixture of denoisers (MoD) objectives. Denoisers include 1) R-Denoiser: a regular span masking, 2) S-Denoiser: which corrupts consecutive tokens of a large sequence and 3) X-Denoiser: which corrupts a large number of tokens randomly. During pre-training, UL2 includes a denoiser token from <math alttext="{R,S,X}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px21.p1.1.m1.3"><semantics id="S3.SS1.SSS1.Px21.p1.1.m1.3a"><mrow id="S3.SS1.SSS1.Px21.p1.1.m1.3.4.2" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.4.1.cmml"><mi id="S3.SS1.SSS1.Px21.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px21.p1.1.m1.1.1.cmml">R</mi><mo id="S3.SS1.SSS1.Px21.p1.1.m1.3.4.2.1" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.Px21.p1.1.m1.2.2" xref="S3.SS1.SSS1.Px21.p1.1.m1.2.2.cmml">S</mi><mo id="S3.SS1.SSS1.Px21.p1.1.m1.3.4.2.2" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS1.Px21.p1.1.m1.3.3" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.3.cmml">X</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px21.p1.1.m1.3b"><list id="S3.SS1.SSS1.Px21.p1.1.m1.3.4.1.cmml" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.4.2"><ci id="S3.SS1.SSS1.Px21.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px21.p1.1.m1.1.1">𝑅</ci><ci id="S3.SS1.SSS1.Px21.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.Px21.p1.1.m1.2.2">𝑆</ci><ci id="S3.SS1.SSS1.Px21.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.Px21.p1.1.m1.3.3">𝑋</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px21.p1.1.m1.3c">{R,S,X}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px21.p1.1.m1.3d">italic_R , italic_S , italic_X</annotation></semantics></math> to represent a denoising setup. It helps improve fine-tuning performance for downstream tasks that bind the task to one of the upstream training modes. This MoD style of training outperforms the T5 model on many benchmarks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px22">
<h5 class="ltx_title ltx_title_paragraph">GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px22.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px22.p1.1">GLM-130B is a bilingual (English and Chinese) model trained using an auto-regressive mask infilling pre-training objective similar to the GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib125" title="">125</a>]</cite>. This training style makes the model bidirectional as compared to GPT-3, which is unidirectional. Opposite to the GLM, the training of GLM-130B includes a small amount of multi-task instruction pre-training data (5% of the total data) along with the self-supervised mask infilling. To stabilize the training, it applies embedding layer gradient shrink.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px23">
<h5 class="ltx_title ltx_title_paragraph">LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px23.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px23.p1.1">A set of decoder-only language models varying from 7B to 70B parameters. LLaMA models series is the most famous among the community for parameter-efficient and instruction tuning. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px23.p1.1.1">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.Px23.p1.1.1.1">LLaMA-1</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib126" title="">126</a>]</cite>:</em> Implements efficient causal attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib127" title="">127</a>]</cite> by not storing and computing masked attention weights and key/query scores. Another optimization is reducing number of activations recomputed in backward pass, as in &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib128" title="">128</a>]</cite>. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.Px23.p1.1.2">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.Px23.p1.1.2.1">LLaMA-2</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite>:</em> This work is more focused towards fine-tuning a safer and better LLaMA-2-Chat model for dialogue generation. The pre-trained model has 40% more training data with a larger context length and grouped-query attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px24">
<h5 class="ltx_title ltx_title_paragraph">PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px24.1.m1.1"><semantics id="S3.SS1.SSS1.Px24.1.m1.1b"><mi id="S3.SS1.SSS1.Px24.1.m1.1.1" mathvariant="normal" xref="S3.SS1.SSS1.Px24.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px24.1.m1.1c"><ci id="S3.SS1.SSS1.Px24.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px24.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px24.1.m1.1d">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px24.1.m1.1e">roman_Σ</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.Px24.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px24.p1.1">An autoregressive model with parameters copied from PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px24.p1.1.m1.1"><semantics id="S3.SS1.SSS1.Px24.p1.1.m1.1a"><mi id="S3.SS1.SSS1.Px24.p1.1.m1.1.1" xref="S3.SS1.SSS1.Px24.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px24.p1.1.m1.1b"><ci id="S3.SS1.SSS1.Px24.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px24.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px24.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px24.p1.1.m1.1d">italic_α</annotation></semantics></math> and extended to a trillion scale with Random Routed Experts (RRE), the architectural diagram is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F10" title="Figure 10 ‣ Xuan Yuan 2.0 [143] ‣ III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">10</span></a>. RRE is similar to the MoE architecture, with distinctions at the second level, where tokens are randomly routed to experts in a domain instead of using a learnable gating method. The model has bottom layers densely activated and shared across all domains, whereas top layers are sparsely activated according to the domain. This training style allows extracting task-specific models and reduces catastrophic forgetting effects in case of continual learning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.5.1.1">III-A</span>2 </span>Coding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib130" title="">130</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">CodeGen has similar architecture to the PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>, i.e., parallel attention, MLP layers, and RoPE embeddings. The model is trained on both natural language and programming language data sequentially (trained on the first dataset, then the second and so on) on the following datasets 1) PILE, 2) BIGQUERY and 3) BIGPYTHON. CodeGen proposed a multi-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen opensource a Multi-Turn Programming Benchmark (MTPB) to evaluate multi-step program synthesis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib131" title="">131</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.1">This LLM is trained on a subset of public Python Github repositories to generate code from docstrings. Computer programming is an iterative process where the programs are often debugged and updated before fulfilling the requirements. Similarly to this, Codex generates 100 versions of a program by repetitive sampling for a given description, which produces a working solution for 77.5% of the problems passing unit tests. Its powerful version powers Github Copilot<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/features/copilot</span></span></span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">AlphaCode&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib132" title="">132</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px3.p1.1">A set of large language models, ranging from 300M to 41B parameters, designed for competition-level code generation tasks. It uses the multi-query attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib133" title="">133</a>]</cite> to reduce memory and cache costs. Since competitive programming problems highly require deep reasoning and an understanding of complex natural language algorithms, the AlphaCode models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on a new competitive programming dataset named CodeContests. The CodeContests dataset mainly contains problems, solutions, and test cases collected from the Codeforces platform<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://codeforces.com/</span></span></span>. The pre-training employs standard language modeling objectives, while GOLD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib134" title="">134</a>]</cite> with tempering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib135" title="">135</a>]</cite> serves as the training objective for the fine-tuning on CodeContests data. To evaluate the performance of AlphaCode, simulated programming competitions are hosted on the Codeforces platform: overall, AlphaCode ranks at the top 54.3% among over 5000 competitors, where its Codeforces rating is within the top 28% of recently participated users.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib34" title="">34</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px4.p1.2">CodeT5+ is based on CodeT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib136" title="">136</a>]</cite>, with shallow encoder and deep decoder, trained in multiple stages initially unimodal data (code) and later bimodal data (text-code pairs). Each training stage has different training objectives and activates different model blocks encoder, decoder, or both according to the task. The unimodal pre-training includes span denoising and CLM objectives, whereas bimodal pre-training objectives contain contrastive learning, matching, and CLM for text-code pairs. CodeT5+ adds special tokens with the text to enable task modes, for example, <math alttext="[CLS]" class="ltx_Math" display="inline" id="S3.SS1.SSS2.Px4.p1.1.m1.1"><semantics id="S3.SS1.SSS2.Px4.p1.1.m1.1a"><mrow id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.2.cmml"><mo id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.2.cmml">C</mi><mo id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.3" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.3.cmml">L</mi><mo id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1a" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.4" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.4.cmml">S</mi></mrow><mo id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1"><times id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.2">𝐶</ci><ci id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.3">𝐿</ci><ci id="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.SSS2.Px4.p1.1.m1.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px4.p1.1.m1.1c">[CLS]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.Px4.p1.1.m1.1d">[ italic_C italic_L italic_S ]</annotation></semantics></math> for contrastive loss, <math alttext="[Match]" class="ltx_Math" display="inline" id="S3.SS1.SSS2.Px4.p1.2.m2.1"><semantics id="S3.SS1.SSS2.Px4.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.2.cmml"><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.2" stretchy="false" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.2" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.2.cmml">M</mi><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.3" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.3.cmml">a</mi><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1a" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.4" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.4.cmml">t</mi><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1b" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.5" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.5.cmml">c</mi><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1c" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.6" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.6.cmml">h</mi></mrow><mo id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.3" stretchy="false" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px4.p1.2.m2.1b"><apply id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1"><times id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.2">𝑀</ci><ci id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.3">𝑎</ci><ci id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.4.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.4">𝑡</ci><ci id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.5.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.5">𝑐</ci><ci id="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.6.cmml" xref="S3.SS1.SSS2.Px4.p1.2.m2.1.1.1.1.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px4.p1.2.m2.1c">[Match]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.Px4.p1.2.m2.1d">[ italic_M italic_a italic_t italic_c italic_h ]</annotation></semantics></math> for text-code matching, etc.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px5">
<h5 class="ltx_title ltx_title_paragraph">StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib137" title="">137</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px5.p1.1">A decoder-only model with SantaCoder architecture, employing Flash attention to scale up the context length to 8k. The StarCoder trains an encoder to filter names, emails, and other personal data from the training data. Its fine-tuned variant outperforms PaLM, LLaMA, and LAMDA on HumanEval and MBPP benchmarks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.5.1.1">III-A</span>3 </span>Scientific Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS3.Px1.p1.1">A large curated corpus of human scientific knowledge with 48 million papers, textbooks, lecture notes, millions of compounds and proteins, scientific websites, encyclopedias, and more are trained using metaseq library3, which is built on PyTorch and fairscale&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib139" title="">139</a>]</cite>. The model wraps reasoning datasets with <math alttext="&lt;work&gt;" class="ltx_Math" display="inline" id="S3.SS1.SSS3.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS3.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.2.cmml"><mo fence="true" id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.2" rspace="0em" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.2.cmml">w</mi><mo id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.3.cmml">o</mi><mo id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1a" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.4" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.4.cmml">r</mi><mo id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1b" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.5" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.5.cmml">k</mi></mrow><mo fence="true" id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.3" lspace="0em" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.2">expectation</csymbol><apply id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1"><times id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.2">𝑤</ci><ci id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.3">𝑜</ci><ci id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.4">𝑟</ci><ci id="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS1.SSS3.Px1.p1.1.m1.1.1.1.1.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.Px1.p1.1.m1.1c">&lt;work&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.Px1.p1.1.m1.1d">&lt; italic_w italic_o italic_r italic_k &gt;</annotation></semantics></math> token to provide step-by-step reasoning context to the model, which has been shown to improve the performance on reasoning tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.5.1.1">III-A</span>4 </span>Dialog</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS4.Px1">
<h5 class="ltx_title ltx_title_paragraph">LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib140" title="">140</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS4.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS4.Px1.p1.1">A decoder-only model pre-trained on public dialog data, public dialog utterances, and public web documents, where more than 90% of the pre-training data is in English. LaMDA is trained with the objective of producing responses that exhibit high levels of quality, safety, and groundedness. To achieve this, discriminative and generative fine-tuning techniques are incorporated to enhance the model’s safety and quality aspects. As a result, the LaMDA models can be utilized as a general language model performing various tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS5.5.1.1">III-A</span>5 </span>Finance</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS5.Px1">
<h5 class="ltx_title ltx_title_paragraph">BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib141" title="">141</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS5.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS5.Px1.p1.1">A non-causal decoder model trained using both financial ("FINPILE" from the Bloomberg archive) and general-purpose datasets. The model’s architecture is similar to the BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> and OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>]</cite>. It allocates 50B parameters to different blocks of the model using the approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib142" title="">142</a>]</cite>. For effective training, BloombergGPT packs documents together with <math alttext="&lt;|endoftext|&gt;" class="ltx_Math" display="inline" id="S3.SS1.SSS5.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS5.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS5.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.3.cmml"></mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.4" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.4.cmml">&lt;</mo><mrow id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.2.cmml"><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.2.cmml">e</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.3.cmml">n</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1a" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.4" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.4.cmml">d</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1b" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.5" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.5.cmml">o</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1c" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.6" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.6.cmml">f</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1d" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.7" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.7.cmml">t</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1e" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.8" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.8.cmml">e</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1f" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.9" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.9.cmml">x</mi><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1g" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.10" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.10.cmml">t</mi></mrow><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.5" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.5.cmml">&gt;</mo><mi id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.6" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1"><and id="S3.SS1.SSS5.Px1.p1.1.m1.1.1a.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1"></and><apply id="S3.SS1.SSS5.Px1.p1.1.m1.1.1b.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1"><lt id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.4"></lt><csymbol cd="latexml" id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.3">absent</csymbol><apply id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1"><abs id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.2.1.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.2"></abs><apply id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1"><times id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.2">𝑒</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.3">𝑛</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.4.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.4">𝑑</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.5.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.5">𝑜</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.6.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.6">𝑓</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.7.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.7">𝑡</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.8.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.8">𝑒</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.9.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.9">𝑥</ci><ci id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.10.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.1.1.10">𝑡</ci></apply></apply></apply><apply id="S3.SS1.SSS5.Px1.p1.1.m1.1.1c.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1"><gt id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.5.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.5"></gt><share href="#S3.SS1.SSS5.Px1.p1.1.m1.1.1.1.cmml" id="S3.SS1.SSS5.Px1.p1.1.m1.1.1d.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1"></share><csymbol cd="latexml" id="S3.SS1.SSS5.Px1.p1.1.m1.1.1.6.cmml" xref="S3.SS1.SSS5.Px1.p1.1.m1.1.1.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.Px1.p1.1.m1.1c">&lt;|endoftext|&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.Px1.p1.1.m1.1d">&lt; | italic_e italic_n italic_d italic_o italic_f italic_t italic_e italic_x italic_t | &gt;</annotation></semantics></math> to use maximum sequence length, use warmup batch size starting from 1024 to 2048, and manually reduces the learning rate multiple times during the training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS5.Px2">
<h5 class="ltx_title ltx_title_paragraph">Xuan Yuan 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib143" title="">143</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS5.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS5.Px2.p1.1">A Chinese financial chat model with BLOOM’s&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> architecture trained on a combination of general purpose, financial, general purpose instructions, and financial institutions datasets. Xuan Yuan 2.0 combined the pre-training and fine-tuning stages to avoid catastrophic forgetting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="406" id="S3.F10.g1" src="./A Comprehensive Overview of Large Language Models_files/pangu_sigma.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>This example illustrates the PanGu-<math alttext="\sum" class="ltx_Math" display="inline" id="S3.F10.2.m1.1"><semantics id="S3.F10.2.m1.1b"><mo id="S3.F10.2.m1.1.1" xref="S3.F10.2.m1.1.1.cmml">∑</mo><annotation-xml encoding="MathML-Content" id="S3.F10.2.m1.1c"><sum id="S3.F10.2.m1.1.1.cmml" xref="S3.F10.2.m1.1.1"></sum></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.2.m1.1d">\sum</annotation><annotation encoding="application/x-llamapun" id="S3.F10.2.m1.1e">∑</annotation></semantics></math> architecture, as depicted in the image sourced from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Noteworthy findings and insights from <em class="ltx_emph ltx_font_italic" id="S3.T1.3.1">pre-trained</em> Large Language Model.</figcaption><div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.2.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.1.2.1.1"><span class="ltx_text" id="S3.T1.1.2.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.2.1.2"><span class="ltx_text" id="S3.T1.1.2.1.2.1" style="background-color:#BFBFBF;">Findings &amp; Insights</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.3.1.1">
T5</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.3.1.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.3.1.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.1.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.3.1.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I1">
<span class="ltx_item" id="S3.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I1.i1.p1">
<span class="ltx_p" id="S3.I1.i1.p1.1">Encoder and decoder with shared parameters perform equivalently when parameters are not shared</span>
</span></span>
<span class="ltx_item" id="S3.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I1.i2.p1">
<span class="ltx_p" id="S3.I1.i2.p1.1">Fine-tuning model layers (adapter layers) work better than the conventional way of training on only classification layers</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.4.2.1">
GPT-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.2.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.4.2.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.4.2.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.2.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.4.2.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I2">
<span class="ltx_item" id="S3.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I2.i1.p1">
<span class="ltx_p" id="S3.I2.i1.p1.1">Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-learners</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.5.3.1">
mT5</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.5.3.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.5.3.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.5.3.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.3.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.5.3.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I3">
<span class="ltx_item" id="S3.I3.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I3.i1.p1">
<span class="ltx_p" id="S3.I3.i1.p1.1">Large multi-lingual models perform equivalently to single language models on downstream tasks. However, smaller multi-lingual models perform worse</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.1.1">
PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mi id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">italic_α</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.1.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.1.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I4">
<span class="ltx_item" id="S3.I4.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I4.i1.p1">
<span class="ltx_p" id="S3.I4.i1.p1.1">LLMs are good at a few shot capabilities</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.6.4.1">
CPM-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.4.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.6.4.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.6.4.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.4.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.6.4.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I5">
<span class="ltx_item" id="S3.I5.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I5.i1.p1">
<span class="ltx_p" id="S3.I5.i1.p1.1">Prompt fine-tuning requires updating very few parameters while achieving performance comparable to full model fine-tuning</span>
</span></span>
<span class="ltx_item" id="S3.I5.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I5.i2.p1">
<span class="ltx_p" id="S3.I5.i2.p1.1">Prompt fine-tuning takes more time to converge as compared to full model fine-tuning</span>
</span></span>
<span class="ltx_item" id="S3.I5.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I5.i3.p1">
<span class="ltx_p" id="S3.I5.i3.p1.1">Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences</span>
</span></span>
<span class="ltx_item" id="S3.I5.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I5.i4.p1">
<span class="ltx_p" id="S3.I5.i4.p1.1">In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator (aggregate information with the input text) for the model</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.7.5.1">
Codex</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.7.5.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.7.5.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.7.5.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.5.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.7.5.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I6">
<span class="ltx_item" id="S3.I6.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I6.i1.p1">
<span class="ltx_p" id="S3.I6.i1.p1.1">This LLM focuses on code evaluations and introduces a novel way of selecting the best code samples.</span>
</span></span>
<span class="ltx_item" id="S3.I6.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I6.i2.p1">
<span class="ltx_p" id="S3.I6.i2.p1.1">The results indicate it is possible to accurately select code samples using heuristic ranking in lieu of a detailed evaluation of each sample, which may not be feasible or feasible in some situations.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.8.6.1">
ERNIE 3.0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.8.6.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.8.6.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.8.6.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.6.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.8.6.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I7">
<span class="ltx_item" id="S3.I7.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I7.i1.p1">
<span class="ltx_p" id="S3.I7.i1.p1.1">ERNIE 3.0 shows that a modular LLM architecture with a universal representation module and task-specific representation module helps in finetuning phase.</span>
</span></span>
<span class="ltx_item" id="S3.I7.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I7.i2.p1">
<span class="ltx_p" id="S3.I7.i2.p1.1">Optimizing the parameters of a task-specific representation network during the fine-tuning phase is an efficient way to take advantage of the powerful pretrained model.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.9.7.1">
Jurassic-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.9.7.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.9.7.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.9.7.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.7.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.9.7.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I8">
<span class="ltx_item" id="S3.I8.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I8.i1.p1">
<span class="ltx_p" id="S3.I8.i1.p1.1">The performance of an LLM is highly related to the network size.</span>
</span></span>
<span class="ltx_item" id="S3.I8.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I8.i2.p1">
<span class="ltx_p" id="S3.I8.i2.p1.1">To improve runtime performance, more operations can be performed
in parallel (width) rather than sequentially (depth).</span>
</span></span>
<span class="ltx_item" id="S3.I8.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I8.i3.p1">
<span class="ltx_p" id="S3.I8.i3.p1.1">To efficiently represent and fit more text in the same context length, the model uses a larger vocabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This tokenizer improvement can further benefit few-shot learning tasks.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.10.8.1">
HyperCLOVA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.10.8.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.10.8.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.10.8.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.8.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.10.8.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I9">
<span class="ltx_item" id="S3.I9.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I9.i1.p1">
<span class="ltx_p" id="S3.I9.i1.p1.1">By employing prompt-based tuning, the performances of models can be improved, often surpassing those of state-of-the-art models when the backward gradients of inputs are accessible.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.11.9.1">
Yuan 1.0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.11.9.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.11.9.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.11.9.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.9.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.11.9.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I10">
<span class="ltx_item" id="S3.I10.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I10.i1.p1">
<span class="ltx_p" id="S3.I10.i1.p1.1">The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.12.10.1">
Gopher</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.12.10.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.12.10.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.12.10.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.10.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.12.10.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I11">
<span class="ltx_item" id="S3.I11.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I11.i1.p1">
<span class="ltx_p" id="S3.I11.i1.p1.1">Relative encodings enable models to be evaluated for longer sequences than those on which it was trained.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.13.11.1">
ERNIE 3.0 Titan</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.13.11.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.13.11.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.13.11.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.11.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.13.11.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I12">
<span class="ltx_item" id="S3.I12.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I12.i1.p1">
<span class="ltx_p" id="S3.I12.i1.p1.1">This LLM builds on top of ERNIE 3.0 and add a self-supervised adversarial loss to distinguish whether a text is generated or the original one.</span>
</span></span>
<span class="ltx_item" id="S3.I12.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I12.i2.p1">
<span class="ltx_p" id="S3.I12.i2.p1.1">This distinction ability between real and generate text improves the LLM’s performance as compared to ERNIE 3.0.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.14.12.1">
GPT-NeoX-20B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.14.12.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.14.12.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.14.12.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.12.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.14.12.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I13">
<span class="ltx_item" id="S3.I13.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I13.i1.p1">
<span class="ltx_p" id="S3.I13.i1.p1.1">Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded layers</span>
</span></span>
<span class="ltx_item" id="S3.I13.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I13.i2.p1">
<span class="ltx_p" id="S3.I13.i2.p1.1">Initializing feed-forward output layers before residuals with scheme in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib144" title="">144</a>]</cite> avoids activations from growing with increasing depth and width</span>
</span></span>
<span class="ltx_item" id="S3.I13.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I13.i3.p1">
<span class="ltx_p" id="S3.I13.i3.p1.1">Training on Pile outperforms GPT-3 on five-shot</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.15.13.1">
OPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.15.13.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.15.13.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.15.13.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.13.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.15.13.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I14">
<span class="ltx_item" id="S3.I14.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I14.i1.p1">
<span class="ltx_p" id="S3.I14.i1.p1.1">Restart training from an earlier checkpoint with a lower learning rate if loss diverges</span>
</span></span>
<span class="ltx_item" id="S3.I14.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I14.i2.p1">
<span class="ltx_p" id="S3.I14.i2.p1.1">Model is prone to generate repetitive text and stuck in a loop</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.16.14.1">
BLOOM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.16.14.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.16.14.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.16.14.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.16.14.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.16.14.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I15">
<span class="ltx_item" id="S3.I15.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I15.i1.p1">
<span class="ltx_p" id="S3.I15.i1.p1.1">None</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.17.15.1">
Galactica</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.17.15.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.17.15.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.17.15.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.17.15.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.17.15.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I16">
<span class="ltx_item" id="S3.I16.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I16.i1.p1">
<span class="ltx_p" id="S3.I16.i1.p1.1">Galactica’s performance has continued to improve across validation set, in-domain, and out-of-domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing research on LLMs.</span>
</span></span>
<span class="ltx_item" id="S3.I16.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I16.i2.p1">
<span class="ltx_p" id="S3.I16.i2.p1.1">A working memory token approach can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%).</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.18.16.1">
GLaM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.18.16.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.18.16.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.18.16.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.16.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.18.16.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I17">
<span class="ltx_item" id="S3.I17.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I17.i1.p1">
<span class="ltx_p" id="S3.I17.i1.p1.1">The feed-forward component of each Transformer layer can be replaced with a mixture-of-experts (MoE) module consisting of a set of independent feed-forward networks (<em class="ltx_emph ltx_font_italic" id="S3.I17.i1.p1.1.1">i.e.</em>, the ‘experts’). By sparsely activating these experts, the model capacity can be maintained while much computation is saved.</span>
</span></span>
<span class="ltx_item" id="S3.I17.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I17.i2.p1">
<span class="ltx_p" id="S3.I17.i2.p1.1">By leveraging sparsity, we can make significant strides toward developing high-quality NLP models while simultaneously reducing energy consumption. Consequently, MoE emerges as a robust candidate for future scaling endeavors.</span>
</span></span>
<span class="ltx_item" id="S3.I17.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I17.i3.p1">
<span class="ltx_p" id="S3.I17.i3.p1.1">The model trained on filtered data shows consistently better performances on both NLG and NLU tasks, where the effect of filtering is more significant on the former tasks.</span>
</span></span>
<span class="ltx_item" id="S3.I17.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I17.i4.p1">
<span class="ltx_p" id="S3.I17.i4.p1.1">Filtered pretraining corpora plays a crucial role in the generation capability of LLMs, especially for the downstream tasks.</span>
</span></span>
<span class="ltx_item" id="S3.I17.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I17.i5.p1">
<span class="ltx_p" id="S3.I17.i5.p1.1">The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given a fixed budget of computation, more experts contribute to better predictions.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.19.17.1">
LaMDA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.19.17.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.19.17.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.19.17.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.19.17.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.19.17.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I18">
<span class="ltx_item" id="S3.I18.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I18.i1.p1">
<span class="ltx_p" id="S3.I18.i1.p1.1">The model can be fine-tuned to learn to call different external information resources and tools.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.20.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.20.18.1">
MT-NLG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.20.18.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.20.18.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.20.18.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.20.18.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.20.18.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I19">
<span class="ltx_item" id="S3.I19.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I19.i1.p1">
<span class="ltx_p" id="S3.I19.i1.p1.1">None.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.21.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.T1.1.21.19.1">
AlphaCode</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T1.1.21.19.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.21.19.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.1.21.19.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.21.19.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T1.1.21.19.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I20">
<span class="ltx_item" id="S3.I20.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I20.i1.p1">
<span class="ltx_p" id="S3.I20.i1.p1.1">For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed with a shallower encoder and a deeper decoder.</span>
</span></span>
<span class="ltx_item" id="S3.I20.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I20.i2.p1">
<span class="ltx_p" id="S3.I20.i2.p1.1">To achieve better performances, it is necessary to employ strategies such as massively scaling up sampling, followed by the filtering and clustering of samples into a compact set.</span>
</span></span>
<span class="ltx_item" id="S3.I20.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I20.i3.p1">
<span class="ltx_p" id="S3.I20.i3.p1.1">The utilization of novel sampling-efficient transformer architectures designed to facilitate large-scale sampling is crucial.</span>
</span></span>
<span class="ltx_item" id="S3.I20.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I20.i4.p1">
<span class="ltx_p" id="S3.I20.i4.p1.1">Simplifying problem descriptions can effectively improve the model’s performance.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_right" id="S3.T1.4">Table Continued on Next Page</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.SS1.SSS5.Px2.1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.2.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.SS1.SSS5.Px2.1.1.2.1.1"><span class="ltx_text" id="S3.SS1.SSS5.Px2.1.1.2.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.SS1.SSS5.Px2.1.1.2.1.2"><span class="ltx_text" id="S3.SS1.SSS5.Px2.1.1.2.1.2.1" style="background-color:#BFBFBF;">Findings &amp; Insights</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.SS1.SSS5.Px2.1.1.3.1.1">
Chinchilla</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.SS1.SSS5.Px2.1.1.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.3.1.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.3.1.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.3.1.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.3.1.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I21">
<span class="ltx_item" id="S3.I21.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I21.i1.p1">
<span class="ltx_p" id="S3.I21.i1.p1.1">The experiments that culminated in the development of Chinchilla determined that for optimal computation during training, the model size and the number of training tokens should be scaled proportionately: for each doubling of the model size, the number of training tokens should be doubled as well.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.4.2.1">
PaLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.4.2.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.4.2.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.4.2.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.4.2.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.4.2.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I22">
<span class="ltx_item" id="S3.I22.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I22.i1.p1">
<span class="ltx_p" id="S3.I22.i1.p1.1">English-centric models produce better translations when translating to English as compared to non-English</span>
</span></span>
<span class="ltx_item" id="S3.I22.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I22.i2.p1">
<span class="ltx_p" id="S3.I22.i2.p1.1">Generalized models can have equivalent performance for language translation to specialized small models</span>
</span></span>
<span class="ltx_item" id="S3.I22.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I22.i3.p1">
<span class="ltx_p" id="S3.I22.i3.p1.1">Larger models have a higher percentage of training data memorization</span>
</span></span>
<span class="ltx_item" id="S3.I22.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I22.i4.p1">
<span class="ltx_p" id="S3.I22.i4.p1.1">Performance has not yet saturated even at 540B scale, which means larger models are likely to perform better</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.5.3.1">
AlexaTM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.5.3.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.5.3.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.5.3.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.5.3.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.5.3.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I23">
<span class="ltx_item" id="S3.I23.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I23.i1.p1">
<span class="ltx_p" id="S3.I23.i1.p1.1">Compared to commonly used Decoder-only Transformer models, seq2seq architecture is more suitable for training generative LLMs given stronger bidirectional attention to the context.</span>
</span></span>
<span class="ltx_item" id="S3.I23.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I23.i2.p1">
<span class="ltx_p" id="S3.I23.i2.p1.1">An extra Causal Language Modeling (CLM) task can be added to benefit the model with a more efficient in-context learning, especially for few-shot learning tasks.</span>
</span></span>
<span class="ltx_item" id="S3.I23.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I23.i3.p1">
<span class="ltx_p" id="S3.I23.i3.p1.1">The key to training powerful seq2seq-based LLMs lies in mixed pre-training, rather than additional multitask training.</span>
</span></span>
<span class="ltx_item" id="S3.I23.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I23.i4.p1">
<span class="ltx_p" id="S3.I23.i4.p1.1">Placing layernorms at the beginning of each transformer layer can improve the training stability of large models.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.6.4.1">
U-PaLM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.6.4.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.6.4.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.6.4.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.6.4.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.6.4.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I24">
<span class="ltx_item" id="S3.I24.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I24.i1.p1">
<span class="ltx_p" id="S3.I24.i1.p1.1">Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs</span>
</span></span>
<span class="ltx_item" id="S3.I24.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I24.i2.p1">
<span class="ltx_p" id="S3.I24.i2.p1.1">Training with a mixture of denoisers improves the infilling ability and open-ended text generation diversity</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.7.5.1">
UL2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.7.5.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.7.5.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.7.5.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.7.5.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.7.5.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I25">
<span class="ltx_item" id="S3.I25.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I25.i1.p1">
<span class="ltx_p" id="S3.I25.i1.p1.1">Mode switching training enables better performance on downstream tasks</span>
</span></span>
<span class="ltx_item" id="S3.I25.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I25.i2.p1">
<span class="ltx_p" id="S3.I25.i2.p1.1">CoT prompting outperforms standard prompting for UL2</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.8.6.1">
GLM-130B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.8.6.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.8.6.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.8.6.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.8.6.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.8.6.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I26">
<span class="ltx_item" id="S3.I26.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I26.i1.p1">
<span class="ltx_p" id="S3.I26.i1.p1.1">Pre-training data with a small proportion of multi-task instruction data improves the overall model performance</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.9.7.1">
CodeGen</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.9.7.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.9.7.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.9.7.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.9.7.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.9.7.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I27">
<span class="ltx_item" id="S3.I27.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I27.i1.p1">
<span class="ltx_p" id="S3.I27.i1.p1.1">Multi-step prompting for code synthesis leads to a better user intent understanding and code generation</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.10.8.1">
LLaMA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.10.8.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.10.8.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.10.8.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.10.8.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.10.8.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I28">
<span class="ltx_item" id="S3.I28.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I28.i1.p1">
<span class="ltx_p" id="S3.I28.i1.p1.1">LLaMA is open-source and can be fine-tuned or continually pre-trained to develop new models or instruction-based tools.</span>
</span></span>
<span class="ltx_item" id="S3.I28.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I28.i2.p1">
<span class="ltx_p" id="S3.I28.i2.p1.1">A few optimizations are proposed to improve the training efficiency of LLaMA, such as efficient implementation of multi-head self-attention and a reduced amount of activations during back-propagation.</span>
</span></span>
<span class="ltx_item" id="S3.I28.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I28.i3.p1">
<span class="ltx_p" id="S3.I28.i3.p1.1">Training exclusively on public data can also achieve state-of-the-art performance.</span>
</span></span>
<span class="ltx_item" id="S3.I28.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I28.i4.p1">
<span class="ltx_p" id="S3.I28.i4.p1.1">A constant performance improvement is gained when scaling the model.</span>
</span></span>
<span class="ltx_item" id="S3.I28.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I28.i5.p1">
<span class="ltx_p" id="S3.I28.i5.p1.1">Smaller models can also realize good performances using more training data and time.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.1.1">
PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1"><semantics id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1a"><mi id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.SSS5.Px2.1.1.1.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1b"><ci id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS5.Px2.1.1.1.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.Px2.1.1.1.1.m1.1d">roman_Σ</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.1.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.1.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.1.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I29">
<span class="ltx_item" id="S3.I29.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I29.i1.p1">
<span class="ltx_p" id="S3.I29.i1.p1.1">Sparse models provide the benefits of large models at a lower computation cost</span>
</span></span>
<span class="ltx_item" id="S3.I29.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I29.i2.p1">
<span class="ltx_p" id="S3.I29.i2.p1.1">Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for continual learning</span>
</span></span>
<span class="ltx_item" id="S3.I29.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I29.i3.p1">
<span class="ltx_p" id="S3.I29.i3.p1.1">Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is cost-efficient while maintaining a performance similar to the original</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.11.9.1">
BloombergGPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.11.9.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.11.9.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.11.9.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.11.9.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.11.9.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I30">
<span class="ltx_item" id="S3.I30.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I30.i1.p1">
<span class="ltx_p" id="S3.I30.i1.p1.1">Pre-training with general-purpose and task-specific data improves task performance without hurting other model capabilities</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.12.10.1">
XuanYuan 2.0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.12.10.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.12.10.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.12.10.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.12.10.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.12.10.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I31">
<span class="ltx_item" id="S3.I31.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I31.i1.p1">
<span class="ltx_p" id="S3.I31.i1.p1.1">Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.13.11.1">
CodeT5+</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.13.11.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.13.11.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.13.11.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.13.11.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.13.11.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I32">
<span class="ltx_item" id="S3.I32.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I32.i1.p1">
<span class="ltx_p" id="S3.I32.i1.p1.1">Causal LM is crucial for a model’s generation capability in encoder-decoder architectures</span>
</span></span>
<span class="ltx_item" id="S3.I32.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I32.i2.p1">
<span class="ltx_p" id="S3.I32.i2.p1.1">Multiple training objectives like span corruption, Causal LM, matching, etc complement each other for better performance</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.14.12.1">
StarCoder</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.14.12.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.14.12.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.14.12.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.14.12.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.14.12.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I33">
<span class="ltx_item" id="S3.I33.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I33.i1.p1">
<span class="ltx_p" id="S3.I33.i1.p1.1">HHH prompt by Anthropic allows the model to follow instructions without fine-tuning</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS1.SSS5.Px2.1.1.15.13.1">
LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.15.13.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.15.13.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.15.13.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.15.13.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.15.13.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I34">
<span class="ltx_item" id="S3.I34.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I34.i1.p1">
<span class="ltx_p" id="S3.I34.i1.p1.1">Model trained on unfiltered data is more toxic but may perform better on downstream tasks after fine-tuning</span>
</span></span>
<span class="ltx_item" id="S3.I34.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I34.i2.p1">
<span class="ltx_p" id="S3.I34.i2.p1.1">Model trained on unfiltered data requires fewer samples for safety alignment</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.SS1.SSS5.Px2.1.1.16.14.1">
PaLM-2</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS1.SSS5.Px2.1.1.16.14.2">
<table class="ltx_tabular ltx_align_middle" id="S3.SS1.SSS5.Px2.1.1.16.14.2.1">
<tbody><tr class="ltx_tr" id="S3.SS1.SSS5.Px2.1.1.16.14.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.SS1.SSS5.Px2.1.1.16.14.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.SS1.SSS5.Px2.1.1.16.14.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I35">
<span class="ltx_item" id="S3.I35.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I35.i1.p1">
<span class="ltx_p" id="S3.I35.i1.p1.1">Data quality is important to train better models</span>
</span></span>
<span class="ltx_item" id="S3.I35.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I35.i2.p1">
<span class="ltx_p" id="S3.I35.i2.p1.1">Model and data size should be scaled with 1:1 proportions</span>
</span></span>
<span class="ltx_item" id="S3.I35.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I35.i3.p1">
<span class="ltx_p" id="S3.I35.i3.p1.1">Smaller models trained for larger iterations outperform larger models</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Key insights and findings from the study of <em class="ltx_emph ltx_font_italic" id="S3.T2.2.1">instruction-tuned</em> Large Language Models. </figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.3.1.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.3.1.1.1"><span class="ltx_text" id="S3.T2.3.1.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.3.1.1.2"><span class="ltx_text" id="S3.T2.3.1.1.2.1" style="background-color:#BFBFBF;">Findings &amp; Insights</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.3.2.1.1">
T0</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.2.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.2.1.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.2.1.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.2.1.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.2.1.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I36">
<span class="ltx_item" id="S3.I36.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I36.i1.p1">
<span class="ltx_p" id="S3.I36.i1.p1.1">Multi-task prompting enables zero-shot generalization and outperforms baselines</span>
</span></span>
<span class="ltx_item" id="S3.I36.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I36.i2.p1">
<span class="ltx_p" id="S3.I36.i2.p1.1">Even a single prompt per dataset task is enough to improve performance</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.3.2.1">
WebGPT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.3.2.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.2.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.3.2.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.3.2.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.3.2.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I37">
<span class="ltx_item" id="S3.I37.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I37.i1.p1">
<span class="ltx_p" id="S3.I37.i1.p1.1">The answer quality of LLMs can be further improved with human feedback.</span>
</span></span>
<span class="ltx_item" id="S3.I37.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I37.i2.p1">
<span class="ltx_p" id="S3.I37.i2.p1.1">To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering questions regarding the usefulness of the retrieved documents.</span>
</span></span>
<span class="ltx_item" id="S3.I37.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I37.i3.p1">
<span class="ltx_p" id="S3.I37.i3.p1.1">Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning.</span>
</span></span>
<span class="ltx_item" id="S3.I37.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I37.i4.p1">
<span class="ltx_p" id="S3.I37.i4.p1.1">Generating answers with references can make labelers easily judge the factual accuracy of answers.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.4.3.1">
Tk-INSTRUCT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.4.3.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.4.3.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.4.3.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.4.3.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.4.3.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I38">
<span class="ltx_item" id="S3.I38.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I38.i1.p1">
<span class="ltx_p" id="S3.I38.i1.p1.1">Instruction tuning leads to a stronger generalization of unseen tasks</span>
</span></span>
<span class="ltx_item" id="S3.I38.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I38.i2.p1">
<span class="ltx_p" id="S3.I38.i2.p1.1">More tasks improve generalization whereas only increasing task instances does not help</span>
</span></span>
<span class="ltx_item" id="S3.I38.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I38.i3.p1">
<span class="ltx_p" id="S3.I38.i3.p1.1">Supervised trained models are better than generalized models</span>
</span></span>
<span class="ltx_item" id="S3.I38.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I38.i4.p1">
<span class="ltx_p" id="S3.I38.i4.p1.1">Models pre-trained with instructions and examples perform well for different types of inputs</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.5.4.1">
mT0 and BLOOMZ</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.4.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.5.4.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.5.4.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.5.4.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.5.4.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I39">
<span class="ltx_item" id="S3.I39.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I39.i1.p1">
<span class="ltx_p" id="S3.I39.i1.p1.1">Instruction tuning enables zero-shot generalization to the tasks never seen before</span>
</span></span>
<span class="ltx_item" id="S3.I39.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I39.i2.p1">
<span class="ltx_p" id="S3.I39.i2.p1.1">Multi-lingual training leads to even better zero-shot generalization for both English and non-English</span>
</span></span>
<span class="ltx_item" id="S3.I39.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I39.i3.p1">
<span class="ltx_p" id="S3.I39.i3.p1.1">Training on machine-translated prompts improves performance for held-out tasks with non-English prompts</span>
</span></span>
<span class="ltx_item" id="S3.I39.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I39.i4.p1">
<span class="ltx_p" id="S3.I39.i4.p1.1">English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.6.5.1">
OPT-IML</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.5.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.6.5.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.6.5.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.6.5.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.6.5.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I40">
<span class="ltx_item" id="S3.I40.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i1.p1">
<span class="ltx_p" id="S3.I40.i1.p1.1">Task size sampling to create a batch with most of the task examples is important for better performance</span>
</span></span>
<span class="ltx_item" id="S3.I40.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i2.p1">
<span class="ltx_p" id="S3.I40.i2.p1.1">Only example proportional sampling is not enough, training datasets/benchmarks should also be proportional for better generalization/performance</span>
</span></span>
<span class="ltx_item" id="S3.I40.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i3.p1">
<span class="ltx_p" id="S3.I40.i3.p1.1">Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no effect</span>
</span></span>
<span class="ltx_item" id="S3.I40.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i4.p1">
<span class="ltx_p" id="S3.I40.i4.p1.1">Including small amounts i.e. 5% of pretraining data during fine-tuning is effective</span>
</span></span>
<span class="ltx_item" id="S3.I40.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i5.p1">
<span class="ltx_p" id="S3.I40.i5.p1.1">Only 1% reasoning data improves the performance, adding more deteriorates performance</span>
</span></span>
<span class="ltx_item" id="S3.I40.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I40.i6.p1">
<span class="ltx_p" id="S3.I40.i6.p1.1">Adding dialogue data makes the performance worse</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.7.6.1">
Flan</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.7.6.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.7.6.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.7.6.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.7.6.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.7.6.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I41">
<span class="ltx_item" id="S3.I41.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i1.p1">
<span class="ltx_p" id="S3.I41.i1.p1.1">Finetuning with CoT improves performance on held-out tasks</span>
</span></span>
<span class="ltx_item" id="S3.I41.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i2.p1">
<span class="ltx_p" id="S3.I41.i2.p1.1">Fine-tuning along with CoT data improves reasoning abilities</span>
</span></span>
<span class="ltx_item" id="S3.I41.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i3.p1">
<span class="ltx_p" id="S3.I41.i3.p1.1">CoT tuning improves zero-shot reasoning</span>
</span></span>
<span class="ltx_item" id="S3.I41.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i4.p1">
<span class="ltx_p" id="S3.I41.i4.p1.1">Performance improves with more tasks</span>
</span></span>
<span class="ltx_item" id="S3.I41.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i5.p1">
<span class="ltx_p" id="S3.I41.i5.p1.1">Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models</span>
</span></span>
<span class="ltx_item" id="S3.I41.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i6.p1">
<span class="ltx_p" id="S3.I41.i6.p1.1">Improving the model’s performance with instruction tuning is compute-efficient</span>
</span></span>
<span class="ltx_item" id="S3.I41.i7" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I41.i7.p1">
<span class="ltx_p" id="S3.I41.i7.p1.1">Multitask prompting enables zero-shot generalization abilities in LLM</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.8.7.1">
Sparrow</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.8.7.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.8.7.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.8.7.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.8.7.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.8.7.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I42">
<span class="ltx_item" id="S3.I42.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I42.i1.p1">
<span class="ltx_p" id="S3.I42.i1.p1.1">The judgments of labelers and the alignments with defined rules can help the model generate better responses.</span>
</span></span>
<span class="ltx_item" id="S3.I42.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I42.i2.p1">
<span class="ltx_p" id="S3.I42.i2.p1.1">Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.</span>
</span></span>
<span class="ltx_item" id="S3.I42.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I42.i3.p1">
<span class="ltx_p" id="S3.I42.i3.p1.1">The combination of reinforcement learning (RL) with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing.</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.9.8.1">
WizardCoder</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.9.8.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.9.8.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.9.8.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.9.8.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.9.8.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I43">
<span class="ltx_item" id="S3.I43.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I43.i1.p1">
<span class="ltx_p" id="S3.I43.i1.p1.1">Fine-tuning with re-written instruction-tuning data into a complex set improves the performance significantly</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.10.9.1">
LLaMA-2-Chat</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.10.9.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.10.9.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.10.9.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.10.9.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.10.9.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I44">
<span class="ltx_item" id="S3.I44.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I44.i1.p1">
<span class="ltx_p" id="S3.I44.i1.p1.1">Model learns to write safe responses with fine-tuning on safe demonstrations, while additional RLHF step further improves model safety and make it less prone to jailbreak attacks</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.3.11.10.1">
LIMA</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.3.11.10.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.11.10.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.3.11.10.2.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.3.11.10.2.1.1.1">
<p class="ltx_p ltx_align_top" id="S3.T2.3.11.10.2.1.1.1.1">
<span class="ltx_itemize" id="S3.I45">
<span class="ltx_item" id="S3.I45.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="S3.I45.i1.p1">
<span class="ltx_p" id="S3.I45.i1.p1.1">Less high quality data is enough for fine-tuned model generalization</span>
</span></span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Fine-Tuned LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Pre-trained LLMs have excellent generalization abilities to unseen tasks. However, because they are generally trained with the objective of next token prediction, LLMs have limited capacity to follow user intent and are prone to generate unethical, toxic or inaccurate responses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>]</cite>. For their effective utilization, LLMs are fine-tuned to follow instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite> and generate safe responses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>]</cite>, which also results in increasing zero-shot, few-shot, and cross-task generalization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite>, with minimal compute increment, e.g., 0.2% of the total pre-training for PaLM 540B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>. 
<br class="ltx_break">We review various fine-tuned LLMs and strategies for effective fine-tuning in this section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Instruction-Tuning with Manually Created Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Numerous hand-crafted instruction-tuning datasets with different design choices are proposed in the literature to instruction-tune LLMs. The performance of fine-tuned LLMs depends on multiple factors, such as dataset, instruction diversity, prompting templates, model size, and training objectives. Keeping this in view, diverse fine-tuned models have emerged in the literature using manually created datasets. 
<br class="ltx_break">The models T0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>]</cite> and mT0 (multi-lingual)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib145" title="">145</a>]</cite> employ templates to convert existing datasets into prompt datasets. They have shown improvements in generalization to zero-shot and held-out tasks. Tk-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite> fine-tuned the T5 model with in-context instructions to study generalization on unseen tasks when given in-context instructions during test time. The model outperformed Instruct-GPT, despite being smaller in size, i.e., 11B parameters as compared to 175B of GPT-3. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">Increasing Tasks and Prompt Setups:</em>
Zero-shot and few-shot performance improves significantly by expanding task collection and prompt styles. OPT-IML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite> and Flan&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite> curated larger 2k and 1.8k task datasets, respectively. While increasing task size alone is not enough, OPT-IML and Flan add more prompting setups in their datasets, zero-shot, few-shot, and CoT. In continuation, CoT Collection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib97" title="">97</a>]</cite> fine-tunes Flan-T5 further on 1.88M CoT samples. Another method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib98" title="">98</a>]</cite> uses symbolic tasks with tasks in T0, Flan, etc. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="298" id="S3.F11.g1" src="./A Comprehensive Overview of Large Language Models_files/Flan.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>An example image shows an instance of the Flan training paradigm, taken from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Instruction-Tuning with LLMs Generated Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Generating an instruction-tuning dataset requires carefully writing instructions and input-output pairs, which are often written by humans, smaller in size, and less diverse. To overcome this, self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib19" title="">19</a>]</cite> proposed an approach to prompt available LLMs to generate instruction-tuning datasets. Self-instruct outperformed models trained on manually created dataset SUPER-NATURALINSTRUCTIONS (a dataset with 1600+ tasks)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite> by 33%. It starts with a seed of 175 tasks, 1 instruction, and 1 sample per task and iteratively generates new instructions (52k) and instances (82k input-output pairs) using GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>. Contrary to this, Dynosaur&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib146" title="">146</a>]</cite> uses the meta-data of datasets on Huggingface to prompt LLMs to generate multiple task instruction-tuning datasets. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">LLaMA Tuned:</em> Various models in literature instruction-tune LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib147" title="">147</a>]</cite> with GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> or GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib148" title="">148</a>]</cite> generated datasets. Among these, Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib149" title="">149</a>]</cite>, Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib150" title="">150</a>]</cite>, and LLaMA-GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib151" title="">151</a>]</cite> are a few general-purpose fine-tuned models, where Alpaca is trained on 52k samples from text-davinci-003, Vicuna on 70k samples from ShareGPT.com, and LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-4. Goat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib152" title="">152</a>]</cite> fine-tunes LLaMA for arithmetic tasks (1 million samples) by generating data from ChatGPT and outperforms GPT-4, PaLM, BLOOM, OPT, etc, attributing its success to the LLaMA’s consistent tokenization of numbers. HuaTuo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib153" title="">153</a>]</cite> is a medical knowledge model, fine-tuned with a generated QA dataset of 8k instructions. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS2.p1.1.2">Complex Instructions:</em> Evol-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib155" title="">155</a>]</cite> prompts LLMs to convert given instructions into a more complex set. The instructions are iteratively evolved with re-writing instructions in complex wording and creating new instructions. With this style of automated instruction generation, WizardLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib154" title="">154</a>]</cite> (fine-tuned LLaMA on 250k instructions), outperforms Vicuna and Alpaca, and WizardCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib155" title="">155</a>]</cite> (fine-tuned StarCoder) beats Claude-Plus, Bard, and others. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS3.5.1.1">III-B</span>3 </span>Aligning with Human Preferences</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Incorporating human preferences into LLMs presents a significant advantage in mitigating undesirable behaviors and ensuring accurate outputs. The initial work on alignment, such as InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>]</cite> aligns GPT-3 using a 3-step approach, instruction-tuning, reward modeling, and fine-tuning with reinforcement learning (RL). The supervised fine-tuned GPT-3 on demonstrations is queried to generate responses, which human labelers rank according to human values, and a reward model is trained on the ranked data. Lastly, the GPT-3 is trained with proximal policy optimization (PPO) using rewards on the generated data from the reward model. LLaMA 2-Chat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite> improves alignment by dividing reward modeling into helpfulness and safety rewards and using rejection sampling in addition to PPO. The initial four versions of LLaMA 2-Chat are fine-tuned with rejection sampling and then with PPO on top of rejection sampling. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.1">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p1.1.1.1">Aligning with Supported Evidence:</span></em> This style of alignment allows the model to generate responses with proofs and facts, reduces hallucination, and assists humans more effectively, which increases trust in the model’s output. Similar to the RLHF training style, a reward model is trained to rank generated responses containing web citations in answers to questions, which is later used to train the model, as in GopherCite&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib156" title="">156</a>]</cite>, WebGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib157" title="">157</a>]</cite>, and Sparrow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib158" title="">158</a>]</cite>. The ranking model in Sparrow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib158" title="">158</a>]</cite> is divided into two branches, preference reward and rule reward, where human annotators adversarial probe the model to break a rule. These two rewards together rank a response to train with RL. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.2">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p1.1.2.1">Aligning Directly with SFT:</span></em> The PPO in the RLHF pipeline is complex, memory-intensive, and unstable, requiring multiple models, reward, value, policy, and reference models. Avoiding this sophisticated alignment pipeline is possible by incorporating minimal changes in the supervised fine-tuning (SFT) pipeline
as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib159" title="">159</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib160" title="">160</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib161" title="">161</a>]</cite>, with better or comparable performance to PPO. Direct preference optimization (DPO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib159" title="">159</a>]</cite> trains a model directly on the human-preferred responses to maximize the likelihood of preferred against unpreferred responses, with per-sample importance weight. Reward ranked fine-tuning RAFT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib160" title="">160</a>]</cite> fine-tunes the model on ranked responses by the reward model. Preference ranking optimization (PRO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib162" title="">162</a>]</cite> and RRHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib161" title="">161</a>]</cite> penalize the model to rank responses with human preferences and supervised loss. On the other hand, chain-of-hindsight (CoH)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib163" title="">163</a>]</cite> provides feedback to the model in language rather than reward, to learn good versus bad responses.
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS3.p1.1.3">Aligning with Synthetic Feedback:</em> Aligning LLMs with human feedback is slow and costly. The literature suggests a semi-automated process to align LLMs by prompting LLMs to generate helpful, honest, and ethical responses to the queries, and fine-tuning using the newly created dataset. Constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib164" title="">164</a>]</cite> replaces human feedback in RLHF with AI, calling it RL from AI feedback (RLAIF). AlpacaFarm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib165" title="">165</a>]</cite> designs prompts to imitate human feedback using LLMs APIs. Opposite to constitutional AI, AlpacaFarm injects noise in feedback to replicate human mistakes. Self-Align&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib94" title="">94</a>]</cite> prompts the LLM with ICL examples, instructing the LLM about what the response should contain to be considered useful and ethical. The same LLM is later fine-tuned with the new dataset. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.4">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p1.1.4.1">Aligning with Prompts:</span></em> LLMs can be steered with prompts to generate desirable responses without training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib167" title="">167</a>]</cite>. The self-correction prompting in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib167" title="">167</a>]</cite> concatenates instructions and CoT with questions, guiding the model to answer its instruction following strategy to ensure moral safety before the actual answer. This strategy is shown to reduce the harm in generated responses significantly. 
<br class="ltx_break">&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p1.1.5">&nbsp;<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p1.1.5.1">Red-Teaming/Jailbreaking/Adversarial Attacks:</span></em> LLMs exhibit harmful behaviors, hallucinations, leaking personal information, and other shortcomings through adversarial probing. The models are susceptible to generating harmful responses even though they are aligned for safety&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib168" title="">168</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib169" title="">169</a>]</cite>. Red-teaming is a common approach to address illicit outputs, where the LLMs are prompted to generate harmful outputs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib170" title="">170</a>]</cite>. The dataset collected through red-teaming is used to fine-tune models for safety. While red-teaming largely relies on human annotators, another work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib171" title="">171</a>]</cite> red-team LLMs to find prompts that lead to harmful outputs of other LLMs. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS4.5.1.1">III-B</span>4 </span>Continue Pre-Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">Although fine-tuning boosts a model’s performance, it leads to catastrophic forgetting of previously learned information. Concatenating fine-tuning data with a few randomly selected pre-training samples in every iteration avoids network forgetting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib143" title="">143</a>]</cite>. This is also effective in adapting LLMs for cases where fine-tuning data is small and the original capacity is to be maintained. Prompt-based continued pre-training (PCP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib173" title="">173</a>]</cite> trains the model with text and instructions related to tasks and then finally instruction-tunes the model for downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS5.5.1.1">III-B</span>5 </span>Sample Efficiency</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1">While fine-tuning data is generally many-fold smaller than the pre-training data, it still has to be large enough for acceptable performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite> and requires proportional computing resources. To study the effects on performance with less data, existing literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib174" title="">174</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib175" title="">175</a>]</cite> finds that the models trained on lesser data can outperform models trained with more data. In&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib174" title="">174</a>]</cite>, 25% of the total downstream data is found enough for state-of-the-art performance.
Selecting coreset-based 0.5% of the total instruction-tuning data improves the model performance by 2% in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib175" title="">175</a>]</cite>, as compared to the complete data tuning. Less is more for alignment (LIMA)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib176" title="">176</a>]</cite> uses only 1000 carefully created demonstrations to fine-tune the model and has achieved comparable performance to GPT-4.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Increasing Context Window</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">LLMs are trained with limited context windows due to expensive attention and high memory requirements. A model trained on limited sequence lengths fails to generalize to unseen lengths at inference time&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib177" title="">177</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib45" title="">45</a>]</cite>. Alternatively, LLMs with ALiBi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib65" title="">65</a>]</cite> positional encodings can perform zero-shot length extrapolation. However, ALiBi has less expressive power&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib66" title="">66</a>]</cite> and inferior performance on multiple benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib42" title="">42</a>]</cite>, and many LLMs use RoPE positional embedding that is unable to perform zero-shot extrapolation. A larger context length has benefits such as a better understanding of longer documents, more samples in in-context learning, execution of bigger reasoning processes, etc. Expanding context length during fine-tuning is slow, inefficient, and computationally expensive&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib45" title="">45</a>]</cite>. Therefore, researchers employ various context window extrapolation techniques discussed below. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS3.p1.1.1">Position Interpolation:</em>
Rather than extrapolating, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib45" title="">45</a>]</cite> shows that interpolating position encodings within the pre-trained context window are more effective. The work demonstrates that only 1000 steps of fine-tuning are enough to achieve better results on larger windows without performance loss compared to the original context size. Giraffe&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib42" title="">42</a>]</cite> uses power scaling in RoPE, and YaRN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib43" title="">43</a>]</cite> proposed NTK-aware interpolation. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS3.p1.1.2">Efficient Attention Mechanism:</em>
Dense global attention is one of the major constraints in training larger context window LLMs. Using efficient attention variants, such as local, sparse, and dilated attention, reduces the computation cost significantly. LongT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib44" title="">44</a>]</cite> proposes transient global attention (TGlobal), applying attention to local and global tokens (windowing token averaging). The model replaces attention in T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> with TGlobal attention, pre-trains the model on 4098 sequence length, fine-tunes on larger window sizes, as large as 16k, and improves task performance with longer inputs. This shows the extrapolation ability of TGlobal attention with only fine-tuning. COLT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib178" title="">178</a>]</cite> uses two branches, one with lightweight and the other with heavyweight attention and feed-forward layers. All tokens are processed from the lightweight branch, and only important tokens are routed to the heavyweight branch. LongNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib179" title="">179</a>]</cite> replaces standard attention with dilated attention, expanding sequence length to 1 billion tokens. LongLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib180" title="">180</a>]</cite> proposes shift-short attention, used during fine-tuning to reduce dense attention costs, while the model during inference can use dense attention and achieve similar performance as full attention fine-tuning.
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS3.p1.1.3">Extrapolation without Training:</em>
LM-Infinite&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib177" title="">177</a>]</cite> and parallel context windows (PCW)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib181" title="">181</a>]</cite> show length extrapolation is possible using pre-trained LLMs. LM-Infinite suggested <math alttext="\Lambda" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS3.p1.1.m1.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\Lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">roman_Λ</annotation></semantics></math>-shaped attention applied within the original context window limits. Likewise, PCW chunks larger inputs into the pre-trained context lengths and applies the same positional encodings to each chunk.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Robotics</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">LLMs have been rapidly adopted across various domains in the scientific community due to their multipurpose capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib46" title="">46</a>]</cite>. In robotics research, the LLMs have very promising applications as well, such as enhancing human-robot interaction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib182" title="">182</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib183" title="">183</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib184" title="">184</a>]</cite>, task planning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib187" title="">187</a>]</cite>, navigation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib188" title="">188</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib189" title="">189</a>]</cite>, and learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib190" title="">190</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib191" title="">191</a>]</cite>. They can enable robots to understand and generate natural language, aiding in instruction following, data annotation, and collaborative problem-solving.
They can facilitate continuous learning by allowing robots to access and integrate information from a wide range of sources. This can help robots acquire new skills, adapt to changes, and refine their performance based on real-time data. 
<br class="ltx_break">LLMs have also started assisting in simulating environments for testing and offer potential for innovative research in robotics, despite challenges like bias mitigation and integration complexity. The work in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib192" title="">192</a>]</cite> focuses on personalizing robot household cleanup tasks. By combining language-based planning and perception with LLMs, such that having users provide object placement examples, which the LLM summarizes to generate generalized preferences, they show that robots can generalize user preferences from a few examples. An embodied LLM is introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib26" title="">26</a>]</cite>, which employs a Transformer-based language model where sensor inputs are embedded alongside language tokens, enabling joint processing to enhance decision-making in real-world scenarios. The model is trained end-to-end for various embodied tasks, achieving positive transfer from diverse training across language and vision domains. LLMs have also been explored as zero-shot human models for enhancing human-robot interaction. 
<br class="ltx_break">The study in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib28" title="">28</a>]</cite> demonstrates that LLMs, trained on vast text data, can serve as effective human models for certain HRI tasks, achieving predictive performance comparable to specialized machine-learning models. However, limitations were identified, such as sensitivity to prompts and difficulties with spatial/numerical reasoning. In another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib193" title="">193</a>]</cite>, the authors enable LLMs to reason over sources of natural language feedback, forming an <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.SS4.p1.1.1">“inner monologue”</span> that enhances their ability to process and plan actions in robotic control scenarios. They combine LLMs with various forms of textual feedback, allowing the LLMs to incorporate conclusions into their decision-making process for improving the execution of user instructions in different domains, including simulated and real-world robotic tasks involving tabletop rearrangement and mobile manipulation. All of these studies employ LLMs as the core mechanism for assimilating everyday intuitive knowledge into the functionality of robotic systems.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS4.p2.1.1">Planning:</em> LLMs are increasingly integral in robotics, particularly for strategic planning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib194" title="">194</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib195" title="">195</a>]</cite>. Their proficiency in processing and generating natural language is crucial for enhancing human-robot interaction and enabling robots to understand and execute complex tasks based on verbal instructions. LLMs also play a key role in task planning, a higher-level cognitive process involving the determination of sequential actions needed to achieve specific goals. This proficiency is crucial across a spectrum of applications, from autonomous manufacturing processes to household chores, where the ability to understand and execute multi-step instructions is of paramount significance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS4.p3.1.1">Manipulation:</em> In the area of manipulation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib196" title="">196</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib197" title="">197</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib198" title="">198</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib199" title="">199</a>]</cite>, LLMs enhance a robot’s dexterity and adaptability, excelling in tasks like object recognition, grasping, and collaboration. They analyze visual and spatial information to determine the most effective approach to interact with objects, proving invaluable in operations requiring precision and flexibility, such as surgical procedures or assembly line tasks. They also enable the integration of sensor inputs and linguistic cues in an embodied framework, enhancing decision-making in real-world scenarios. It enhances the model’s performance across various embodied tasks by allowing it to gather insights and generalize from diverse training data spanning language and vision domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS4.p4.1.1">Navigation:</em> LLMs have revolutionized the navigation in robotics&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib200" title="">200</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib201" title="">201</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib202" title="">202</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib203" title="">203</a>]</cite>, offering significant potential to enhance a robot’s ability to navigate complex environments with precision and adaptability. Motion planning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib188" title="">188</a>]</cite>, in particular, stands out as a critical domain where LLMs have shown remarkable promise, excelling in generating feasible paths and trajectories for robots, accounting for intricate environmental details. This ability proves particularly valuable in scenarios requiring precise and dynamically adaptable navigation, as observed in environments like warehouses, transport and healthcare facilities, and smart residences. LLMs have also played a key role in localization and mapping, which are foundational components for successful robot navigation. They empower robots to determine their precise position within an environment while concurrently constructing or updating a spatial representation of their surroundings. This capability is crucial for tasks demanding spatial awareness, including autonomous exploration, search and rescue missions, and the operations of mobile robots. They have also contributed significantly to the proficiency of collision-free navigation within the environment while accounting for obstacles and dynamic alterations, playing an important role in scenarios where robots are tasked with traversing predefined paths with accuracy and reliability, as seen in the operations of automated guided vehicles (AGVs) and delivery robots (e.g., SADRs – pedestrian sized robots that deliver items to customers without the involvement of a delivery person).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Multimodal LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Inspired by the success of LLMs in natural language processing applications, an increasing number of research works are now facilitating LLMs to perceive different modalities of information like image&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib204" title="">204</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib205" title="">205</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib206" title="">206</a>]</cite>, video&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib207" title="">207</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib208" title="">208</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib209" title="">209</a>]</cite>, audio&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib210" title="">210</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib209" title="">209</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib211" title="">211</a>]</cite>, <em class="ltx_emph ltx_font_italic" id="S3.SS5.p1.1.1">etc.</em> Multimodal LLMs (MLLMs) present substantial benefits compared to standard LLMs that process only text. By incorporating information from various modalities, MLLMs can achieve a deeper understanding of context, leading to more intelligent responses infused with a variety of expressions. Importantly, MLLMs align closely with human perceptual experiences, leveraging the synergistic nature of our multisensory inputs to form a comprehensive understanding of the world&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib211" title="">211</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib26" title="">26</a>]</cite>. Coupled with a user-friendly interface, MLLMs can offer intuitive, flexible, and adaptable interactions, allowing users to engage with intelligent assistants through a spectrum of input methods. According to the ways of constructing models, current MLLMs can be generally divided into three streams: pre-training, fine-tuning, and prompting. In this section, we will discuss more details of these main streams, as well as the important application of MLLMs in visual reasoning. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS5.p1.1.2">Pre-training:</em> This stream of MLLMs intends to support different modalities using unified end-to-end models. For instance, Flamingo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib204" title="">204</a>]</cite> applies gated cross-attention to fuse vision and language modalities, which are collected from pre-trained and frozen visual encoder and LLM, respectively. Moreover, BLIP-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib205" title="">205</a>]</cite> proposes a two-stage strategy to pre-train a Querying Transformer (Q-Former) for the alignment between vision and language modalities: in the first stage, vision-language representation learning is bootstrapped from a frozen visual encoder; and in the second stage, a frozen LLM bootstraps vision-to-language generative learning for zero-shot image-to-text generation. Similarly, MiniGPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib212" title="">212</a>]</cite> also deploys pre-trained and frozen ViT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib213" title="">213</a>]</cite>, Q-Former and Vicuna LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib150" title="">150</a>]</cite>, while only a linear projection layer needs to be trained for vision and language modalities alignment. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS5.p1.1.3">Fine-tuning:</em> Derived from instruction tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite> for NLP tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite>, researchers are now fine-tuning pre-trained LLMs using multimodal instructions. Following this method, LLMs can be easily and effectively extended as multimodal chatbots&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib212" title="">212</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib206" title="">206</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib29" title="">29</a>]</cite> and multimodal task solvers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib214" title="">214</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib215" title="">215</a>]</cite>. The key issue of this stream of MLLMs is to collect multimodal instruction-following data for fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib216" title="">216</a>]</cite>. To address this issue, the solutions of benchmark adaptation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib214" title="">214</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib217" title="">217</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib218" title="">218</a>]</cite>, self-instruction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib219" title="">219</a>]</cite>, and hybrid composition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib220" title="">220</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib215" title="">215</a>]</cite> are employed, respectively. To mitigate the gap between the original language modality and additional modalities, the learnable interface is introduced to connect different modalities from frozen pre-trained models. Particularly, the learnable interface is expected to work in a parameter-efficient tuning manner: <em class="ltx_emph ltx_font_italic" id="S3.SS5.p1.1.4">e.g.</em>, LLaMA-Adapter&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib221" title="">221</a>]</cite> applies an efficient transformer-based adapter module for training, and LaVIN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib220" title="">220</a>]</cite> dynamically learns the multimodal feature weights using a mixture-of-modality adapter. Different from the learnable interface, the expert models can directly convert multimodalities into language: <em class="ltx_emph ltx_font_italic" id="S3.SS5.p1.1.5">e.g.</em>, VideoChat-Text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib207" title="">207</a>]</cite> incorporates Whisper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib222" title="">222</a>]</cite>, a speech recognition expert model, to generate the captions of given videos for the understanding of following LLMs. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS5.p1.1.6">Prompting:</em> Different from the fine-tuning technique that directly updates the model parameters given task-specific datasets, the prompting technique provides certain context, examples, or instructions to the model, fulfilling specialized tasks without changing the model parameters. Since prompting can significantly reduce the need for large-scale multimodal data, this technique is widely used to construct MLLMs. Particularly, to solve multimodal Chain of Thought (CoT) problems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib99" title="">99</a>]</cite>, LLMs are prompted to generate both the reasoning process and the answer given multimodal inputs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib223" title="">223</a>]</cite>. On this front, different learning paradigms are exploited in practice: for example, Multimodal-CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib223" title="">223</a>]</cite> involves two stages of rationale generation and answer inference, where the input of the second stage is a combination of the original input and the output of the first stage; and CoT-PT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib224" title="">224</a>]</cite> applies both prompt tuning and specific visual bias to generate a chain of reasoning implicitly. In addition to CoT problems, LLMs can also be prompted with multimodal descriptions and tools, effectively dividing complex tasks into sub-tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib225" title="">225</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib226" title="">226</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS5.p1.1.7">Visual Reasoning Application:</em> Recent visual reasoning systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib227" title="">227</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib228" title="">228</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib229" title="">229</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib230" title="">230</a>]</cite> tend to apply LLMs for better visual information analysis and visual-language integration. Different from previous works&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib231" title="">231</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib232" title="">232</a>]</cite> that rely on limited VQA datasets and small-scale neural networks, current LLM-aided methods offer benefits of stronger generalization ability, emergent ability, and interactivity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib216" title="">216</a>]</cite>. To realize visual reasoning with the help of LLMs, prompting and fine-tuning techniques can also be utilized: for example, PointClip V2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib228" title="">228</a>]</cite> applies LLMs to generate 3D-specific prompts, which are encoded as textual features and then combined with visual features for 3D recognition; and GPT4Tools&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib31" title="">31</a>]</cite> employs LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib233" title="">233</a>]</cite> to fine-tune LLMs following tool-related instructions. Serving as a controller&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib230" title="">230</a>]</cite>, decision maker&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib234" title="">234</a>]</cite>, or semantics refiner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib227" title="">227</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib235" title="">235</a>]</cite>, LLMs significantly facilitates the progress of visual reasoning research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.5.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.6.2">Augmented LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">LLMs are capable of learning from the examples concatenated with the input, known as context augmentation, in-context learning (ICL), or few-shot prompting. They show excellent generalization to unseen tasks with few-shot prompting, enabling LLMs to answer queries beyond the capacity acquired during training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib51" title="">51</a>]</cite>. These emergent abilities allow for adapting the model without fine-tuning - a costly process. Aside from this, hallucination, producing inaccurate, unsafe or factually incorrect responses, is common for LLMs, which is avoided by augmenting contextual data. While the user can provide in-context samples in the query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib32" title="">32</a>]</cite>, here we specifically refer to the methods that access external storage programmatically, calling them augmented LLMs.
<br class="ltx_break">The literature suggests various external memory designs to augment LLMs, long-term&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib236" title="">236</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib237" title="">237</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib238" title="">238</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib239" title="">239</a>]</cite>, short-term&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib240" title="">240</a>]</cite>, symbolic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib241" title="">241</a>]</cite>, and non-symbolic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib242" title="">242</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib243" title="">243</a>]</cite>. The memory can be maintained in different formats such as documents, vectors, or databases. A few systems maintain intermediate memory representations to retain information across multiple iterations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib239" title="">239</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib237" title="">237</a>]</cite>, while others extract important information from the datasets and save it in memory for recall&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib244" title="">244</a>]</cite>. The memory read and write operations are performed either with or without LLMs cooperation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib237" title="">237</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib245" title="">245</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib239" title="">239</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib246" title="">246</a>]</cite>, acting as a feedback signal in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib240" title="">240</a>]</cite>.
We discuss different types of augmented LLMs below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="S3.F12.g1" src="./A Comprehensive Overview of Large Language Models_files/RAG.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A flow diagram of Retrieval Augmented <span class="ltx_text" id="S3.F12.2.1" style="color:#000000;">LLMs.</span> The retriever extracts a similar context to the input and forwards it to the LLM either in simple language or encoded through Fusion-in-Decoder (FiD). Depending on the task, retrieval and generation may repeat multiple times. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS6.SSS1.5.1.1">III-F</span>1 </span>Retrieval Augmented LLMs</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">LLMs may have limited memory and outdated information, leading to inaccurate responses. Retrieving relevant information from external up-to-date storage enables the LLMs to accurately answer with references and utilize more information. With retrieval augmentation, smaller models have been shown to perform at par with larger models. For instance, the 11B model can become competitive to 540B PaLM in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>]</cite> and 7.5B to 280B Gopher in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib238" title="">238</a>]</cite>. Retrieval augmented language modeling (RALM) has two major components, shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F12" title="Figure 12 ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">12</span></a>, namely: 1) retriever and 2) language model. In RALM, the retriever plays a crucial role in driving LLM response, where incorrect information can steer LLMs to false behavior. This leads to the development of various methods to retrieve accurate information and fuse with the query for better performance.
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS1.p1.1.1">Zero-Shot Retrieval Augmentation:</em>
This kind of augmentation keeps the original LLM architecture and weights unchanged and uses BM25&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib247" title="">247</a>]</cite>, nearest neighbors, or frozen pre-trained models like Bert&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib7" title="">7</a>]</cite> as a retriever. The retrieved information is provided as input to the model for response generation, shown to improve performance over LLMs without retrieval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib243" title="">243</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib248" title="">248</a>]</cite>.
In some scenarios, multiple retrieval iterations are required to complete the task. The output generated in the first iteration is forwarded to the retriever to fetch similar documents. Forward-looking active retrieval (FLARE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib242" title="">242</a>]</cite> initially generates the response and corrects the output by retrieving relevant documents if the response contains low-confidence tokens. Similarly, RepoCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib249" title="">249</a>]</cite> fetches code snippets recursively for code completion. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS1.p1.1.2">Training with Retrieval Augmentation:</em>
To reduce failures in retrieval augmentation generation (RAG), researchers train or fine-tune retrievers and LLMs with a retrieval augmentation pipeline. We discuss the literature below based on their focus on the respective training processes of the pipeline. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S3.SS6.SSS1.p1.1.3">Training LLM:</span> Retrieval-enhanced transformer (RETRO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib238" title="">238</a>]</cite> shows pre-training smaller LLMs with RAG pipeline outperforms larger LLMs, such as GPT-3 trained without RAG. RETRO uses a 2-trillion token subset of MassiveText as a database. The retrieval pipeline divides the input query into subsets and retrieves relevant chunks from the database for each subset, encoded together with input intermediate representations for generating tokens. It uses cross-chunked attention to attend to previous chunks auto-regressively. A study on RETRO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib250" title="">250</a>]</cite> shows models pre-trained without RAG but fine-tuned using RAG lack the performance gains obtained by pre-training with RAG. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S3.SS6.SSS1.p1.1.4">Training Retriever:</span> Quality of responses generated by LLMs is highly dependent on the in-context examples. Therefore,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib251" title="">251</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib252" title="">252</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib253" title="">253</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib254" title="">254</a>]</cite> train retrievers to retrieve accurate few-shot samples while keeping the LLM frozen for generation. Retrieved samples are ranked to build ground-truth data to train retrievers with contrastive learning in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib251" title="">251</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib253" title="">253</a>]</cite>. RoBERTa is trained for downstream tasks in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib252" title="">252</a>]</cite> for ICL samples retrieval. REPLUG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib254" title="">254</a>]</cite> trains the retriever with supervised signals from the frozen LLM-generated outputs. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S3.SS6.SSS1.p1.1.5">Training Retriever and LLM:</span> Further benefits are achieved by training both the retriever and the model in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib255" title="">255</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib256" title="">256</a>]</cite>. In this case, the error propagates back to the retriever, updating both the language model and the retriever. While masked language modeling (MLM) is a common pre-training objective&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib256" title="">256</a>]</cite>, retrieval pre-trained transformer (RPT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib255" title="">255</a>]</cite> used document chunk prediction as a pre-training objective for long text modeling. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS1.p1.1.6">Encoded Context Augmentation:</em>
Concatenating retrieved documents with the query becomes infeasible as the sequence length and sample size grow. Encoding the context and fusing it with the decoder (Fusion-in-Decoder) using cross-attention makes it possible to augment more samples without increasing computation costs significantly&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib257" title="">257</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib238" title="">238</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib255" title="">255</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS1.p1.1.7">Web Augmented:</em>
Locally stored memory, but external to LLM, has limited information. However, a large amount of information is available on the internet, which gets updated regularly. Rather than storing information locally, various methods retrieve query-related context through a web search and forward it to LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib258" title="">258</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib259" title="">259</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib157" title="">157</a>]</cite>. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS6.SSS2.5.1.1">III-F</span>2 </span>Tool Augmented LLMs</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">While RAG relies on the retriever to provide context to the LLM to answer queries, tool augmented LLMs capitalize on the reasoning abilities of LLMs to iteratively plan by dividing tasks into sub-tasks, selecting necessary tools, and taking actions to complete the task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib260" title="">260</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib261" title="">261</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib262" title="">262</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib27" title="">27</a>]</cite>. A generic pipeline of tool-augmented LLMs is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F13" title="Figure 13 ‣ III-F2 Tool Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">13</span></a>, where different modules in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F13" title="Figure 13 ‣ III-F2 Tool Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">13</span></a> are selected in a loop until the task completion.
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="540" id="S3.F13.g1" src="./A Comprehensive Overview of Large Language Models_files/TALLM.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A basic flow diagram of tool augmented <span class="ltx_text" id="S3.F13.2.1" style="color:#000000;">LLMs.</span> Given an input and a set of available tools, the model generates a plan to complete the task. The tool augmented LLMs utilize different modules iteratively, such as retriever, tool execution, read-write to memory, feedback, etc., depending on the task. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS6.SSS2.p2">
<p class="ltx_p" id="S3.SS6.SSS2.p2.4"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS2.p2.4.1">Zero-Shot Tool Augmentation:</em> LLMs in-context learning and reasoning abilities enable them to interact with tools without training. Automatic reasoning and tool-use (ART)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib262" title="">262</a>]</cite> builds a task library with demonstrations of reasoning steps and calling external tools. It retrieves similar task examples and provides the context to the LLM for inference. Aside from this,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib263" title="">263</a>]</cite> shows tool documentation is enough to teach LLMs to use tools without demonstrations. RestGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib264" title="">264</a>]</cite> integrates LLMs with RESTful APIs by decomposing tasks into planning and API selection steps. The API selector understands the API documentation to select a suitable API for the task and plan the execution. ToolkenGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib265" title="">265</a>]</cite> uses tools as tokens by concatenating tool embeddings with other token embeddings. During inference, the LLM generates the tool tokens representing the tool call, stops text generation, and restarts using the tool execution output. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS2.p2.4.2">Training with Tool Augmentation:</em> LLMs are trained to interact with diverse tools, enhancing planning abilities to overcome the limitations of zero-shot tool augmentation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib266" title="">266</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib267" title="">267</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib268" title="">268</a>]</cite>. Gorilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib266" title="">266</a>]</cite> instruction-tunes LLaMA with information retrieval from API documentation. It uses self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib19" title="">19</a>]</cite> data generation pipeline with GPT-4 by providing in-context examples retrieved from API documentation. Tool augmented language model (TALM)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib27" title="">27</a>]</cite> fine-tunes T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> for tool use with a self-play approach, where it iteratively completes tool manipulation tasks and includes them back in the training set. ToolLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib268" title="">268</a>]</cite> collects 16k APIs from RapidAPI. It samples APIs from the list to generate an instruction-tuning dataset using ChatGPT in single-tool and multi-tool scenarios. For high-quality datasets, ToolLLM suggested a depth-first search-based decision tree (DFSDT) method to generate ground-truths with diverse reasoning and planning. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS6.SSS2.p2.4.3">Multimodal Tool Augmentation:</em> The compositional reasoning capacity of LLMs allows them to manipulate tools in multimodal settings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib260" title="">260</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib261" title="">261</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib269" title="">269</a>]</cite>. Following the pipeline shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F13" title="Figure 13 ‣ III-F2 Tool Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">13</span></a>, the LLM outlines a plan, generally executing in a sequence: Plan <math alttext="\,\to\," class="ltx_Math" display="inline" id="S3.SS6.SSS2.p2.1.m1.1"><semantics id="S3.SS6.SSS2.p2.1.m1.1a"><mo id="S3.SS6.SSS2.p2.1.m1.1.1" stretchy="false" xref="S3.SS6.SSS2.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.1.m1.1b"><ci id="S3.SS6.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS6.SSS2.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.1.m1.1c">\,\to\,</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.1.m1.1d">→</annotation></semantics></math> Tool selection <math alttext="\,\to\," class="ltx_Math" display="inline" id="S3.SS6.SSS2.p2.2.m2.1"><semantics id="S3.SS6.SSS2.p2.2.m2.1a"><mo id="S3.SS6.SSS2.p2.2.m2.1.1" stretchy="false" xref="S3.SS6.SSS2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.2.m2.1b"><ci id="S3.SS6.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS6.SSS2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.2.m2.1c">\,\to\,</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.2.m2.1d">→</annotation></semantics></math> Execute <math alttext="\,\to\," class="ltx_Math" display="inline" id="S3.SS6.SSS2.p2.3.m3.1"><semantics id="S3.SS6.SSS2.p2.3.m3.1a"><mo id="S3.SS6.SSS2.p2.3.m3.1.1" stretchy="false" xref="S3.SS6.SSS2.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.3.m3.1b"><ci id="S3.SS6.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS6.SSS2.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.3.m3.1c">\,\to\,</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.3.m3.1d">→</annotation></semantics></math> Inspect <math alttext="\,\to\," class="ltx_Math" display="inline" id="S3.SS6.SSS2.p2.4.m4.1"><semantics id="S3.SS6.SSS2.p2.4.m4.1a"><mo id="S3.SS6.SSS2.p2.4.m4.1.1" stretchy="false" xref="S3.SS6.SSS2.p2.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS2.p2.4.m4.1b"><ci id="S3.SS6.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS6.SSS2.p2.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS2.p2.4.m4.1c">\,\to\,</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS2.p2.4.m4.1d">→</annotation></semantics></math> Generate, to respond to the user query. Here, the database of tools is rich in modalities, including text, images, etc. Many of the multimodal tool augmentation systems employ multimodal LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib270" title="">270</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib271" title="">271</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib269" title="">269</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib261" title="">261</a>]</cite>, while others utilize single modality LLMs and generate a plan on using different modality tools to solve multimodal queries&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib272" title="">272</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S3.F14.g1" src="./A Comprehensive Overview of Large Language Models_files/peft.png" width="1196">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Illustration of parameter-efficient fine-tuning paradigms, where <math alttext="x" class="ltx_Math" display="inline" id="S3.F14.3.m1.1"><semantics id="S3.F14.3.m1.1b"><mi id="S3.F14.3.m1.1.1" xref="S3.F14.3.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F14.3.m1.1c"><ci id="S3.F14.3.m1.1.1.cmml" xref="S3.F14.3.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F14.3.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F14.3.m1.1e">italic_x</annotation></semantics></math> is input and <math alttext="h" class="ltx_Math" display="inline" id="S3.F14.4.m2.1"><semantics id="S3.F14.4.m2.1b"><mi id="S3.F14.4.m2.1.1" xref="S3.F14.4.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.F14.4.m2.1c"><ci id="S3.F14.4.m2.1.1.cmml" xref="S3.F14.4.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F14.4.m2.1d">h</annotation><annotation encoding="application/x-llamapun" id="S3.F14.4.m2.1e">italic_h</annotation></semantics></math> is hidden state, figure courtesy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib38" title="">38</a>]</cite>.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS7.5.1.1">III-G</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS7.6.2">Efficient LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS7.SSS1.5.1.1">III-G</span>1 </span>Parameter Efficient Fine-Tuning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS7.SSS1.p1">
<p class="ltx_p" id="S3.SS7.SSS1.p1.1">Fine-tuning LLMs with tens or hundreds of billions of parameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG (540B), etc., is hardware-extensive and time-consuming. To avoid complete model fine-tuning, numerous parameter-efficient fine-tuning (PEFT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib273" title="">273</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib39" title="">39</a>]</cite> are trying to reach full model fine-tuning performance at reduced costs. PEFT outperforms low-resource tasks, achieves comparable performance on medium-resource tasks, and underperforms on high-resource tasks compared to full fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib274" title="">274</a>]</cite>. An overview of different PEFT approaches is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.F14" title="Figure 14 ‣ III-F2 Tool Augmented LLMs ‣ III-F Augmented LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">14</span></a>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS1.p1.1.1">Adapter Tuning:</em> Adds a few trainable parameters within the transformer block. The adapter layer is a sequence of feature downscaling, non-linearity, and upscaling&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib102" title="">102</a>]</cite>. Variants of adapter tuning inject adapter layers sequentially&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib102" title="">102</a>]</cite> and parallel&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib275" title="">275</a>]</cite>, whereas the mixture of adapter (AdaMix)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib276" title="">276</a>]</cite> employs multiple adapter modules in a single layer. AdaMix routes input instances randomly to one of the multiple downscale and upscale modules. The mixture of adapters is averaged out for inference to avoid additional latency. Low-Rank Adaptation (LoRA)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib233" title="">233</a>]</cite> learns low-rank decomposed matrices to freeze original weights. Learned weights are fused with the original weights for inference, avoiding latency. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS1.p1.1.2">Prompt Tuning:</em> Prompting is an effective way to adapt a pre-trained LLM for the downstream task. However, manual prompts bring uncertainty in the model’s prediction, where a change in a single word drops the performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib273" title="">273</a>]</cite>. Prompt tuning alleviates this problem by fine-tuning only 0.001%-3% additional parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib277" title="">277</a>]</cite>. It concatenates trainable prompt parameters with the model embeddings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib273" title="">273</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib277" title="">277</a>]</cite>. Task-specific fixed discrete prompts are concatenated with input embeddings in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib40" title="">40</a>]</cite>. As discrete prompts bring instability, prompts are encoded through a learnable mapping in P-Tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib273" title="">273</a>]</cite>, naming continuous prompts, which are appended with the discrete prompts. Only the prompt encoder is trainable in the model. In an extension of P-Tuning, continuous prompts are concatenated with each layer of the network in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib277" title="">277</a>]</cite>. Progressive prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib278" title="">278</a>]</cite> avoid catastrophic forgetting and transfer previously learned knowledge by sequentially adding trainable prompt embeddings to the previously frozen task embeddings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS7.SSS1.p2">
<p class="ltx_p" id="S3.SS7.SSS1.p2.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS1.p2.1.1">Prefix Tuning:</em> A set of trainable task-specific prefix vectors are appended to the frozen transformer layers in prefix tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib41" title="">41</a>]</cite>. The prefix vectors are virtual tokens attended by the context tokens on the right. In addition, adaptive prefix tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib279" title="">279</a>]</cite> applies a gating mechanism to control the information from the prefix and actual tokens. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS1.p2.1.2">Bias Tuning:</em> Fine-tuning only bias terms in small to medium training data has been found effective in BitFit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib280" title="">280</a>]</cite>. This method achieves full fine-tuning performance for tasks with less training data and comparable performance with larger training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS7.SSS2.5.1.1">III-G</span>2 </span>Quantization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS7.SSS2.p1">
<p class="ltx_p" id="S3.SS7.SSS2.p1.1">LLMs require extensive computing and memory for inference. Deploying the GPT-3 175B model needs at least 5x80GB A100 GPUs and 350GB of memory to store in FP16 format&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib281" title="">281</a>]</cite>. Such demanding requirements for deploying LLMs make it harder for smaller organizations to utilize them. Model compression is an effective solution but comes at the cost of degrading performance, especially at large scales greater than 6B. These models exhibit very large magnitude outliers that do not exist in smaller models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib282" title="">282</a>]</cite>, making it challenging and requiring specialized methods for quantizing LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib281" title="">281</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib283" title="">283</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS2.p1.1.1">Post-Training Quantization:</em>
Minimal or no training is required in this quantization type without compromising the model’s performance. LLM-8-bit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib282" title="">282</a>]</cite> uses full-precision matrix multiplication for weights associated with outlier features and 8-bit for remaining. The lower precision multiplication outputs are converted to FP-16 and concatenated with others. The quantized models have homogenous word embeddings, degrading their performance. To fix this, token-level knowledge distillation is employed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib284" title="">284</a>]</cite> along with independent quantization scaling factors for each module due to varying weight distribution. Feature distributions are asymmetric and appear in different channels; outlier suppresion+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib285" title="">285</a>]</cite> shifts and scales per-channel activation distributions for effective quantization. SmoothQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib281" title="">281</a>]</cite> quantizes activations and weights to INT8 by smoothing activations and migrating the quantization difficulty toward weights. It multiplies the inverse of the smoothing factor with weights, which introduces a few outliers in the weights but is easier to quantify than unsmoothed activations. OPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib283" title="">283</a>]</cite> uses the optimal brain compression (OBC)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib286" title="">286</a>]</cite> algorithm to quantize the model layer-by-layer and update weights to compensate for quantization error. To improve speed and performance, OPTQ updates weights in arbitrary order, employs lazy updates, and uses better Cholesky kernels. Outlier-aware weight quantization (OWQ)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib287" title="">287</a>]</cite> uses the OPTQ algorithm for quantization but assigns higher precision to vulnerable weights, causing outliers and lower precision for others. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS2.p1.1.2">Quantization-Aware Training:</em>
To compensate for the performance degradation, the quantized model is fine-tuned in quantization-aware training (QAT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib288" title="">288</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib289" title="">289</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib290" title="">290</a>]</cite>. Alphatuning quantizes the model using binary coding quantization (BCQ)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib291" title="">291</a>]</cite> and fine-tunes only quantization scaling factors. This approach improves performance over parameter-efficient fine-tuning of a pre-trained model and then quantizing and fine-tuning. Similarly, parameter-efficient and quantization-aware adaptation (PEQA)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib292" title="">292</a>]</cite> reduces the precision of fully-connected layers and fine-tunes only quantization scaling parameters. LLM-QAT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib290" title="">290</a>]</cite> generates training data from the pre-trained network and trains a quantized student model with knowledge distillation. QLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib289" title="">289</a>]</cite> fine-tunes 4-bit quantized pre-trained LLM with LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib233" title="">233</a>]</cite> using 4-bit normal float, which shows better performance over 4-bit integer and float.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS7.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS7.SSS3.5.1.1">III-G</span>3 </span>Pruning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS7.SSS3.p1">
<p class="ltx_p" id="S3.SS7.SSS3.p1.1">Pruning is an alternative approach to quantization to compress model size, thereby reducing LLMs deployment costs significantly. Compared to task-agnostic pruning, task-specific pruning is easily achievable with good performance, where a model is fine-tuned on the downstream task and pruned for faster inference.
It is possible to prune LLMs for individual tasks, but the cost of pruning and deploying task-specific models is high. To overcome this, many structured and unstructured pruning methods for LLMs have been proposed to maintain reasonable performance while shrinking the model size&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib293" title="">293</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib294" title="">294</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib295" title="">295</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS3.p1.1.1">Unstructured Pruning:</em> This kind of pruning removes less important weights without maintaining any structure. Existing LLM pruning methods take advantage of the unique characteristics of LLMs, uncommon for smaller models, where a small subset of hidden states are activated with large magnitude&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib282" title="">282</a>]</cite>. Pruning by weights and activations (Wanda)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib293" title="">293</a>]</cite> prunes weights in every row based on importance, calculated by multiplying the weights with the norm of input. The pruned model does not require fine-tuning, saving large models’ computational costs. Outlier weighed layerwise sparsity (OWL)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib296" title="">296</a>]</cite> extends Wanda with non-uniform layer pruning. It shows that the number of outliers varies for different layers; therefore, the model should have variable pruning ratios for better performance for every layer. Contrastive pruning (CAP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib297" title="">297</a>]</cite> iteratively prunes the model by training the sparse model using contrastive loss between pre-trained, fine-tuning, and snapshots of previous sparse models to learn task-specific and task-agnostic knowledge. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS7.SSS3.p1.1.2">Structured Pruning:</em> Here, the parameters are removed in groups, rows, columns, or matrices, which speeds up the inference because of effective NVIDIA tensor core utilization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib293" title="">293</a>]</cite>. LLM-Pruner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib294" title="">294</a>]</cite> employs a 3-stage structured pruning strategy, identifying the groups of hidden states causing each other to activate during forward-pass, keeping important groups and removing less important ones, and fine-tuning the pruned model with LoRA. Sparsity-induced mask learning (SIMPLE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib298" title="">298</a>]</cite> prunes the network using learnable masks. Similarly, another method prunes LLMs by learning masks and removing unimportant rank-1 components of the factorized weight matrix&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib295" title="">295</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Summary of pre-trained LLMs (&gt;10B). Only the LLMs discussed individually in the previous sections are summarized. <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.49.1">“Data/Tokens”</span> is the model’s pre-training data, which is either the number of tokens or data size.
<span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.50.2">“Data Cleaning”</span> indicates whether the data cleaning is performed or not. This includes heuristics (Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.51.3">“Cost”</span> is the calculated training cost obtained by multiplying the GPUs/TPUs hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate, re-training, number of employees working on the problem, etc. <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.52.4">“Training Parallelism”</span> indicates distributed training using data parallelism (D), tensor parallelism (T), pipeline parallelism (P), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.53.5">“Library”</span> column, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.54.6">“DS”</span> is a short form for Deep Speed. In column <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T3.55.7">“Commercial Use”</span>, we assumed a model is for non-commercial purposes if its license is unavailable.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.41" style="width:433.6pt;height:274.7pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-295.4pt,186.8pt) scale(0.423313919858637,0.423313919858637) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.41.41">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.41.41.42.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.1" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.2" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.2.1" style="background-color:#BFBFBF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.2.1.1" style="background-color:#BFBFBF;">
<span class="ltx_tr" id="S3.T3.41.41.42.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.2.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.2.1.1.1.1.1">Publication</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.2.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.2.1.1.2.1.1" style="background-color:#FFFFFF;">Venue</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.3" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.3.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.3.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.3.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.3.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.3.1.1.1.1.1">License</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.3.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.3.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.3.1.1.2.1.1">Type</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.4" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.4.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.4.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.4.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.4.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.4.1.1.1.1.1">Model</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.4.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.4.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.4.1.1.2.1.1">Creators</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.5.1" style="background-color:#FFFFFF;">Purpose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.6" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.T3.41.41.42.1.6.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.6.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.6.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.6.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.6.1.1.1.1.1">No. of</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.6.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.6.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.6.1.1.2.1.1">Params</span></span></span>
</span></span><span class="ltx_text" id="S3.T3.41.41.42.1.6.2" style="background-color:#FFFFFF;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.7" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.7.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.7.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.7.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.7.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.7.1.1.1.1.1">&nbsp;&nbsp;Commercial</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.7.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.7.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.7.1.1.2.1.1">Use</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.8" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.8.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.8.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.8.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.8.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.8.1.1.1.1.1">Steps</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.8.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.8.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.8.1.1.2.1.1">Trained</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.9" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.9.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.9.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.9.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.9.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.9.1.1.1.1.1">Data/</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.9.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.9.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.9.1.1.2.1.1">Tokens</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.10" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.10.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.10.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.10.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.10.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.10.1.1.1.1.1">Data</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.10.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.10.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.10.1.1.2.1.1">Cleaning</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.11" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.11.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.11.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.11.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.11.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.11.1.1.1.1.1">No. of</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.11.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.11.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.11.1.1.2.1.1">Processing Units</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.12" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.12.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.12.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.12.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.12.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.12.1.1.1.1.1">Processing</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.12.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.12.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.12.1.1.2.1.1">Unit Type</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.13" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.13.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.13.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.13.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.13.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.13.1.1.1.1.1">Training</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.13.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.13.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.13.1.1.2.1.1">Time</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.14" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.14.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.14.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.14.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.14.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.14.1.1.1.1.1">Calculated</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.14.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.14.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.14.1.1.2.1.1">Train. Cost</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.15" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.15.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.15.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.15.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.15.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.15.1.1.1.1.1">Training</span></span></span>
<span class="ltx_tr" id="S3.T3.41.41.42.1.15.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.15.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.15.1.1.2.1.1">Parallelism</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.41.41.42.1.16" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.42.1.16.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.41.41.42.1.16.1.1">
<span class="ltx_tr" id="S3.T3.41.41.42.1.16.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.41.41.42.1.16.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.41.41.42.1.16.1.1.1.1.1">Library</span></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.2.1" style="background-color:#DFDFDF;">T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.3.1" style="background-color:#DFDFDF;">JMLR'20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.7.1" style="background-color:#DFDFDF;">11B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.1.1.1.1.m1.1a"><mi id="S3.T3.1.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.8.1" style="background-color:#DFDFDF;">1M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.9.1" style="background-color:#DFDFDF;">1T</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.10.1" style="background-color:#DFDFDF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.11.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.12.1" style="background-color:#DFDFDF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.15.1" style="background-color:#DFDFDF;">D+M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.1.1.1.16.1" style="background-color:#DFDFDF;">Mesh TensorFlow</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.2.2.2.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.2.1" style="background-color:#FFFFFF;">GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.2.2.2.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.3.1" style="background-color:#FFFFFF;">NeurIPS'20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.5.1" style="background-color:#FFFFFF;">OpenAI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.7.1" style="background-color:#FFFFFF;">175B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.2.2.2.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.2.2.2.1.m1.1a"><mo id="S3.T3.2.2.2.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.m1.1b"><times id="S3.T3.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.9.1" style="background-color:#FFFFFF;">300B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.10.1" style="background-color:#FFFFFF;">Dedup+QF</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.12.1" style="background-color:#FFFFFF;">V100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.15.1" style="background-color:#FFFFFF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.2.2.2.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3.3" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.3.3.3.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.2.1" style="background-color:#DFDFDF;">mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.3.3.3.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.3.1" style="background-color:#DFDFDF;">NAACL'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.7.1" style="background-color:#DFDFDF;">13B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.3.3.3.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.3.3.3.1.m1.1a"><mi id="S3.T3.3.3.3.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><ci id="S3.T3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.8.1" style="background-color:#DFDFDF;">1M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.9.1" style="background-color:#DFDFDF;">1T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.3.3.3.16.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5.5" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.4.4.4.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.4.4.4.1.1" style="background-color:#FFFFFF;">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.T3.4.4.4.1.1.m1.1"><semantics id="S3.T3.4.4.4.1.1.m1.1a"><mi id="S3.T3.4.4.4.1.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.4.4.4.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.1.1.m1.1b"><ci id="S3.T3.4.4.4.1.1.m1.1.1.cmml" xref="S3.T3.4.4.4.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.1.1.m1.1d">italic_α</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib104" title="">104</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.5.5.5.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.3.1" style="background-color:#FFFFFF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.5.1" style="background-color:#FFFFFF;">Huawei</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.7.1" style="background-color:#FFFFFF;">200B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.2" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.5.5.5.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.5.5.5.2.m1.1a"><mi id="S3.T3.5.5.5.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.5.5.5.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.2.m1.1b"><ci id="S3.T3.5.5.5.2.m1.1.1.cmml" xref="S3.T3.5.5.5.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.8.1" style="background-color:#FFFFFF;">260k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.9.1" style="background-color:#FFFFFF;">1.1TB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.10.1" style="background-color:#FFFFFF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.11.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.12.1" style="background-color:#FFFFFF;">Ascend 910</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.15.1" style="background-color:#FFFFFF;">D+OP+P+O+R</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.5.5.5.16.1" style="background-color:#FFFFFF;">MindSpore</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6.6" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.6.6.6.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.2.1" style="background-color:#DFDFDF;">CPM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib12" title="">12</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.6.6.6.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.3.1" style="background-color:#DFDFDF;">AI Open'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.4.1" style="background-color:#DFDFDF;">MIT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.5.1" style="background-color:#DFDFDF;">Tsinghua</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.7.1" style="background-color:#DFDFDF;">198B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.6.6.6.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.6.6.6.1.m1.1a"><mi id="S3.T3.6.6.6.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.6.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.1.m1.1b"><ci id="S3.T3.6.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.6.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.8.1" style="background-color:#DFDFDF;">1M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.9.1" style="background-color:#DFDFDF;">2.6TB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.10.1" style="background-color:#DFDFDF;">Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.15.1" style="background-color:#DFDFDF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.6.6.6.16.1" style="background-color:#DFDFDF;">JAXFormer</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.7.7.7" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.7.7.7.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.2.1" style="background-color:#FFFFFF;">Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib131" title="">131</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.7.7.7.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.3.1" style="background-color:#FFFFFF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.5.1" style="background-color:#FFFFFF;">OpenAI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.6.1" style="background-color:#FFFFFF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.7.1" style="background-color:#FFFFFF;">12B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.7.7.7.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.7.7.7.1.m1.1a"><mo id="S3.T3.7.7.7.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.1.m1.1b"><times id="S3.T3.7.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.7.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.9.1" style="background-color:#FFFFFF;">100B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.10.1" style="background-color:#FFFFFF;">Heur</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.7.7.7.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.10.10.10" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.10.10.10.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.4.1" style="background-color:#DFDFDF;">ERNIE 3.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib107" title="">107</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.10.10.10.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.5.1" style="background-color:#DFDFDF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.7.1" style="background-color:#DFDFDF;">Baidu</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.8.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.9.1" style="background-color:#DFDFDF;">10B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.8.8.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.8.8.8.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.8.8.8.1.m1.1a"><mo id="S3.T3.8.8.8.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.1.m1.1b"><times id="S3.T3.8.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.8.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.9.9.9.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.9.9.9.2.1" style="background-color:#DFDFDF;">120k<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.9.9.9.2.1.m1.1"><semantics id="S3.T3.9.9.9.2.1.m1.1a"><msup id="S3.T3.9.9.9.2.1.m1.1.1" xref="S3.T3.9.9.9.2.1.m1.1.1.cmml"><mi id="S3.T3.9.9.9.2.1.m1.1.1a" xref="S3.T3.9.9.9.2.1.m1.1.1.cmml"></mi><mo id="S3.T3.9.9.9.2.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S3.T3.9.9.9.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.2.1.m1.1b"><apply id="S3.T3.9.9.9.2.1.m1.1.1.cmml" xref="S3.T3.9.9.9.2.1.m1.1.1"><times id="S3.T3.9.9.9.2.1.m1.1.1.1.cmml" xref="S3.T3.9.9.9.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.9.9.9.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.10.1" style="background-color:#DFDFDF;">375B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.11.1" style="background-color:#DFDFDF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.12.1" style="background-color:#DFDFDF;">384</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.13.1" style="background-color:#DFDFDF;">V100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.3.1" style="background-color:#DFDFDF;">M<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.10.10.10.3.1.m1.1"><semantics id="S3.T3.10.10.10.3.1.m1.1a"><msup id="S3.T3.10.10.10.3.1.m1.1.1" xref="S3.T3.10.10.10.3.1.m1.1.1.cmml"><mi id="S3.T3.10.10.10.3.1.m1.1.1a" xref="S3.T3.10.10.10.3.1.m1.1.1.cmml"></mi><mo id="S3.T3.10.10.10.3.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S3.T3.10.10.10.3.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.10.3.1.m1.1b"><apply id="S3.T3.10.10.10.3.1.m1.1.1.cmml" xref="S3.T3.10.10.10.3.1.m1.1.1"><times id="S3.T3.10.10.10.3.1.m1.1.1.1.cmml" xref="S3.T3.10.10.10.3.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.10.3.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.10.10.10.3.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.10.10.10.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.10.10.10.16.1" style="background-color:#DFDFDF;">PaddlePaddle</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.11.11.11" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.11.11.11.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.2.1" style="background-color:#FFFFFF;">Jurassic-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib109" title="">109</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.11.11.11.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.3.1" style="background-color:#FFFFFF;">White-Paper'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.5.1" style="background-color:#FFFFFF;">AI21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.7.1" style="background-color:#FFFFFF;">178B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.11.11.11.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.11.11.11.1.m1.1a"><mi id="S3.T3.11.11.11.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.11.11.11.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.11.1.m1.1b"><ci id="S3.T3.11.11.11.1.m1.1.1.cmml" xref="S3.T3.11.11.11.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.11.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.11.11.11.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.9.1" style="background-color:#FFFFFF;">300B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.11.1" style="background-color:#FFFFFF;">800</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.12.1" style="background-color:#FFFFFF;">GPU</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.15.1" style="background-color:#FFFFFF;">D+M+P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.11.11.11.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.11.11.11.16.1" style="background-color:#FFFFFF;">Megatron+DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.12.12.12" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.12.12.12.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.2.1" style="background-color:#DFDFDF;">HyperCLOVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib111" title="">111</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.12.12.12.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.3.1" style="background-color:#DFDFDF;">EMNLP'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.5.1" style="background-color:#DFDFDF;">Naver</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.7.1" style="background-color:#DFDFDF;">82B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.12.12.12.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.12.12.12.1.m1.1a"><mo id="S3.T3.12.12.12.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.12.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.12.1.m1.1b"><times id="S3.T3.12.12.12.1.m1.1.1.cmml" xref="S3.T3.12.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.12.12.12.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.9.1" style="background-color:#DFDFDF;">300B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.10.1" style="background-color:#DFDFDF;">Clf+Dedup+PF</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.11.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.12.1" style="background-color:#DFDFDF;">A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.13.1" style="background-color:#DFDFDF;">321h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.14.1" style="background-color:#DFDFDF;">1.32 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.12.12.12.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.12.12.12.16.1" style="background-color:#DFDFDF;">Megatron</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.14.14.14" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.14.14.14.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.3.1" style="background-color:#FFFFFF;">Yuan 1.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib112" title="">112</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.14.14.14.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.4.1" style="background-color:#FFFFFF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.5.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.6.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.7.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.8.1" style="background-color:#FFFFFF;">245B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.13.13.13.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.13.13.13.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.13.13.13.1.m1.1a"><mi id="S3.T3.13.13.13.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.13.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.13.13.13.1.m1.1b"><ci id="S3.T3.13.13.13.1.m1.1.1.cmml" xref="S3.T3.13.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.13.13.13.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.13.13.13.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.2.1" style="background-color:#FFFFFF;">26k<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.14.14.14.2.1.m1.1"><semantics id="S3.T3.14.14.14.2.1.m1.1a"><msup id="S3.T3.14.14.14.2.1.m1.1.1" xref="S3.T3.14.14.14.2.1.m1.1.1.cmml"><mi id="S3.T3.14.14.14.2.1.m1.1.1a" xref="S3.T3.14.14.14.2.1.m1.1.1.cmml"></mi><mo id="S3.T3.14.14.14.2.1.m1.1.1.1" mathbackground="#FFFFFF" xref="S3.T3.14.14.14.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.14.14.14.2.1.m1.1b"><apply id="S3.T3.14.14.14.2.1.m1.1.1.cmml" xref="S3.T3.14.14.14.2.1.m1.1.1"><times id="S3.T3.14.14.14.2.1.m1.1.1.1.cmml" xref="S3.T3.14.14.14.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.14.14.14.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.14.14.14.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.9.1" style="background-color:#FFFFFF;">180B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.10.1" style="background-color:#FFFFFF;">Heur+Clf+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.11.1" style="background-color:#FFFFFF;">2128</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.12.1" style="background-color:#FFFFFF;">GPU</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.15.1" style="background-color:#FFFFFF;">D+T+P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.14.14.14.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.14.14.14.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.15.15.15" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.15.15.15.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.2.1" style="background-color:#DFDFDF;">Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib113" title="">113</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.15.15.15.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.3.1" style="background-color:#DFDFDF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.7.1" style="background-color:#DFDFDF;">280B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.15.15.15.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.15.15.15.1.m1.1a"><mo id="S3.T3.15.15.15.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.15.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.15.15.15.1.m1.1b"><times id="S3.T3.15.15.15.1.m1.1.1.cmml" xref="S3.T3.15.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.15.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.15.15.15.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.9.1" style="background-color:#DFDFDF;">300B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.10.1" style="background-color:#DFDFDF;">QF+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.11.1" style="background-color:#DFDFDF;">4096</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.12.1" style="background-color:#DFDFDF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.13.1" style="background-color:#DFDFDF;">920h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.14.1" style="background-color:#DFDFDF;">13.19 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.15.1" style="background-color:#DFDFDF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.15.15.15.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.15.15.15.16.1" style="background-color:#DFDFDF;">JAX+Haiku</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.16.16.16" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.16.16.16.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.2.1" style="background-color:#FFFFFF;">ERNIE 3.0 Titan&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib35" title="">35</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.16.16.16.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.3.1" style="background-color:#FFFFFF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.5.1" style="background-color:#FFFFFF;">Baidu</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.7.1" style="background-color:#FFFFFF;">260B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.16.16.16.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.16.16.16.1.m1.1a"><mo id="S3.T3.16.16.16.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.16.16.16.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.16.16.16.1.m1.1b"><times id="S3.T3.16.16.16.1.m1.1.1.cmml" xref="S3.T3.16.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.16.16.16.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.16.16.16.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.9.1" style="background-color:#FFFFFF;">300B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.10.1" style="background-color:#FFFFFF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.12.1" style="background-color:#FFFFFF;">Ascend 910</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.15.1" style="background-color:#FFFFFF;">D+M+P+D*</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.16.16.16.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.16.16.16.16.1" style="background-color:#FFFFFF;">PaddlePaddle</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.17.17.17" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.17.17.17.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.2.1" style="background-color:#DFDFDF;">GPT-NeoX-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib299" title="">299</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.17.17.17.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.3.1" style="background-color:#DFDFDF;">BigScience'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.5.1" style="background-color:#DFDFDF;">EleutherAI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.7.1" style="background-color:#DFDFDF;">20B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.17.17.17.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.17.17.17.1.m1.1a"><mi id="S3.T3.17.17.17.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.17.17.17.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.17.17.17.1.m1.1b"><ci id="S3.T3.17.17.17.1.m1.1.1.cmml" xref="S3.T3.17.17.17.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.17.17.17.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.17.17.17.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.8.1" style="background-color:#DFDFDF;">150k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.9.1" style="background-color:#DFDFDF;">825GB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.10.1" style="background-color:#DFDFDF;">None</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.11.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.12.1" style="background-color:#DFDFDF;">40G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.17.17.17.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.17.17.17.16.1" style="background-color:#DFDFDF;">Megatron+DS+PyTorch</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.18.18.18" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.18.18.18.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.2.1" style="background-color:#FFFFFF;">OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.18.18.18.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.4.1" style="background-color:#FFFFFF;">MIT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.5.1" style="background-color:#FFFFFF;">Meta</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.7.1" style="background-color:#FFFFFF;">175B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.18.18.18.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.18.18.18.1.m1.1a"><mi id="S3.T3.18.18.18.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.18.18.18.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.18.18.18.1.m1.1b"><ci id="S3.T3.18.18.18.1.m1.1.1.cmml" xref="S3.T3.18.18.18.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.18.18.18.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.18.18.18.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.8.1" style="background-color:#FFFFFF;">150k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.9.1" style="background-color:#FFFFFF;">180B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.10.1" style="background-color:#FFFFFF;">Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.11.1" style="background-color:#FFFFFF;">992</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.12.1" style="background-color:#FFFFFF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.15.1" style="background-color:#FFFFFF;">D+T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.18.18.18.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.18.18.18.16.1" style="background-color:#FFFFFF;">Megatron</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.19.19.19" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.19.19.19.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.2.1" style="background-color:#DFDFDF;">BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.19.19.19.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.3.1" style="background-color:#DFDFDF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.4.1" style="background-color:#DFDFDF;">RAIL-1.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.5.1" style="background-color:#DFDFDF;">BigScience</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.7.1" style="background-color:#DFDFDF;">176B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.19.19.19.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.19.19.19.1.m1.1a"><mi id="S3.T3.19.19.19.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.19.19.19.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.19.19.19.1.m1.1b"><ci id="S3.T3.19.19.19.1.m1.1.1.cmml" xref="S3.T3.19.19.19.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.19.19.19.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.19.19.19.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.9.1" style="background-color:#DFDFDF;">366B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.10.1" style="background-color:#DFDFDF;">Dedup+PR</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.11.1" style="background-color:#DFDFDF;">384</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.12.1" style="background-color:#DFDFDF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.13.1" style="background-color:#DFDFDF;">2520h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.14.1" style="background-color:#DFDFDF;">3.87 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.15.1" style="background-color:#DFDFDF;">D+T+P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.19.19.19.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.19.19.19.16.1" style="background-color:#DFDFDF;">Megatron+DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.20.20.20" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.20.20.20.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.2.1" style="background-color:#FFFFFF;">Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.20.20.20.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.5.1" style="background-color:#FFFFFF;">Meta</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.6.1" style="background-color:#FFFFFF;">Science</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.7.1" style="background-color:#FFFFFF;">120B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.20.20.20.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.20.20.20.1.m1.1a"><mo id="S3.T3.20.20.20.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.20.20.20.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.20.20.20.1.m1.1b"><times id="S3.T3.20.20.20.1.m1.1.1.cmml" xref="S3.T3.20.20.20.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.20.20.20.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.20.20.20.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.8.1" style="background-color:#FFFFFF;">225k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.9.1" style="background-color:#FFFFFF;">106B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.10.1" style="background-color:#FFFFFF;">Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.11.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.12.1" style="background-color:#FFFFFF;">80GB A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.20.20.20.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.20.20.20.16.1" style="background-color:#FFFFFF;">Metaseq</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.22.22.22" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.22.22.22.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.3.1" style="background-color:#DFDFDF;">GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.22.22.22.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.4.1" style="background-color:#DFDFDF;">ICML'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.6.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.7.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.8.1" style="background-color:#DFDFDF;">1.2T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.21.21.21.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.21.21.21.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.21.21.21.1.m1.1a"><mo id="S3.T3.21.21.21.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.21.21.21.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.21.21.21.1.m1.1b"><times id="S3.T3.21.21.21.1.m1.1.1.cmml" xref="S3.T3.21.21.21.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.21.21.21.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.21.21.21.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.2.1" style="background-color:#DFDFDF;">600k<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.22.22.22.2.1.m1.1"><semantics id="S3.T3.22.22.22.2.1.m1.1a"><msup id="S3.T3.22.22.22.2.1.m1.1.1" xref="S3.T3.22.22.22.2.1.m1.1.1.cmml"><mi id="S3.T3.22.22.22.2.1.m1.1.1a" xref="S3.T3.22.22.22.2.1.m1.1.1.cmml"></mi><mo id="S3.T3.22.22.22.2.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S3.T3.22.22.22.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.22.22.22.2.1.m1.1b"><apply id="S3.T3.22.22.22.2.1.m1.1.1.cmml" xref="S3.T3.22.22.22.2.1.m1.1.1"><times id="S3.T3.22.22.22.2.1.m1.1.1.1.cmml" xref="S3.T3.22.22.22.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.22.22.22.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.22.22.22.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.9.1" style="background-color:#DFDFDF;">600B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.10.1" style="background-color:#DFDFDF;">Clf</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.11.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.12.1" style="background-color:#DFDFDF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.22.22.22.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.22.22.22.16.1" style="background-color:#DFDFDF;">GSPMD</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.23.23.23" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.23.23.23.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.2.1" style="background-color:#FFFFFF;">LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib140" title="">140</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.23.23.23.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.5.1" style="background-color:#FFFFFF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.6.1" style="background-color:#FFFFFF;">Dialog</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.7.1" style="background-color:#FFFFFF;">137B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.23.23.23.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.23.23.23.1.m1.1a"><mo id="S3.T3.23.23.23.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.23.23.23.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.23.23.23.1.m1.1b"><times id="S3.T3.23.23.23.1.m1.1.1.cmml" xref="S3.T3.23.23.23.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.23.23.23.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.23.23.23.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.8.1" style="background-color:#FFFFFF;">3M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.9.1" style="background-color:#FFFFFF;">2.81T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.10.1" style="background-color:#FFFFFF;">Filtered</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.11.1" style="background-color:#FFFFFF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.12.1" style="background-color:#FFFFFF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.13.1" style="background-color:#FFFFFF;">1384h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.14.1" style="background-color:#FFFFFF;">4.96 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.15.1" style="background-color:#FFFFFF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.23.23.23.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.23.23.23.16.1" style="background-color:#FFFFFF;">Lingvo</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.24.24.24" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.24.24.24.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.2.1" style="background-color:#DFDFDF;">MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.24.24.24.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.3.1" style="background-color:#DFDFDF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.4.1" style="background-color:#DFDFDF;">Apache-v2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.5.1" style="background-color:#DFDFDF;">MS.+Nvidia</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.7.1" style="background-color:#DFDFDF;">530B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.24.24.24.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.24.24.24.1.m1.1a"><mo id="S3.T3.24.24.24.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.24.24.24.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.24.24.24.1.m1.1b"><times id="S3.T3.24.24.24.1.m1.1.1.cmml" xref="S3.T3.24.24.24.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.24.24.24.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.24.24.24.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.9.1" style="background-color:#DFDFDF;">270B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.11.1" style="background-color:#DFDFDF;">4480</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.12.1" style="background-color:#DFDFDF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.15.1" style="background-color:#DFDFDF;">D+T+P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.24.24.24.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.24.24.24.16.1" style="background-color:#DFDFDF;">Megatron+DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.25.25.25" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.25.25.25.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.2.1" style="background-color:#FFFFFF;">AlphaCode&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib132" title="">132</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.25.25.25.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.3.1" style="background-color:#FFFFFF;">Science'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.4.1" style="background-color:#FFFFFF;">Apache-v2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.5.1" style="background-color:#FFFFFF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.6.1" style="background-color:#FFFFFF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.7.1" style="background-color:#FFFFFF;">41B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.25.25.25.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.25.25.25.1.m1.1a"><mi id="S3.T3.25.25.25.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.25.25.25.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.25.25.25.1.m1.1b"><ci id="S3.T3.25.25.25.1.m1.1.1.cmml" xref="S3.T3.25.25.25.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.25.25.25.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.25.25.25.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.8.1" style="background-color:#FFFFFF;">205k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.9.1" style="background-color:#FFFFFF;">967B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.10.1" style="background-color:#FFFFFF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.12.1" style="background-color:#FFFFFF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.15.1" style="background-color:#FFFFFF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.25.25.25.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.25.25.25.16.1" style="background-color:#FFFFFF;">JAX+Haiku</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.26.26.26" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.26.26.26.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.2.1" style="background-color:#DFDFDF;">Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib121" title="">121</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.26.26.26.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.3.1" style="background-color:#DFDFDF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.7.1" style="background-color:#DFDFDF;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.26.26.26.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.26.26.26.1.m1.1a"><mo id="S3.T3.26.26.26.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.26.26.26.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.26.26.26.1.m1.1b"><times id="S3.T3.26.26.26.1.m1.1.1.cmml" xref="S3.T3.26.26.26.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.26.26.26.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.26.26.26.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.9.1" style="background-color:#DFDFDF;">1.4T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.10.1" style="background-color:#DFDFDF;">QF+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.12.1" style="background-color:#DFDFDF;">TPUv4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.26.26.26.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.26.26.26.16.1" style="background-color:#DFDFDF;">JAX+Haiku</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.27.27.27" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.27.27.27.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.2.1" style="background-color:#FFFFFF;">PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.27.27.27.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.5.1" style="background-color:#FFFFFF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.7.1" style="background-color:#FFFFFF;">540B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.27.27.27.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.27.27.27.1.m1.1a"><mo id="S3.T3.27.27.27.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.27.27.27.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.27.27.27.1.m1.1b"><times id="S3.T3.27.27.27.1.m1.1.1.cmml" xref="S3.T3.27.27.27.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.27.27.27.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.27.27.27.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.8.1" style="background-color:#FFFFFF;">255k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.9.1" style="background-color:#FFFFFF;">780B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.10.1" style="background-color:#FFFFFF;">Heur</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.11.1" style="background-color:#FFFFFF;">6144</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.12.1" style="background-color:#FFFFFF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.15.1" style="background-color:#FFFFFF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.27.27.27.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.27.27.27.16.1" style="background-color:#FFFFFF;">JAX+T5X</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.28.28.28" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.28.28.28.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.2.1" style="background-color:#DFDFDF;">AlexaTM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.28.28.28.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.3.1" style="background-color:#DFDFDF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.4.1" style="background-color:#DFDFDF;">Apache v2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.5.1" style="background-color:#DFDFDF;">Amazon</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.7.1" style="background-color:#DFDFDF;">20B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.28.28.28.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.28.28.28.1.m1.1a"><mo id="S3.T3.28.28.28.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.28.28.28.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.28.28.28.1.m1.1b"><times id="S3.T3.28.28.28.1.m1.1.1.cmml" xref="S3.T3.28.28.28.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.28.28.28.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.28.28.28.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.8.1" style="background-color:#DFDFDF;">500k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.9.1" style="background-color:#DFDFDF;">1.1T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.10.1" style="background-color:#DFDFDF;">Filtered</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.11.1" style="background-color:#DFDFDF;">128</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.12.1" style="background-color:#DFDFDF;">A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.13.1" style="background-color:#DFDFDF;">2880h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.14.1" style="background-color:#DFDFDF;">1.47 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.28.28.28.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.28.28.28.16.1" style="background-color:#DFDFDF;">DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.29.29.29" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.29.29.29.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.2.1" style="background-color:#FFFFFF;">U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib124" title="">124</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.29.29.29.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.5.1" style="background-color:#FFFFFF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.7.1" style="background-color:#FFFFFF;">540B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.29.29.29.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.29.29.29.1.m1.1a"><mo id="S3.T3.29.29.29.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.29.29.29.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.29.29.29.1.m1.1b"><times id="S3.T3.29.29.29.1.m1.1.1.cmml" xref="S3.T3.29.29.29.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.29.29.29.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.29.29.29.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.8.1" style="background-color:#FFFFFF;">20k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.9.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.11.1" style="background-color:#FFFFFF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.12.1" style="background-color:#FFFFFF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.13.1" style="background-color:#FFFFFF;">120h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.14.1" style="background-color:#FFFFFF;">0.25 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.29.29.29.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.29.29.29.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.30.30.30" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.30.30.30.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.2.1" style="background-color:#DFDFDF;">UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.30.30.30.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.3.1" style="background-color:#DFDFDF;">ICLR'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.7.1" style="background-color:#DFDFDF;">20B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.30.30.30.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.30.30.30.1.m1.1a"><mi id="S3.T3.30.30.30.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.30.30.30.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.30.30.30.1.m1.1b"><ci id="S3.T3.30.30.30.1.m1.1.1.cmml" xref="S3.T3.30.30.30.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.30.30.30.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.30.30.30.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.8.1" style="background-color:#DFDFDF;">2M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.9.1" style="background-color:#DFDFDF;">1T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.11.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.12.1" style="background-color:#DFDFDF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.30.30.30.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.30.30.30.16.1" style="background-color:#DFDFDF;">JAX+T5X</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.31.31.31" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.31.31.31.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.2.1" style="background-color:#FFFFFF;">GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.31.31.31.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.3.1" style="background-color:#FFFFFF;">ICLR'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.5.1" style="background-color:#FFFFFF;">Multiple</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.7.1" style="background-color:#FFFFFF;">130B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.31.31.31.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.31.31.31.1.m1.1a"><mo id="S3.T3.31.31.31.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.31.31.31.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.31.31.31.1.m1.1b"><times id="S3.T3.31.31.31.1.m1.1.1.cmml" xref="S3.T3.31.31.31.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.31.31.31.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.31.31.31.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.9.1" style="background-color:#FFFFFF;">400B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.11.1" style="background-color:#FFFFFF;">768</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.12.1" style="background-color:#FFFFFF;">40G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.13.1" style="background-color:#FFFFFF;">1440h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.14.1" style="background-color:#FFFFFF;">3.37 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.15.1" style="background-color:#FFFFFF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.31.31.31.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.31.31.31.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.32.32.32" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.32.32.32.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.2.1" style="background-color:#DFDFDF;">CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib130" title="">130</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.32.32.32.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.3.1" style="background-color:#DFDFDF;">ICLR'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.5.1" style="background-color:#DFDFDF;">Salesforce</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.6.1" style="background-color:#DFDFDF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.7.1" style="background-color:#DFDFDF;">16B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.32.32.32.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.32.32.32.1.m1.1a"><mi id="S3.T3.32.32.32.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.32.32.32.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.32.32.32.1.m1.1b"><ci id="S3.T3.32.32.32.1.m1.1.1.cmml" xref="S3.T3.32.32.32.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.32.32.32.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.32.32.32.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.8.1" style="background-color:#DFDFDF;">650k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.9.1" style="background-color:#DFDFDF;">577B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.10.1" style="background-color:#DFDFDF;">Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.12.1" style="background-color:#DFDFDF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.15.1" style="background-color:#DFDFDF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.32.32.32.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.32.32.32.16.1" style="background-color:#DFDFDF;">JAXFormer</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.33.33.33" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.33.33.33.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.2.1" style="background-color:#FFFFFF;">LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib126" title="">126</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.33.33.33.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.3.1" style="background-color:#FFFFFF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.5.1" style="background-color:#FFFFFF;">Meta</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.7.1" style="background-color:#FFFFFF;">65B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.33.33.33.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.33.33.33.1.m1.1a"><mo id="S3.T3.33.33.33.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.33.33.33.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.33.33.33.1.m1.1b"><times id="S3.T3.33.33.33.1.m1.1.1.cmml" xref="S3.T3.33.33.33.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.33.33.33.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.33.33.33.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.8.1" style="background-color:#FFFFFF;">350k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.9.1" style="background-color:#FFFFFF;">1.4T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.10.1" style="background-color:#FFFFFF;">Clf+Heur+Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.11.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.12.1" style="background-color:#FFFFFF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.13.1" style="background-color:#FFFFFF;">504h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.14.1" style="background-color:#FFFFFF;">4.12 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.15.1" style="background-color:#FFFFFF;">D+M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.33.33.33.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.33.33.33.16.1" style="background-color:#FFFFFF;">xFormers</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.35.35.35" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.34.34.34.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.34.34.34.1.1" style="background-color:#DFDFDF;">PanGu<math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.T3.34.34.34.1.1.m1.1"><semantics id="S3.T3.34.34.34.1.1.m1.1a"><mi id="S3.T3.34.34.34.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.34.34.34.1.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S3.T3.34.34.34.1.1.m1.1b"><ci id="S3.T3.34.34.34.1.1.m1.1.1.cmml" xref="S3.T3.34.34.34.1.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.34.34.34.1.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S3.T3.34.34.34.1.1.m1.1d">roman_Σ</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.35.35.35.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.3.1" style="background-color:#DFDFDF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.5.1" style="background-color:#DFDFDF;">Huawei</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.7.1" style="background-color:#DFDFDF;">1.085T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.2" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.35.35.35.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.35.35.35.2.m1.1a"><mo id="S3.T3.35.35.35.2.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.35.35.35.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.35.35.35.2.m1.1b"><times id="S3.T3.35.35.35.2.m1.1.1.cmml" xref="S3.T3.35.35.35.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.35.35.35.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.35.35.35.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.9.1" style="background-color:#DFDFDF;">329B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.11.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.12.1" style="background-color:#DFDFDF;">Ascend 910</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.13.1" style="background-color:#DFDFDF;">2400h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.15.1" style="background-color:#DFDFDF;">D+OP+P+O+R</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.35.35.35.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.35.35.35.16.1" style="background-color:#DFDFDF;">MindSpore</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.36.36.36" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.36.36.36.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.2.1" style="background-color:#FFFFFF;">BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib141" title="">141</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.36.36.36.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.3.1" style="background-color:#FFFFFF;">arXiv23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.5.1" style="background-color:#FFFFFF;">Bloomberg</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.6.1" style="background-color:#FFFFFF;">Finance</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.7.1" style="background-color:#FFFFFF;">50B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.36.36.36.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.36.36.36.1.m1.1a"><mo id="S3.T3.36.36.36.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T3.36.36.36.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.36.36.36.1.m1.1b"><times id="S3.T3.36.36.36.1.m1.1.1.cmml" xref="S3.T3.36.36.36.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.36.36.36.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.36.36.36.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.8.1" style="background-color:#FFFFFF;">139k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.9.1" style="background-color:#FFFFFF;">569B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.10.1" style="background-color:#FFFFFF;">Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.11.1" style="background-color:#FFFFFF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.12.1" style="background-color:#FFFFFF;">40G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.13.1" style="background-color:#FFFFFF;">1272h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.14.1" style="background-color:#FFFFFF;">1.97 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.15.1" style="background-color:#FFFFFF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.36.36.36.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.36.36.36.16.1" style="background-color:#FFFFFF;">PyTorch</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.37.37.37" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.37.37.37.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.2.1" style="background-color:#DFDFDF;">Xuan Yuan 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib143" title="">143</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.37.37.37.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.3.1" style="background-color:#DFDFDF;">arXiv23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.4.1" style="background-color:#DFDFDF;">RAIL-1.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.5.1" style="background-color:#DFDFDF;">Du Xiaoman</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.6.1" style="background-color:#DFDFDF;">Finance</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.7.1" style="background-color:#DFDFDF;">176B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.37.37.37.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.37.37.37.1.m1.1a"><mi id="S3.T3.37.37.37.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.37.37.37.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.37.37.37.1.m1.1b"><ci id="S3.T3.37.37.37.1.m1.1.1.cmml" xref="S3.T3.37.37.37.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.37.37.37.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.37.37.37.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.9.1" style="background-color:#DFDFDF;">366B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.10.1" style="background-color:#DFDFDF;">Filtered</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.11.1" style="background-color:#DFDFDF;">80GB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.12.1" style="background-color:#DFDFDF;">A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.15.1" style="background-color:#DFDFDF;">P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.37.37.37.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.37.37.37.16.1" style="background-color:#DFDFDF;">DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.38.38.38" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.38.38.38.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.2.1" style="background-color:#FFFFFF;">CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.38.38.38.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.3.1" style="background-color:#FFFFFF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.4.1" style="background-color:#FFFFFF;">BSD-3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.5.1" style="background-color:#FFFFFF;">Salesforce</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.6.1" style="background-color:#FFFFFF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.7.1" style="background-color:#FFFFFF;">16B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.38.38.38.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.38.38.38.1.m1.1a"><mi id="S3.T3.38.38.38.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.38.38.38.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.38.38.38.1.m1.1b"><ci id="S3.T3.38.38.38.1.m1.1.1.cmml" xref="S3.T3.38.38.38.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.38.38.38.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.38.38.38.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.8.1" style="background-color:#FFFFFF;">110k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.9.1" style="background-color:#FFFFFF;">51.5B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.10.1" style="background-color:#FFFFFF;">Dedup</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.11.1" style="background-color:#FFFFFF;">16</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.12.1" style="background-color:#FFFFFF;">40G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.38.38.38.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.38.38.38.16.1" style="background-color:#FFFFFF;">DS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.39.39.39" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T3.39.39.39.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.2.1" style="background-color:#DFDFDF;">StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib137" title="">137</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.39.39.39.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.3.1" style="background-color:#DFDFDF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.4.1" style="background-color:#DFDFDF;">OpenRAIL-M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.5.1" style="background-color:#DFDFDF;">BigCode</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.6.1" style="background-color:#DFDFDF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.7.1" style="background-color:#DFDFDF;">15.5B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.39.39.39.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.39.39.39.1.m1.1a"><mi id="S3.T3.39.39.39.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T3.39.39.39.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.39.39.39.1.m1.1b"><ci id="S3.T3.39.39.39.1.m1.1.1.cmml" xref="S3.T3.39.39.39.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.39.39.39.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.39.39.39.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.8.1" style="background-color:#DFDFDF;">250k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.9.1" style="background-color:#DFDFDF;">1T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.10.1" style="background-color:#DFDFDF;">Dedup+QF+PF</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.11.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.12.1" style="background-color:#DFDFDF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.13.1" style="background-color:#DFDFDF;">624h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.14.1" style="background-color:#DFDFDF;">1.28 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.15.1" style="background-color:#DFDFDF;">D+T+P</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.39.39.39.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.39.39.39.16.1" style="background-color:#DFDFDF;">Megatron-LM</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.40.40.40" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T3.40.40.40.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.2.1" style="background-color:#FFFFFF;">LLaMA-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T3.40.40.40.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.3.1" style="background-color:#FFFFFF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.4.1" style="background-color:#FFFFFF;">LLaMA-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.5.1" style="background-color:#FFFFFF;">Meta</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.7.1" style="background-color:#FFFFFF;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T3.40.40.40.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T3.40.40.40.1.m1.1a"><mi id="S3.T3.40.40.40.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T3.40.40.40.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T3.40.40.40.1.m1.1b"><ci id="S3.T3.40.40.40.1.m1.1.1.cmml" xref="S3.T3.40.40.40.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.40.40.40.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T3.40.40.40.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.8.1" style="background-color:#FFFFFF;">500k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.9.1" style="background-color:#FFFFFF;">2T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.10.1" style="background-color:#FFFFFF;">Minimal Filtering</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.12.1" style="background-color:#FFFFFF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.13.1" style="background-color:#FFFFFF;">1.7Mh</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.40.40.40.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.40.40.40.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.41.41.41" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.41.41.41.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.2.1" style="background-color:#DFDFDF;">PaLM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib123" title="">123</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.41.41.41.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.3.1" style="background-color:#DFDFDF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T3.41.41.41.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T3.41.41.41.1.m1.1a"><mo id="S3.T3.41.41.41.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T3.41.41.41.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T3.41.41.41.1.m1.1b"><times id="S3.T3.41.41.41.1.m1.1.1.cmml" xref="S3.T3.41.41.41.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.41.41.41.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T3.41.41.41.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.10.1" style="background-color:#DFDFDF;">Ddedup+PF+QF</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.41.41.41.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T3.41.41.41.16.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Summary of instruction tuned LLMs (&gt;10B). All abbreviations are the same as Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.T3" title="TABLE III ‣ III-G3 Pruning ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a>. Entries in <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T4.14.1">“Data/Tokens”</span> starting with <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.T4.15.2">“S-”</span> represents the number of training samples.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T4.11" style="width:433.6pt;height:117.3pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-217.6pt,58.6pt) scale(0.499095935129324,0.499095935129324) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.11.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.11.11.12.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.1" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.2" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.2.1" style="background-color:#BFBFBF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.2.1.1" style="background-color:#BFBFBF;">
<span class="ltx_tr" id="S3.T4.11.11.12.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.2.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.2.1.1.1.1.1">Publication</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.2.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.2.1.1.2.1.1" style="background-color:#FFFFFF;">Venue</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.3" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.3.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.3.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.3.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.3.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.3.1.1.1.1.1">License</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.3.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.3.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.3.1.1.2.1.1">Type</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.4" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.4.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.4.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.4.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.4.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.4.1.1.1.1.1">Model</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.4.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.4.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.4.1.1.2.1.1">Creators</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.5.1" style="background-color:#FFFFFF;">Purpose</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.6" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.T4.11.11.12.1.6.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.6.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.6.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.6.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.6.1.1.1.1.1">No. of</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.6.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.6.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.6.1.1.2.1.1">Params</span></span></span>
</span></span><span class="ltx_text" id="S3.T4.11.11.12.1.6.2" style="background-color:#FFFFFF;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.7" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.7.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.7.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.7.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.7.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.7.1.1.1.1.1">Commercial</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.7.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.7.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.7.1.1.2.1.1">Use</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.8" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.8.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.8.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.8.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.8.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.8.1.1.1.1.1">Pre-trained</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.8.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.8.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.8.1.1.2.1.1">Models</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.9" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.9.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.9.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.9.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.9.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.9.1.1.1.1.1">Steps</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.9.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.9.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.9.1.1.2.1.1">Trained</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.10" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.10.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.10.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.10.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.10.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.10.1.1.1.1.1">Data/</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.10.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.10.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.10.1.1.2.1.1">Tokens</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.11" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.11.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.11.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.11.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.11.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.11.1.1.1.1.1">No. of</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.11.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.11.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.11.1.1.2.1.1">Processing Units</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.12" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.12.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.12.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.12.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.12.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.12.1.1.1.1.1">Processing</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.12.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.12.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.12.1.1.2.1.1">Unit Type</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.13" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.13.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.13.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.13.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.13.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.13.1.1.1.1.1">Train.</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.13.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.13.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.13.1.1.2.1.1">Time</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.14" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.14.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.14.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.14.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.14.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.14.1.1.1.1.1">Calculated</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.14.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.14.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.14.1.1.2.1.1">Train. Cost</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.15" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.15.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.15.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.15.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.15.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.15.1.1.1.1.1">Train.</span></span></span>
<span class="ltx_tr" id="S3.T4.11.11.12.1.15.1.1.2">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.15.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.15.1.1.2.1.1">Parallelism</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.11.11.12.1.16" rowspan="-2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.12.1.16.1" style="background-color:#FFFFFF;">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.11.11.12.1.16.1.1">
<span class="ltx_tr" id="S3.T4.11.11.12.1.16.1.1.1">
<span class="ltx_td ltx_align_center" id="S3.T4.11.11.12.1.16.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.11.11.12.1.16.1.1.1.1.1">Library</span></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1.1" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.2.1" style="background-color:#DFDFDF;">WebGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib157" title="">157</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.3.1" style="background-color:#DFDFDF;">arXiv'21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.5.1" style="background-color:#DFDFDF;">OpenAI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.7.1" style="background-color:#DFDFDF;">175B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T4.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T4.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><times id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.8.1" style="background-color:#DFDFDF;">GPT-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.1.1.1.16.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.2" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T4.2.2.2.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.2.1" style="background-color:#FFFFFF;">T0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.2.2.2.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.3.1" style="background-color:#FFFFFF;">ICLR'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.5.1" style="background-color:#FFFFFF;">BigScience</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.7.1" style="background-color:#FFFFFF;">11B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.2.2.2.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.2.2.2.1.m1.1a"><mi id="S3.T4.2.2.2.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T4.2.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.1.m1.1b"><ci id="S3.T4.2.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.2.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.8.1" style="background-color:#FFFFFF;">T5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.9.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.10.1" style="background-color:#FFFFFF;">250B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.11.1" style="background-color:#FFFFFF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.12.1" style="background-color:#FFFFFF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.13.1" style="background-color:#FFFFFF;">270h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.14.1" style="background-color:#FFFFFF;">0.48 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.2.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.2.2.2.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.3" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T4.3.3.3.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.2.1" style="background-color:#DFDFDF;">Tk-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.3.3.3.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.3.1" style="background-color:#DFDFDF;">EMNLP'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.4.1" style="background-color:#DFDFDF;">MIT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.5.1" style="background-color:#DFDFDF;">AI2+</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.7.1" style="background-color:#DFDFDF;">11B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.3.3.3.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T4.3.3.3.1.m1.1a"><mi id="S3.T4.3.3.3.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T4.3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.1.m1.1b"><ci id="S3.T4.3.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.3.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.8.1" style="background-color:#DFDFDF;">T5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.9.1" style="background-color:#DFDFDF;">1000</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.11.1" style="background-color:#DFDFDF;">256</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.12.1" style="background-color:#DFDFDF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.13.1" style="background-color:#DFDFDF;">4h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.14.1" style="background-color:#DFDFDF;">0.0036 Mil</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.3.3.3.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.3.3.3.16.1" style="background-color:#DFDFDF;">Google T5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.4" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T4.4.4.4.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.2.1" style="background-color:#FFFFFF;">OPT-IML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.4.4.4.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.3.1" style="background-color:#FFFFFF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.5.1" style="background-color:#FFFFFF;">Meta</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.7.1" style="background-color:#FFFFFF;">175B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T4.4.4.4.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.4.4.4.1.m1.1a"><mo id="S3.T4.4.4.4.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T4.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.4.1.m1.1b"><times id="S3.T4.4.4.4.1.m1.1.1.cmml" xref="S3.T4.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.4.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.8.1" style="background-color:#FFFFFF;">OPT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.9.1" style="background-color:#FFFFFF;">8k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.10.1" style="background-color:#FFFFFF;">2B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.11.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.12.1" style="background-color:#FFFFFF;">40G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.15.1" style="background-color:#FFFFFF;">D+T</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.4.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.4.4.4.16.1" style="background-color:#FFFFFF;">Megatron</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.5.5.5" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T4.5.5.5.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.2.1" style="background-color:#DFDFDF;">Flan-U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.5.5.5.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.3.1" style="background-color:#DFDFDF;">ICLR'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.7.1" style="background-color:#DFDFDF;">540B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.5.5.5.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T4.5.5.5.1.m1.1a"><mi id="S3.T4.5.5.5.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T4.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.5.1.m1.1b"><ci id="S3.T4.5.5.5.1.m1.1.1.cmml" xref="S3.T4.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.5.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.8.1" style="background-color:#DFDFDF;">U-PaLM</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.9.1" style="background-color:#DFDFDF;">30k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.11.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.12.1" style="background-color:#DFDFDF;">TPU v4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.5.5.5.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.5.5.5.16.1" style="background-color:#DFDFDF;">JAX+T5X</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.6.6.6" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T4.6.6.6.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.2.1" style="background-color:#FFFFFF;">mT0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib145" title="">145</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.6.6.6.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.3.1" style="background-color:#FFFFFF;">ACL'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.5.1" style="background-color:#FFFFFF;">&nbsp;HuggingFace+</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.7.1" style="background-color:#FFFFFF;">13B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.6.6.6.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.6.6.6.1.m1.1a"><mi id="S3.T4.6.6.6.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T4.6.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.6.1.m1.1b"><ci id="S3.T4.6.6.6.1.m1.1.1.cmml" xref="S3.T4.6.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.6.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.8.1" style="background-color:#FFFFFF;">mT5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.9.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.6.6.6.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.6.6.6.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.7.7.7" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T4.7.7.7.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.2.1" style="background-color:#DFDFDF;">Sparrow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib158" title="">158</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.7.7.7.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.3.1" style="background-color:#DFDFDF;">arXiv'22</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.5.1" style="background-color:#DFDFDF;">Google</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.6.1" style="background-color:#DFDFDF;">Dialog</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.7.1" style="background-color:#DFDFDF;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T4.7.7.7.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T4.7.7.7.1.m1.1a"><mo id="S3.T4.7.7.7.1.m1.1.1" mathbackground="#DFDFDF" xref="S3.T4.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.7.1.m1.1b"><times id="S3.T4.7.7.7.1.m1.1.1.cmml" xref="S3.T4.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.7.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.8.1" style="background-color:#DFDFDF;">Chinchilla</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.11.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.12.1" style="background-color:#DFDFDF;">TPU v3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.15.1" style="background-color:#DFDFDF;">M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.7.7.7.16.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.8.8.8" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T4.8.8.8.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.2.1" style="background-color:#FFFFFF;">WizardCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib155" title="">155</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.8.8.8.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.3.1" style="background-color:#FFFFFF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.5.1" style="background-color:#FFFFFF;">HK Bapt.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.6.1" style="background-color:#FFFFFF;">Coding</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.7.1" style="background-color:#FFFFFF;">15B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T4.8.8.8.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.8.8.8.1.m1.1a"><mo id="S3.T4.8.8.8.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T4.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.8.1.m1.1b"><times id="S3.T4.8.8.8.1.m1.1.1.cmml" xref="S3.T4.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T4.8.8.8.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.8.1" style="background-color:#FFFFFF;">StarCoder</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.9.1" style="background-color:#FFFFFF;">200</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.10.1" style="background-color:#FFFFFF;">S-78k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.8.8.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.8.8.8.16.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.9.9.9" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T4.9.9.9.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.2.1" style="background-color:#DFDFDF;">Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib149" title="">149</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.9.9.9.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.3.1" style="background-color:#DFDFDF;">Github'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.4.1" style="background-color:#DFDFDF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.5.1" style="background-color:#DFDFDF;">Stanford</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.6.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.7.1" style="background-color:#DFDFDF;">13B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.9.9.9.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S3.T4.9.9.9.1.m1.1a"><mi id="S3.T4.9.9.9.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S3.T4.9.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.9.9.9.1.m1.1b"><ci id="S3.T4.9.9.9.1.m1.1.1.cmml" xref="S3.T4.9.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.9.9.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.9.9.9.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.8.1" style="background-color:#DFDFDF;">LLaMA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.9.1" style="background-color:#DFDFDF;">3-Epoch</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.10.1" style="background-color:#DFDFDF;">S-52k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.11.1" style="background-color:#DFDFDF;">8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.12.1" style="background-color:#DFDFDF;">80G A100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.13.1" style="background-color:#DFDFDF;">3h</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.14.1" style="background-color:#DFDFDF;">600</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.15.1" style="background-color:#DFDFDF;">FSDP</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.9.9.9.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.9.9.9.16.1" style="background-color:#DFDFDF;">PyTorch</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.10.10.10" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S3.T4.10.10.10.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.2.1" style="background-color:#FFFFFF;">Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib150" title="">150</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.10.10.10.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.3.1" style="background-color:#FFFFFF;">Github'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.5.1" style="background-color:#FFFFFF;">LMSYS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.7.1" style="background-color:#FFFFFF;">13B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S3.T4.10.10.10.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.10.10.10.1.m1.1a"><mi id="S3.T4.10.10.10.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S3.T4.10.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T4.10.10.10.1.m1.1b"><ci id="S3.T4.10.10.10.1.m1.1.1.cmml" xref="S3.T4.10.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.10.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S3.T4.10.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.8.1" style="background-color:#FFFFFF;">LLaMA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.9.1" style="background-color:#FFFFFF;">3-Epoch</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.10.1" style="background-color:#FFFFFF;">S-125k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.15.1" style="background-color:#FFFFFF;">FSDP</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.10.10.10.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.10.10.10.16.1" style="background-color:#FFFFFF;">PyTorch</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.11.11.13.1" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S3.T4.11.11.13.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.1.1" style="background-color:#DFDFDF;">LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib176" title="">176</a>]</cite></span></td>
<td class="ltx_td ltx_align_left" id="S3.T4.11.11.13.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.2.1" style="background-color:#DFDFDF;">arXiv'23</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.3.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.4.1" style="background-color:#DFDFDF;">Meta+</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.5.1" style="background-color:#DFDFDF;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.6.1" style="background-color:#DFDFDF;">65B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.8.1" style="background-color:#DFDFDF;">LLaMA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.9.1" style="background-color:#DFDFDF;">15-Epoch</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.10.1" style="background-color:#DFDFDF;">S-1000</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.11.11.13.1.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.13.1.16.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.11.11.11" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.11.11.11.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.2.1" style="background-color:#FFFFFF;">Koala&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib300" title="">300</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.11.11.11.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.3.1" style="background-color:#FFFFFF;">Github'23</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.4.1" style="background-color:#FFFFFF;">Apache-2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.5.1" style="background-color:#FFFFFF;">UC-Berkley</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.6" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.6.1" style="background-color:#FFFFFF;">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.7" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.7.1" style="background-color:#FFFFFF;">13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.1" style="padding-left:0.0pt;padding-right:0.0pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T4.11.11.11.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S3.T4.11.11.11.1.m1.1a"><mo id="S3.T4.11.11.11.1.m1.1.1" mathbackground="#FFFFFF" xref="S3.T4.11.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T4.11.11.11.1.m1.1b"><times id="S3.T4.11.11.11.1.m1.1.1.cmml" xref="S3.T4.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.11.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T4.11.11.11.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.8" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.8.1" style="background-color:#FFFFFF;">LLaMA</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.9" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.9.1" style="background-color:#FFFFFF;">2-Epoch</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.10" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.10.1" style="background-color:#FFFFFF;">S-472k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.11" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.11.1" style="background-color:#FFFFFF;">8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.12" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.12.1" style="background-color:#FFFFFF;">A100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.13" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.13.1" style="background-color:#FFFFFF;">6h</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.14" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.14.1" style="background-color:#FFFFFF;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.15" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.15.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.11.11.11.16" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S3.T4.11.11.11.16.1" style="background-color:#FFFFFF;">JAX/FLAX</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Findings &amp; Insights</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Training a billion-scale model is difficult as compared to a smaller model. LLMs are prone to various instabilities during training, such as hardware failure and instability. Other than this, LLMs exhibit different behaviors such as emergent abilities, improved zero-shot, few-shot, and reasoning abilities. Researchers report these essential details in their papers for results reproduction and field progress. We identify critical information in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.T1" title="TABLE I ‣ Xuan Yuan 2.0 [143] ‣ III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">I</span></a>&nbsp;and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.T2" title="TABLE II ‣ Xuan Yuan 2.0 [143] ‣ III-A5 Finance ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">II</span></a> such as architecture, training strategies, and pipelines that improve LLMs’ performance or other abilities acquired because of changes mentioned in section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3" title="III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Model Configurations</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We provide different statistics of pre-trained and instruction-tuned models in this section. This includes information such as publication venue, license type, model creators, steps trained, parallelism, etc in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.T3" title="TABLE III ‣ III-G3 Pruning ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">III</span></a> and Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.T4" title="TABLE IV ‣ III-G3 Pruning ‣ III-G Efficient LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">IV</span></a>. Architecture details of pre-trained LLMs are available in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5.T5" title="TABLE V ‣ V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">V</span></a>. Providing these details for instruction-tuned models is unnecessary because it fine-tunes pre-trained models for instruction datasets. Hence, architectural details are the same as the baselines. Moreover, optimization settings for various LLMs are available in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5.T6" title="TABLE VI ‣ V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VI</span></a> and Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5.T7" title="TABLE VII ‣ V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VII</span></a>. We do not include details on precision, warmup, and weight decay in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S5.T7" title="TABLE VII ‣ V Model Configurations ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VII</span></a>. Neither of these details are important as others to mention for instruction-tuned models nor provided by the papers. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Architecture details of LLMs. Here, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.T5.25.1">“PE”</span> is the positional embedding, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.T5.26.2">“nL”</span> is the number of layers, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.T5.27.3">“nH”</span> is the number of attention heads, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S5.T5.28.4">“HS”</span> is the size of hidden states.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.20" style="width:433.6pt;height:422.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.6pt,133.3pt) scale(0.613397914076865,0.613397914076865) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.20.20">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.20.20.21.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.2.1" style="background-color:#BFBFBF;">Type</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.20.20.21.1.3.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S5.T5.20.20.21.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.21.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.3.1.1.1.1" style="background-color:#BFBFBF;">Training</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.21.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.21.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.3.1.2.1.1" style="background-color:#FFFFFF;">Objective</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.4.1" style="background-color:#BFBFBF;">Attention</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.5"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.5.1" style="background-color:#BFBFBF;">Vocab</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.6"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.6.1" style="background-color:#BFBFBF;">Tokenizer</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.7"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.7.1" style="background-color:#BFBFBF;">Norm</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.8"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.8.1" style="background-color:#BFBFBF;">PE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.9"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.9.1" style="background-color:#BFBFBF;">Activation</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.10"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.10.1" style="background-color:#BFBFBF;">Bias</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.11"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.11.1" style="background-color:#BFBFBF;">nL</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.12"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.12.1" style="background-color:#BFBFBF;">nH</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S5.T5.20.20.21.1.13"><span class="ltx_text ltx_font_bold" id="S5.T5.20.20.21.1.13.1" style="background-color:#BFBFBF;">HS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.1.1.1.2"><span class="ltx_text" id="S5.T5.1.1.1.2.1" style="background-color:#FFFFFF;">T5&nbsp;(11B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.3"><span class="ltx_text" id="S5.T5.1.1.1.3.1" style="background-color:#FFFFFF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.4"><span class="ltx_text" id="S5.T5.1.1.1.4.1" style="background-color:#FFFFFF;">Span Corruption</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.5"><span class="ltx_text" id="S5.T5.1.1.1.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.6"><span class="ltx_text" id="S5.T5.1.1.1.6.1" style="background-color:#FFFFFF;">32k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.7"><span class="ltx_text" id="S5.T5.1.1.1.7.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.8"><span class="ltx_text" id="S5.T5.1.1.1.8.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.9"><span class="ltx_text" id="S5.T5.1.1.1.9.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1.10"><span class="ltx_text" id="S5.T5.1.1.1.10.1" style="background-color:#FFFFFF;">ReLU</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T5.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><times id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.11"><span class="ltx_text" id="S5.T5.1.1.1.11.1" style="background-color:#FFFFFF;">24</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.12"><span class="ltx_text" id="S5.T5.1.1.1.12.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.13"><span class="ltx_text" id="S5.T5.1.1.1.13.1" style="background-color:#FFFFFF;">1024</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.22.1" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.22.1.1"><span class="ltx_text" id="S5.T5.20.20.22.1.1.1" style="background-color:#DFDFDF;">GPT3&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.2"><span class="ltx_text" id="S5.T5.20.20.22.1.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.22.1.3"><span class="ltx_text" id="S5.T5.20.20.22.1.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.4"><span class="ltx_text" id="S5.T5.20.20.22.1.4.1" style="background-color:#DFDFDF;">Dense+Sparse</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.5"><span class="ltx_text" id="S5.T5.20.20.22.1.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.6"><span class="ltx_text" id="S5.T5.20.20.22.1.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.7"><span class="ltx_text" id="S5.T5.20.20.22.1.7.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.8"><span class="ltx_text" id="S5.T5.20.20.22.1.8.1" style="background-color:#DFDFDF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.22.1.9"><span class="ltx_text" id="S5.T5.20.20.22.1.9.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.22.1.10"><span class="ltx_text" id="S5.T5.20.20.22.1.10.1" style="background-color:#DFDFDF;">✓</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.22.1.11"><span class="ltx_text" id="S5.T5.20.20.22.1.11.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.22.1.12"><span class="ltx_text" id="S5.T5.20.20.22.1.12.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.22.1.13"><span class="ltx_text" id="S5.T5.20.20.22.1.13.1" style="background-color:#DFDFDF;">12288</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.23.2" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.23.2.1"><span class="ltx_text" id="S5.T5.20.20.23.2.1.1" style="background-color:#FFFFFF;">mT5&nbsp;(13B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.2"><span class="ltx_text" id="S5.T5.20.20.23.2.2.1" style="background-color:#FFFFFF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.23.2.3"><span class="ltx_text" id="S5.T5.20.20.23.2.3.1" style="background-color:#FFFFFF;">Span Corruption</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.4"><span class="ltx_text" id="S5.T5.20.20.23.2.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.5"><span class="ltx_text" id="S5.T5.20.20.23.2.5.1" style="background-color:#FFFFFF;">250k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.6"><span class="ltx_text" id="S5.T5.20.20.23.2.6.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.7"><span class="ltx_text" id="S5.T5.20.20.23.2.7.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.8"><span class="ltx_text" id="S5.T5.20.20.23.2.8.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.23.2.9"><span class="ltx_text" id="S5.T5.20.20.23.2.9.1" style="background-color:#FFFFFF;">ReLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.23.2.10"><span class="ltx_text" id="S5.T5.20.20.23.2.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.23.2.11"><span class="ltx_text" id="S5.T5.20.20.23.2.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.23.2.12"><span class="ltx_text" id="S5.T5.20.20.23.2.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.23.2.13"><span class="ltx_text" id="S5.T5.20.20.23.2.13.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2.2" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.2.2.2.1"><span class="ltx_text" id="S5.T5.2.2.2.1.1" style="background-color:#DFDFDF;">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T5.2.2.2.1.1.m1.1"><semantics id="S5.T5.2.2.2.1.1.m1.1a"><mi id="S5.T5.2.2.2.1.1.m1.1.1" mathbackground="#DFDFDF" xref="S5.T5.2.2.2.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.1.m1.1b"><ci id="S5.T5.2.2.2.1.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.1.1.m1.1d">italic_α</annotation></semantics></math>&nbsp;(200B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.2"><span class="ltx_text" id="S5.T5.2.2.2.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.2.2.2.3"><span class="ltx_text" id="S5.T5.2.2.2.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.4"><span class="ltx_text" id="S5.T5.2.2.2.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.5"><span class="ltx_text" id="S5.T5.2.2.2.5.1" style="background-color:#DFDFDF;">40k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.6"><span class="ltx_text" id="S5.T5.2.2.2.6.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.7"><span class="ltx_text" id="S5.T5.2.2.2.7.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.8"><span class="ltx_text" id="S5.T5.2.2.2.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2.9"><span class="ltx_text" id="S5.T5.2.2.2.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.2.2.2.10"><span class="ltx_text" id="S5.T5.2.2.2.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.2.2.2.11"><span class="ltx_text" id="S5.T5.2.2.2.11.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.2.2.2.12"><span class="ltx_text" id="S5.T5.2.2.2.12.1" style="background-color:#DFDFDF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.2.2.2.13"><span class="ltx_text" id="S5.T5.2.2.2.13.1" style="background-color:#DFDFDF;">16384</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.24.3" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.24.3.1"><span class="ltx_text" id="S5.T5.20.20.24.3.1.1" style="background-color:#FFFFFF;">CPM-2&nbsp;(198B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.2"><span class="ltx_text" id="S5.T5.20.20.24.3.2.1" style="background-color:#FFFFFF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.24.3.3"><span class="ltx_text" id="S5.T5.20.20.24.3.3.1" style="background-color:#FFFFFF;">Span Corruption</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.4"><span class="ltx_text" id="S5.T5.20.20.24.3.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.5"><span class="ltx_text" id="S5.T5.20.20.24.3.5.1" style="background-color:#FFFFFF;">250k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.6"><span class="ltx_text" id="S5.T5.20.20.24.3.6.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.7"><span class="ltx_text" id="S5.T5.20.20.24.3.7.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.8"><span class="ltx_text" id="S5.T5.20.20.24.3.8.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.24.3.9"><span class="ltx_text" id="S5.T5.20.20.24.3.9.1" style="background-color:#FFFFFF;">ReLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.24.3.10"><span class="ltx_text" id="S5.T5.20.20.24.3.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.24.3.11"><span class="ltx_text" id="S5.T5.20.20.24.3.11.1" style="background-color:#FFFFFF;">24</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.24.3.12"><span class="ltx_text" id="S5.T5.20.20.24.3.12.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.24.3.13"><span class="ltx_text" id="S5.T5.20.20.24.3.13.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.25.4" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.25.4.1"><span class="ltx_text" id="S5.T5.20.20.25.4.1.1" style="background-color:#DFDFDF;">Codex&nbsp;(12B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.2"><span class="ltx_text" id="S5.T5.20.20.25.4.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.25.4.3"><span class="ltx_text" id="S5.T5.20.20.25.4.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.4"><span class="ltx_text" id="S5.T5.20.20.25.4.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.5"><span class="ltx_text" id="S5.T5.20.20.25.4.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.6"><span class="ltx_text" id="S5.T5.20.20.25.4.6.1" style="background-color:#DFDFDF;">BPE+</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.7"><span class="ltx_text" id="S5.T5.20.20.25.4.7.1" style="background-color:#DFDFDF;">Pre-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.8"><span class="ltx_text" id="S5.T5.20.20.25.4.8.1" style="background-color:#DFDFDF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.25.4.9"><span class="ltx_text" id="S5.T5.20.20.25.4.9.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.25.4.10"><span class="ltx_text" id="S5.T5.20.20.25.4.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.25.4.11"><span class="ltx_text" id="S5.T5.20.20.25.4.11.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.25.4.12"><span class="ltx_text" id="S5.T5.20.20.25.4.12.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.25.4.13"><span class="ltx_text" id="S5.T5.20.20.25.4.13.1" style="background-color:#DFDFDF;">12288</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.26.5" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.26.5.1"><span class="ltx_text" id="S5.T5.20.20.26.5.1.1" style="background-color:#FFFFFF;">ERNIE 3.0&nbsp;(10B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.2"><span class="ltx_text" id="S5.T5.20.20.26.5.2.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.26.5.3"><span class="ltx_text" id="S5.T5.20.20.26.5.3.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.4"><span class="ltx_text" id="S5.T5.20.20.26.5.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.5"><span class="ltx_text" id="S5.T5.20.20.26.5.5.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.6"><span class="ltx_text" id="S5.T5.20.20.26.5.6.1" style="background-color:#FFFFFF;">WordPiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.7"><span class="ltx_text" id="S5.T5.20.20.26.5.7.1" style="background-color:#FFFFFF;">Post-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.8"><span class="ltx_text" id="S5.T5.20.20.26.5.8.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.26.5.9"><span class="ltx_text" id="S5.T5.20.20.26.5.9.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.26.5.10"><span class="ltx_text" id="S5.T5.20.20.26.5.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.26.5.11"><span class="ltx_text" id="S5.T5.20.20.26.5.11.1" style="background-color:#FFFFFF;">48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.26.5.12"><span class="ltx_text" id="S5.T5.20.20.26.5.12.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.26.5.13"><span class="ltx_text" id="S5.T5.20.20.26.5.13.1" style="background-color:#FFFFFF;">4096</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.4.4" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.4.4.4.3"><span class="ltx_text" id="S5.T5.4.4.4.3.1" style="background-color:#DFDFDF;">Jurassic-1&nbsp;(178B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.4"><span class="ltx_text" id="S5.T5.4.4.4.4.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.4.4.4.5"><span class="ltx_text" id="S5.T5.4.4.4.5.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.6"><span class="ltx_text" id="S5.T5.4.4.4.6.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.7"><span class="ltx_text" id="S5.T5.4.4.4.7.1" style="background-color:#DFDFDF;">256k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.3.1"><span class="ltx_text" id="S5.T5.3.3.3.1.1" style="background-color:#DFDFDF;">SentencePiece<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S5.T5.3.3.3.1.1.m1.1"><semantics id="S5.T5.3.3.3.1.1.m1.1a"><msup id="S5.T5.3.3.3.1.1.m1.1.1" xref="S5.T5.3.3.3.1.1.m1.1.1.cmml"><mi id="S5.T5.3.3.3.1.1.m1.1.1a" xref="S5.T5.3.3.3.1.1.m1.1.1.cmml"></mi><mo id="S5.T5.3.3.3.1.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S5.T5.3.3.3.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.1.1.m1.1b"><apply id="S5.T5.3.3.3.1.1.m1.1.1.cmml" xref="S5.T5.3.3.3.1.1.m1.1.1"><times id="S5.T5.3.3.3.1.1.m1.1.1.1.cmml" xref="S5.T5.3.3.3.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.8"><span class="ltx_text" id="S5.T5.4.4.4.8.1" style="background-color:#DFDFDF;">Pre-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.9"><span class="ltx_text" id="S5.T5.4.4.4.9.1" style="background-color:#DFDFDF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.4.4.4.10"><span class="ltx_text" id="S5.T5.4.4.4.10.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.4.4.4.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.4.4.4.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.4.4.4.2.m1.1a"><mi id="S5.T5.4.4.4.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T5.4.4.4.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.2.m1.1b"><ci id="S5.T5.4.4.4.2.m1.1.1.cmml" xref="S5.T5.4.4.4.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.4.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.4.4.4.11"><span class="ltx_text" id="S5.T5.4.4.4.11.1" style="background-color:#DFDFDF;">76</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.4.4.4.12"><span class="ltx_text" id="S5.T5.4.4.4.12.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.4.4.4.13"><span class="ltx_text" id="S5.T5.4.4.4.13.1" style="background-color:#DFDFDF;">13824</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.27.6" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.27.6.1"><span class="ltx_text" id="S5.T5.20.20.27.6.1.1" style="background-color:#FFFFFF;">HyperCLOVA&nbsp;(82B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.2"><span class="ltx_text" id="S5.T5.20.20.27.6.2.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.27.6.3"><span class="ltx_text" id="S5.T5.20.20.27.6.3.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.4"><span class="ltx_text" id="S5.T5.20.20.27.6.4.1" style="background-color:#FFFFFF;">Dense+Sparse</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.5"><span class="ltx_text" id="S5.T5.20.20.27.6.5.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.6"><span class="ltx_text" id="S5.T5.20.20.27.6.6.1" style="background-color:#FFFFFF;">BPE*</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.7"><span class="ltx_text" id="S5.T5.20.20.27.6.7.1" style="background-color:#FFFFFF;">Pre-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.8"><span class="ltx_text" id="S5.T5.20.20.27.6.8.1" style="background-color:#FFFFFF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.27.6.9"><span class="ltx_text" id="S5.T5.20.20.27.6.9.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.27.6.10"><span class="ltx_text" id="S5.T5.20.20.27.6.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.27.6.11"><span class="ltx_text" id="S5.T5.20.20.27.6.11.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.27.6.12"><span class="ltx_text" id="S5.T5.20.20.27.6.12.1" style="background-color:#FFFFFF;">80</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.27.6.13"><span class="ltx_text" id="S5.T5.20.20.27.6.13.1" style="background-color:#FFFFFF;">10240</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.28.7" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.28.7.1"><span class="ltx_text" id="S5.T5.20.20.28.7.1.1" style="background-color:#DFDFDF;">Yuan 1.0&nbsp;(245B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.2"><span class="ltx_text" id="S5.T5.20.20.28.7.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.28.7.3"><span class="ltx_text" id="S5.T5.20.20.28.7.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.4"><span class="ltx_text" id="S5.T5.20.20.28.7.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.5"><span class="ltx_text" id="S5.T5.20.20.28.7.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.6"><span class="ltx_text" id="S5.T5.20.20.28.7.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.7"><span class="ltx_text" id="S5.T5.20.20.28.7.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.8"><span class="ltx_text" id="S5.T5.20.20.28.7.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.28.7.9"><span class="ltx_text" id="S5.T5.20.20.28.7.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.28.7.10"><span class="ltx_text" id="S5.T5.20.20.28.7.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.28.7.11"><span class="ltx_text" id="S5.T5.20.20.28.7.11.1" style="background-color:#DFDFDF;">76</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.28.7.12"><span class="ltx_text" id="S5.T5.20.20.28.7.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.28.7.13"><span class="ltx_text" id="S5.T5.20.20.28.7.13.1" style="background-color:#DFDFDF;">16384</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.5.5" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.5.5.5.2"><span class="ltx_text" id="S5.T5.5.5.5.2.1" style="background-color:#FFFFFF;">Gopher&nbsp;(280B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.3"><span class="ltx_text" id="S5.T5.5.5.5.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.5.5.5.4"><span class="ltx_text" id="S5.T5.5.5.5.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.5"><span class="ltx_text" id="S5.T5.5.5.5.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.6"><span class="ltx_text" id="S5.T5.5.5.5.6.1" style="background-color:#FFFFFF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.7"><span class="ltx_text" id="S5.T5.5.5.5.7.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.8"><span class="ltx_text" id="S5.T5.5.5.5.8.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.9"><span class="ltx_text" id="S5.T5.5.5.5.9.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5.10"><span class="ltx_text" id="S5.T5.5.5.5.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.5.5.5.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.5.5.5.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.5.5.5.1.m1.1a"><mi id="S5.T5.5.5.5.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.1.m1.1b"><ci id="S5.T5.5.5.5.1.m1.1.1.cmml" xref="S5.T5.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.5.5.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.5.5.5.11"><span class="ltx_text" id="S5.T5.5.5.5.11.1" style="background-color:#FFFFFF;">80</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.5.5.5.12"><span class="ltx_text" id="S5.T5.5.5.5.12.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.5.5.5.13"><span class="ltx_text" id="S5.T5.5.5.5.13.1" style="background-color:#FFFFFF;">16384</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.29.8" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.29.8.1"><span class="ltx_text" id="S5.T5.20.20.29.8.1.1" style="background-color:#DFDFDF;">ERNIE 3.0 Titan&nbsp;(260B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.2"><span class="ltx_text" id="S5.T5.20.20.29.8.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.29.8.3"><span class="ltx_text" id="S5.T5.20.20.29.8.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.4"><span class="ltx_text" id="S5.T5.20.20.29.8.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.5"><span class="ltx_text" id="S5.T5.20.20.29.8.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.6"><span class="ltx_text" id="S5.T5.20.20.29.8.6.1" style="background-color:#DFDFDF;">WordPiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.7"><span class="ltx_text" id="S5.T5.20.20.29.8.7.1" style="background-color:#DFDFDF;">Post-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.8"><span class="ltx_text" id="S5.T5.20.20.29.8.8.1" style="background-color:#DFDFDF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.29.8.9"><span class="ltx_text" id="S5.T5.20.20.29.8.9.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.29.8.10"><span class="ltx_text" id="S5.T5.20.20.29.8.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.29.8.11"><span class="ltx_text" id="S5.T5.20.20.29.8.11.1" style="background-color:#DFDFDF;">48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.29.8.12"><span class="ltx_text" id="S5.T5.20.20.29.8.12.1" style="background-color:#DFDFDF;">192</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.29.8.13"><span class="ltx_text" id="S5.T5.20.20.29.8.13.1" style="background-color:#DFDFDF;">12288</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.6.6" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.6.6.6.2"><span class="ltx_text" id="S5.T5.6.6.6.2.1" style="background-color:#FFFFFF;">GPT-NeoX-20B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.3"><span class="ltx_text" id="S5.T5.6.6.6.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.6.6.6.4"><span class="ltx_text" id="S5.T5.6.6.6.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.5"><span class="ltx_text" id="S5.T5.6.6.6.5.1" style="background-color:#FFFFFF;">Parallel</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.6"><span class="ltx_text" id="S5.T5.6.6.6.6.1" style="background-color:#FFFFFF;">50k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.7"><span class="ltx_text" id="S5.T5.6.6.6.7.1" style="background-color:#FFFFFF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.8"><span class="ltx_text" id="S5.T5.6.6.6.8.1" style="background-color:#FFFFFF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.9"><span class="ltx_text" id="S5.T5.6.6.6.9.1" style="background-color:#FFFFFF;">Rotary</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.6.10"><span class="ltx_text" id="S5.T5.6.6.6.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.6.6.6.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.6.6.6.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.6.6.6.1.m1.1a"><mi id="S5.T5.6.6.6.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.6.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.1.m1.1b"><ci id="S5.T5.6.6.6.1.m1.1.1.cmml" xref="S5.T5.6.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.6.6.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.6.6.6.11"><span class="ltx_text" id="S5.T5.6.6.6.11.1" style="background-color:#FFFFFF;">44</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.6.6.6.12"><span class="ltx_text" id="S5.T5.6.6.6.12.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.6.6.6.13"><span class="ltx_text" id="S5.T5.6.6.6.13.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7.7" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.7.7.7.2"><span class="ltx_text" id="S5.T5.7.7.7.2.1" style="background-color:#DFDFDF;">OPT&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.3"><span class="ltx_text" id="S5.T5.7.7.7.3.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.7.7.7.4"><span class="ltx_text" id="S5.T5.7.7.7.4.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.5"><span class="ltx_text" id="S5.T5.7.7.7.5.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.6"><span class="ltx_text" id="S5.T5.7.7.7.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.7"><span class="ltx_text" id="S5.T5.7.7.7.7.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.8"><span class="ltx_text" id="S5.T5.7.7.7.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.9"><span class="ltx_text" id="S5.T5.7.7.7.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.7.7.10"><span class="ltx_text" id="S5.T5.7.7.7.10.1" style="background-color:#DFDFDF;">ReLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.7.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.7.7.7.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.7.7.7.1.m1.1a"><mi id="S5.T5.7.7.7.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T5.7.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.1.m1.1b"><ci id="S5.T5.7.7.7.1.m1.1.1.cmml" xref="S5.T5.7.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.7.7.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.7.7.7.11"><span class="ltx_text" id="S5.T5.7.7.7.11.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.7.7.7.12"><span class="ltx_text" id="S5.T5.7.7.7.12.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.7.7.7.13"><span class="ltx_text" id="S5.T5.7.7.7.13.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.8.8.8" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.8.8.8.2"><span class="ltx_text" id="S5.T5.8.8.8.2.1" style="background-color:#FFFFFF;">BLOOM&nbsp;(176B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.3"><span class="ltx_text" id="S5.T5.8.8.8.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.8.8.8.4"><span class="ltx_text" id="S5.T5.8.8.8.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.5"><span class="ltx_text" id="S5.T5.8.8.8.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.6"><span class="ltx_text" id="S5.T5.8.8.8.6.1" style="background-color:#FFFFFF;">250k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.7"><span class="ltx_text" id="S5.T5.8.8.8.7.1" style="background-color:#FFFFFF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.8"><span class="ltx_text" id="S5.T5.8.8.8.8.1" style="background-color:#FFFFFF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.9"><span class="ltx_text" id="S5.T5.8.8.8.9.1" style="background-color:#FFFFFF;">ALiBi</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.8.8.8.10"><span class="ltx_text" id="S5.T5.8.8.8.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.8.8.8.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.8.8.8.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.8.8.8.1.m1.1a"><mi id="S5.T5.8.8.8.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.8.8.8.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.8.1.m1.1b"><ci id="S5.T5.8.8.8.1.m1.1.1.cmml" xref="S5.T5.8.8.8.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.8.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.8.8.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.8.8.8.11"><span class="ltx_text" id="S5.T5.8.8.8.11.1" style="background-color:#FFFFFF;">70</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.8.8.8.12"><span class="ltx_text" id="S5.T5.8.8.8.12.1" style="background-color:#FFFFFF;">112</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.8.8.8.13"><span class="ltx_text" id="S5.T5.8.8.8.13.1" style="background-color:#FFFFFF;">14336</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.9.9" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.9.9.9.2"><span class="ltx_text" id="S5.T5.9.9.9.2.1" style="background-color:#DFDFDF;">Galactica&nbsp;(120B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.3"><span class="ltx_text" id="S5.T5.9.9.9.3.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.9.9.9.4"><span class="ltx_text" id="S5.T5.9.9.9.4.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.5"><span class="ltx_text" id="S5.T5.9.9.9.5.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.6"><span class="ltx_text" id="S5.T5.9.9.9.6.1" style="background-color:#DFDFDF;">50k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.7"><span class="ltx_text" id="S5.T5.9.9.9.7.1" style="background-color:#DFDFDF;">BPE+custom</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.8"><span class="ltx_text" id="S5.T5.9.9.9.8.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.9"><span class="ltx_text" id="S5.T5.9.9.9.9.1" style="background-color:#DFDFDF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.9.10"><span class="ltx_text" id="S5.T5.9.9.9.10.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.9.9.9.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T5.9.9.9.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.9.9.9.1.m1.1a"><mo id="S5.T5.9.9.9.1.m1.1.1" mathbackground="#DFDFDF" xref="S5.T5.9.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.9.1.m1.1b"><times id="S5.T5.9.9.9.1.m1.1.1.cmml" xref="S5.T5.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.9.9.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.9.9.9.11"><span class="ltx_text" id="S5.T5.9.9.9.11.1" style="background-color:#DFDFDF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.9.9.9.12"><span class="ltx_text" id="S5.T5.9.9.9.12.1" style="background-color:#DFDFDF;">80</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.9.9.9.13"><span class="ltx_text" id="S5.T5.9.9.9.13.1" style="background-color:#DFDFDF;">10240</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.10.10.10" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.10.10.10.2"><span class="ltx_text" id="S5.T5.10.10.10.2.1" style="background-color:#FFFFFF;">GLaM&nbsp;(1.2T)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.3"><span class="ltx_text" id="S5.T5.10.10.10.3.1" style="background-color:#FFFFFF;">MoE-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.10.10.10.4"><span class="ltx_text" id="S5.T5.10.10.10.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.5"><span class="ltx_text" id="S5.T5.10.10.10.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.6"><span class="ltx_text" id="S5.T5.10.10.10.6.1" style="background-color:#FFFFFF;">256k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.7"><span class="ltx_text" id="S5.T5.10.10.10.7.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.8"><span class="ltx_text" id="S5.T5.10.10.10.8.1" style="background-color:#FFFFFF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.9"><span class="ltx_text" id="S5.T5.10.10.10.9.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.10.10.10.10"><span class="ltx_text" id="S5.T5.10.10.10.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.10.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.10.10.10.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.10.10.10.1.m1.1a"><mi id="S5.T5.10.10.10.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.10.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.10.10.10.1.m1.1b"><ci id="S5.T5.10.10.10.1.m1.1.1.cmml" xref="S5.T5.10.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.10.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.10.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.10.10.10.11"><span class="ltx_text" id="S5.T5.10.10.10.11.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.10.10.10.12"><span class="ltx_text" id="S5.T5.10.10.10.12.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.10.10.10.13"><span class="ltx_text" id="S5.T5.10.10.10.13.1" style="background-color:#FFFFFF;">32768</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.30.9" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.30.9.1"><span class="ltx_text" id="S5.T5.20.20.30.9.1.1" style="background-color:#DFDFDF;">LaMDA&nbsp;(137B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.2"><span class="ltx_text" id="S5.T5.20.20.30.9.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.30.9.3"><span class="ltx_text" id="S5.T5.20.20.30.9.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.4"><span class="ltx_text" id="S5.T5.20.20.30.9.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.5"><span class="ltx_text" id="S5.T5.20.20.30.9.5.1" style="background-color:#DFDFDF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.6"><span class="ltx_text" id="S5.T5.20.20.30.9.6.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.7"><span class="ltx_text" id="S5.T5.20.20.30.9.7.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.8"><span class="ltx_text" id="S5.T5.20.20.30.9.8.1" style="background-color:#DFDFDF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.30.9.9"><span class="ltx_text" id="S5.T5.20.20.30.9.9.1" style="background-color:#DFDFDF;">GeGLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.30.9.10"><span class="ltx_text" id="S5.T5.20.20.30.9.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.30.9.11"><span class="ltx_text" id="S5.T5.20.20.30.9.11.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.30.9.12"><span class="ltx_text" id="S5.T5.20.20.30.9.12.1" style="background-color:#DFDFDF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.30.9.13"><span class="ltx_text" id="S5.T5.20.20.30.9.13.1" style="background-color:#DFDFDF;">8192</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.11.11.11" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.11.11.11.2"><span class="ltx_text" id="S5.T5.11.11.11.2.1" style="background-color:#FFFFFF;">MT-NLG&nbsp;(530B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.3"><span class="ltx_text" id="S5.T5.11.11.11.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.11.11.11.4"><span class="ltx_text" id="S5.T5.11.11.11.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.5"><span class="ltx_text" id="S5.T5.11.11.11.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.6"><span class="ltx_text" id="S5.T5.11.11.11.6.1" style="background-color:#FFFFFF;">50k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.7"><span class="ltx_text" id="S5.T5.11.11.11.7.1" style="background-color:#FFFFFF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.8"><span class="ltx_text" id="S5.T5.11.11.11.8.1" style="background-color:#FFFFFF;">Pre-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.9"><span class="ltx_text" id="S5.T5.11.11.11.9.1" style="background-color:#FFFFFF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.11.11.11.10"><span class="ltx_text" id="S5.T5.11.11.11.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.11.11.11.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.11.11.11.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.11.11.11.1.m1.1a"><mi id="S5.T5.11.11.11.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.11.11.11.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.11.11.11.1.m1.1b"><ci id="S5.T5.11.11.11.1.m1.1.1.cmml" xref="S5.T5.11.11.11.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.11.11.11.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.11.11.11.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.11.11.11.11"><span class="ltx_text" id="S5.T5.11.11.11.11.1" style="background-color:#FFFFFF;">105</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.11.11.11.12"><span class="ltx_text" id="S5.T5.11.11.11.12.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.11.11.11.13"><span class="ltx_text" id="S5.T5.11.11.11.13.1" style="background-color:#FFFFFF;">20480</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.31.10" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.31.10.1"><span class="ltx_text" id="S5.T5.20.20.31.10.1.1" style="background-color:#DFDFDF;">AlphaCode&nbsp;(41B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.2"><span class="ltx_text" id="S5.T5.20.20.31.10.2.1" style="background-color:#DFDFDF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.31.10.3"><span class="ltx_text" id="S5.T5.20.20.31.10.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.4"><span class="ltx_text" id="S5.T5.20.20.31.10.4.1" style="background-color:#DFDFDF;">Multi-query</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.5"><span class="ltx_text" id="S5.T5.20.20.31.10.5.1" style="background-color:#DFDFDF;">8k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.6"><span class="ltx_text" id="S5.T5.20.20.31.10.6.1" style="background-color:#DFDFDF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.7"><span class="ltx_text" id="S5.T5.20.20.31.10.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.8"><span class="ltx_text" id="S5.T5.20.20.31.10.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.31.10.9"><span class="ltx_text" id="S5.T5.20.20.31.10.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.31.10.10"><span class="ltx_text" id="S5.T5.20.20.31.10.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.31.10.11"><span class="ltx_text" id="S5.T5.20.20.31.10.11.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.31.10.12"><span class="ltx_text" id="S5.T5.20.20.31.10.12.1" style="background-color:#DFDFDF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.31.10.13"><span class="ltx_text" id="S5.T5.20.20.31.10.13.1" style="background-color:#DFDFDF;">6144</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.12.12.12" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.12.12.12.2"><span class="ltx_text" id="S5.T5.12.12.12.2.1" style="background-color:#FFFFFF;">Chinchilla&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.3"><span class="ltx_text" id="S5.T5.12.12.12.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.12.12.12.4"><span class="ltx_text" id="S5.T5.12.12.12.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.5"><span class="ltx_text" id="S5.T5.12.12.12.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.6"><span class="ltx_text" id="S5.T5.12.12.12.6.1" style="background-color:#FFFFFF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.7"><span class="ltx_text" id="S5.T5.12.12.12.7.1" style="background-color:#FFFFFF;">SentencePiece-NFKC</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.8"><span class="ltx_text" id="S5.T5.12.12.12.8.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.9"><span class="ltx_text" id="S5.T5.12.12.12.9.1" style="background-color:#FFFFFF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.12.12.12.10"><span class="ltx_text" id="S5.T5.12.12.12.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.12.12.12.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.12.12.12.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.12.12.12.1.m1.1a"><mi id="S5.T5.12.12.12.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.12.12.12.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.12.12.12.1.m1.1b"><ci id="S5.T5.12.12.12.1.m1.1.1.cmml" xref="S5.T5.12.12.12.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.12.12.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.12.12.12.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.12.12.12.11"><span class="ltx_text" id="S5.T5.12.12.12.11.1" style="background-color:#FFFFFF;">80</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.12.12.12.12"><span class="ltx_text" id="S5.T5.12.12.12.12.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.12.12.12.13"><span class="ltx_text" id="S5.T5.12.12.12.13.1" style="background-color:#FFFFFF;">8192</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.13.13.13" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.13.13.13.2"><span class="ltx_text" id="S5.T5.13.13.13.2.1" style="background-color:#DFDFDF;">PaLM&nbsp;(540B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.3"><span class="ltx_text" id="S5.T5.13.13.13.3.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.13.13.13.4"><span class="ltx_text" id="S5.T5.13.13.13.4.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.5"><span class="ltx_text" id="S5.T5.13.13.13.5.1" style="background-color:#DFDFDF;">Parallel+Multi-query</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.6"><span class="ltx_text" id="S5.T5.13.13.13.6.1" style="background-color:#DFDFDF;">256k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.7"><span class="ltx_text" id="S5.T5.13.13.13.7.1" style="background-color:#DFDFDF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.8"><span class="ltx_text" id="S5.T5.13.13.13.8.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.9"><span class="ltx_text" id="S5.T5.13.13.13.9.1" style="background-color:#DFDFDF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.13.13.13.10"><span class="ltx_text" id="S5.T5.13.13.13.10.1" style="background-color:#DFDFDF;">SwiGLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.13.13.13.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T5.13.13.13.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.13.13.13.1.m1.1a"><mo id="S5.T5.13.13.13.1.m1.1.1" mathbackground="#DFDFDF" xref="S5.T5.13.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T5.13.13.13.1.m1.1b"><times id="S5.T5.13.13.13.1.m1.1.1.cmml" xref="S5.T5.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T5.13.13.13.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.13.13.13.11"><span class="ltx_text" id="S5.T5.13.13.13.11.1" style="background-color:#DFDFDF;">118</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.13.13.13.12"><span class="ltx_text" id="S5.T5.13.13.13.12.1" style="background-color:#DFDFDF;">48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.13.13.13.13"><span class="ltx_text" id="S5.T5.13.13.13.13.1" style="background-color:#DFDFDF;">18432</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.14.14.14" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.14.14.14.2"><span class="ltx_text" id="S5.T5.14.14.14.2.1" style="background-color:#FFFFFF;">AlexaTM&nbsp;(20B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.3"><span class="ltx_text" id="S5.T5.14.14.14.3.1" style="background-color:#FFFFFF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.14.14.14.4"><span class="ltx_text" id="S5.T5.14.14.14.4.1" style="background-color:#FFFFFF;">Denoising</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.5"><span class="ltx_text" id="S5.T5.14.14.14.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.6"><span class="ltx_text" id="S5.T5.14.14.14.6.1" style="background-color:#FFFFFF;">150k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.7"><span class="ltx_text" id="S5.T5.14.14.14.7.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.8"><span class="ltx_text" id="S5.T5.14.14.14.8.1" style="background-color:#FFFFFF;">Pre-Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.9"><span class="ltx_text" id="S5.T5.14.14.14.9.1" style="background-color:#FFFFFF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.14.14.14.10"><span class="ltx_text" id="S5.T5.14.14.14.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.14.14.14.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.14.14.14.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.14.14.14.1.m1.1a"><mi id="S5.T5.14.14.14.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.14.14.14.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.14.14.14.1.m1.1b"><ci id="S5.T5.14.14.14.1.m1.1.1.cmml" xref="S5.T5.14.14.14.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.14.14.14.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.14.14.14.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.14.14.14.11"><span class="ltx_text" id="S5.T5.14.14.14.11.1" style="background-color:#FFFFFF;">78</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.14.14.14.12"><span class="ltx_text" id="S5.T5.14.14.14.12.1" style="background-color:#FFFFFF;">32</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.14.14.14.13"><span class="ltx_text" id="S5.T5.14.14.14.13.1" style="background-color:#FFFFFF;">4096</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.16.16.16" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.16.16.16.3"><span class="ltx_text" id="S5.T5.16.16.16.3.1" style="background-color:#DFDFDF;">Sparrow&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.4"><span class="ltx_text" id="S5.T5.16.16.16.4.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.16.16.16.5"><span class="ltx_text" id="S5.T5.16.16.16.5.1" style="background-color:#DFDFDF;">Pref.&amp;Rule RM</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.6"><span class="ltx_text" id="S5.T5.16.16.16.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.7"><span class="ltx_text" id="S5.T5.16.16.16.7.1" style="background-color:#DFDFDF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.8"><span class="ltx_text" id="S5.T5.16.16.16.8.1" style="background-color:#DFDFDF;">SentencePiece-NFKC</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.9"><span class="ltx_text" id="S5.T5.16.16.16.9.1" style="background-color:#DFDFDF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.10"><span class="ltx_text" id="S5.T5.16.16.16.10.1" style="background-color:#DFDFDF;">Relative</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.16.16.16.11"><span class="ltx_text" id="S5.T5.16.16.16.11.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.15.15.15.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.15.15.15.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.15.15.15.1.m1.1a"><mi id="S5.T5.15.15.15.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T5.15.15.15.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.15.15.15.1.m1.1b"><ci id="S5.T5.15.15.15.1.m1.1.1.cmml" xref="S5.T5.15.15.15.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.15.15.15.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.15.15.15.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.16.16.16.2"><span class="ltx_text" id="S5.T5.16.16.16.2.1" style="background-color:#DFDFDF;">16<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S5.T5.16.16.16.2.1.m1.1"><semantics id="S5.T5.16.16.16.2.1.m1.1a"><msup id="S5.T5.16.16.16.2.1.m1.1.1" xref="S5.T5.16.16.16.2.1.m1.1.1.cmml"><mi id="S5.T5.16.16.16.2.1.m1.1.1a" xref="S5.T5.16.16.16.2.1.m1.1.1.cmml"></mi><mo id="S5.T5.16.16.16.2.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S5.T5.16.16.16.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T5.16.16.16.2.1.m1.1b"><apply id="S5.T5.16.16.16.2.1.m1.1.1.cmml" xref="S5.T5.16.16.16.2.1.m1.1.1"><times id="S5.T5.16.16.16.2.1.m1.1.1.1.cmml" xref="S5.T5.16.16.16.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.16.16.16.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.16.16.16.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.16.16.16.12"><span class="ltx_text" id="S5.T5.16.16.16.12.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.16.16.16.13"><span class="ltx_text" id="S5.T5.16.16.16.13.1" style="background-color:#DFDFDF;">8192</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.17.17.17" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.17.17.17.2"><span class="ltx_text" id="S5.T5.17.17.17.2.1" style="background-color:#FFFFFF;">U-PaLM&nbsp;(540B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.3"><span class="ltx_text" id="S5.T5.17.17.17.3.1" style="background-color:#FFFFFF;">Non-Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.17.17.17.4"><span class="ltx_text" id="S5.T5.17.17.17.4.1" style="background-color:#FFFFFF;">MoD</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.5"><span class="ltx_text" id="S5.T5.17.17.17.5.1" style="background-color:#FFFFFF;">Parallel+Multi-query</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.6"><span class="ltx_text" id="S5.T5.17.17.17.6.1" style="background-color:#FFFFFF;">256k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.7"><span class="ltx_text" id="S5.T5.17.17.17.7.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.8"><span class="ltx_text" id="S5.T5.17.17.17.8.1" style="background-color:#FFFFFF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.9"><span class="ltx_text" id="S5.T5.17.17.17.9.1" style="background-color:#FFFFFF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.17.17.17.10"><span class="ltx_text" id="S5.T5.17.17.17.10.1" style="background-color:#FFFFFF;">SwiGLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.17.17.17.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T5.17.17.17.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.17.17.17.1.m1.1a"><mo id="S5.T5.17.17.17.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T5.17.17.17.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T5.17.17.17.1.m1.1b"><times id="S5.T5.17.17.17.1.m1.1.1.cmml" xref="S5.T5.17.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.17.17.17.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T5.17.17.17.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.17.17.17.11"><span class="ltx_text" id="S5.T5.17.17.17.11.1" style="background-color:#FFFFFF;">118</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.17.17.17.12"><span class="ltx_text" id="S5.T5.17.17.17.12.1" style="background-color:#FFFFFF;">48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.17.17.17.13"><span class="ltx_text" id="S5.T5.17.17.17.13.1" style="background-color:#FFFFFF;">18432</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.32.11" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.32.11.1"><span class="ltx_text" id="S5.T5.20.20.32.11.1.1" style="background-color:#DFDFDF;">UL2&nbsp;(20B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.2"><span class="ltx_text" id="S5.T5.20.20.32.11.2.1" style="background-color:#DFDFDF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.32.11.3"><span class="ltx_text" id="S5.T5.20.20.32.11.3.1" style="background-color:#DFDFDF;">MoD</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.4"><span class="ltx_text" id="S5.T5.20.20.32.11.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.5"><span class="ltx_text" id="S5.T5.20.20.32.11.5.1" style="background-color:#DFDFDF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.6"><span class="ltx_text" id="S5.T5.20.20.32.11.6.1" style="background-color:#DFDFDF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.7"><span class="ltx_text" id="S5.T5.20.20.32.11.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.8"><span class="ltx_text" id="S5.T5.20.20.32.11.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.32.11.9"><span class="ltx_text" id="S5.T5.20.20.32.11.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.32.11.10"><span class="ltx_text" id="S5.T5.20.20.32.11.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.32.11.11"><span class="ltx_text" id="S5.T5.20.20.32.11.11.1" style="background-color:#DFDFDF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.32.11.12"><span class="ltx_text" id="S5.T5.20.20.32.11.12.1" style="background-color:#DFDFDF;">16</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.32.11.13"><span class="ltx_text" id="S5.T5.20.20.32.11.13.1" style="background-color:#DFDFDF;">4096</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.33.12" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.33.12.1"><span class="ltx_text" id="S5.T5.20.20.33.12.1.1" style="background-color:#FFFFFF;">GLM (130B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.2"><span class="ltx_text" id="S5.T5.20.20.33.12.2.1" style="background-color:#FFFFFF;">Non-Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.33.12.3"><span class="ltx_text" id="S5.T5.20.20.33.12.3.1" style="background-color:#FFFFFF;">AR Blank Infilling</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.4"><span class="ltx_text" id="S5.T5.20.20.33.12.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.5"><span class="ltx_text" id="S5.T5.20.20.33.12.5.1" style="background-color:#FFFFFF;">130k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.6"><span class="ltx_text" id="S5.T5.20.20.33.12.6.1" style="background-color:#FFFFFF;">SentencePiece</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.7"><span class="ltx_text" id="S5.T5.20.20.33.12.7.1" style="background-color:#FFFFFF;">Deep</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.8"><span class="ltx_text" id="S5.T5.20.20.33.12.8.1" style="background-color:#FFFFFF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.33.12.9"><span class="ltx_text" id="S5.T5.20.20.33.12.9.1" style="background-color:#FFFFFF;">GeGLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.33.12.10"><span class="ltx_text" id="S5.T5.20.20.33.12.10.1" style="background-color:#FFFFFF;">✓</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.33.12.11"><span class="ltx_text" id="S5.T5.20.20.33.12.11.1" style="background-color:#FFFFFF;">70</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.33.12.12"><span class="ltx_text" id="S5.T5.20.20.33.12.12.1" style="background-color:#FFFFFF;">96</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.33.12.13"><span class="ltx_text" id="S5.T5.20.20.33.12.13.1" style="background-color:#FFFFFF;">12288</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.34.13" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.34.13.1"><span class="ltx_text" id="S5.T5.20.20.34.13.1.1" style="background-color:#DFDFDF;">CodeGen&nbsp;(16B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.2"><span class="ltx_text" id="S5.T5.20.20.34.13.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.34.13.3"><span class="ltx_text" id="S5.T5.20.20.34.13.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.4"><span class="ltx_text" id="S5.T5.20.20.34.13.4.1" style="background-color:#DFDFDF;">Parallel</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.5"><span class="ltx_text" id="S5.T5.20.20.34.13.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.6"><span class="ltx_text" id="S5.T5.20.20.34.13.6.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.7"><span class="ltx_text" id="S5.T5.20.20.34.13.7.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.8"><span class="ltx_text" id="S5.T5.20.20.34.13.8.1" style="background-color:#DFDFDF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.34.13.9"><span class="ltx_text" id="S5.T5.20.20.34.13.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.34.13.10"><span class="ltx_text" id="S5.T5.20.20.34.13.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.34.13.11"><span class="ltx_text" id="S5.T5.20.20.34.13.11.1" style="background-color:#DFDFDF;">34</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.34.13.12"><span class="ltx_text" id="S5.T5.20.20.34.13.12.1" style="background-color:#DFDFDF;">24</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.34.13.13"><span class="ltx_text" id="S5.T5.20.20.34.13.13.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.35.14" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.35.14.1"><span class="ltx_text" id="S5.T5.20.20.35.14.1.1" style="background-color:#FFFFFF;">LLaMA&nbsp;(65B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.2"><span class="ltx_text" id="S5.T5.20.20.35.14.2.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.35.14.3"><span class="ltx_text" id="S5.T5.20.20.35.14.3.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.4"><span class="ltx_text" id="S5.T5.20.20.35.14.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.5"><span class="ltx_text" id="S5.T5.20.20.35.14.5.1" style="background-color:#FFFFFF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.6"><span class="ltx_text" id="S5.T5.20.20.35.14.6.1" style="background-color:#FFFFFF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.7"><span class="ltx_text" id="S5.T5.20.20.35.14.7.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.8"><span class="ltx_text" id="S5.T5.20.20.35.14.8.1" style="background-color:#FFFFFF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.35.14.9"><span class="ltx_text" id="S5.T5.20.20.35.14.9.1" style="background-color:#FFFFFF;">SwiGLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.35.14.10"><span class="ltx_text" id="S5.T5.20.20.35.14.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.35.14.11"><span class="ltx_text" id="S5.T5.20.20.35.14.11.1" style="background-color:#FFFFFF;">80</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.35.14.12"><span class="ltx_text" id="S5.T5.20.20.35.14.12.1" style="background-color:#FFFFFF;">64</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.35.14.13"><span class="ltx_text" id="S5.T5.20.20.35.14.13.1" style="background-color:#FFFFFF;">8192</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.18.18.18" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.18.18.18.1"><span class="ltx_text" id="S5.T5.18.18.18.1.1" style="background-color:#DFDFDF;">PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S5.T5.18.18.18.1.1.m1.1"><semantics id="S5.T5.18.18.18.1.1.m1.1a"><mi id="S5.T5.18.18.18.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T5.18.18.18.1.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S5.T5.18.18.18.1.1.m1.1b"><ci id="S5.T5.18.18.18.1.1.m1.1.1.cmml" xref="S5.T5.18.18.18.1.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.18.18.18.1.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S5.T5.18.18.18.1.1.m1.1d">roman_Σ</annotation></semantics></math>&nbsp;(1085B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.2"><span class="ltx_text" id="S5.T5.18.18.18.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.18.18.18.3"><span class="ltx_text" id="S5.T5.18.18.18.3.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.4"><span class="ltx_text" id="S5.T5.18.18.18.4.1" style="background-color:#DFDFDF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.5"><span class="ltx_text" id="S5.T5.18.18.18.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.6"><span class="ltx_text" id="S5.T5.18.18.18.6.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.7"><span class="ltx_text" id="S5.T5.18.18.18.7.1" style="background-color:#DFDFDF;">Fused Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.8"><span class="ltx_text" id="S5.T5.18.18.18.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.18.18.18.9"><span class="ltx_text" id="S5.T5.18.18.18.9.1" style="background-color:#DFDFDF;">FastGeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.18.18.18.10"><span class="ltx_text" id="S5.T5.18.18.18.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.18.18.18.11"><span class="ltx_text" id="S5.T5.18.18.18.11.1" style="background-color:#DFDFDF;">40</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.18.18.18.12"><span class="ltx_text" id="S5.T5.18.18.18.12.1" style="background-color:#DFDFDF;">40</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.18.18.18.13"><span class="ltx_text" id="S5.T5.18.18.18.13.1" style="background-color:#DFDFDF;">5120</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.19.19.19" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.19.19.19.2"><span class="ltx_text" id="S5.T5.19.19.19.2.1" style="background-color:#FFFFFF;">BloombergGPT&nbsp;(50B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.3"><span class="ltx_text" id="S5.T5.19.19.19.3.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.19.19.19.4"><span class="ltx_text" id="S5.T5.19.19.19.4.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.5"><span class="ltx_text" id="S5.T5.19.19.19.5.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.6"><span class="ltx_text" id="S5.T5.19.19.19.6.1" style="background-color:#FFFFFF;">131k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.7"><span class="ltx_text" id="S5.T5.19.19.19.7.1" style="background-color:#FFFFFF;">Unigram</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.8"><span class="ltx_text" id="S5.T5.19.19.19.8.1" style="background-color:#FFFFFF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.9"><span class="ltx_text" id="S5.T5.19.19.19.9.1" style="background-color:#FFFFFF;">ALiBi</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.19.19.19.10"><span class="ltx_text" id="S5.T5.19.19.19.10.1" style="background-color:#FFFFFF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.19.19.19.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.19.19.19.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T5.19.19.19.1.m1.1a"><mi id="S5.T5.19.19.19.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T5.19.19.19.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.19.19.19.1.m1.1b"><ci id="S5.T5.19.19.19.1.m1.1.1.cmml" xref="S5.T5.19.19.19.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.19.19.19.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.19.19.19.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.19.19.19.11"><span class="ltx_text" id="S5.T5.19.19.19.11.1" style="background-color:#FFFFFF;">70</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.19.19.19.12"><span class="ltx_text" id="S5.T5.19.19.19.12.1" style="background-color:#FFFFFF;">40</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.19.19.19.13"><span class="ltx_text" id="S5.T5.19.19.19.13.1" style="background-color:#FFFFFF;">7680</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.20" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.20.2"><span class="ltx_text" id="S5.T5.20.20.20.2.1" style="background-color:#DFDFDF;">Xuan Yuan 2.0&nbsp;(176B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.3"><span class="ltx_text" id="S5.T5.20.20.20.3.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.20.4"><span class="ltx_text" id="S5.T5.20.20.20.4.1" style="background-color:#DFDFDF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.5"><span class="ltx_text" id="S5.T5.20.20.20.5.1" style="background-color:#DFDFDF;">Self</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.6"><span class="ltx_text" id="S5.T5.20.20.20.6.1" style="background-color:#DFDFDF;">250k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.7"><span class="ltx_text" id="S5.T5.20.20.20.7.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.8"><span class="ltx_text" id="S5.T5.20.20.20.8.1" style="background-color:#DFDFDF;">Layer</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.9"><span class="ltx_text" id="S5.T5.20.20.20.9.1" style="background-color:#DFDFDF;">ALiBi</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.20.10"><span class="ltx_text" id="S5.T5.20.20.20.10.1" style="background-color:#DFDFDF;">GeLU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.20.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T5.20.20.20.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T5.20.20.20.1.m1.1a"><mi id="S5.T5.20.20.20.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T5.20.20.20.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T5.20.20.20.1.m1.1b"><ci id="S5.T5.20.20.20.1.m1.1.1.cmml" xref="S5.T5.20.20.20.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.20.20.20.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T5.20.20.20.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.20.11"><span class="ltx_text" id="S5.T5.20.20.20.11.1" style="background-color:#DFDFDF;">70</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.20.12"><span class="ltx_text" id="S5.T5.20.20.20.12.1" style="background-color:#DFDFDF;">112</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.20.13"><span class="ltx_text" id="S5.T5.20.20.20.13.1" style="background-color:#DFDFDF;">14336</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.36.15" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.36.15.1"><span class="ltx_text" id="S5.T5.20.20.36.15.1.1" style="background-color:#FFFFFF;">CodeT5+&nbsp;(16B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.2"><span class="ltx_text" id="S5.T5.20.20.36.15.2.1" style="background-color:#FFFFFF;">Enc-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.36.15.3"><span class="ltx_text" id="S5.T5.20.20.36.15.3.1" style="background-color:#FFFFFF;">SC+NT+Cont.+Match</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.4"><span class="ltx_text" id="S5.T5.20.20.36.15.4.1" style="background-color:#FFFFFF;">Standard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.5"><span class="ltx_text" id="S5.T5.20.20.36.15.5.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.6"><span class="ltx_text" id="S5.T5.20.20.36.15.6.1" style="background-color:#FFFFFF;">Code-Specific</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.7"><span class="ltx_text" id="S5.T5.20.20.36.15.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.8"><span class="ltx_text" id="S5.T5.20.20.36.15.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.36.15.9"><span class="ltx_text" id="S5.T5.20.20.36.15.9.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.36.15.10"><span class="ltx_text" id="S5.T5.20.20.36.15.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.36.15.11"><span class="ltx_text" id="S5.T5.20.20.36.15.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.36.15.12"><span class="ltx_text" id="S5.T5.20.20.36.15.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.36.15.13"><span class="ltx_text" id="S5.T5.20.20.36.15.13.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.37.16" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.37.16.1"><span class="ltx_text" id="S5.T5.20.20.37.16.1.1" style="background-color:#DFDFDF;">StarCoder&nbsp;(15.5B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.2"><span class="ltx_text" id="S5.T5.20.20.37.16.2.1" style="background-color:#DFDFDF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.37.16.3"><span class="ltx_text" id="S5.T5.20.20.37.16.3.1" style="background-color:#DFDFDF;">FIM</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.4"><span class="ltx_text" id="S5.T5.20.20.37.16.4.1" style="background-color:#DFDFDF;">Multi-query</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.5"><span class="ltx_text" id="S5.T5.20.20.37.16.5.1" style="background-color:#DFDFDF;">49k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.6"><span class="ltx_text" id="S5.T5.20.20.37.16.6.1" style="background-color:#DFDFDF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.7"><span class="ltx_text" id="S5.T5.20.20.37.16.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.8"><span class="ltx_text" id="S5.T5.20.20.37.16.8.1" style="background-color:#DFDFDF;">Learned</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.37.16.9"><span class="ltx_text" id="S5.T5.20.20.37.16.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.37.16.10"><span class="ltx_text" id="S5.T5.20.20.37.16.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.37.16.11"><span class="ltx_text" id="S5.T5.20.20.37.16.11.1" style="background-color:#DFDFDF;">40</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.37.16.12"><span class="ltx_text" id="S5.T5.20.20.37.16.12.1" style="background-color:#DFDFDF;">48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.37.16.13"><span class="ltx_text" id="S5.T5.20.20.37.16.13.1" style="background-color:#DFDFDF;">6144</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.38.17" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T5.20.20.38.17.1"><span class="ltx_text" id="S5.T5.20.20.38.17.1.1" style="background-color:#FFFFFF;">LLaMA&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.2"><span class="ltx_text" id="S5.T5.20.20.38.17.2.1" style="background-color:#FFFFFF;">Causal-Dec</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.38.17.3"><span class="ltx_text" id="S5.T5.20.20.38.17.3.1" style="background-color:#FFFFFF;">Next Token</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.4"><span class="ltx_text" id="S5.T5.20.20.38.17.4.1" style="background-color:#FFFFFF;">Grouped-query</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.5"><span class="ltx_text" id="S5.T5.20.20.38.17.5.1" style="background-color:#FFFFFF;">32k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.6"><span class="ltx_text" id="S5.T5.20.20.38.17.6.1" style="background-color:#FFFFFF;">BPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.7"><span class="ltx_text" id="S5.T5.20.20.38.17.7.1" style="background-color:#FFFFFF;">Pre-RMS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.8"><span class="ltx_text" id="S5.T5.20.20.38.17.8.1" style="background-color:#FFFFFF;">RoPE</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.20.20.38.17.9"><span class="ltx_text" id="S5.T5.20.20.38.17.9.1" style="background-color:#FFFFFF;">SwiGLUE</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.38.17.10"><span class="ltx_text" id="S5.T5.20.20.38.17.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.38.17.11"><span class="ltx_text" id="S5.T5.20.20.38.17.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.38.17.12"><span class="ltx_text" id="S5.T5.20.20.38.17.12.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.20.20.38.17.13"><span class="ltx_text" id="S5.T5.20.20.38.17.13.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.20.20.39.18" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.20.20.39.18.1"><span class="ltx_text" id="S5.T5.20.20.39.18.1.1" style="background-color:#DFDFDF;">PaLM-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.2"><span class="ltx_text" id="S5.T5.20.20.39.18.2.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.20.20.39.18.3"><span class="ltx_text" id="S5.T5.20.20.39.18.3.1" style="background-color:#DFDFDF;">MoD</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.4"><span class="ltx_text" id="S5.T5.20.20.39.18.4.1" style="background-color:#DFDFDF;">Parallel</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.5"><span class="ltx_text" id="S5.T5.20.20.39.18.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.6"><span class="ltx_text" id="S5.T5.20.20.39.18.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.7"><span class="ltx_text" id="S5.T5.20.20.39.18.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.8"><span class="ltx_text" id="S5.T5.20.20.39.18.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.20.20.39.18.9"><span class="ltx_text" id="S5.T5.20.20.39.18.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.20.20.39.18.10"><span class="ltx_text" id="S5.T5.20.20.39.18.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.20.20.39.18.11"><span class="ltx_text" id="S5.T5.20.20.39.18.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.20.20.39.18.12"><span class="ltx_text" id="S5.T5.20.20.39.18.12.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.20.20.39.18.13"><span class="ltx_text" id="S5.T5.20.20.39.18.13.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively, for most of the LLMs.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.132" style="width:433.6pt;height:445.1pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.3pt,101.8pt) scale(0.685853505769741,0.685853505769741) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T6.132.132">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.132.132.133.1">
<td class="ltx_td ltx_border_tt" id="S5.T6.132.132.133.1.1"></td>
<td class="ltx_td ltx_border_tt" id="S5.T6.132.132.133.1.2"></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T6.132.132.133.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.3.1" style="background-color:#BFBFBF;">Sequence</span></td>
<td class="ltx_td ltx_border_tt" id="S5.T6.132.132.133.1.4"></td>
<td class="ltx_td ltx_border_tt" id="S5.T6.132.132.133.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.132.132.133.1.6"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.6.1" style="background-color:#BFBFBF;">LR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T6.132.132.133.1.7"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.7.1" style="background-color:#BFBFBF;">Optimizers</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T6.132.132.133.1.8"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.8.1" style="background-color:#BFBFBF;">Precision</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.132.132.133.1.9"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.9.1" style="background-color:#BFBFBF;">Weight</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.132.132.133.1.10"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.133.1.10.1" style="background-color:#BFBFBF;">Grad</span></td>
<td class="ltx_td ltx_border_tt" id="S5.T6.132.132.133.1.11"></td>
</tr>
<tr class="ltx_tr" id="S5.T6.132.132.134.2" style="background-color:#BFBFBF;">
<td class="ltx_td ltx_align_left" id="S5.T6.132.132.134.2.1"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.1.1" style="background-color:#BFBFBF;">Models</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.132.132.134.2.2"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.2.1" style="background-color:#BFBFBF;">Batch Size</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.132.132.134.2.3"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.3.1" style="background-color:#BFBFBF;">Length</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.134.2.4"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.4.1" style="background-color:#BFBFBF;">LR</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.134.2.5"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.5.1" style="background-color:#BFBFBF;">Warmup</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.132.132.134.2.6"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.6.1" style="background-color:#BFBFBF;">Decay</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.134.2.7" style="width:29.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.7.1" style="background-color:#BFBFBF;">AdaFactor</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.134.2.8" style="width:17.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.8.1" style="background-color:#BFBFBF;">Adam</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.132.132.134.2.9" style="width:24.2pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.9.1" style="background-color:#BFBFBF;">AdamW</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.134.2.10" style="width:14.2pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.10.1" style="background-color:#BFBFBF;">FP16</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.134.2.11" style="width:14.2pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.11.1" style="background-color:#BFBFBF;">BF16</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.132.132.134.2.12" style="width:17.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T6.132.132.134.2.12.1" style="background-color:#BFBFBF;">Mixed</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.134.2.13"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.13.1" style="background-color:#BFBFBF;">Decay</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.134.2.14"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.14.1" style="background-color:#BFBFBF;">Clip</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.134.2.15"><span class="ltx_text ltx_font_bold" id="S5.T6.132.132.134.2.15.1" style="background-color:#BFBFBF;">Dropout</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.4.4.4" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.4.4.4.5"><span class="ltx_text" id="S5.T6.4.4.4.5.1" style="background-color:#DFDFDF;">T5&nbsp;(11B)</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T6.1.1.1.1"><span class="ltx_text" id="S5.T6.1.1.1.1.1" style="background-color:#DFDFDF;">2<math alttext="{}^{11}" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.1.m1.1a"><msup id="S5.T6.1.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T6.1.1.1.1.1.m1.1.1a" xref="S5.T6.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S5.T6.1.1.1.1.1.m1.1.1.1" mathbackground="#DFDFDF" xref="S5.T6.1.1.1.1.1.m1.1.1.1.cmml">11</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.1.m1.1b"><apply id="S5.T6.1.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.1.m1.1.1"><cn id="S5.T6.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S5.T6.1.1.1.1.1.m1.1.1.1">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.1.m1.1c">{}^{11}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 11 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T6.4.4.4.6"><span class="ltx_text" id="S5.T6.4.4.4.6.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.4.7"><span class="ltx_text" id="S5.T6.4.4.4.7.1" style="background-color:#DFDFDF;">0.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.2.2"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.2.2.2.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.2.2.2.2.m1.1a"><mo id="S5.T6.2.2.2.2.m1.1.1" mathbackground="#DFDFDF" xref="S5.T6.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.m1.1b"><times id="S5.T6.2.2.2.2.m1.1.1.cmml" xref="S5.T6.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.4.4.4.8"><span class="ltx_text" id="S5.T6.4.4.4.8.1" style="background-color:#DFDFDF;">inverse square root</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T6.3.3.3.3" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.3.3.3.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.3.3.3.3.1.1.m1.1"><semantics id="S5.T6.3.3.3.3.1.1.m1.1a"><mi id="S5.T6.3.3.3.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.3.3.3.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.3.3.1.1.m1.1b"><ci id="S5.T6.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T6.3.3.3.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.3.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.3.3.3.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_t" id="S5.T6.4.4.4.9" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T6.4.4.4.10" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T6.4.4.4.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.4.4.4.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T6.4.4.4.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.4.4.4.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S5.T6.4.4.4.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.4.4.4.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.4.14"><span class="ltx_text" id="S5.T6.4.4.4.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.4.15"><span class="ltx_text" id="S5.T6.4.4.4.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.4.4.4.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.4.4.4.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.4.4.4.4.m1.1a"><mi id="S5.T6.4.4.4.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.4.4.4.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.4.4.4.4.m1.1b"><ci id="S5.T6.4.4.4.4.m1.1.1.cmml" xref="S5.T6.4.4.4.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.4.4.4.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.4.4.4.4.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.9.9" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.9.9.9.6"><span class="ltx_text" id="S5.T6.9.9.9.6.1" style="background-color:#FFFFFF;">GPT3&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.9.9.9.7"><span class="ltx_text" id="S5.T6.9.9.9.7.1" style="background-color:#FFFFFF;">32K</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.9.9.9.8"><span class="ltx_text" id="S5.T6.9.9.9.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.9.9"><span class="ltx_text" id="S5.T6.9.9.9.9.1" style="background-color:#FFFFFF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.5.5.5.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.5.5.5.1.m1.1a"><mi id="S5.T6.5.5.5.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.5.5.5.1.m1.1b"><ci id="S5.T6.5.5.5.1.m1.1.1.cmml" xref="S5.T6.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.5.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.5.5.5.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.9.9.9.10"><span class="ltx_text" id="S5.T6.9.9.9.10.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.9.9.9.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.6.6.6.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.6.6.6.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.6.6.6.2.1.1.m1.1"><semantics id="S5.T6.6.6.6.2.1.1.m1.1a"><mi id="S5.T6.6.6.6.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.6.6.6.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.6.6.6.2.1.1.m1.1b"><ci id="S5.T6.6.6.6.2.1.1.m1.1.1.cmml" xref="S5.T6.6.6.6.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.6.6.6.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.6.6.6.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.9.9.9.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.7.7.7.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.7.7.7.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.7.7.7.3.1.1.m1.1"><semantics id="S5.T6.7.7.7.3.1.1.m1.1a"><mi id="S5.T6.7.7.7.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.7.7.7.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.7.7.7.3.1.1.m1.1b"><ci id="S5.T6.7.7.7.3.1.1.m1.1.1.cmml" xref="S5.T6.7.7.7.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.7.7.7.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.7.7.7.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.9.9.9.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.9.9.9.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.8.8.8.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.8.8.8.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.8.8.8.4.m1.1a"><mi id="S5.T6.8.8.8.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.8.8.8.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.8.8.8.4.m1.1b"><ci id="S5.T6.8.8.8.4.m1.1.1.cmml" xref="S5.T6.8.8.8.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.8.8.8.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.8.8.8.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.9.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.9.9.9.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.9.9.9.5.m1.1a"><mi id="S5.T6.9.9.9.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.9.9.9.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.9.9.9.5.m1.1b"><ci id="S5.T6.9.9.9.5.m1.1.1.cmml" xref="S5.T6.9.9.9.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.9.9.9.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.9.9.9.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.9.15"><span class="ltx_text" id="S5.T6.9.9.9.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.11.11.11" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.11.11.11.3"><span class="ltx_text" id="S5.T6.11.11.11.3.1" style="background-color:#DFDFDF;">mT5&nbsp;(13B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.11.11.11.4"><span class="ltx_text" id="S5.T6.11.11.11.4.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.11.11.11.5"><span class="ltx_text" id="S5.T6.11.11.11.5.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.11.11.11.6"><span class="ltx_text" id="S5.T6.11.11.11.6.1" style="background-color:#DFDFDF;">0.01</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.11.11.11.7"><span class="ltx_text" id="S5.T6.11.11.11.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.11.11.11.8"><span class="ltx_text" id="S5.T6.11.11.11.8.1" style="background-color:#DFDFDF;">inverse square root</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.10.10.10.1" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.10.10.10.1.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.10.10.10.1.1.1.m1.1"><semantics id="S5.T6.10.10.10.1.1.1.m1.1a"><mi id="S5.T6.10.10.10.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.10.10.10.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.10.10.10.1.1.1.m1.1b"><ci id="S5.T6.10.10.10.1.1.1.m1.1.1.cmml" xref="S5.T6.10.10.10.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.10.10.10.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.10.10.10.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.11.11.11.9" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.11.11.11.10" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.11.11.11.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.11.11.11.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.11.11.11.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.11.11.11.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.11.11.11.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.11.11.11.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.11.11.11.14"><span class="ltx_text" id="S5.T6.11.11.11.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.11.11.11.15"><span class="ltx_text" id="S5.T6.11.11.11.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.11.11.11.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.11.11.11.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.11.11.11.2.m1.1a"><mi id="S5.T6.11.11.11.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.11.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.11.11.11.2.m1.1b"><ci id="S5.T6.11.11.11.2.m1.1.1.cmml" xref="S5.T6.11.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.11.11.11.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.11.11.11.2.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.13.13.13" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.12.12.12.1"><span class="ltx_text" id="S5.T6.12.12.12.1.1" style="background-color:#FFFFFF;">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T6.12.12.12.1.1.m1.1"><semantics id="S5.T6.12.12.12.1.1.m1.1a"><mi id="S5.T6.12.12.12.1.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T6.12.12.12.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T6.12.12.12.1.1.m1.1b"><ci id="S5.T6.12.12.12.1.1.m1.1.1.cmml" xref="S5.T6.12.12.12.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.12.12.12.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T6.12.12.12.1.1.m1.1d">italic_α</annotation></semantics></math>&nbsp;(200B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.13.13.13.3"><span class="ltx_text" id="S5.T6.13.13.13.3.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.13.13.13.4"><span class="ltx_text" id="S5.T6.13.13.13.4.1" style="background-color:#FFFFFF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.13.13.13.5"><span class="ltx_text" id="S5.T6.13.13.13.5.1" style="background-color:#FFFFFF;">2e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.13.13.13.6"><span class="ltx_text" id="S5.T6.13.13.13.6.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.13.13.13.7"><span class="ltx_text" id="S5.T6.13.13.13.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.13.13.13.8" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.8.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.13.13.13.9" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.9.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.13.13.13.10" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.10.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.13.13.13.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.11.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.13.13.13.2" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.13.13.13.2.1.1.m1.1"><semantics id="S5.T6.13.13.13.2.1.1.m1.1a"><mi id="S5.T6.13.13.13.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.13.13.13.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.13.13.13.2.1.1.m1.1b"><ci id="S5.T6.13.13.13.2.1.1.m1.1.1.cmml" xref="S5.T6.13.13.13.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.13.13.13.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.13.13.13.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.13.13.13.12" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.13.13.13.12.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.13.13.13.13"><span class="ltx_text" id="S5.T6.13.13.13.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.13.13.13.14"><span class="ltx_text" id="S5.T6.13.13.13.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.13.13.13.15"><span class="ltx_text" id="S5.T6.13.13.13.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.15.15.15" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.15.15.15.3"><span class="ltx_text" id="S5.T6.15.15.15.3.1" style="background-color:#DFDFDF;">CPM-2&nbsp;(198B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.15.15.15.4"><span class="ltx_text" id="S5.T6.15.15.15.4.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.15.15.15.5"><span class="ltx_text" id="S5.T6.15.15.15.5.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.15.15.15.6"><span class="ltx_text" id="S5.T6.15.15.15.6.1" style="background-color:#DFDFDF;">0.001</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.15.15.15.7"><span class="ltx_text" id="S5.T6.15.15.15.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.15.15.15.8"><span class="ltx_text" id="S5.T6.15.15.15.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.14.14.14.1" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.14.14.14.1.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.14.14.14.1.1.1.m1.1"><semantics id="S5.T6.14.14.14.1.1.1.m1.1a"><mi id="S5.T6.14.14.14.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.14.14.14.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.14.14.14.1.1.1.m1.1b"><ci id="S5.T6.14.14.14.1.1.1.m1.1.1.cmml" xref="S5.T6.14.14.14.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.14.14.14.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.14.14.14.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.15.15.15.9" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.15.15.15.10" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.15.15.15.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.15.15.15.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.15.15.15.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.15.15.15.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.15.15.15.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.15.15.15.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.15.15.15.14"><span class="ltx_text" id="S5.T6.15.15.15.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.15.15.15.15"><span class="ltx_text" id="S5.T6.15.15.15.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.15.15.15.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.15.15.15.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.15.15.15.2.m1.1a"><mi id="S5.T6.15.15.15.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.15.15.15.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.15.15.15.2.m1.1b"><ci id="S5.T6.15.15.15.2.m1.1.1.cmml" xref="S5.T6.15.15.15.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.15.15.15.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.15.15.15.2.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.19.19.19" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.19.19.19.5"><span class="ltx_text" id="S5.T6.19.19.19.5.1" style="background-color:#FFFFFF;">Codex&nbsp;(12B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.19.19.19.6"><span class="ltx_text" id="S5.T6.19.19.19.6.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.19.19.19.7"><span class="ltx_text" id="S5.T6.19.19.19.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.19.19.19.8"><span class="ltx_text" id="S5.T6.19.19.19.8.1" style="background-color:#FFFFFF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.16.16.16.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.16.16.16.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.16.16.16.1.m1.1a"><mi id="S5.T6.16.16.16.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.16.16.16.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.16.16.16.1.m1.1b"><ci id="S5.T6.16.16.16.1.m1.1.1.cmml" xref="S5.T6.16.16.16.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.16.16.16.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.16.16.16.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.19.19.19.9"><span class="ltx_text" id="S5.T6.19.19.19.9.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.19.19.19.10" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.17.17.17.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.17.17.17.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.17.17.17.2.1.1.m1.1"><semantics id="S5.T6.17.17.17.2.1.1.m1.1a"><mi id="S5.T6.17.17.17.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.17.17.17.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.17.17.17.2.1.1.m1.1b"><ci id="S5.T6.17.17.17.2.1.1.m1.1.1.cmml" xref="S5.T6.17.17.17.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.17.17.17.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.17.17.17.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.19.19.19.11" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.18.18.18.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.18.18.18.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.18.18.18.3.1.1.m1.1"><semantics id="S5.T6.18.18.18.3.1.1.m1.1a"><mi id="S5.T6.18.18.18.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.18.18.18.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.18.18.18.3.1.1.m1.1b"><ci id="S5.T6.18.18.18.3.1.1.m1.1.1.cmml" xref="S5.T6.18.18.18.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.18.18.18.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.18.18.18.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.19.19.19.12" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.19.19.19.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.19.19.19.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.19.19.19.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.19.19.19.4.m1.1a"><mi id="S5.T6.19.19.19.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.19.19.19.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.19.19.19.4.m1.1b"><ci id="S5.T6.19.19.19.4.m1.1.1.cmml" xref="S5.T6.19.19.19.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.19.19.19.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.19.19.19.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.19.19.19.14"><span class="ltx_text" id="S5.T6.19.19.19.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.19.19.19.15"><span class="ltx_text" id="S5.T6.19.19.19.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.22.22.22" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.22.22.22.4"><span class="ltx_text" id="S5.T6.22.22.22.4.1" style="background-color:#DFDFDF;">ERNIE 3.0&nbsp;(12B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.22.22.22.5"><span class="ltx_text" id="S5.T6.22.22.22.5.1" style="background-color:#DFDFDF;">6144</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.22.22.22.6"><span class="ltx_text" id="S5.T6.22.22.22.6.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.22.22.22.7"><span class="ltx_text" id="S5.T6.22.22.22.7.1" style="background-color:#DFDFDF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.20.20.20.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.20.20.20.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.20.20.20.1.m1.1a"><mi id="S5.T6.20.20.20.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.20.20.20.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.20.20.20.1.m1.1b"><ci id="S5.T6.20.20.20.1.m1.1.1.cmml" xref="S5.T6.20.20.20.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.20.20.20.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.20.20.20.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.22.22.22.8"><span class="ltx_text" id="S5.T6.22.22.22.8.1" style="background-color:#DFDFDF;">linear</span></td>
<td class="ltx_td" id="S5.T6.22.22.22.9" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.21.21.21.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.21.21.21.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.21.21.21.2.1.1.m1.1"><semantics id="S5.T6.21.21.21.2.1.1.m1.1a"><mi id="S5.T6.21.21.21.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.21.21.21.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.21.21.21.2.1.1.m1.1b"><ci id="S5.T6.21.21.21.2.1.1.m1.1.1.cmml" xref="S5.T6.21.21.21.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.21.21.21.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.21.21.21.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.22.22.22.10" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.22.22.22.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.22.22.22.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.22.22.22.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.22.22.22.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.22.22.22.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.22.22.22.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.22.22.22.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.22.22.22.3.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.22.22.22.3.m1.1a"><mi id="S5.T6.22.22.22.3.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.22.22.22.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.22.22.22.3.m1.1b"><ci id="S5.T6.22.22.22.3.m1.1.1.cmml" xref="S5.T6.22.22.22.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.22.22.22.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.22.22.22.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.22.22.22.14"><span class="ltx_text" id="S5.T6.22.22.22.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.22.22.22.15"><span class="ltx_text" id="S5.T6.22.22.22.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.27.27.27" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.27.27.27.6"><span class="ltx_text" id="S5.T6.27.27.27.6.1" style="background-color:#FFFFFF;">Jurassic-1&nbsp;(178B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.27.27.27.7"><span class="ltx_text" id="S5.T6.27.27.27.7.1" style="background-color:#FFFFFF;">3.2M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.27.27.27.8"><span class="ltx_text" id="S5.T6.27.27.27.8.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.27.27.27.9"><span class="ltx_text" id="S5.T6.27.27.27.9.1" style="background-color:#FFFFFF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.23.23.23.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.23.23.23.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.23.23.23.1.m1.1a"><mi id="S5.T6.23.23.23.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.23.23.23.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.23.23.23.1.m1.1b"><ci id="S5.T6.23.23.23.1.m1.1.1.cmml" xref="S5.T6.23.23.23.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.23.23.23.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.23.23.23.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.27.27.27.10"><span class="ltx_text" id="S5.T6.27.27.27.10.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.27.27.27.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.24.24.24.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.24.24.24.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.24.24.24.2.1.1.m1.1"><semantics id="S5.T6.24.24.24.2.1.1.m1.1a"><mi id="S5.T6.24.24.24.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.24.24.24.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.24.24.24.2.1.1.m1.1b"><ci id="S5.T6.24.24.24.2.1.1.m1.1.1.cmml" xref="S5.T6.24.24.24.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.24.24.24.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.24.24.24.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.27.27.27.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.25.25.25.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.25.25.25.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.25.25.25.3.1.1.m1.1"><semantics id="S5.T6.25.25.25.3.1.1.m1.1a"><mi id="S5.T6.25.25.25.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.25.25.25.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.25.25.25.3.1.1.m1.1b"><ci id="S5.T6.25.25.25.3.1.1.m1.1.1.cmml" xref="S5.T6.25.25.25.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.25.25.25.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.25.25.25.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.27.27.27.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.27.27.27.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.26.26.26.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.26.26.26.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.26.26.26.4.m1.1a"><mi id="S5.T6.26.26.26.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.26.26.26.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.26.26.26.4.m1.1b"><ci id="S5.T6.26.26.26.4.m1.1.1.cmml" xref="S5.T6.26.26.26.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.26.26.26.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.26.26.26.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.27.27.27.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.27.27.27.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.27.27.27.5.m1.1a"><mi id="S5.T6.27.27.27.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.27.27.27.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.27.27.27.5.m1.1b"><ci id="S5.T6.27.27.27.5.m1.1.1.cmml" xref="S5.T6.27.27.27.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.27.27.27.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.27.27.27.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.27.27.27.15"><span class="ltx_text" id="S5.T6.27.27.27.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.29.29.29" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.29.29.29.3"><span class="ltx_text" id="S5.T6.29.29.29.3.1" style="background-color:#DFDFDF;">HyperCLOVA&nbsp;(82B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.29.29.29.4"><span class="ltx_text" id="S5.T6.29.29.29.4.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.29.29.29.5"><span class="ltx_text" id="S5.T6.29.29.29.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.29.29.29.6"><span class="ltx_text" id="S5.T6.29.29.29.6.1" style="background-color:#DFDFDF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.29.29.29.7"><span class="ltx_text" id="S5.T6.29.29.29.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.29.29.29.8"><span class="ltx_text" id="S5.T6.29.29.29.8.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.29.29.29.9" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.29.29.29.10" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.28.28.28.1" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.28.28.28.1.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.28.28.28.1.1.1.m1.1"><semantics id="S5.T6.28.28.28.1.1.1.m1.1a"><mi id="S5.T6.28.28.28.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.28.28.28.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.28.28.28.1.1.1.m1.1b"><ci id="S5.T6.28.28.28.1.1.1.m1.1.1.cmml" xref="S5.T6.28.28.28.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.28.28.28.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.28.28.28.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.29.29.29.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.29.29.29.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.29.29.29.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.29.29.29.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.29.29.29.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.29.29.29.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.29.29.29.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.29.29.29.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.29.29.29.2.m1.1a"><mi id="S5.T6.29.29.29.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.29.29.29.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.29.29.29.2.m1.1b"><ci id="S5.T6.29.29.29.2.m1.1.1.cmml" xref="S5.T6.29.29.29.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.29.29.29.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.29.29.29.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.29.29.29.14"><span class="ltx_text" id="S5.T6.29.29.29.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.29.29.29.15"><span class="ltx_text" id="S5.T6.29.29.29.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.33.33.33" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.33.33.33.5"><span class="ltx_text" id="S5.T6.33.33.33.5.1" style="background-color:#FFFFFF;">Yuan 1.0&nbsp;(245B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.30.30.30.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S5.T6.30.30.30.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.30.30.30.1.m1.1a"><mo id="S5.T6.30.30.30.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T6.30.30.30.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.30.30.30.1.m1.1b"><lt id="S5.T6.30.30.30.1.m1.1.1.cmml" xref="S5.T6.30.30.30.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.30.30.30.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S5.T6.30.30.30.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text" id="S5.T6.30.30.30.1.1" style="background-color:#FFFFFF;">10M</span>
</td>
<td class="ltx_td ltx_align_right" id="S5.T6.33.33.33.6"><span class="ltx_text" id="S5.T6.33.33.33.6.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.33.33.33.7"><span class="ltx_text" id="S5.T6.33.33.33.7.1" style="background-color:#FFFFFF;">1.6e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.31.31.31.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.31.31.31.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.31.31.31.2.m1.1a"><mi id="S5.T6.31.31.31.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.31.31.31.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.31.31.31.2.m1.1b"><ci id="S5.T6.31.31.31.2.m1.1.1.cmml" xref="S5.T6.31.31.31.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.31.31.31.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.31.31.31.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.33.33.33.8"><span class="ltx_text" id="S5.T6.33.33.33.8.1" style="background-color:#FFFFFF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.33.33.33.9" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.32.32.32.3" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.32.32.32.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.32.32.32.3.1.1.m1.1"><semantics id="S5.T6.32.32.32.3.1.1.m1.1a"><mi id="S5.T6.32.32.32.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.32.32.32.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.32.32.32.3.1.1.m1.1b"><ci id="S5.T6.32.32.32.3.1.1.m1.1.1.cmml" xref="S5.T6.32.32.32.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.32.32.32.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.32.32.32.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.33.33.33.10" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.33.33.33.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.33.33.33.11.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.33.33.33.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.33.33.33.12.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.33.33.33.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.33.33.33.13.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.33.33.33.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.33.33.33.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.33.33.33.4.m1.1a"><mi id="S5.T6.33.33.33.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.33.33.33.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.33.33.33.4.m1.1b"><ci id="S5.T6.33.33.33.4.m1.1.1.cmml" xref="S5.T6.33.33.33.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.33.33.33.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.33.33.33.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.33.33.33.14"><span class="ltx_text" id="S5.T6.33.33.33.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.33.33.33.15"><span class="ltx_text" id="S5.T6.33.33.33.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.37.37.37" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.37.37.37.5"><span class="ltx_text" id="S5.T6.37.37.37.5.1" style="background-color:#DFDFDF;">Gopher&nbsp;(280B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.37.37.37.6"><span class="ltx_text" id="S5.T6.37.37.37.6.1" style="background-color:#DFDFDF;">3M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.37.37.37.7"><span class="ltx_text" id="S5.T6.37.37.37.7.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.37.37.37.8"><span class="ltx_text" id="S5.T6.37.37.37.8.1" style="background-color:#DFDFDF;">4e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.34.34.34.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.34.34.34.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.34.34.34.1.m1.1a"><mi id="S5.T6.34.34.34.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.34.34.34.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.34.34.34.1.m1.1b"><ci id="S5.T6.34.34.34.1.m1.1.1.cmml" xref="S5.T6.34.34.34.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.34.34.34.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.34.34.34.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.37.37.37.9"><span class="ltx_text" id="S5.T6.37.37.37.9.1" style="background-color:#DFDFDF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.37.37.37.10" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.35.35.35.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.35.35.35.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.35.35.35.2.1.1.m1.1"><semantics id="S5.T6.35.35.35.2.1.1.m1.1a"><mi id="S5.T6.35.35.35.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.35.35.35.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.35.35.35.2.1.1.m1.1b"><ci id="S5.T6.35.35.35.2.1.1.m1.1.1.cmml" xref="S5.T6.35.35.35.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.35.35.35.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.35.35.35.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.37.37.37.11" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.37.37.37.12" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.36.36.36.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.36.36.36.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.36.36.36.3.1.1.m1.1"><semantics id="S5.T6.36.36.36.3.1.1.m1.1a"><mi id="S5.T6.36.36.36.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.36.36.36.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.36.36.36.3.1.1.m1.1b"><ci id="S5.T6.36.36.36.3.1.1.m1.1.1.cmml" xref="S5.T6.36.36.36.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.36.36.36.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.36.36.36.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.37.37.37.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.37.37.37.14"><span class="ltx_text" id="S5.T6.37.37.37.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.37.37.37.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.37.37.37.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.37.37.37.4.m1.1a"><mi id="S5.T6.37.37.37.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.37.37.37.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.37.37.37.4.m1.1b"><ci id="S5.T6.37.37.37.4.m1.1.1.cmml" xref="S5.T6.37.37.37.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.37.37.37.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.37.37.37.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.37.37.37.15"><span class="ltx_text" id="S5.T6.37.37.37.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.42.42.42" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.42.42.42.6"><span class="ltx_text" id="S5.T6.42.42.42.6.1" style="background-color:#FFFFFF;">ERNIE 3.0 Titan&nbsp;(260B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.42.42.42.7"><span class="ltx_text" id="S5.T6.42.42.42.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.42.42.42.8"><span class="ltx_text" id="S5.T6.42.42.42.8.1" style="background-color:#FFFFFF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.42.42.42.9"><span class="ltx_text" id="S5.T6.42.42.42.9.1" style="background-color:#FFFFFF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.38.38.38.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.38.38.38.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.38.38.38.1.m1.1a"><mi id="S5.T6.38.38.38.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.38.38.38.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.38.38.38.1.m1.1b"><ci id="S5.T6.38.38.38.1.m1.1.1.cmml" xref="S5.T6.38.38.38.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.38.38.38.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.38.38.38.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.42.42.42.10"><span class="ltx_text" id="S5.T6.42.42.42.10.1" style="background-color:#FFFFFF;">linear</span></td>
<td class="ltx_td" id="S5.T6.42.42.42.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.39.39.39.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.39.39.39.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.39.39.39.2.1.1.m1.1"><semantics id="S5.T6.39.39.39.2.1.1.m1.1a"><mi id="S5.T6.39.39.39.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.39.39.39.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.39.39.39.2.1.1.m1.1b"><ci id="S5.T6.39.39.39.2.1.1.m1.1.1.cmml" xref="S5.T6.39.39.39.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.39.39.39.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.39.39.39.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.42.42.42.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.40.40.40.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.40.40.40.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.40.40.40.3.1.1.m1.1"><semantics id="S5.T6.40.40.40.3.1.1.m1.1a"><mi id="S5.T6.40.40.40.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.40.40.40.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.40.40.40.3.1.1.m1.1b"><ci id="S5.T6.40.40.40.3.1.1.m1.1.1.cmml" xref="S5.T6.40.40.40.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.40.40.40.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.40.40.40.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.42.42.42.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.42.42.42.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.41.41.41.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.41.41.41.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.41.41.41.4.m1.1a"><mi id="S5.T6.41.41.41.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.41.41.41.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.41.41.41.4.m1.1b"><ci id="S5.T6.41.41.41.4.m1.1.1.cmml" xref="S5.T6.41.41.41.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.41.41.41.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.41.41.41.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.42.42.42.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.42.42.42.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.42.42.42.5.m1.1a"><mi id="S5.T6.42.42.42.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.42.42.42.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.42.42.42.5.m1.1b"><ci id="S5.T6.42.42.42.5.m1.1.1.cmml" xref="S5.T6.42.42.42.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.42.42.42.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.42.42.42.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.42.42.42.15"><span class="ltx_text" id="S5.T6.42.42.42.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.48.48.48" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.48.48.48.7"><span class="ltx_text" id="S5.T6.48.48.48.7.1" style="background-color:#DFDFDF;">GPT-NeoX-20B</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.48.48.48.8"><span class="ltx_text" id="S5.T6.48.48.48.8.1" style="background-color:#DFDFDF;">1538</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.48.48.48.9"><span class="ltx_text" id="S5.T6.48.48.48.9.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.48.48.48.10"><span class="ltx_text" id="S5.T6.48.48.48.10.1" style="background-color:#DFDFDF;">0.97e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.43.43.43.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.43.43.43.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.43.43.43.1.m1.1a"><mi id="S5.T6.43.43.43.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.43.43.43.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.43.43.43.1.m1.1b"><ci id="S5.T6.43.43.43.1.m1.1.1.cmml" xref="S5.T6.43.43.43.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.43.43.43.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.43.43.43.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.48.48.48.11"><span class="ltx_text" id="S5.T6.48.48.48.11.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.48.48.48.12" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.48.48.48.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.44.44.44.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.44.44.44.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.44.44.44.2.1.1.m1.1"><semantics id="S5.T6.44.44.44.2.1.1.m1.1a"><mi id="S5.T6.44.44.44.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.44.44.44.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.44.44.44.2.1.1.m1.1b"><ci id="S5.T6.44.44.44.2.1.1.m1.1.1.cmml" xref="S5.T6.44.44.44.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.44.44.44.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.44.44.44.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.45.45.45.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.45.45.45.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.45.45.45.3.1.1.m1.1"><semantics id="S5.T6.45.45.45.3.1.1.m1.1a"><mi id="S5.T6.45.45.45.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.45.45.45.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.45.45.45.3.1.1.m1.1b"><ci id="S5.T6.45.45.45.3.1.1.m1.1.1.cmml" xref="S5.T6.45.45.45.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.45.45.45.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.45.45.45.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.48.48.48.14" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.48.48.48.15" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.46.46.46.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.46.46.46.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.46.46.46.4.m1.1a"><mi id="S5.T6.46.46.46.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.46.46.46.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.46.46.46.4.m1.1b"><ci id="S5.T6.46.46.46.4.m1.1.1.cmml" xref="S5.T6.46.46.46.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.46.46.46.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.46.46.46.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.47.47.47.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.47.47.47.5.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.47.47.47.5.m1.1a"><mi id="S5.T6.47.47.47.5.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.47.47.47.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.47.47.47.5.m1.1b"><ci id="S5.T6.47.47.47.5.m1.1.1.cmml" xref="S5.T6.47.47.47.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.47.47.47.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.47.47.47.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.48.48.48.6"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.48.48.48.6.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.48.48.48.6.m1.1a"><mo id="S5.T6.48.48.48.6.m1.1.1" mathbackground="#DFDFDF" xref="S5.T6.48.48.48.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.48.48.48.6.m1.1b"><times id="S5.T6.48.48.48.6.m1.1.1.cmml" xref="S5.T6.48.48.48.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.48.48.48.6.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.48.48.48.6.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.53.53.53" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.53.53.53.6"><span class="ltx_text" id="S5.T6.53.53.53.6.1" style="background-color:#FFFFFF;">OPT&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.53.53.53.7"><span class="ltx_text" id="S5.T6.53.53.53.7.1" style="background-color:#FFFFFF;">2M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.53.53.53.8"><span class="ltx_text" id="S5.T6.53.53.53.8.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.53.53.53.9"><span class="ltx_text" id="S5.T6.53.53.53.9.1" style="background-color:#FFFFFF;">1.2e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.53.53.53.10"><span class="ltx_text" id="S5.T6.53.53.53.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.53.53.53.11"><span class="ltx_text" id="S5.T6.53.53.53.11.1" style="background-color:#FFFFFF;">linear</span></td>
<td class="ltx_td" id="S5.T6.53.53.53.12" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.53.53.53.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.49.49.49.1" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.49.49.49.1.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.49.49.49.1.1.1.m1.1"><semantics id="S5.T6.49.49.49.1.1.1.m1.1a"><mi id="S5.T6.49.49.49.1.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.49.49.49.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.49.49.49.1.1.1.m1.1b"><ci id="S5.T6.49.49.49.1.1.1.m1.1.1.cmml" xref="S5.T6.49.49.49.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.49.49.49.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.49.49.49.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.50.50.50.2" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.50.50.50.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.50.50.50.2.1.1.m1.1"><semantics id="S5.T6.50.50.50.2.1.1.m1.1a"><mi id="S5.T6.50.50.50.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.50.50.50.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.50.50.50.2.1.1.m1.1b"><ci id="S5.T6.50.50.50.2.1.1.m1.1.1.cmml" xref="S5.T6.50.50.50.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.50.50.50.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.50.50.50.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.53.53.53.14" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.53.53.53.15" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.51.51.51.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.51.51.51.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.51.51.51.3.m1.1a"><mi id="S5.T6.51.51.51.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.51.51.51.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.51.51.51.3.m1.1b"><ci id="S5.T6.51.51.51.3.m1.1.1.cmml" xref="S5.T6.51.51.51.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.51.51.51.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.51.51.51.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.52.52.52.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.52.52.52.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.52.52.52.4.m1.1a"><mi id="S5.T6.52.52.52.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.52.52.52.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.52.52.52.4.m1.1b"><ci id="S5.T6.52.52.52.4.m1.1.1.cmml" xref="S5.T6.52.52.52.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.52.52.52.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.52.52.52.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.53.53.53.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.53.53.53.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.53.53.53.5.m1.1a"><mi id="S5.T6.53.53.53.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.53.53.53.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.53.53.53.5.m1.1b"><ci id="S5.T6.53.53.53.5.m1.1.1.cmml" xref="S5.T6.53.53.53.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.53.53.53.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.53.53.53.5.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.59.59.59" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.59.59.59.7"><span class="ltx_text" id="S5.T6.59.59.59.7.1" style="background-color:#DFDFDF;">BLOOM&nbsp;(176B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.59.59.59.8"><span class="ltx_text" id="S5.T6.59.59.59.8.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.59.59.59.9"><span class="ltx_text" id="S5.T6.59.59.59.9.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.59.59.59.10"><span class="ltx_text" id="S5.T6.59.59.59.10.1" style="background-color:#DFDFDF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.54.54.54.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.54.54.54.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.54.54.54.1.m1.1a"><mi id="S5.T6.54.54.54.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.54.54.54.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.54.54.54.1.m1.1b"><ci id="S5.T6.54.54.54.1.m1.1.1.cmml" xref="S5.T6.54.54.54.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.54.54.54.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.54.54.54.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.59.59.59.11"><span class="ltx_text" id="S5.T6.59.59.59.11.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.59.59.59.12" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.55.55.55.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.55.55.55.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.55.55.55.2.1.1.m1.1"><semantics id="S5.T6.55.55.55.2.1.1.m1.1a"><mi id="S5.T6.55.55.55.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.55.55.55.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.55.55.55.2.1.1.m1.1b"><ci id="S5.T6.55.55.55.2.1.1.m1.1.1.cmml" xref="S5.T6.55.55.55.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.55.55.55.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.55.55.55.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.59.59.59.13" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.59.59.59.14" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.56.56.56.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.56.56.56.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.56.56.56.3.1.1.m1.1"><semantics id="S5.T6.56.56.56.3.1.1.m1.1a"><mi id="S5.T6.56.56.56.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.56.56.56.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.56.56.56.3.1.1.m1.1b"><ci id="S5.T6.56.56.56.3.1.1.m1.1.1.cmml" xref="S5.T6.56.56.56.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.56.56.56.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.56.56.56.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.59.59.59.15" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.57.57.57.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.57.57.57.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.57.57.57.4.m1.1a"><mi id="S5.T6.57.57.57.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.57.57.57.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.57.57.57.4.m1.1b"><ci id="S5.T6.57.57.57.4.m1.1.1.cmml" xref="S5.T6.57.57.57.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.57.57.57.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.57.57.57.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.58.58.58.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.58.58.58.5.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.58.58.58.5.m1.1a"><mi id="S5.T6.58.58.58.5.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.58.58.58.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.58.58.58.5.m1.1b"><ci id="S5.T6.58.58.58.5.m1.1.1.cmml" xref="S5.T6.58.58.58.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.58.58.58.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.58.58.58.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.59.59.59.6"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.59.59.59.6.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.59.59.59.6.m1.1a"><mo id="S5.T6.59.59.59.6.m1.1.1" mathbackground="#DFDFDF" xref="S5.T6.59.59.59.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.59.59.59.6.m1.1b"><times id="S5.T6.59.59.59.6.m1.1.1.cmml" xref="S5.T6.59.59.59.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.59.59.59.6.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.59.59.59.6.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.64.64.64" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.64.64.64.6"><span class="ltx_text" id="S5.T6.64.64.64.6.1" style="background-color:#FFFFFF;">Galactica&nbsp;(120B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.64.64.64.7"><span class="ltx_text" id="S5.T6.64.64.64.7.1" style="background-color:#FFFFFF;">2M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.64.64.64.8"><span class="ltx_text" id="S5.T6.64.64.64.8.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.64.64.64.9"><span class="ltx_text" id="S5.T6.64.64.64.9.1" style="background-color:#FFFFFF;">7e-6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.60.60.60.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.60.60.60.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.60.60.60.1.m1.1a"><mi id="S5.T6.60.60.60.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.60.60.60.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.60.60.60.1.m1.1b"><ci id="S5.T6.60.60.60.1.m1.1.1.cmml" xref="S5.T6.60.60.60.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.60.60.60.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.60.60.60.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.64.64.64.10"><span class="ltx_text" id="S5.T6.64.64.64.10.1" style="background-color:#FFFFFF;">linear decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.64.64.64.11" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.64.64.64.12" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.61.61.61.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.61.61.61.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.61.61.61.2.1.1.m1.1"><semantics id="S5.T6.61.61.61.2.1.1.m1.1a"><mi id="S5.T6.61.61.61.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.61.61.61.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.61.61.61.2.1.1.m1.1b"><ci id="S5.T6.61.61.61.2.1.1.m1.1.1.cmml" xref="S5.T6.61.61.61.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.61.61.61.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.61.61.61.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.64.64.64.13" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.64.64.64.13.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.64.64.64.14" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.64.64.64.14.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.64.64.64.15" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.64.64.64.15.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.62.62.62.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.62.62.62.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.62.62.62.3.m1.1a"><mi id="S5.T6.62.62.62.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.62.62.62.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.62.62.62.3.m1.1b"><ci id="S5.T6.62.62.62.3.m1.1.1.cmml" xref="S5.T6.62.62.62.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.62.62.62.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.62.62.62.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.63.63.63.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.63.63.63.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.63.63.63.4.m1.1a"><mi id="S5.T6.63.63.63.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.63.63.63.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.63.63.63.4.m1.1b"><ci id="S5.T6.63.63.63.4.m1.1.1.cmml" xref="S5.T6.63.63.63.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.63.63.63.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.63.63.63.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.64.64.64.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.64.64.64.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.64.64.64.5.m1.1a"><mi id="S5.T6.64.64.64.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.64.64.64.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.64.64.64.5.m1.1b"><ci id="S5.T6.64.64.64.5.m1.1.1.cmml" xref="S5.T6.64.64.64.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.64.64.64.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.64.64.64.5.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.68.68.68" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.68.68.68.5"><span class="ltx_text" id="S5.T6.68.68.68.5.1" style="background-color:#DFDFDF;">GLaM&nbsp;(1.2T)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.68.68.68.6"><span class="ltx_text" id="S5.T6.68.68.68.6.1" style="background-color:#DFDFDF;">1M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.68.68.68.7"><span class="ltx_text" id="S5.T6.68.68.68.7.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.68.68.68.8"><span class="ltx_text" id="S5.T6.68.68.68.8.1" style="background-color:#DFDFDF;">0.01</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.68.68.68.9"><span class="ltx_text" id="S5.T6.68.68.68.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.68.68.68.10"><span class="ltx_text" id="S5.T6.68.68.68.10.1" style="background-color:#DFDFDF;">inverse square root</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.65.65.65.1" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.65.65.65.1.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.65.65.65.1.1.1.m1.1"><semantics id="S5.T6.65.65.65.1.1.1.m1.1a"><mi id="S5.T6.65.65.65.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.65.65.65.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.65.65.65.1.1.1.m1.1b"><ci id="S5.T6.65.65.65.1.1.1.m1.1.1.cmml" xref="S5.T6.65.65.65.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.65.65.65.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.65.65.65.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.68.68.68.11" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.68.68.68.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S5.T6.66.66.66.2"><span class="ltx_text" id="S5.T6.66.66.66.2.1" style="background-color:#DFDFDF;">FP32 + <math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.66.66.66.2.1.m1.1"><semantics id="S5.T6.66.66.66.2.1.m1.1a"><mi id="S5.T6.66.66.66.2.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.66.66.66.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.66.66.66.2.1.m1.1b"><ci id="S5.T6.66.66.66.2.1.m1.1.1.cmml" xref="S5.T6.66.66.66.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.66.66.66.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.66.66.66.2.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.68.68.68.13"><span class="ltx_text" id="S5.T6.68.68.68.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.67.67.67.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.67.67.67.3.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.67.67.67.3.m1.1a"><mi id="S5.T6.67.67.67.3.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.67.67.67.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.67.67.67.3.m1.1b"><ci id="S5.T6.67.67.67.3.m1.1.1.cmml" xref="S5.T6.67.67.67.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.67.67.67.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.67.67.67.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.68.68.68.4"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.68.68.68.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.68.68.68.4.m1.1a"><mo id="S5.T6.68.68.68.4.m1.1.1" mathbackground="#DFDFDF" xref="S5.T6.68.68.68.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.68.68.68.4.m1.1b"><times id="S5.T6.68.68.68.4.m1.1.1.cmml" xref="S5.T6.68.68.68.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.68.68.68.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.68.68.68.4.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.132.132.135.3" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.132.132.135.3.1"><span class="ltx_text" id="S5.T6.132.132.135.3.1.1" style="background-color:#FFFFFF;">LaMDA&nbsp;(137B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.132.132.135.3.2"><span class="ltx_text" id="S5.T6.132.132.135.3.2.1" style="background-color:#FFFFFF;">256K</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.132.132.135.3.3"><span class="ltx_text" id="S5.T6.132.132.135.3.3.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.135.3.4"><span class="ltx_text" id="S5.T6.132.132.135.3.4.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.135.3.5"><span class="ltx_text" id="S5.T6.132.132.135.3.5.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.132.132.135.3.6"><span class="ltx_text" id="S5.T6.132.132.135.3.6.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.135.3.7" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.7.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.135.3.8" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.8.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.132.132.135.3.9" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.9.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.135.3.10" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.10.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.132.132.135.3.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.11.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.132.132.135.3.12" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.132.132.135.3.12.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.135.3.13"><span class="ltx_text" id="S5.T6.132.132.135.3.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.135.3.14"><span class="ltx_text" id="S5.T6.132.132.135.3.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.132.132.135.3.15"><span class="ltx_text" id="S5.T6.132.132.135.3.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.73.73.73" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.73.73.73.6"><span class="ltx_text" id="S5.T6.73.73.73.6.1" style="background-color:#DFDFDF;">MT-NLG&nbsp;(530B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.73.73.73.7"><span class="ltx_text" id="S5.T6.73.73.73.7.1" style="background-color:#DFDFDF;">1920</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.73.73.73.8"><span class="ltx_text" id="S5.T6.73.73.73.8.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.73.73.73.9"><span class="ltx_text" id="S5.T6.73.73.73.9.1" style="background-color:#DFDFDF;">5e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.69.69.69.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.69.69.69.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.69.69.69.1.m1.1a"><mi id="S5.T6.69.69.69.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.69.69.69.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.69.69.69.1.m1.1b"><ci id="S5.T6.69.69.69.1.m1.1.1.cmml" xref="S5.T6.69.69.69.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.69.69.69.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.69.69.69.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.73.73.73.10"><span class="ltx_text" id="S5.T6.73.73.73.10.1" style="background-color:#DFDFDF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.73.73.73.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.70.70.70.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.70.70.70.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.70.70.70.2.1.1.m1.1"><semantics id="S5.T6.70.70.70.2.1.1.m1.1a"><mi id="S5.T6.70.70.70.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.70.70.70.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.70.70.70.2.1.1.m1.1b"><ci id="S5.T6.70.70.70.2.1.1.m1.1.1.cmml" xref="S5.T6.70.70.70.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.70.70.70.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.70.70.70.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.73.73.73.12" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.73.73.73.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.71.71.71.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.71.71.71.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.71.71.71.3.1.1.m1.1"><semantics id="S5.T6.71.71.71.3.1.1.m1.1a"><mi id="S5.T6.71.71.71.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.71.71.71.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.71.71.71.3.1.1.m1.1b"><ci id="S5.T6.71.71.71.3.1.1.m1.1.1.cmml" xref="S5.T6.71.71.71.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.71.71.71.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.71.71.71.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.73.73.73.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.72.72.72.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.72.72.72.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.72.72.72.4.m1.1a"><mi id="S5.T6.72.72.72.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.72.72.72.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.72.72.72.4.m1.1b"><ci id="S5.T6.72.72.72.4.m1.1.1.cmml" xref="S5.T6.72.72.72.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.72.72.72.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.72.72.72.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.73.73.73.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.73.73.73.5.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.73.73.73.5.m1.1a"><mi id="S5.T6.73.73.73.5.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.73.73.73.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.73.73.73.5.m1.1b"><ci id="S5.T6.73.73.73.5.m1.1.1.cmml" xref="S5.T6.73.73.73.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.73.73.73.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.73.73.73.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.73.73.73.15"><span class="ltx_text" id="S5.T6.73.73.73.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.78.78.78" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.78.78.78.6"><span class="ltx_text" id="S5.T6.78.78.78.6.1" style="background-color:#FFFFFF;">AlphaCode&nbsp;(41B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.78.78.78.7"><span class="ltx_text" id="S5.T6.78.78.78.7.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.78.78.78.8"><span class="ltx_text" id="S5.T6.78.78.78.8.1" style="background-color:#FFFFFF;">1536+768</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.78.78.78.9"><span class="ltx_text" id="S5.T6.78.78.78.9.1" style="background-color:#FFFFFF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.74.74.74.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.74.74.74.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.74.74.74.1.m1.1a"><mi id="S5.T6.74.74.74.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.74.74.74.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.74.74.74.1.m1.1b"><ci id="S5.T6.74.74.74.1.m1.1.1.cmml" xref="S5.T6.74.74.74.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.74.74.74.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.74.74.74.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.78.78.78.10"><span class="ltx_text" id="S5.T6.78.78.78.10.1" style="background-color:#FFFFFF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.78.78.78.11" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.78.78.78.12" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.75.75.75.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.75.75.75.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.75.75.75.2.1.1.m1.1"><semantics id="S5.T6.75.75.75.2.1.1.m1.1a"><mi id="S5.T6.75.75.75.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.75.75.75.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.75.75.75.2.1.1.m1.1b"><ci id="S5.T6.75.75.75.2.1.1.m1.1.1.cmml" xref="S5.T6.75.75.75.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.75.75.75.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.75.75.75.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.78.78.78.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.76.76.76.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.76.76.76.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.76.76.76.3.1.1.m1.1"><semantics id="S5.T6.76.76.76.3.1.1.m1.1a"><mi id="S5.T6.76.76.76.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.76.76.76.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.76.76.76.3.1.1.m1.1b"><ci id="S5.T6.76.76.76.3.1.1.m1.1.1.cmml" xref="S5.T6.76.76.76.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.76.76.76.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.76.76.76.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.78.78.78.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.77.77.77.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.77.77.77.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.77.77.77.4.m1.1a"><mi id="S5.T6.77.77.77.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.77.77.77.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.77.77.77.4.m1.1b"><ci id="S5.T6.77.77.77.4.m1.1.1.cmml" xref="S5.T6.77.77.77.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.77.77.77.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.77.77.77.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.78.78.78.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.78.78.78.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.78.78.78.5.m1.1a"><mi id="S5.T6.78.78.78.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.78.78.78.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.78.78.78.5.m1.1b"><ci id="S5.T6.78.78.78.5.m1.1.1.cmml" xref="S5.T6.78.78.78.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.78.78.78.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.78.78.78.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.78.78.78.15"><span class="ltx_text" id="S5.T6.78.78.78.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.81.81.81" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.81.81.81.4"><span class="ltx_text" id="S5.T6.81.81.81.4.1" style="background-color:#DFDFDF;">Chinchilla&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.81.81.81.5"><span class="ltx_text" id="S5.T6.81.81.81.5.1" style="background-color:#DFDFDF;">1.5M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.81.81.81.6"><span class="ltx_text" id="S5.T6.81.81.81.6.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.81.81.81.7"><span class="ltx_text" id="S5.T6.81.81.81.7.1" style="background-color:#DFDFDF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.79.79.79.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.79.79.79.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.79.79.79.1.m1.1a"><mi id="S5.T6.79.79.79.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.79.79.79.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.79.79.79.1.m1.1b"><ci id="S5.T6.79.79.79.1.m1.1.1.cmml" xref="S5.T6.79.79.79.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.79.79.79.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.79.79.79.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.81.81.81.8"><span class="ltx_text" id="S5.T6.81.81.81.8.1" style="background-color:#DFDFDF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.81.81.81.9" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.81.81.81.10" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.80.80.80.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.80.80.80.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.80.80.80.2.1.1.m1.1"><semantics id="S5.T6.80.80.80.2.1.1.m1.1a"><mi id="S5.T6.80.80.80.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.80.80.80.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.80.80.80.2.1.1.m1.1b"><ci id="S5.T6.80.80.80.2.1.1.m1.1.1.cmml" xref="S5.T6.80.80.80.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.80.80.80.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.80.80.80.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.81.81.81.11" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.81.81.81.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.81.81.81.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.81.81.81.3.1.1.m1.1"><semantics id="S5.T6.81.81.81.3.1.1.m1.1a"><mi id="S5.T6.81.81.81.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.81.81.81.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.81.81.81.3.1.1.m1.1b"><ci id="S5.T6.81.81.81.3.1.1.m1.1.1.cmml" xref="S5.T6.81.81.81.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.81.81.81.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.81.81.81.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.81.81.81.12" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.81.81.81.13"><span class="ltx_text" id="S5.T6.81.81.81.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.81.81.81.14"><span class="ltx_text" id="S5.T6.81.81.81.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.81.81.81.15"><span class="ltx_text" id="S5.T6.81.81.81.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.85.85.85" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.85.85.85.5"><span class="ltx_text" id="S5.T6.85.85.85.5.1" style="background-color:#FFFFFF;">PaLM&nbsp;(540B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.85.85.85.6"><span class="ltx_text" id="S5.T6.85.85.85.6.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.85.85.85.7"><span class="ltx_text" id="S5.T6.85.85.85.7.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.85.85.85.8"><span class="ltx_text" id="S5.T6.85.85.85.8.1" style="background-color:#FFFFFF;">0.01</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.85.85.85.9"><span class="ltx_text" id="S5.T6.85.85.85.9.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.85.85.85.10"><span class="ltx_text" id="S5.T6.85.85.85.10.1" style="background-color:#FFFFFF;">inverse square root</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.82.82.82.1" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.82.82.82.1.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.82.82.82.1.1.1.m1.1"><semantics id="S5.T6.82.82.82.1.1.1.m1.1a"><mi id="S5.T6.82.82.82.1.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.82.82.82.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.82.82.82.1.1.1.m1.1b"><ci id="S5.T6.82.82.82.1.1.1.m1.1.1.cmml" xref="S5.T6.82.82.82.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.82.82.82.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.82.82.82.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.85.85.85.11" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.85.85.85.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.85.85.85.13" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.85.85.85.13.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.85.85.85.14" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.85.85.85.14.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.85.85.85.15" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.85.85.85.15.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.83.83.83.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.83.83.83.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.83.83.83.2.m1.1a"><mi id="S5.T6.83.83.83.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.83.83.83.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.83.83.83.2.m1.1b"><ci id="S5.T6.83.83.83.2.m1.1.1.cmml" xref="S5.T6.83.83.83.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.83.83.83.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.83.83.83.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.84.84.84.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.84.84.84.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.84.84.84.3.m1.1a"><mi id="S5.T6.84.84.84.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.84.84.84.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.84.84.84.3.m1.1b"><ci id="S5.T6.84.84.84.3.m1.1.1.cmml" xref="S5.T6.84.84.84.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.84.84.84.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.84.84.84.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.85.85.85.4"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.85.85.85.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.85.85.85.4.m1.1a"><mo id="S5.T6.85.85.85.4.m1.1.1" mathbackground="#FFFFFF" xref="S5.T6.85.85.85.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.85.85.85.4.m1.1b"><times id="S5.T6.85.85.85.4.m1.1.1.cmml" xref="S5.T6.85.85.85.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.85.85.85.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.85.85.85.4.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.89.89.89" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.89.89.89.5"><span class="ltx_text" id="S5.T6.89.89.89.5.1" style="background-color:#DFDFDF;">AlexaTM&nbsp;(20B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.89.89.89.6"><span class="ltx_text" id="S5.T6.89.89.89.6.1" style="background-color:#DFDFDF;">2M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.89.89.89.7"><span class="ltx_text" id="S5.T6.89.89.89.7.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.89.89.89.8"><span class="ltx_text" id="S5.T6.89.89.89.8.1" style="background-color:#DFDFDF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.89.89.89.9"><span class="ltx_text" id="S5.T6.89.89.89.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.89.89.89.10"><span class="ltx_text" id="S5.T6.89.89.89.10.1" style="background-color:#DFDFDF;">linear decay to 5%</span></td>
<td class="ltx_td" id="S5.T6.89.89.89.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.86.86.86.1" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.86.86.86.1.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.86.86.86.1.1.1.m1.1"><semantics id="S5.T6.86.86.86.1.1.1.m1.1a"><mi id="S5.T6.86.86.86.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.86.86.86.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.86.86.86.1.1.1.m1.1b"><ci id="S5.T6.86.86.86.1.1.1.m1.1.1.cmml" xref="S5.T6.86.86.86.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.86.86.86.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.86.86.86.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.89.89.89.12" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.89.89.89.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.87.87.87.2" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.87.87.87.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.87.87.87.2.1.1.m1.1"><semantics id="S5.T6.87.87.87.2.1.1.m1.1a"><mi id="S5.T6.87.87.87.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.87.87.87.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.87.87.87.2.1.1.m1.1b"><ci id="S5.T6.87.87.87.2.1.1.m1.1.1.cmml" xref="S5.T6.87.87.87.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.87.87.87.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.87.87.87.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.89.89.89.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.88.88.88.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.88.88.88.3.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.88.88.88.3.m1.1a"><mi id="S5.T6.88.88.88.3.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.88.88.88.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.88.88.88.3.m1.1b"><ci id="S5.T6.88.88.88.3.m1.1.1.cmml" xref="S5.T6.88.88.88.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.88.88.88.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.88.88.88.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.89.89.89.15"><span class="ltx_text" id="S5.T6.89.89.89.15.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.89.89.89.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.89.89.89.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.89.89.89.4.m1.1a"><mi id="S5.T6.89.89.89.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.89.89.89.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.89.89.89.4.m1.1b"><ci id="S5.T6.89.89.89.4.m1.1.1.cmml" xref="S5.T6.89.89.89.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.89.89.89.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.89.89.89.4.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.90.90.90" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.90.90.90.2"><span class="ltx_text" id="S5.T6.90.90.90.2.1" style="background-color:#FFFFFF;">U-PaLM&nbsp;(540B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.90.90.90.3"><span class="ltx_text" id="S5.T6.90.90.90.3.1" style="background-color:#FFFFFF;">32</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.90.90.90.4"><span class="ltx_text" id="S5.T6.90.90.90.4.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.90.90.90.5"><span class="ltx_text" id="S5.T6.90.90.90.5.1" style="background-color:#FFFFFF;">1e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.90.90.90.6"><span class="ltx_text" id="S5.T6.90.90.90.6.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.90.90.90.7"><span class="ltx_text" id="S5.T6.90.90.90.7.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.90.90.90.1" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.90.90.90.1.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.90.90.90.1.1.1.m1.1"><semantics id="S5.T6.90.90.90.1.1.1.m1.1a"><mi id="S5.T6.90.90.90.1.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.90.90.90.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.90.90.90.1.1.1.m1.1b"><ci id="S5.T6.90.90.90.1.1.1.m1.1.1.cmml" xref="S5.T6.90.90.90.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.90.90.90.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.90.90.90.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.90.90.90.8" style="width:17.1pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.90.90.90.9" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.90.90.90.10" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.90.90.90.10.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.90.90.90.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.90.90.90.11.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.90.90.90.12" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.90.90.90.12.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.90.90.90.13"><span class="ltx_text" id="S5.T6.90.90.90.13.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.90.90.90.14"><span class="ltx_text" id="S5.T6.90.90.90.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.90.90.90.15"><span class="ltx_text" id="S5.T6.90.90.90.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.91.91.91" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.91.91.91.2"><span class="ltx_text" id="S5.T6.91.91.91.2.1" style="background-color:#DFDFDF;">UL2&nbsp;(20B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.91.91.91.3"><span class="ltx_text" id="S5.T6.91.91.91.3.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.91.91.91.4"><span class="ltx_text" id="S5.T6.91.91.91.4.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.91.91.91.5"><span class="ltx_text" id="S5.T6.91.91.91.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.91.91.91.6"><span class="ltx_text" id="S5.T6.91.91.91.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.91.91.91.7"><span class="ltx_text" id="S5.T6.91.91.91.7.1" style="background-color:#DFDFDF;">inverse square root</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.91.91.91.8" style="width:29.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.8.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.91.91.91.9" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.9.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.91.91.91.10" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.10.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.91.91.91.11" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.11.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.91.91.91.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.91.91.91.13" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.91.91.91.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.91.91.91.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.91.91.91.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.91.91.91.1.m1.1a"><mo id="S5.T6.91.91.91.1.m1.1.1" mathbackground="#DFDFDF" xref="S5.T6.91.91.91.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.91.91.91.1.m1.1b"><times id="S5.T6.91.91.91.1.m1.1.1.cmml" xref="S5.T6.91.91.91.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.91.91.91.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.91.91.91.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.91.91.91.14"><span class="ltx_text" id="S5.T6.91.91.91.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.91.91.91.15"><span class="ltx_text" id="S5.T6.91.91.91.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.97.97.97" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.97.97.97.7"><span class="ltx_text" id="S5.T6.97.97.97.7.1" style="background-color:#FFFFFF;">GLM&nbsp;(130B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.97.97.97.8"><span class="ltx_text" id="S5.T6.97.97.97.8.1" style="background-color:#FFFFFF;">4224</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.97.97.97.9"><span class="ltx_text" id="S5.T6.97.97.97.9.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.97.97.97.10"><span class="ltx_text" id="S5.T6.97.97.97.10.1" style="background-color:#FFFFFF;">8e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.92.92.92.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.92.92.92.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.92.92.92.1.m1.1a"><mi id="S5.T6.92.92.92.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.92.92.92.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.92.92.92.1.m1.1b"><ci id="S5.T6.92.92.92.1.m1.1.1.cmml" xref="S5.T6.92.92.92.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.92.92.92.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.92.92.92.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.97.97.97.11"><span class="ltx_text" id="S5.T6.97.97.97.11.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.97.97.97.12" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.97.97.97.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.93.93.93.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.93.93.93.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.93.93.93.2.1.1.m1.1"><semantics id="S5.T6.93.93.93.2.1.1.m1.1a"><mi id="S5.T6.93.93.93.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.93.93.93.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.93.93.93.2.1.1.m1.1b"><ci id="S5.T6.93.93.93.2.1.1.m1.1.1.cmml" xref="S5.T6.93.93.93.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.93.93.93.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.93.93.93.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.94.94.94.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.94.94.94.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.94.94.94.3.1.1.m1.1"><semantics id="S5.T6.94.94.94.3.1.1.m1.1a"><mi id="S5.T6.94.94.94.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.94.94.94.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.94.94.94.3.1.1.m1.1b"><ci id="S5.T6.94.94.94.3.1.1.m1.1.1.cmml" xref="S5.T6.94.94.94.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.94.94.94.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.94.94.94.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.97.97.97.14" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.97.97.97.15" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.95.95.95.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.95.95.95.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.95.95.95.4.m1.1a"><mi id="S5.T6.95.95.95.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.95.95.95.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.95.95.95.4.m1.1b"><ci id="S5.T6.95.95.95.4.m1.1.1.cmml" xref="S5.T6.95.95.95.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.95.95.95.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.95.95.95.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.96.96.96.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.96.96.96.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.96.96.96.5.m1.1a"><mi id="S5.T6.96.96.96.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.96.96.96.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.96.96.96.5.m1.1b"><ci id="S5.T6.96.96.96.5.m1.1.1.cmml" xref="S5.T6.96.96.96.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.96.96.96.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.96.96.96.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.97.97.97.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.97.97.97.6.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.97.97.97.6.m1.1a"><mi id="S5.T6.97.97.97.6.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.97.97.97.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.97.97.97.6.m1.1b"><ci id="S5.T6.97.97.97.6.m1.1.1.cmml" xref="S5.T6.97.97.97.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.97.97.97.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.97.97.97.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.101.101.101" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.101.101.101.5"><span class="ltx_text" id="S5.T6.101.101.101.5.1" style="background-color:#DFDFDF;">CodeGen&nbsp;(16B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.101.101.101.6"><span class="ltx_text" id="S5.T6.101.101.101.6.1" style="background-color:#DFDFDF;">2M</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.101.101.101.7"><span class="ltx_text" id="S5.T6.101.101.101.7.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.101.101.101.8"><span class="ltx_text" id="S5.T6.101.101.101.8.1" style="background-color:#DFDFDF;">5e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.98.98.98.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.98.98.98.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.98.98.98.1.m1.1a"><mi id="S5.T6.98.98.98.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.98.98.98.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.98.98.98.1.m1.1b"><ci id="S5.T6.98.98.98.1.m1.1.1.cmml" xref="S5.T6.98.98.98.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.98.98.98.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.98.98.98.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.101.101.101.9"><span class="ltx_text" id="S5.T6.101.101.101.9.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.101.101.101.10" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.99.99.99.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.99.99.99.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.99.99.99.2.1.1.m1.1"><semantics id="S5.T6.99.99.99.2.1.1.m1.1a"><mi id="S5.T6.99.99.99.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.99.99.99.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.99.99.99.2.1.1.m1.1b"><ci id="S5.T6.99.99.99.2.1.1.m1.1.1.cmml" xref="S5.T6.99.99.99.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.99.99.99.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.99.99.99.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.101.101.101.11" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.101.101.101.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.101.101.101.12.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.101.101.101.13" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.101.101.101.13.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.101.101.101.14" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.101.101.101.14.1" style="background-color:#DFDFDF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.100.100.100.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.100.100.100.3.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.100.100.100.3.m1.1a"><mi id="S5.T6.100.100.100.3.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.100.100.100.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.100.100.100.3.m1.1b"><ci id="S5.T6.100.100.100.3.m1.1.1.cmml" xref="S5.T6.100.100.100.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.100.100.100.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.100.100.100.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.101.101.101.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.101.101.101.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.101.101.101.4.m1.1a"><mi id="S5.T6.101.101.101.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.101.101.101.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.101.101.101.4.m1.1b"><ci id="S5.T6.101.101.101.4.m1.1.1.cmml" xref="S5.T6.101.101.101.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.101.101.101.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.101.101.101.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.101.101.101.15"><span class="ltx_text" id="S5.T6.101.101.101.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.105.105.105" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.105.105.105.5"><span class="ltx_text" id="S5.T6.105.105.105.5.1" style="background-color:#FFFFFF;">LLaMA&nbsp;(65B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.105.105.105.6"><span class="ltx_text" id="S5.T6.105.105.105.6.1" style="background-color:#FFFFFF;">4M Tokens</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.105.105.105.7"><span class="ltx_text" id="S5.T6.105.105.105.7.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.105.105.105.8"><span class="ltx_text" id="S5.T6.105.105.105.8.1" style="background-color:#FFFFFF;">1.5e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.102.102.102.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.102.102.102.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.102.102.102.1.m1.1a"><mi id="S5.T6.102.102.102.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.102.102.102.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.102.102.102.1.m1.1b"><ci id="S5.T6.102.102.102.1.m1.1.1.cmml" xref="S5.T6.102.102.102.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.102.102.102.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.102.102.102.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.105.105.105.9"><span class="ltx_text" id="S5.T6.105.105.105.9.1" style="background-color:#FFFFFF;">cosine decay to 10%</span></td>
<td class="ltx_td" id="S5.T6.105.105.105.10" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.105.105.105.11" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.103.103.103.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.103.103.103.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.103.103.103.2.1.1.m1.1"><semantics id="S5.T6.103.103.103.2.1.1.m1.1a"><mi id="S5.T6.103.103.103.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.103.103.103.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.103.103.103.2.1.1.m1.1b"><ci id="S5.T6.103.103.103.2.1.1.m1.1.1.cmml" xref="S5.T6.103.103.103.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.103.103.103.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.103.103.103.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.105.105.105.12" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.105.105.105.12.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T6.105.105.105.13" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.105.105.105.13.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.105.105.105.14" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.105.105.105.14.1" style="background-color:#FFFFFF;">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.104.104.104.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.104.104.104.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.104.104.104.3.m1.1a"><mi id="S5.T6.104.104.104.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.104.104.104.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.104.104.104.3.m1.1b"><ci id="S5.T6.104.104.104.3.m1.1.1.cmml" xref="S5.T6.104.104.104.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.104.104.104.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.104.104.104.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.105.105.105.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.105.105.105.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.105.105.105.4.m1.1a"><mi id="S5.T6.105.105.105.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.105.105.105.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.105.105.105.4.m1.1b"><ci id="S5.T6.105.105.105.4.m1.1.1.cmml" xref="S5.T6.105.105.105.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.105.105.105.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.105.105.105.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.105.105.105.15"><span class="ltx_text" id="S5.T6.105.105.105.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.109.109.109" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.106.106.106.1"><span class="ltx_text" id="S5.T6.106.106.106.1.1" style="background-color:#DFDFDF;">PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S5.T6.106.106.106.1.1.m1.1"><semantics id="S5.T6.106.106.106.1.1.m1.1a"><mi id="S5.T6.106.106.106.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.106.106.106.1.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S5.T6.106.106.106.1.1.m1.1b"><ci id="S5.T6.106.106.106.1.1.m1.1.1.cmml" xref="S5.T6.106.106.106.1.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.106.106.106.1.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S5.T6.106.106.106.1.1.m1.1d">roman_Σ</annotation></semantics></math>&nbsp;(1.085T)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.109.109.109.5"><span class="ltx_text" id="S5.T6.109.109.109.5.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.109.109.109.6"><span class="ltx_text" id="S5.T6.109.109.109.6.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.109.109.109.7"><span class="ltx_text" id="S5.T6.109.109.109.7.1" style="background-color:#DFDFDF;">2e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.107.107.107.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.107.107.107.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.107.107.107.2.m1.1a"><mi id="S5.T6.107.107.107.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.107.107.107.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.107.107.107.2.m1.1b"><ci id="S5.T6.107.107.107.2.m1.1.1.cmml" xref="S5.T6.107.107.107.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.107.107.107.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.107.107.107.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.109.109.109.8"><span class="ltx_text" id="S5.T6.109.109.109.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td" id="S5.T6.109.109.109.9" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.108.108.108.3" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.108.108.108.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.108.108.108.3.1.1.m1.1"><semantics id="S5.T6.108.108.108.3.1.1.m1.1a"><mi id="S5.T6.108.108.108.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.108.108.108.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.108.108.108.3.1.1.m1.1b"><ci id="S5.T6.108.108.108.3.1.1.m1.1.1.cmml" xref="S5.T6.108.108.108.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.108.108.108.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.108.108.108.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.109.109.109.10" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.109.109.109.11" style="width:14.2pt;"></td>
<td class="ltx_td" id="S5.T6.109.109.109.12" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.109.109.109.4" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.109.109.109.4.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.109.109.109.4.1.1.m1.1"><semantics id="S5.T6.109.109.109.4.1.1.m1.1a"><mi id="S5.T6.109.109.109.4.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.109.109.109.4.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.109.109.109.4.1.1.m1.1b"><ci id="S5.T6.109.109.109.4.1.1.m1.1.1.cmml" xref="S5.T6.109.109.109.4.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.109.109.109.4.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.109.109.109.4.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.109.109.109.13"><span class="ltx_text" id="S5.T6.109.109.109.13.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.109.109.109.14"><span class="ltx_text" id="S5.T6.109.109.109.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.109.109.109.15"><span class="ltx_text" id="S5.T6.109.109.109.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.115.115.115" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.115.115.115.7"><span class="ltx_text" id="S5.T6.115.115.115.7.1" style="background-color:#FFFFFF;">BloombergGPT&nbsp;(50B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.115.115.115.8"><span class="ltx_text" id="S5.T6.115.115.115.8.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.115.115.115.9"><span class="ltx_text" id="S5.T6.115.115.115.9.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.115.115.115.10"><span class="ltx_text" id="S5.T6.115.115.115.10.1" style="background-color:#FFFFFF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.110.110.110.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.110.110.110.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.110.110.110.1.m1.1a"><mi id="S5.T6.110.110.110.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.110.110.110.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.110.110.110.1.m1.1b"><ci id="S5.T6.110.110.110.1.m1.1.1.cmml" xref="S5.T6.110.110.110.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.110.110.110.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.110.110.110.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.115.115.115.11"><span class="ltx_text" id="S5.T6.115.115.115.11.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.115.115.115.12" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.115.115.115.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.111.111.111.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.111.111.111.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.111.111.111.2.1.1.m1.1"><semantics id="S5.T6.111.111.111.2.1.1.m1.1a"><mi id="S5.T6.111.111.111.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.111.111.111.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.111.111.111.2.1.1.m1.1b"><ci id="S5.T6.111.111.111.2.1.1.m1.1.1.cmml" xref="S5.T6.111.111.111.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.111.111.111.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.111.111.111.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.115.115.115.14" style="width:14.2pt;"></td>
<td class="ltx_td" id="S5.T6.115.115.115.15" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.112.112.112.3" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.112.112.112.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.112.112.112.3.1.1.m1.1"><semantics id="S5.T6.112.112.112.3.1.1.m1.1a"><mi id="S5.T6.112.112.112.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.112.112.112.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.112.112.112.3.1.1.m1.1b"><ci id="S5.T6.112.112.112.3.1.1.m1.1.1.cmml" xref="S5.T6.112.112.112.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.112.112.112.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.112.112.112.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.113.113.113.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.113.113.113.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.113.113.113.4.m1.1a"><mi id="S5.T6.113.113.113.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.113.113.113.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.113.113.113.4.m1.1b"><ci id="S5.T6.113.113.113.4.m1.1.1.cmml" xref="S5.T6.113.113.113.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.113.113.113.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.113.113.113.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.114.114.114.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.114.114.114.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.114.114.114.5.m1.1a"><mi id="S5.T6.114.114.114.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.114.114.114.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.114.114.114.5.m1.1b"><ci id="S5.T6.114.114.114.5.m1.1.1.cmml" xref="S5.T6.114.114.114.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.114.114.114.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.114.114.114.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.115.115.115.6"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.115.115.115.6.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.115.115.115.6.m1.1a"><mo id="S5.T6.115.115.115.6.m1.1.1" mathbackground="#FFFFFF" xref="S5.T6.115.115.115.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.115.115.115.6.m1.1b"><times id="S5.T6.115.115.115.6.m1.1.1.cmml" xref="S5.T6.115.115.115.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.115.115.115.6.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.115.115.115.6.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T6.120.120.120" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.120.120.120.6"><span class="ltx_text" id="S5.T6.120.120.120.6.1" style="background-color:#DFDFDF;">Xuan Yuan 2.0&nbsp;(176B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.120.120.120.7"><span class="ltx_text" id="S5.T6.120.120.120.7.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.120.120.120.8"><span class="ltx_text" id="S5.T6.120.120.120.8.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.120.120.120.9"><span class="ltx_text" id="S5.T6.120.120.120.9.1" style="background-color:#DFDFDF;">6e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.116.116.116.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.116.116.116.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.116.116.116.1.m1.1a"><mi id="S5.T6.116.116.116.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.116.116.116.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.116.116.116.1.m1.1b"><ci id="S5.T6.116.116.116.1.m1.1.1.cmml" xref="S5.T6.116.116.116.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.116.116.116.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.116.116.116.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.120.120.120.10"><span class="ltx_text" id="S5.T6.120.120.120.10.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.120.120.120.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.117.117.117.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.117.117.117.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.117.117.117.2.1.1.m1.1"><semantics id="S5.T6.117.117.117.2.1.1.m1.1a"><mi id="S5.T6.117.117.117.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.117.117.117.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.117.117.117.2.1.1.m1.1b"><ci id="S5.T6.117.117.117.2.1.1.m1.1.1.cmml" xref="S5.T6.117.117.117.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.117.117.117.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.117.117.117.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.120.120.120.12" style="width:24.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.118.118.118.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.118.118.118.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.118.118.118.3.1.1.m1.1"><semantics id="S5.T6.118.118.118.3.1.1.m1.1a"><mi id="S5.T6.118.118.118.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.118.118.118.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.118.118.118.3.1.1.m1.1b"><ci id="S5.T6.118.118.118.3.1.1.m1.1.1.cmml" xref="S5.T6.118.118.118.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.118.118.118.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.118.118.118.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.120.120.120.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.120.120.120.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.119.119.119.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.119.119.119.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.119.119.119.4.m1.1a"><mi id="S5.T6.119.119.119.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.119.119.119.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.119.119.119.4.m1.1b"><ci id="S5.T6.119.119.119.4.m1.1.1.cmml" xref="S5.T6.119.119.119.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.119.119.119.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.119.119.119.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.120.120.120.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.120.120.120.5.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.120.120.120.5.m1.1a"><mi id="S5.T6.120.120.120.5.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.120.120.120.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.120.120.120.5.m1.1b"><ci id="S5.T6.120.120.120.5.m1.1.1.cmml" xref="S5.T6.120.120.120.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.120.120.120.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.120.120.120.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.120.120.120.15"><span class="ltx_text" id="S5.T6.120.120.120.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.123.123.123" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T6.123.123.123.4"><span class="ltx_text" id="S5.T6.123.123.123.4.1" style="background-color:#FFFFFF;">CodeT5+&nbsp;(16B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.123.123.123.5"><span class="ltx_text" id="S5.T6.123.123.123.5.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.123.123.123.6"><span class="ltx_text" id="S5.T6.123.123.123.6.1" style="background-color:#FFFFFF;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.123.123.123.7"><span class="ltx_text" id="S5.T6.123.123.123.7.1" style="background-color:#FFFFFF;">2e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.123.123.123.8"><span class="ltx_text" id="S5.T6.123.123.123.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.123.123.123.9"><span class="ltx_text" id="S5.T6.123.123.123.9.1" style="background-color:#FFFFFF;">linear</span></td>
<td class="ltx_td" id="S5.T6.123.123.123.10" style="width:29.9pt;"></td>
<td class="ltx_td" id="S5.T6.123.123.123.11" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.121.121.121.1" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.121.121.121.1.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.121.121.121.1.1.1.m1.1"><semantics id="S5.T6.121.121.121.1.1.1.m1.1a"><mi id="S5.T6.121.121.121.1.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.121.121.121.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.121.121.121.1.1.1.m1.1b"><ci id="S5.T6.121.121.121.1.1.1.m1.1.1.cmml" xref="S5.T6.121.121.121.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.121.121.121.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.121.121.121.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S5.T6.123.123.123.12" style="width:14.2pt;"></td>
<td class="ltx_td" id="S5.T6.123.123.123.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" id="S5.T6.122.122.122.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.122.122.122.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.122.122.122.2.1.1.m1.1"><semantics id="S5.T6.122.122.122.2.1.1.m1.1a"><mi id="S5.T6.122.122.122.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.122.122.122.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.122.122.122.2.1.1.m1.1b"><ci id="S5.T6.122.122.122.2.1.1.m1.1.1.cmml" xref="S5.T6.122.122.122.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.122.122.122.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.122.122.122.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.123.123.123.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.123.123.123.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.123.123.123.3.m1.1a"><mi id="S5.T6.123.123.123.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.123.123.123.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.123.123.123.3.m1.1b"><ci id="S5.T6.123.123.123.3.m1.1.1.cmml" xref="S5.T6.123.123.123.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.123.123.123.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.123.123.123.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.123.123.123.14"><span class="ltx_text" id="S5.T6.123.123.123.14.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.123.123.123.15"><span class="ltx_text" id="S5.T6.123.123.123.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.127.127.127" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T6.127.127.127.5"><span class="ltx_text" id="S5.T6.127.127.127.5.1" style="background-color:#DFDFDF;">StarCoder&nbsp;(15.5B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.127.127.127.6"><span class="ltx_text" id="S5.T6.127.127.127.6.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_right" id="S5.T6.127.127.127.7"><span class="ltx_text" id="S5.T6.127.127.127.7.1" style="background-color:#DFDFDF;">8k</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.127.127.127.8"><span class="ltx_text" id="S5.T6.127.127.127.8.1" style="background-color:#DFDFDF;">3e-4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.124.124.124.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.124.124.124.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.124.124.124.1.m1.1a"><mi id="S5.T6.124.124.124.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.124.124.124.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.124.124.124.1.m1.1b"><ci id="S5.T6.124.124.124.1.m1.1.1.cmml" xref="S5.T6.124.124.124.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.124.124.124.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.124.124.124.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.127.127.127.9"><span class="ltx_text" id="S5.T6.127.127.127.9.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T6.127.127.127.10" style="width:29.9pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.125.125.125.2" style="width:17.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.125.125.125.2.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.125.125.125.2.1.1.m1.1"><semantics id="S5.T6.125.125.125.2.1.1.m1.1a"><mi id="S5.T6.125.125.125.2.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.125.125.125.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.125.125.125.2.1.1.m1.1b"><ci id="S5.T6.125.125.125.2.1.1.m1.1.1.cmml" xref="S5.T6.125.125.125.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.125.125.125.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.125.125.125.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.127.127.127.11" style="width:24.2pt;"></td>
<td class="ltx_td" id="S5.T6.127.127.127.12" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.126.126.126.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.126.126.126.3.1.1" style="background-color:#DFDFDF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.126.126.126.3.1.1.m1.1"><semantics id="S5.T6.126.126.126.3.1.1.m1.1a"><mi id="S5.T6.126.126.126.3.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.126.126.126.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.126.126.126.3.1.1.m1.1b"><ci id="S5.T6.126.126.126.3.1.1.m1.1.1.cmml" xref="S5.T6.126.126.126.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.126.126.126.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.126.126.126.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r" id="S5.T6.127.127.127.13" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.127.127.127.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.127.127.127.4.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T6.127.127.127.4.m1.1a"><mi id="S5.T6.127.127.127.4.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T6.127.127.127.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.127.127.127.4.m1.1b"><ci id="S5.T6.127.127.127.4.m1.1.1.cmml" xref="S5.T6.127.127.127.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.127.127.127.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.127.127.127.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T6.127.127.127.14"><span class="ltx_text" id="S5.T6.127.127.127.14.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.127.127.127.15"><span class="ltx_text" id="S5.T6.127.127.127.15.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.132.132.132" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T6.132.132.132.6"><span class="ltx_text" id="S5.T6.132.132.132.6.1" style="background-color:#FFFFFF;">LLaMA-2&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T6.132.132.132.7"><span class="ltx_text" id="S5.T6.132.132.132.7.1" style="background-color:#FFFFFF;">4M Tokens</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T6.132.132.132.8"><span class="ltx_text" id="S5.T6.132.132.132.8.1" style="background-color:#FFFFFF;">4k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.132.132.132.9"><span class="ltx_text" id="S5.T6.132.132.132.9.1" style="background-color:#FFFFFF;">1.5e-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.128.128.128.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.128.128.128.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.128.128.128.1.m1.1a"><mi id="S5.T6.128.128.128.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.128.128.128.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.128.128.128.1.m1.1b"><ci id="S5.T6.128.128.128.1.m1.1.1.cmml" xref="S5.T6.128.128.128.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.128.128.128.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.128.128.128.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.132.132.132.10"><span class="ltx_text" id="S5.T6.132.132.132.10.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td ltx_border_bb" id="S5.T6.132.132.132.11" style="width:29.9pt;"></td>
<td class="ltx_td ltx_border_bb" id="S5.T6.132.132.132.12" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r" id="S5.T6.129.129.129.2" style="width:24.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.129.129.129.2.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.129.129.129.2.1.1.m1.1"><semantics id="S5.T6.129.129.129.2.1.1.m1.1a"><mi id="S5.T6.129.129.129.2.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.129.129.129.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.129.129.129.2.1.1.m1.1b"><ci id="S5.T6.129.129.129.2.1.1.m1.1.1.cmml" xref="S5.T6.129.129.129.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.129.129.129.2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.129.129.129.2.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_bb" id="S5.T6.132.132.132.13" style="width:14.2pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T6.130.130.130.3" style="width:14.2pt;">
<p class="ltx_p ltx_align_top" id="S5.T6.130.130.130.3.1.1" style="background-color:#FFFFFF;"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.130.130.130.3.1.1.m1.1"><semantics id="S5.T6.130.130.130.3.1.1.m1.1a"><mi id="S5.T6.130.130.130.3.1.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.130.130.130.3.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.130.130.130.3.1.1.m1.1b"><ci id="S5.T6.130.130.130.3.1.1.m1.1.1.cmml" xref="S5.T6.130.130.130.3.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.130.130.130.3.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.130.130.130.3.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S5.T6.132.132.132.14" style="width:17.1pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.131.131.131.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.131.131.131.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.131.131.131.4.m1.1a"><mi id="S5.T6.131.131.131.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.131.131.131.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.131.131.131.4.m1.1b"><ci id="S5.T6.131.131.131.4.m1.1.1.cmml" xref="S5.T6.131.131.131.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.131.131.131.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.131.131.131.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.132.132.132.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T6.132.132.132.5.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T6.132.132.132.5.m1.1a"><mi id="S5.T6.132.132.132.5.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T6.132.132.132.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T6.132.132.132.5.m1.1b"><ci id="S5.T6.132.132.132.5.m1.1.1.cmml" xref="S5.T6.132.132.132.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.132.132.132.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T6.132.132.132.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.132.132.132.15"><span class="ltx_text" id="S5.T6.132.132.132.15.1" style="background-color:#FFFFFF;">-</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while no model uses weight decay for instruction tuning.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T7.24" style="width:433.6pt;height:178.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.4pt,19.0pt) scale(0.82361506000975,0.82361506000975) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.24.24">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.24.24.25.1">
<td class="ltx_td ltx_border_tt" id="S5.T7.24.24.25.1.1"></td>
<td class="ltx_td ltx_border_tt" id="S5.T7.24.24.25.1.2"></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T7.24.24.25.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.25.1.3.1" style="background-color:#BFBFBF;">Sequence</span></td>
<td class="ltx_td ltx_border_tt" id="S5.T7.24.24.25.1.4"></td>
<td class="ltx_td ltx_border_tt" id="S5.T7.24.24.25.1.5"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T7.24.24.25.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T7.24.24.25.1.7"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.25.1.7.1" style="background-color:#BFBFBF;">Optimizers<span class="ltx_text ltx_font_medium" id="S5.T7.24.24.25.1.7.1.1" style="background-color:#BFBFBF;"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.24.24.25.1.8"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.25.1.8.1" style="background-color:#BFBFBF;">Grad</span></td>
<td class="ltx_td ltx_border_tt" id="S5.T7.24.24.25.1.9"></td>
</tr>
<tr class="ltx_tr" id="S5.T7.24.24.26.2" style="background-color:#BFBFBF;">
<td class="ltx_td ltx_align_left" id="S5.T7.24.24.26.2.1"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.1.1" style="background-color:#BFBFBF;">Models</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.24.24.26.2.2"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.2.1" style="background-color:#BFBFBF;">Batch Size</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.24.24.26.2.3"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.3.1" style="background-color:#BFBFBF;">Length</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.4"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.4.1" style="background-color:#BFBFBF;">LR</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.5"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.5.1" style="background-color:#BFBFBF;">Warmup</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.24.24.26.2.6"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.6.1" style="background-color:#BFBFBF;">LR_Decay</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.7"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.7.1" style="background-color:#BFBFBF;">AdaFactor</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.8"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.8.1" style="background-color:#BFBFBF;">Adam</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.24.24.26.2.9"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.9.1" style="background-color:#BFBFBF;">AdamW</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.10"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.10.1" style="background-color:#BFBFBF;">Clip</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.26.2.11"><span class="ltx_text ltx_font_bold" id="S5.T7.24.24.26.2.11.1" style="background-color:#BFBFBF;">Dropout</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.1" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T7.1.1.1.2"><span class="ltx_text" id="S5.T7.1.1.1.2.1" style="background-color:#DFDFDF;">WebGPT&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T7.1.1.1.3"><span class="ltx_text" id="S5.T7.1.1.1.3.1" style="background-color:#DFDFDF;">BC:512, RM:32</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T7.1.1.1.4"><span class="ltx_text" id="S5.T7.1.1.1.4.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.5"><span class="ltx_text" id="S5.T7.1.1.1.5.1" style="background-color:#DFDFDF;">6e-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.6"><span class="ltx_text" id="S5.T7.1.1.1.6.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.1.7"><span class="ltx_text" id="S5.T7.1.1.1.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_border_t" id="S5.T7.1.1.1.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.1.1.1.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.1.1.1.1.m1.1a"><mi id="S5.T7.1.1.1.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><ci id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T7.1.1.1.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.10"><span class="ltx_text" id="S5.T7.1.1.1.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.11"><span class="ltx_text" id="S5.T7.1.1.1.11.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.3.3" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T7.3.3.3.3"><span class="ltx_text" id="S5.T7.3.3.3.3.1" style="background-color:#FFFFFF;">T0&nbsp;(11B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.3.3.3.4"><span class="ltx_text" id="S5.T7.3.3.3.4.1" style="background-color:#FFFFFF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.3.3.3.5"><span class="ltx_text" id="S5.T7.3.3.3.5.1" style="background-color:#FFFFFF;">1280</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.3.6"><span class="ltx_text" id="S5.T7.3.3.3.6.1" style="background-color:#FFFFFF;">1e-3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.3.7"><span class="ltx_text" id="S5.T7.3.3.3.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.3.3.3.8"><span class="ltx_text" id="S5.T7.3.3.3.8.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.2.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.2.2.2.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.2.2.2.1.m1.1a"><mi id="S5.T7.2.2.2.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.2.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.1.m1.1b"><ci id="S5.T7.2.2.2.1.m1.1.1.cmml" xref="S5.T7.2.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.2.2.2.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T7.3.3.3.9"></td>
<td class="ltx_td ltx_border_r" id="S5.T7.3.3.3.10"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.3.11"><span class="ltx_text" id="S5.T7.3.3.3.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.3.3.3.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.3.3.3.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.3.3.3.2.m1.1a"><mi id="S5.T7.3.3.3.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.3.3.3.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.3.2.m1.1b"><ci id="S5.T7.3.3.3.2.m1.1.1.cmml" xref="S5.T7.3.3.3.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.3.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.3.3.3.2.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.24.24.27.3" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T7.24.24.27.3.1"><span class="ltx_text" id="S5.T7.24.24.27.3.1.1" style="background-color:#DFDFDF;">Tk-Instruct&nbsp;(11B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.24.24.27.3.2"><span class="ltx_text" id="S5.T7.24.24.27.3.2.1" style="background-color:#DFDFDF;">1024</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.24.24.27.3.3"><span class="ltx_text" id="S5.T7.24.24.27.3.3.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.4"><span class="ltx_text" id="S5.T7.24.24.27.3.4.1" style="background-color:#DFDFDF;">1e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.5"><span class="ltx_text" id="S5.T7.24.24.27.3.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.24.24.27.3.6"><span class="ltx_text" id="S5.T7.24.24.27.3.6.1" style="background-color:#DFDFDF;">constant</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.7"><span class="ltx_text" id="S5.T7.24.24.27.3.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.8"><span class="ltx_text" id="S5.T7.24.24.27.3.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.24.24.27.3.9"><span class="ltx_text" id="S5.T7.24.24.27.3.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.10"><span class="ltx_text" id="S5.T7.24.24.27.3.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.24.24.27.3.11"><span class="ltx_text" id="S5.T7.24.24.27.3.11.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.7.7.7" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T7.7.7.7.5"><span class="ltx_text" id="S5.T7.7.7.7.5.1" style="background-color:#FFFFFF;">OPT-IML&nbsp;(175B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.7.7.7.6"><span class="ltx_text" id="S5.T7.7.7.7.6.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.7.7.7.7"><span class="ltx_text" id="S5.T7.7.7.7.7.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.7.7.7.8"><span class="ltx_text" id="S5.T7.7.7.7.8.1" style="background-color:#FFFFFF;">5e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.4.4.4.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T7.4.4.4.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.4.4.4.1.m1.1a"><mo id="S5.T7.4.4.4.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T7.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T7.4.4.4.1.m1.1b"><times id="S5.T7.4.4.4.1.m1.1.1.cmml" xref="S5.T7.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T7.4.4.4.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.7.7.7.9"><span class="ltx_text" id="S5.T7.7.7.7.9.1" style="background-color:#FFFFFF;">linear</span></td>
<td class="ltx_td" id="S5.T7.7.7.7.10"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.5.5.5.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.5.5.5.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.5.5.5.2.m1.1a"><mi id="S5.T7.5.5.5.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.5.5.5.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.5.5.5.2.m1.1b"><ci id="S5.T7.5.5.5.2.m1.1.1.cmml" xref="S5.T7.5.5.5.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.5.5.5.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.5.5.5.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r" id="S5.T7.7.7.7.11"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.6.6.6.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.6.6.6.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.6.6.6.3.m1.1a"><mi id="S5.T7.6.6.6.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.6.6.6.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.6.6.6.3.m1.1b"><ci id="S5.T7.6.6.6.3.m1.1.1.cmml" xref="S5.T7.6.6.6.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.6.6.6.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.6.6.6.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T7.7.7.7.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.7.7.7.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.7.7.7.4.m1.1a"><mi id="S5.T7.7.7.7.4.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.7.7.7.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.7.7.7.4.m1.1b"><ci id="S5.T7.7.7.7.4.m1.1.1.cmml" xref="S5.T7.7.7.7.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.7.7.7.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.7.7.7.4.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.9.9.9" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T7.9.9.9.3"><span class="ltx_text" id="S5.T7.9.9.9.3.1" style="background-color:#DFDFDF;">Flan-U-PaLM&nbsp;(540B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.9.9.9.4"><span class="ltx_text" id="S5.T7.9.9.9.4.1" style="background-color:#DFDFDF;">32</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.9.9.9.5"><span class="ltx_text" id="S5.T7.9.9.9.5.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.9.9.9.6"><span class="ltx_text" id="S5.T7.9.9.9.6.1" style="background-color:#DFDFDF;">1e-3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.9.9.9.7"><span class="ltx_text" id="S5.T7.9.9.9.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.9.9.9.8"><span class="ltx_text" id="S5.T7.9.9.9.8.1" style="background-color:#DFDFDF;">constant</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.8.8.8.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.8.8.8.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.8.8.8.1.m1.1a"><mi id="S5.T7.8.8.8.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.8.8.8.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.8.8.8.1.m1.1b"><ci id="S5.T7.8.8.8.1.m1.1.1.cmml" xref="S5.T7.8.8.8.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.8.8.8.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.8.8.8.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T7.9.9.9.9"></td>
<td class="ltx_td ltx_border_r" id="S5.T7.9.9.9.10"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.9.9.9.11"><span class="ltx_text" id="S5.T7.9.9.9.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.9.9.9.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.9.9.9.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.9.9.9.2.m1.1a"><mi id="S5.T7.9.9.9.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.9.9.9.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.9.9.9.2.m1.1b"><ci id="S5.T7.9.9.9.2.m1.1.1.cmml" xref="S5.T7.9.9.9.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.9.9.9.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.9.9.9.2.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.13.13.13" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T7.13.13.13.5"><span class="ltx_text" id="S5.T7.13.13.13.5.1" style="background-color:#FFFFFF;">Sparrow&nbsp;(70B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.13.13.13.6"><span class="ltx_text" id="S5.T7.13.13.13.6.1" style="background-color:#FFFFFF;">RM: 8+16, RL:16</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.13.13.13.7"><span class="ltx_text" id="S5.T7.13.13.13.7.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.13.13.13.8"><span class="ltx_text" id="S5.T7.13.13.13.8.1" style="background-color:#FFFFFF;">2e-6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.10.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.10.10.10.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.10.10.10.1.m1.1a"><mi id="S5.T7.10.10.10.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.10.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.10.10.10.1.m1.1b"><ci id="S5.T7.10.10.10.1.m1.1.1.cmml" xref="S5.T7.10.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.10.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.10.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.13.13.13.9"><span class="ltx_text" id="S5.T7.13.13.13.9.1" style="background-color:#FFFFFF;">cosine decay to 10%</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.11.11.11.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.11.11.11.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.11.11.11.2.m1.1a"><mi id="S5.T7.11.11.11.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.11.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.11.11.11.2.m1.1b"><ci id="S5.T7.11.11.11.2.m1.1.1.cmml" xref="S5.T7.11.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.11.11.11.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.11.11.11.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td" id="S5.T7.13.13.13.10"></td>
<td class="ltx_td ltx_border_r" id="S5.T7.13.13.13.11"></td>
<td class="ltx_td ltx_align_center" id="S5.T7.12.12.12.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.12.12.12.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.12.12.12.3.m1.1a"><mi id="S5.T7.12.12.12.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.12.12.12.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.12.12.12.3.m1.1b"><ci id="S5.T7.12.12.12.3.m1.1.1.cmml" xref="S5.T7.12.12.12.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.12.12.12.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.12.12.12.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T7.13.13.13.4"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T7.13.13.13.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.13.13.13.4.m1.1a"><mo id="S5.T7.13.13.13.4.m1.1.1" mathbackground="#FFFFFF" xref="S5.T7.13.13.13.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T7.13.13.13.4.m1.1b"><times id="S5.T7.13.13.13.4.m1.1.1.cmml" xref="S5.T7.13.13.13.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.13.13.13.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T7.13.13.13.4.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.14.14.14" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T7.14.14.14.2"><span class="ltx_text" id="S5.T7.14.14.14.2.1" style="background-color:#DFDFDF;">WizardCoder&nbsp;(15B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.14.14.14.3"><span class="ltx_text" id="S5.T7.14.14.14.3.1" style="background-color:#DFDFDF;">512</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.14.14.14.4"><span class="ltx_text" id="S5.T7.14.14.14.4.1" style="background-color:#DFDFDF;">2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.5"><span class="ltx_text" id="S5.T7.14.14.14.5.1" style="background-color:#DFDFDF;">2e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.14.14.14.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.14.14.14.1.m1.1a"><mi id="S5.T7.14.14.14.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.14.14.14.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.14.14.14.1.m1.1b"><ci id="S5.T7.14.14.14.1.m1.1.1.cmml" xref="S5.T7.14.14.14.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.14.14.14.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.14.14.14.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.14.14.14.6"><span class="ltx_text" id="S5.T7.14.14.14.6.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.7"><span class="ltx_text" id="S5.T7.14.14.14.7.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.8"><span class="ltx_text" id="S5.T7.14.14.14.8.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.14.14.14.9"><span class="ltx_text" id="S5.T7.14.14.14.9.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.10"><span class="ltx_text" id="S5.T7.14.14.14.10.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.14.14.14.11"><span class="ltx_text" id="S5.T7.14.14.14.11.1" style="background-color:#DFDFDF;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.18.18.18" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left" id="S5.T7.18.18.18.5"><span class="ltx_text" id="S5.T7.18.18.18.5.1" style="background-color:#FFFFFF;">Alpaca&nbsp;(13B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.18.18.18.6"><span class="ltx_text" id="S5.T7.18.18.18.6.1" style="background-color:#FFFFFF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.18.18.18.7"><span class="ltx_text" id="S5.T7.18.18.18.7.1" style="background-color:#FFFFFF;">512</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.18.18.18.8"><span class="ltx_text" id="S5.T7.18.18.18.8.1" style="background-color:#FFFFFF;">1e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.15.15.15.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.15.15.15.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.15.15.15.1.m1.1a"><mi id="S5.T7.15.15.15.1.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.15.15.15.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.15.15.15.1.m1.1b"><ci id="S5.T7.15.15.15.1.m1.1.1.cmml" xref="S5.T7.15.15.15.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.15.15.15.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.15.15.15.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.18.18.18.9"><span class="ltx_text" id="S5.T7.18.18.18.9.1" style="background-color:#FFFFFF;">cosine</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.18.18.18.10"><span class="ltx_text" id="S5.T7.18.18.18.10.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.18.18.18.11"><span class="ltx_text" id="S5.T7.18.18.18.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.16.16.16.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.16.16.16.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.16.16.16.2.m1.1a"><mi id="S5.T7.16.16.16.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.16.16.16.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.16.16.16.2.m1.1b"><ci id="S5.T7.16.16.16.2.m1.1.1.cmml" xref="S5.T7.16.16.16.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.16.16.16.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.16.16.16.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T7.17.17.17.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.17.17.17.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.17.17.17.3.m1.1a"><mi id="S5.T7.17.17.17.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.17.17.17.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.17.17.17.3.m1.1b"><ci id="S5.T7.17.17.17.3.m1.1.1.cmml" xref="S5.T7.17.17.17.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.17.17.17.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.17.17.17.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T7.18.18.18.4"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T7.18.18.18.4.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.18.18.18.4.m1.1a"><mo id="S5.T7.18.18.18.4.m1.1.1" mathbackground="#FFFFFF" xref="S5.T7.18.18.18.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T7.18.18.18.4.m1.1b"><times id="S5.T7.18.18.18.4.m1.1.1.cmml" xref="S5.T7.18.18.18.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.18.18.18.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T7.18.18.18.4.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.21.21.21" style="background-color:#DFDFDF;">
<td class="ltx_td ltx_align_left" id="S5.T7.21.21.21.4"><span class="ltx_text" id="S5.T7.21.21.21.4.1" style="background-color:#DFDFDF;">Vicuna&nbsp;(13B)</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.21.21.21.5"><span class="ltx_text" id="S5.T7.21.21.21.5.1" style="background-color:#DFDFDF;">128</span></td>
<td class="ltx_td ltx_align_right" id="S5.T7.21.21.21.6"><span class="ltx_text" id="S5.T7.21.21.21.6.1" style="background-color:#DFDFDF;">-2048</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.21.21.21.7"><span class="ltx_text" id="S5.T7.21.21.21.7.1" style="background-color:#DFDFDF;">2e-5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.19.19.19.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.19.19.19.1.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.19.19.19.1.m1.1a"><mi id="S5.T7.19.19.19.1.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.19.19.19.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.19.19.19.1.m1.1b"><ci id="S5.T7.19.19.19.1.m1.1.1.cmml" xref="S5.T7.19.19.19.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.19.19.19.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.19.19.19.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.21.21.21.8"><span class="ltx_text" id="S5.T7.21.21.21.8.1" style="background-color:#DFDFDF;">cosine</span></td>
<td class="ltx_td" id="S5.T7.21.21.21.9"></td>
<td class="ltx_td" id="S5.T7.21.21.21.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.20.20.20.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.20.20.20.2.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.20.20.20.2.m1.1a"><mi id="S5.T7.20.20.20.2.m1.1.1" mathbackground="#DFDFDF" mathvariant="normal" xref="S5.T7.20.20.20.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.20.20.20.2.m1.1b"><ci id="S5.T7.20.20.20.2.m1.1.1.cmml" xref="S5.T7.20.20.20.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.20.20.20.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.20.20.20.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T7.21.21.21.11"><span class="ltx_text" id="S5.T7.21.21.21.11.1" style="background-color:#DFDFDF;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.21.21.21.3"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T7.21.21.21.3.m1.1" style="background-color:#DFDFDF;"><semantics id="S5.T7.21.21.21.3.m1.1a"><mo id="S5.T7.21.21.21.3.m1.1.1" mathbackground="#DFDFDF" xref="S5.T7.21.21.21.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T7.21.21.21.3.m1.1b"><times id="S5.T7.21.21.21.3.m1.1.1.cmml" xref="S5.T7.21.21.21.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.21.21.21.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T7.21.21.21.3.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T7.24.24.24" style="background-color:#FFFFFF;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T7.24.24.24.4"><span class="ltx_text" id="S5.T7.24.24.24.4.1" style="background-color:#FFFFFF;">LIMA&nbsp;(65B)</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T7.24.24.24.5"><span class="ltx_text" id="S5.T7.24.24.24.5.1" style="background-color:#FFFFFF;">32</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T7.24.24.24.6"><span class="ltx_text" id="S5.T7.24.24.24.6.1" style="background-color:#FFFFFF;">2048</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.24.24.24.7"><span class="ltx_text" id="S5.T7.24.24.24.7.1" style="background-color:#FFFFFF;">1e-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.22.22.22.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T7.22.22.22.1.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.22.22.22.1.m1.1a"><mo id="S5.T7.22.22.22.1.m1.1.1" mathbackground="#FFFFFF" xref="S5.T7.22.22.22.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T7.22.22.22.1.m1.1b"><times id="S5.T7.22.22.22.1.m1.1.1.cmml" xref="S5.T7.22.22.22.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.22.22.22.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T7.22.22.22.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T7.24.24.24.8"><span class="ltx_text" id="S5.T7.24.24.24.8.1" style="background-color:#FFFFFF;">linear</span></td>
<td class="ltx_td ltx_border_bb" id="S5.T7.24.24.24.9"></td>
<td class="ltx_td ltx_border_bb" id="S5.T7.24.24.24.10"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T7.23.23.23.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.23.23.23.2.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.23.23.23.2.m1.1a"><mi id="S5.T7.23.23.23.2.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.23.23.23.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.23.23.23.2.m1.1b"><ci id="S5.T7.23.23.23.2.m1.1.1.cmml" xref="S5.T7.23.23.23.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.23.23.23.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.23.23.23.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.24.24.24.11"><span class="ltx_text" id="S5.T7.24.24.24.11.1" style="background-color:#FFFFFF;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.24.24.24.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T7.24.24.24.3.m1.1" style="background-color:#FFFFFF;"><semantics id="S5.T7.24.24.24.3.m1.1a"><mi id="S5.T7.24.24.24.3.m1.1.1" mathbackground="#FFFFFF" mathvariant="normal" xref="S5.T7.24.24.24.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T7.24.24.24.3.m1.1b"><ci id="S5.T7.24.24.24.3.m1.1.1.cmml" xref="S5.T7.24.24.24.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.24.24.24.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T7.24.24.24.3.m1.1d">✓</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Datasets and Evaluation</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Generating training and evaluation datasets is expensive because of the large-scale data demand of LLMs. Hence, datasets for training and benchmarking these models are topics of key importance. In Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.F15" title="Figure 15 ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">15</span></a>, we show the distribution of the existing datasets for various NLP tasks. We restrict our distribution to only the most important tasks in the literature by including tasks with at least 20 datasets. LLMs can directly benefit from these datasets for training and evaluation. A summary of the training and evaluation datasets commonly used by LLMs is provided next.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Training Datasets</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The performance of LLMs largely depends on the training data’s quality, size, and diversity. Preparing training datasets of high quality at a large scale is laborious. Researchers have suggested various pre-training and fine-tuning datasets to enhance LLMs capabilities. We summarize these efforts in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.T8" title="TABLE VIII ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VIII</span></a>. While numerous training datasets are available in the literature, we cover the most widely used ones in our summary.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Evaluation Datasets and Tasks</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The evaluation of LLMs is important in gauging their proficiency and limitations. This process measures the model’s ability to comprehend, generate, and interact with human language across a spectrum of tasks. Evaluating a language model (LM) is divided into two broader categories: 1) natural language understanding (NLU) and 2) natural language generation (NLG). It is emphasized that tasks in NLU and NLG are softly categorized and are often used interchangeably in the literature. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">Natural Language Understanding:</span> This task measures the language understanding capacity of LMs. It encompasses multiple tasks, including sentiment analysis, text classification, natural language inference (NLI), question answering (QA), commonsense reasoning (CR), mathematical reasoning (MR), reading comprehension (RC), etc. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">Natural Language Generation:</span> This task assesses the language generation capabilities of LLMs by understanding the provided input context. It includes tasks such as summarization, sentence completion, machine translation (MT), dialogue generation, etc. 
<br class="ltx_break">Numerous datasets are proposed for each task, evaluating LLMs against different characteristics. To provide an overview of evaluation datasets, we briefly discuss a few famous datasets within each category and offer a comprehensive list of datasets in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.T9" title="TABLE IX ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">IX</span></a>. Moreover, we show a detailed overview of the training datasets and evaluation tasks and benchmarks used by various pre-trained LLMs in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.T10" title="TABLE X ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">X</span></a> and fine-tuned LLMs in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.T11" title="TABLE XI ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">XI</span></a>. We also compare the top-performing LLMs in various NLP tasks in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S6.T12" title="TABLE XII ‣ StereoSet [410] ‣ VI-B12 Biases and Ethics in AI ‣ VI-B Evaluation Datasets and Tasks ‣ VI Datasets and Evaluation ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">XII</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S6.T8.1" style="width:433.6pt;height:218.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-294.9pt,148.8pt) scale(0.423733502907929,0.423733502907929) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T8.1.1.1.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.1"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1" style="background-color:#BFBFBF;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.2"><span class="ltx_text" id="S6.T8.1.1.1.1.2.1" style="background-color:#BFBFBF;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.3"><span class="ltx_text" id="S6.T8.1.1.1.1.3.1" style="background-color:#BFBFBF;">Size/Samples</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.4"><span class="ltx_text" id="S6.T8.1.1.1.1.4.1" style="background-color:#BFBFBF;">Tasks</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.5"><span class="ltx_text" id="S6.T8.1.1.1.1.5.1" style="background-color:#BFBFBF;">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1.6"><span class="ltx_text" id="S6.T8.1.1.1.1.6.1" style="background-color:#BFBFBF;">Creation</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t" id="S6.T8.1.1.1.1.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.1.1.7.1" style="background-color:#BFBFBF;">Comments</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T8.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.1">C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.3">806GB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.5">Common Crawl</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T8.1.1.2.1.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S6.T8.1.1.2.1.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.2.1.7.1">A clean, multilingual dataset with billions of tokens</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.1">mC4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.3">38.49TB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.5">Common Crawl</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.3.2.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.3.2.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.3.2.7.1">A multilingual extension of the C4 dataset, mC4 identifies over 100 languages using cld3 from 71 monthly web scrapes of Common Crawl.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.1">PILE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib301" title="">301</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.3">825GB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.4.3.5.1">
<tbody><tr class="ltx_tr" id="S6.T8.1.1.4.3.5.1.1">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.4.3.5.1.1.1">Common Crawl, PubMed Central,</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.4.3.5.1.2">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.4.3.5.1.2.1">OpenWebText2, ArXiv, GitHub,</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.4.3.5.1.3">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.4.3.5.1.3.1">Books3, and others</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.4.3.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.4.3.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.4.3.7.1">A massive dataset comprised of 22 constituent sub-datasets</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.1">ROOTs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib302" title="">302</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.3">1.61TB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.5">498 Hugging Face datasets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.5.4.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.5.4.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.5.4.7.1">46 natural and 13 programming languages</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.1">MassiveText&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib113" title="">113</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.3">10.5TB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.6.5.5.1">
<tbody><tr class="ltx_tr" id="S6.T8.1.1.6.5.5.1.1">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.6.5.5.1.1.1">MassiveWeb, Books, News,</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.6.5.5.1.2">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.6.5.5.1.2.1">Wikipedia, Github, C4</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.6.5.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.6.5.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.6.5.7.1">99% of the data is in English</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.1">Wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib303" title="">303</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.5">Wikipedia</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.7.6.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.7.6.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.7.6.7.1">Dump of wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.1">RedPajama&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib304" title="">304</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.3">5TB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.8.7.5.1">
<tbody><tr class="ltx_tr" id="S6.T8.1.1.8.7.5.1.1">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.8.7.5.1.1.1">CommonCrawl, C4, Wikipedia,</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.8.7.5.1.2">
<td class="ltx_td ltx_align_center" id="S6.T8.1.1.8.7.5.1.2.1">Github, Books, StackExchange</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.8.7.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.8.7.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.8.7.7.1">Open-source replica of LLaMA dataset</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.1">PushShift.io Reddit</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.3">21.1GB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.5">Reddit</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.9.8.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.9.8.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.9.8.7.1">Submissions and comments on Reddit from 2005 to 2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.1">BigPython&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib130" title="">130</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.2">Pretrain</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.3">5.5TB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.4">Coding</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.5">GitHub</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.10.9.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.10.9.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.10.9.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.1">Pool of Prompt (P3)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.3">12M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.4">62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.5">PromptSource</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.11.10.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.11.10.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.11.10.7.1">A Subset of PromptSource, created from 177 datasets including summarization, QA, classification, etc.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.1">xP3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib145" title="">145</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.3">81M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.4">71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.5">P3+Multilingual datasets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.12.11.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.12.11.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.12.11.7.1">Extending P3 to total 46 languages</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.1">Super-NaturalInstructions (SNI)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.3">12.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.4">1616</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.5">Multiple datasets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.13.12.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.13.12.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.13.12.7.1">Extending P3 with additional multi-lingual datasets, total 46 languages</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.1">Flan&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.3">15M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.4">1836</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.5">Muffin+T0-SF+NIV2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.14.13.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.14.13.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.14.13.7.1">Total 60 languages</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.15.14">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.1">OPT-IML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.3">18.1M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.4">1667</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.15.14.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.15.14.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.15.14.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.1">Self-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.3">82k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.4">175</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.16.15.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.16.15.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.16.15.7.1">Generated 52k instructions with 82k samples from 175 seed tasks using GPT-3</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.1">Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib149" title="">149</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.3">52k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.17.16.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.17.16.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.17.16.7.1">Employed self-instruct method to generate data from text-davinci-003</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.18.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.1">Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib150" title="">150</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.3">125k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.5">ShareGPT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.18.17.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.18.17.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.18.17.7.1">Conversations shared by users on ShareGPT using public APIs</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.1">LLaMA-GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib151" title="">151</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.3">52k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.5">Alpaca</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.19.18.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.19.18.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.19.18.7.1">Recreated Alpaca dataset with GPT-4 in English and Chinese</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.20.19">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.1">Unnatural Instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib305" title="">305</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.3">68k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.5">15-Seeds (SNI)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.20.19.6">Automated</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.20.19.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.20.19.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.21.20">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.1">LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib176" title="">176</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.2">Instructions</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.3">1k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.5">Multiple datasets</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.21.20.6">Manual</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T8.1.1.21.20.7" style="width:165.0pt;">
<p class="ltx_p ltx_align_top" id="S6.T8.1.1.21.20.7.1">Carefully created samples to test performance with fine-tuning on less data</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.22.21">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.1">Anthropic-HH-RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib306" title="">306</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.2">Alignment</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.3">142k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T8.1.1.22.21.6">Manual</td>
<td class="ltx_td ltx_border_t" id="S6.T8.1.1.22.21.7" style="width:165.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.1.23.22">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.1">Anthropic-HH-RLHF-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib169" title="">169</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.2">Alignment</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.3">39k</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.1.23.22.6">Manual</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S6.T8.1.1.23.22.7" style="width:165.0pt;"></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S6.F15.g1" src="./A Comprehensive Overview of Large Language Models_files/Datasets_for_Tasks.png" width="538">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>A distribution of datasets proposed for different NLP tasks. We include only the tasks for which at least 20 datasets have already been proposed.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>Categorized evaluation datasets used in evaluating LLMs.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T9.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T9.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T9.1.1.1.1.1">Type</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.1.1.2" style="width:369.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S6.T9.1.1.1.2.1">Datasets/Benchmarks</span></td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.2.2.1">Multi-Task</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.2.2.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.2.2.2.1">MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib307" title="">307</a>]</cite>, SuperGLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib2" title="">2</a>]</cite>, BIG-bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib308" title="">308</a>]</cite>, GLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib309" title="">309</a>]</cite>, BBH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib308" title="">308</a>]</cite>, CUGE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib310" title="">310</a>]</cite>, ZeroCLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, FewCLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, Blended Skill Talk&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib313" title="">313</a>]</cite>, HELM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib314" title="">314</a>]</cite>, KLUE-STS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib315" title="">315</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.3.3.1">Language Understanding</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.3.3.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.3.3.2.1">CoQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib316" title="">316</a>]</cite>, WiC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib317" title="">317</a>]</cite>, Wikitext103&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib318" title="">318</a>]</cite>, PG19&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib319" title="">319</a>]</cite>, LCQMC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib320" title="">320</a>]</cite>, QQP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib321" title="">321</a>]</cite>, WinoGender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib322" title="">322</a>]</cite>, CB&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib323" title="">323</a>]</cite>, FinRE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib324" title="">324</a>]</cite>, SanWen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib325" title="">325</a>]</cite>, AFQMC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, BQ Corpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib326" title="">326</a>]</cite>, CNSS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib327" title="">327</a>]</cite>, CKBQA 13&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib328" title="">328</a>]</cite>, CLUENER&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, Weibo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib329" title="">329</a>]</cite>, AQuA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib330" title="">330</a>]</cite>, OntoNotes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib331" title="">331</a>]</cite>, HeadQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib332" title="">332</a>]</cite>, Twitter Dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib333" title="">333</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.4.4.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T9.1.4.4.1.1">
<tbody><tr class="ltx_tr" id="S6.T9.1.4.4.1.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.4.4.1.1.1.1">Story Cloze and</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.4.4.1.1.2">
<td class="ltx_td ltx_align_left" id="S6.T9.1.4.4.1.1.2.1">Sentence Completion</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.4.4.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.4.4.2.1">StoryCloze&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib334" title="">334</a>]</cite>, LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib335" title="">335</a>]</cite>, LCSTS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib336" title="">336</a>]</cite>, AdGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib337" title="">337</a>]</cite>, E2E&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib338" title="">338</a>]</cite>, CHID&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib339" title="">339</a>]</cite>, CHID-FC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.5.5.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T9.1.5.5.1.1">
<tbody><tr class="ltx_tr" id="S6.T9.1.5.5.1.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.5.5.1.1.1.1">Physical Knowledge and</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.5.5.1.1.2">
<td class="ltx_td ltx_align_left" id="S6.T9.1.5.5.1.1.2.1">World Understanding</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.5.5.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.5.5.2.1">PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib340" title="">340</a>]</cite>, TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib341" title="">341</a>]</cite>, ARC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>, ARC-Easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>, ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>, PROST&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib343" title="">343</a>]</cite>, OpenBookQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib344" title="">344</a>]</cite>, WebNLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib345" title="">345</a>]</cite>, DogWhistle Insider &amp; Outsider&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib346" title="">346</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.6.6.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T9.1.6.6.1.1">
<tbody><tr class="ltx_tr" id="S6.T9.1.6.6.1.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.6.6.1.1.1.1">Contextual Language</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.6.6.1.1.2">
<td class="ltx_td ltx_align_left" id="S6.T9.1.6.6.1.1.2.1">Understanding</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.6.6.2" style="width:369.9pt;">
<table class="ltx_tabular ltx_align_top" id="S6.T9.1.6.6.2.1">
<tbody><tr class="ltx_tr" id="S6.T9.1.6.6.2.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.6.6.2.1.1.1">RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>, RACE-Middle&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>, RACE-High&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>, QuAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib348" title="">348</a>]</cite>, StrategyQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib349" title="">349</a>]</cite>, Quiz Bowl&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib350" title="">350</a>]</cite>,</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.6.6.2.1.2">
<td class="ltx_td ltx_align_left" id="S6.T9.1.6.6.2.1.2.1">cMedQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib351" title="">351</a>]</cite>, cMedQA2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib352" title="">352</a>]</cite>, MATINF-QA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib353" title="">353</a>]</cite>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.7.7.1">Commonsense Reasoning</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.7.7.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.7.7.2.1">WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib354" title="">354</a>]</cite>, HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib355" title="">355</a>]</cite>, COPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib356" title="">356</a>]</cite>, WSC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib357" title="">357</a>]</cite>, CSQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib358" title="">358</a>]</cite>, SIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib359" title="">359</a>]</cite>, C<sup class="ltx_sup" id="S6.T9.1.7.7.2.1.1">3</sup>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib360" title="">360</a>]</cite>, CLUEWSC2020&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, CLUEWSC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, CLUEWSC-FC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, ReCoRD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib361" title="">361</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.8.8.1">Reading Comprehension</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.8.8.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.8.8.2.1">SQuAD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib362" title="">362</a>]</cite>, BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib363" title="">363</a>]</cite>, SQUADv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib364" title="">364</a>]</cite>, DROP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib365" title="">365</a>]</cite>, RTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib366" title="">366</a>]</cite>, WebQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib367" title="">367</a>]</cite>, CMRC2017&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib368" title="">368</a>]</cite>, CMRC2018&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib369" title="">369</a>]</cite>, CMRC2019&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib370" title="">370</a>]</cite>, COTE-BD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib371" title="">371</a>]</cite>, COTE-DP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib371" title="">371</a>]</cite>, COTE-MFW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib371" title="">371</a>]</cite>, MultiRC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib372" title="">372</a>]</cite>, Natural Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib373" title="">373</a>]</cite>, CNSE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib327" title="">327</a>]</cite>, DRCD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib374" title="">374</a>]</cite>, DuReader&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib375" title="">375</a>]</cite>, Dureader<sub class="ltx_sub" id="S6.T9.1.8.8.2.1.1">robust</sub>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib376" title="">376</a>]</cite>, DuReader-QG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib375" title="">375</a>]</cite>, SciQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib377" title="">377</a>]</cite>, Sogou-log&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib378" title="">378</a>]</cite>, Dureader<sub class="ltx_sub" id="S6.T9.1.8.8.2.1.2">robust</sub>-QG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib376" title="">376</a>]</cite>, QA4MRE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib379" title="">379</a>]</cite>, KorQuAD 1.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib380" title="">380</a>]</cite>, CAIL2018-Task1 &amp; Task2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib381" title="">381</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.9.9.1">Mathematical Reasoning</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.9.9.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.9.9.2.1">MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib382" title="">382</a>]</cite>, Math23k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib383" title="">383</a>]</cite>, GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib384" title="">384</a>]</cite>, MathQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib385" title="">385</a>]</cite>, MGSM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib386" title="">386</a>]</cite>, MultiArith&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib387" title="">387</a>]</cite>, ASDiv&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib388" title="">388</a>]</cite>, MAWPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib389" title="">389</a>]</cite>, SVAMP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib390" title="">390</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.10.10.1">Problem Solving</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.10.10.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.10.10.2.1">HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib391" title="">391</a>]</cite>, DS-1000&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib392" title="">392</a>]</cite>, MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib393" title="">393</a>]</cite>, APPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib382" title="">382</a>]</cite>, CodeContests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib132" title="">132</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.11.11.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T9.1.11.11.1.1">
<tbody><tr class="ltx_tr" id="S6.T9.1.11.11.1.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.11.11.1.1.1.1">Natural Language Inference</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.11.11.1.1.2">
<td class="ltx_td ltx_align_left" id="S6.T9.1.11.11.1.1.2.1">&amp; Logical Reasoning</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.11.11.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.11.11.2.1">ANLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib394" title="">394</a>]</cite>, MNLI-m&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib395" title="">395</a>]</cite>, MNLI-mm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib395" title="">395</a>]</cite>,QNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib362" title="">362</a>]</cite>, WNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib357" title="">357</a>]</cite>, OCNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, CMNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, ANLI R1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib394" title="">394</a>]</cite>, ANLI R2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib394" title="">394</a>]</cite>, ANLI R3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib394" title="">394</a>]</cite>, HANS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib396" title="">396</a>]</cite>, OCNLI-FC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, LogiQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib397" title="">397</a>]</cite>, StrategyQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib349" title="">349</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.12.12.1">Cross-Lingual Understanding</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.12.12.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.12.12.2.1">MLQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib398" title="">398</a>]</cite>, XNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib399" title="">399</a>]</cite>, PAWS-X&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib400" title="">400</a>]</cite>, XSum&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib401" title="">401</a>]</cite>, XCOPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib402" title="">402</a>]</cite>, XWinograd&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib403" title="">403</a>]</cite>, TyDiQA-GoldP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib404" title="">404</a>]</cite>, MLSum&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib405" title="">405</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.13.13.1">Truthfulness and Fact Checking</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.13.13.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.13.13.2.1">TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib406" title="">406</a>]</cite>, MultiFC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib407" title="">407</a>]</cite>, Fact Checking on Fever&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib408" title="">408</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.14.14.1">Biases and Ethics in AI</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.14.14.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.14.14.2.1">ETHOS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib409" title="">409</a>]</cite>, StereoSet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib410" title="">410</a>]</cite>, BBQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib411" title="">411</a>]</cite>, Winobias&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib412" title="">412</a>]</cite>, CrowS-Pairs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib413" title="">413</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.15.15.1">Toxicity</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.15.15.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.15.15.2.1">RealToxicityPrompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib414" title="">414</a>]</cite>, CivilComments toxicity classification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib415" title="">415</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.16.16.1">Language Translation</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.16.16.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.16.16.2.1">WMT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib416" title="">416</a>]</cite>, WMT20&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib417" title="">417</a>]</cite>, WMT20-enzh&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib417" title="">417</a>]</cite>, EPRSTMT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, CCPM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib418" title="">418</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.17.17.1">Scientific Knowledge</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.17.17.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.17.17.2.1">AminoProbe&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite>, BioLAMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite>, Chemical Reactions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite>, Galaxy Clusters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite>, Mineral Groups&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T9.1.18.18.1">Dialogue</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.1.18.18.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.18.18.2.1">Wizard of Wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib419" title="">419</a>]</cite>, Empathetic Dialogues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib420" title="">420</a>]</cite>, DPC-generated&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib121" title="">121</a>]</cite> dialogues, ConvAI2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib421" title="">421</a>]</cite>, KdConv&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib422" title="">422</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T9.1.19.19.1">Topic Classification</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="S6.T9.1.19.19.2" style="width:369.9pt;">
<p class="ltx_p ltx_align_top" id="S6.T9.1.19.19.2.1">TNEWS-FC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, YNAT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib315" title="">315</a>]</cite>, KLUE-TC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib315" title="">315</a>]</cite>, CSL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib311" title="">311</a>]</cite>, CSL-FC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib312" title="">312</a>]</cite>, IFLYTEK&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib423" title="">423</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE X: </span>An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.176.1">“QA”</span> is question-answering, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.177.2">“Clf”</span> is classification, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.178.3">“NLI”</span> is natural language inference, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.179.4">“MT”</span> is machine translation, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.180.5">“RC”</span> is reading comprehension, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.181.6">“CR”</span> is commonsense reasoning, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.182.7">“MR”</span> is mathematical reasoning, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T10.183.8">“Mem.”</span> is memorization.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S6.T10.167" style="width:433.6pt;height:390.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-207.1pt,186.7pt) scale(0.511460624846972,0.511460624846972) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T10.167.167">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T10.167.167.168.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.2" style="width:142.3pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S6.T10.167.167.168.1.3"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.168.1.3.1" style="background-color:#BFBFBF;">Benchmark<span class="ltx_text ltx_font_medium" id="S6.T10.167.167.168.1.3.1.1" style="background-color:#BFBFBF;"></span></span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.4"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.5"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.6"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.7"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.8"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.9"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.10"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.11"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T10.167.167.168.1.12"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S6.T10.167.167.168.1.13"></th>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T10.167.167.169.2.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.2" style="width:142.3pt;">
<table class="ltx_tabular ltx_align_top" id="S6.T10.167.167.169.2.2.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T10.167.167.169.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.2.1.1.1.1" style="background-color:#BFBFBF;">Training Dataset</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T10.167.167.169.2.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T10.167.167.169.2.3.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T10.167.167.169.2.3.1.1">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.3.1.1.1">BIG-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.3.1.2">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.3.1.2.1">bench</td>
</tr>
</tbody></table>
<span class="ltx_text" id="S6.T10.167.167.169.2.3.2" style="background-color:#BFBFBF;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T10.167.167.169.2.4"><span class="ltx_text" id="S6.T10.167.167.169.2.4.1" style="background-color:#BFBFBF;">MMLU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T10.167.167.169.2.5.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T10.167.167.169.2.5.1.1">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.5.1.1.1">Super</td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.5.1.2">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.5.1.2.1">GLUE</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.6"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.6.1" style="background-color:#BFBFBF;">QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.7"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.7.1" style="background-color:#BFBFBF;">Clf</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.8"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.8.1" style="background-color:#BFBFBF;">NLI</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.9"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.9.1" style="background-color:#BFBFBF;">MT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.10">
<table class="ltx_tabular ltx_align_middle" id="S6.T10.167.167.169.2.10.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T10.167.167.169.2.10.1.1">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.10.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.10.1.1.1.1" style="background-color:#BFBFBF;">Cloze/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.10.1.2">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.10.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.10.1.2.1.1">Completion</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.11"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.11.1" style="background-color:#BFBFBF;">RC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.12"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.12.1" style="background-color:#BFBFBF;">CR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.13"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.13.1" style="background-color:#BFBFBF;">MR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S6.T10.167.167.169.2.14"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.14.1" style="background-color:#BFBFBF;">Coding</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T10.167.167.169.2.15">
<table class="ltx_tabular ltx_align_middle" id="S6.T10.167.167.169.2.15.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T10.167.167.169.2.15.1.1">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.15.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.15.1.1.1.1" style="background-color:#BFBFBF;">Truthful/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.15.1.2">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.15.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.15.1.2.1.1">Bias/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.15.1.3">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.15.1.3.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.15.1.3.1.1">Toxicity/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.169.2.15.1.4">
<td class="ltx_td ltx_align_center" id="S6.T10.167.167.169.2.15.1.4.1"><span class="ltx_text ltx_font_bold" id="S6.T10.167.167.169.2.15.1.4.1.1">Mem.</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T10.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.9">T5</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.10" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.8.8.8.10.1">C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.11"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.1.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.1.1.1.1.m1.1"><semantics id="S6.T10.1.1.1.1.m1.1a"><mi id="S6.T10.1.1.1.1.m1.1.1" mathvariant="normal" xref="S6.T10.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.1.1.1.1.m1.1b"><ci id="S6.T10.1.1.1.1.m1.1.1.cmml" xref="S6.T10.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.1.1.1.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.2.2.2.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.2.2.2.2.m1.1"><semantics id="S6.T10.2.2.2.2.m1.1a"><mi id="S6.T10.2.2.2.2.m1.1.1" mathvariant="normal" xref="S6.T10.2.2.2.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.2.2.2.2.m1.1b"><ci id="S6.T10.2.2.2.2.m1.1.1.cmml" xref="S6.T10.2.2.2.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.2.2.2.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.2.2.2.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.3.3.3.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.3.3.3.3.m1.1"><semantics id="S6.T10.3.3.3.3.m1.1a"><mi id="S6.T10.3.3.3.3.m1.1.1" mathvariant="normal" xref="S6.T10.3.3.3.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.3.3.3.3.m1.1b"><ci id="S6.T10.3.3.3.3.m1.1.1.cmml" xref="S6.T10.3.3.3.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.3.3.3.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.3.3.3.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.4.4.4.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.4.4.4.4.m1.1"><semantics id="S6.T10.4.4.4.4.m1.1a"><mi id="S6.T10.4.4.4.4.m1.1.1" mathvariant="normal" xref="S6.T10.4.4.4.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.4.4.4.4.m1.1b"><ci id="S6.T10.4.4.4.4.m1.1.1.cmml" xref="S6.T10.4.4.4.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.4.4.4.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.4.4.4.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.5.5.5.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.5.5.5.5.m1.1"><semantics id="S6.T10.5.5.5.5.m1.1a"><mi id="S6.T10.5.5.5.5.m1.1.1" mathvariant="normal" xref="S6.T10.5.5.5.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.5.5.5.5.m1.1b"><ci id="S6.T10.5.5.5.5.m1.1.1.cmml" xref="S6.T10.5.5.5.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.5.5.5.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.5.5.5.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.6.6.6.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.6.6.6.6.m1.1"><semantics id="S6.T10.6.6.6.6.m1.1a"><mi id="S6.T10.6.6.6.6.m1.1.1" mathvariant="normal" xref="S6.T10.6.6.6.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.6.6.6.6.m1.1b"><ci id="S6.T10.6.6.6.6.m1.1.1.cmml" xref="S6.T10.6.6.6.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.6.6.6.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.6.6.6.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.7.7.7.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.7.7.7.7.m1.1"><semantics id="S6.T10.7.7.7.7.m1.1a"><mi id="S6.T10.7.7.7.7.m1.1.1" mathvariant="normal" xref="S6.T10.7.7.7.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.7.7.7.7.m1.1b"><ci id="S6.T10.7.7.7.7.m1.1.1.cmml" xref="S6.T10.7.7.7.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.7.7.7.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.7.7.7.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.8"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.8.8.8.8.m1.1"><semantics id="S6.T10.8.8.8.8.m1.1a"><mi id="S6.T10.8.8.8.8.m1.1.1" mathvariant="normal" xref="S6.T10.8.8.8.8.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.8.8.8.8.m1.1b"><ci id="S6.T10.8.8.8.8.m1.1.1.cmml" xref="S6.T10.8.8.8.8.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.8.8.8.8.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.8.8.8.8.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T10.8.8.8.14"></td>
<td class="ltx_td ltx_border_tt" id="S6.T10.8.8.8.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.14.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.14.14.14.7">GPT-3</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.14.14.14.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.14.14.14.8.1">Common Crawl, WebText, Books Corpora, Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.9.9.9.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.9.9.9.1.m1.1"><semantics id="S6.T10.9.9.9.1.m1.1a"><mi id="S6.T10.9.9.9.1.m1.1.1" mathvariant="normal" xref="S6.T10.9.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.9.9.9.1.m1.1b"><ci id="S6.T10.9.9.9.1.m1.1.1.cmml" xref="S6.T10.9.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.9.9.9.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.9.9.9.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.10.10.10.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.10.10.10.2.m1.1"><semantics id="S6.T10.10.10.10.2.m1.1a"><mi id="S6.T10.10.10.10.2.m1.1.1" mathvariant="normal" xref="S6.T10.10.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.10.10.10.2.m1.1b"><ci id="S6.T10.10.10.10.2.m1.1.1.cmml" xref="S6.T10.10.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.10.10.10.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.10.10.10.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.11.11.11.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.11.11.11.3.m1.1"><semantics id="S6.T10.11.11.11.3.m1.1a"><mi id="S6.T10.11.11.11.3.m1.1.1" mathvariant="normal" xref="S6.T10.11.11.11.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.11.11.11.3.m1.1b"><ci id="S6.T10.11.11.11.3.m1.1.1.cmml" xref="S6.T10.11.11.11.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.11.11.11.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.11.11.11.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.12.12.12.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.12.12.12.4.m1.1"><semantics id="S6.T10.12.12.12.4.m1.1a"><mi id="S6.T10.12.12.12.4.m1.1.1" mathvariant="normal" xref="S6.T10.12.12.12.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.12.12.12.4.m1.1b"><ci id="S6.T10.12.12.12.4.m1.1.1.cmml" xref="S6.T10.12.12.12.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.12.12.12.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.12.12.12.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.13.13.13.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.13.13.13.5.m1.1"><semantics id="S6.T10.13.13.13.5.m1.1a"><mi id="S6.T10.13.13.13.5.m1.1.1" mathvariant="normal" xref="S6.T10.13.13.13.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.13.13.13.5.m1.1b"><ci id="S6.T10.13.13.13.5.m1.1.1.cmml" xref="S6.T10.13.13.13.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.13.13.13.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.13.13.13.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.14.14.14.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.14.14.14.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.14.14.14.6.m1.1"><semantics id="S6.T10.14.14.14.6.m1.1a"><mi id="S6.T10.14.14.14.6.m1.1.1" mathvariant="normal" xref="S6.T10.14.14.14.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.14.14.14.6.m1.1b"><ci id="S6.T10.14.14.14.6.m1.1.1.cmml" xref="S6.T10.14.14.14.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.14.14.14.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.14.14.14.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.17.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.17.17.17.4">mT5</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.17.17.17.5" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.17.17.17.5.1">mC4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.15.15.15.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.15.15.15.1.m1.1"><semantics id="S6.T10.15.15.15.1.m1.1a"><mi id="S6.T10.15.15.15.1.m1.1.1" mathvariant="normal" xref="S6.T10.15.15.15.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.15.15.15.1.m1.1b"><ci id="S6.T10.15.15.15.1.m1.1.1.cmml" xref="S6.T10.15.15.15.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.15.15.15.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.15.15.15.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.16.16.16.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.16.16.16.2.m1.1"><semantics id="S6.T10.16.16.16.2.m1.1a"><mi id="S6.T10.16.16.16.2.m1.1.1" mathvariant="normal" xref="S6.T10.16.16.16.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.16.16.16.2.m1.1b"><ci id="S6.T10.16.16.16.2.m1.1.1.cmml" xref="S6.T10.16.16.16.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.16.16.16.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.16.16.16.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.17.17.17.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.17.17.17.3.m1.1"><semantics id="S6.T10.17.17.17.3.m1.1a"><mi id="S6.T10.17.17.17.3.m1.1.1" mathvariant="normal" xref="S6.T10.17.17.17.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.17.17.17.3.m1.1b"><ci id="S6.T10.17.17.17.3.m1.1.1.cmml" xref="S6.T10.17.17.17.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.17.17.17.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.17.17.17.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.17.17.17.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.17.17.17.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.23.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.18.18.18.1">PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S6.T10.18.18.18.1.m1.1"><semantics id="S6.T10.18.18.18.1.m1.1a"><mi id="S6.T10.18.18.18.1.m1.1.1" xref="S6.T10.18.18.18.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S6.T10.18.18.18.1.m1.1b"><ci id="S6.T10.18.18.18.1.m1.1.1.cmml" xref="S6.T10.18.18.18.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.18.18.18.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S6.T10.18.18.18.1.m1.1d">italic_α</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.23.23.23.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.23.23.23.7.1">1.1TB Chinese Text Corpus</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.19.19.19.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.19.19.19.2.m1.1"><semantics id="S6.T10.19.19.19.2.m1.1a"><mi id="S6.T10.19.19.19.2.m1.1.1" mathvariant="normal" xref="S6.T10.19.19.19.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.19.19.19.2.m1.1b"><ci id="S6.T10.19.19.19.2.m1.1.1.cmml" xref="S6.T10.19.19.19.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.19.19.19.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.19.19.19.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.20.20.20.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.20.20.20.3.m1.1"><semantics id="S6.T10.20.20.20.3.m1.1a"><mi id="S6.T10.20.20.20.3.m1.1.1" mathvariant="normal" xref="S6.T10.20.20.20.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.20.20.20.3.m1.1b"><ci id="S6.T10.20.20.20.3.m1.1.1.cmml" xref="S6.T10.20.20.20.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.20.20.20.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.20.20.20.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.21.21.21.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.21.21.21.4.m1.1"><semantics id="S6.T10.21.21.21.4.m1.1a"><mi id="S6.T10.21.21.21.4.m1.1.1" mathvariant="normal" xref="S6.T10.21.21.21.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.21.21.21.4.m1.1b"><ci id="S6.T10.21.21.21.4.m1.1.1.cmml" xref="S6.T10.21.21.21.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.21.21.21.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.21.21.21.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.22.22.22.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.22.22.22.5.m1.1"><semantics id="S6.T10.22.22.22.5.m1.1a"><mi id="S6.T10.22.22.22.5.m1.1.1" mathvariant="normal" xref="S6.T10.22.22.22.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.22.22.22.5.m1.1b"><ci id="S6.T10.22.22.22.5.m1.1.1.cmml" xref="S6.T10.22.22.22.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.22.22.22.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.22.22.22.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.23.23.23.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.23.23.23.6.m1.1"><semantics id="S6.T10.23.23.23.6.m1.1a"><mi id="S6.T10.23.23.23.6.m1.1.1" mathvariant="normal" xref="S6.T10.23.23.23.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.23.23.23.6.m1.1b"><ci id="S6.T10.23.23.23.6.m1.1.1.cmml" xref="S6.T10.23.23.23.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.23.23.23.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.23.23.23.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.23.23.23.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.23.23.23.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.25.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.25.25.25.3">CPM-2</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.25.25.25.4" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.25.25.25.4.1">WuDaoCorpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib105" title="">105</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.24.24.24.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.24.24.24.1.m1.1"><semantics id="S6.T10.24.24.24.1.m1.1a"><mi id="S6.T10.24.24.24.1.m1.1.1" mathvariant="normal" xref="S6.T10.24.24.24.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.24.24.24.1.m1.1b"><ci id="S6.T10.24.24.24.1.m1.1.1.cmml" xref="S6.T10.24.24.24.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.24.24.24.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.24.24.24.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.25.25.25.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.25.25.25.2.m1.1"><semantics id="S6.T10.25.25.25.2.m1.1a"><mi id="S6.T10.25.25.25.2.m1.1.1" mathvariant="normal" xref="S6.T10.25.25.25.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.25.25.25.2.m1.1b"><ci id="S6.T10.25.25.25.2.m1.1.1.cmml" xref="S6.T10.25.25.25.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.25.25.25.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.25.25.25.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.25.25.25.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.25.25.25.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.26.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.26.26.26.2">Codex</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.26.26.26.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.26.26.26.3.1">54 million public repositories from Github</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.26.26.26.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.26.26.26.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.26.26.26.1.m1.1"><semantics id="S6.T10.26.26.26.1.m1.1a"><mi id="S6.T10.26.26.26.1.m1.1.1" mathvariant="normal" xref="S6.T10.26.26.26.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.26.26.26.1.m1.1b"><ci id="S6.T10.26.26.26.1.m1.1.1.cmml" xref="S6.T10.26.26.26.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.26.26.26.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.26.26.26.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.26.26.26.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.34.34.34">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.34.34.34.9">ERNIE-3.0</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.34.34.34.10" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.34.34.34.10.1">Chinese text corpora, Baidu Search, Web text, QA-long, QA-short, Poetry and Couplet Domain-specific data from medical, law, and financial area Baidu knowledge graph with more than 50 million facts</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.34.34.34.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.34.34.34.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.27.27.27.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.27.27.27.1.m1.1"><semantics id="S6.T10.27.27.27.1.m1.1a"><mi id="S6.T10.27.27.27.1.m1.1.1" mathvariant="normal" xref="S6.T10.27.27.27.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.27.27.27.1.m1.1b"><ci id="S6.T10.27.27.27.1.m1.1.1.cmml" xref="S6.T10.27.27.27.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.27.27.27.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.27.27.27.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.28.28.28.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.28.28.28.2.m1.1"><semantics id="S6.T10.28.28.28.2.m1.1a"><mi id="S6.T10.28.28.28.2.m1.1.1" mathvariant="normal" xref="S6.T10.28.28.28.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.28.28.28.2.m1.1b"><ci id="S6.T10.28.28.28.2.m1.1.1.cmml" xref="S6.T10.28.28.28.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.28.28.28.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.28.28.28.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.29.29.29.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.29.29.29.3.m1.1"><semantics id="S6.T10.29.29.29.3.m1.1a"><mi id="S6.T10.29.29.29.3.m1.1.1" mathvariant="normal" xref="S6.T10.29.29.29.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.29.29.29.3.m1.1b"><ci id="S6.T10.29.29.29.3.m1.1.1.cmml" xref="S6.T10.29.29.29.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.29.29.29.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.29.29.29.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.30.30.30.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.30.30.30.4.m1.1"><semantics id="S6.T10.30.30.30.4.m1.1a"><mi id="S6.T10.30.30.30.4.m1.1.1" mathvariant="normal" xref="S6.T10.30.30.30.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.30.30.30.4.m1.1b"><ci id="S6.T10.30.30.30.4.m1.1.1.cmml" xref="S6.T10.30.30.30.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.30.30.30.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.30.30.30.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.31.31.31.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.31.31.31.5.m1.1"><semantics id="S6.T10.31.31.31.5.m1.1a"><mi id="S6.T10.31.31.31.5.m1.1.1" mathvariant="normal" xref="S6.T10.31.31.31.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.31.31.31.5.m1.1b"><ci id="S6.T10.31.31.31.5.m1.1.1.cmml" xref="S6.T10.31.31.31.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.31.31.31.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.31.31.31.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.32.32.32.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.32.32.32.6.m1.1"><semantics id="S6.T10.32.32.32.6.m1.1a"><mi id="S6.T10.32.32.32.6.m1.1.1" mathvariant="normal" xref="S6.T10.32.32.32.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.32.32.32.6.m1.1b"><ci id="S6.T10.32.32.32.6.m1.1.1.cmml" xref="S6.T10.32.32.32.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.32.32.32.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.32.32.32.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.33.33.33.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.33.33.33.7.m1.1"><semantics id="S6.T10.33.33.33.7.m1.1a"><mi id="S6.T10.33.33.33.7.m1.1.1" mathvariant="normal" xref="S6.T10.33.33.33.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.33.33.33.7.m1.1b"><ci id="S6.T10.33.33.33.7.m1.1.1.cmml" xref="S6.T10.33.33.33.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.33.33.33.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.33.33.33.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.34.34.34.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.34.34.34.8"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.34.34.34.8.m1.1"><semantics id="S6.T10.34.34.34.8.m1.1a"><mi id="S6.T10.34.34.34.8.m1.1.1" mathvariant="normal" xref="S6.T10.34.34.34.8.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.34.34.34.8.m1.1b"><ci id="S6.T10.34.34.34.8.m1.1.1.cmml" xref="S6.T10.34.34.34.8.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.34.34.34.8.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.34.34.34.8.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.34.34.34.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.34.34.34.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.38.38.38">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.38.38.38.5">Jurassic-1</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.38.38.38.6" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.38.38.38.6.1">Wikipedia, OWT, Books, C4,
Pile&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib301" title="">301</a>]</cite>, arXiv, GitHub</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.35.35.35.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.35.35.35.1.m1.1"><semantics id="S6.T10.35.35.35.1.m1.1a"><mi id="S6.T10.35.35.35.1.m1.1.1" mathvariant="normal" xref="S6.T10.35.35.35.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.35.35.35.1.m1.1b"><ci id="S6.T10.35.35.35.1.m1.1.1.cmml" xref="S6.T10.35.35.35.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.35.35.35.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.35.35.35.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.36.36.36.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.36.36.36.2.m1.1"><semantics id="S6.T10.36.36.36.2.m1.1a"><mi id="S6.T10.36.36.36.2.m1.1.1" mathvariant="normal" xref="S6.T10.36.36.36.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.36.36.36.2.m1.1b"><ci id="S6.T10.36.36.36.2.m1.1.1.cmml" xref="S6.T10.36.36.36.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.36.36.36.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.36.36.36.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.37.37.37.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.37.37.37.3.m1.1"><semantics id="S6.T10.37.37.37.3.m1.1a"><mi id="S6.T10.37.37.37.3.m1.1.1" mathvariant="normal" xref="S6.T10.37.37.37.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.37.37.37.3.m1.1b"><ci id="S6.T10.37.37.37.3.m1.1.1.cmml" xref="S6.T10.37.37.37.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.37.37.37.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.37.37.37.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.38.38.38.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.38.38.38.4.m1.1"><semantics id="S6.T10.38.38.38.4.m1.1a"><mi id="S6.T10.38.38.38.4.m1.1.1" mathvariant="normal" xref="S6.T10.38.38.38.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.38.38.38.4.m1.1b"><ci id="S6.T10.38.38.38.4.m1.1.1.cmml" xref="S6.T10.38.38.38.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.38.38.38.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.38.38.38.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.38.38.38.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.38.38.38.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.39.39.39">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.39.39.39.2">HyperCLOVA</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.39.39.39.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.39.39.39.3.1">Korean blogs, Community sites, News, KiN Korean Wikipedia, Wikipedia (English and Japanese), Modu-Corpus: Messenger, News, Spoken and written language corpus, Web corpus</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.39.39.39.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.39.39.39.1.m1.1"><semantics id="S6.T10.39.39.39.1.m1.1a"><mi id="S6.T10.39.39.39.1.m1.1.1" mathvariant="normal" xref="S6.T10.39.39.39.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.39.39.39.1.m1.1b"><ci id="S6.T10.39.39.39.1.m1.1.1.cmml" xref="S6.T10.39.39.39.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.39.39.39.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.39.39.39.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.39.39.39.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.39.39.39.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.43.43.43">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.43.43.43.5">Yuan 1.0</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.43.43.43.6" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.43.43.43.6.1">Common Crawl, SogouT, Sogou News,
Baidu Baike, Wikipedia, Books</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.40.40.40.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.40.40.40.1.m1.1"><semantics id="S6.T10.40.40.40.1.m1.1a"><mi id="S6.T10.40.40.40.1.m1.1.1" mathvariant="normal" xref="S6.T10.40.40.40.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.40.40.40.1.m1.1b"><ci id="S6.T10.40.40.40.1.m1.1.1.cmml" xref="S6.T10.40.40.40.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.40.40.40.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.40.40.40.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.41.41.41.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.41.41.41.2.m1.1"><semantics id="S6.T10.41.41.41.2.m1.1a"><mi id="S6.T10.41.41.41.2.m1.1.1" mathvariant="normal" xref="S6.T10.41.41.41.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.41.41.41.2.m1.1b"><ci id="S6.T10.41.41.41.2.m1.1.1.cmml" xref="S6.T10.41.41.41.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.41.41.41.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.41.41.41.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.42.42.42.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.42.42.42.3.m1.1"><semantics id="S6.T10.42.42.42.3.m1.1a"><mi id="S6.T10.42.42.42.3.m1.1.1" mathvariant="normal" xref="S6.T10.42.42.42.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.42.42.42.3.m1.1b"><ci id="S6.T10.42.42.42.3.m1.1.1.cmml" xref="S6.T10.42.42.42.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.42.42.42.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.42.42.42.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.43.43.43.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.43.43.43.4.m1.1"><semantics id="S6.T10.43.43.43.4.m1.1a"><mi id="S6.T10.43.43.43.4.m1.1.1" mathvariant="normal" xref="S6.T10.43.43.43.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.43.43.43.4.m1.1b"><ci id="S6.T10.43.43.43.4.m1.1.1.cmml" xref="S6.T10.43.43.43.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.43.43.43.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.43.43.43.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.43.43.43.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.43.43.43.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.50.50.50">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.50.50.50.8">Gopher</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.50.50.50.9" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.50.50.50.9.1">subsets of MassiveWeb
Books, C4, News, GitHub and
Wikipedia samples from MassiveText</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.44.44.44.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.44.44.44.1.m1.1"><semantics id="S6.T10.44.44.44.1.m1.1a"><mi id="S6.T10.44.44.44.1.m1.1.1" mathvariant="normal" xref="S6.T10.44.44.44.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.44.44.44.1.m1.1b"><ci id="S6.T10.44.44.44.1.m1.1.1.cmml" xref="S6.T10.44.44.44.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.44.44.44.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.44.44.44.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.45.45.45.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.45.45.45.2.m1.1"><semantics id="S6.T10.45.45.45.2.m1.1a"><mi id="S6.T10.45.45.45.2.m1.1.1" mathvariant="normal" xref="S6.T10.45.45.45.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.45.45.45.2.m1.1b"><ci id="S6.T10.45.45.45.2.m1.1.1.cmml" xref="S6.T10.45.45.45.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.45.45.45.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.45.45.45.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.46.46.46.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.46.46.46.3.m1.1"><semantics id="S6.T10.46.46.46.3.m1.1a"><mi id="S6.T10.46.46.46.3.m1.1.1" mathvariant="normal" xref="S6.T10.46.46.46.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.46.46.46.3.m1.1b"><ci id="S6.T10.46.46.46.3.m1.1.1.cmml" xref="S6.T10.46.46.46.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.46.46.46.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.46.46.46.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.47.47.47.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.47.47.47.4.m1.1"><semantics id="S6.T10.47.47.47.4.m1.1a"><mi id="S6.T10.47.47.47.4.m1.1.1" mathvariant="normal" xref="S6.T10.47.47.47.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.47.47.47.4.m1.1b"><ci id="S6.T10.47.47.47.4.m1.1.1.cmml" xref="S6.T10.47.47.47.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.47.47.47.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.47.47.47.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.48.48.48.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.48.48.48.5.m1.1"><semantics id="S6.T10.48.48.48.5.m1.1a"><mi id="S6.T10.48.48.48.5.m1.1.1" mathvariant="normal" xref="S6.T10.48.48.48.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.48.48.48.5.m1.1b"><ci id="S6.T10.48.48.48.5.m1.1.1.cmml" xref="S6.T10.48.48.48.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.48.48.48.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.48.48.48.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.49.49.49.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.49.49.49.6.m1.1"><semantics id="S6.T10.49.49.49.6.m1.1a"><mi id="S6.T10.49.49.49.6.m1.1.1" mathvariant="normal" xref="S6.T10.49.49.49.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.49.49.49.6.m1.1b"><ci id="S6.T10.49.49.49.6.m1.1.1.cmml" xref="S6.T10.49.49.49.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.49.49.49.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.49.49.49.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.50.50.50.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.50.50.50.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.50.50.50.7.m1.1"><semantics id="S6.T10.50.50.50.7.m1.1a"><mi id="S6.T10.50.50.50.7.m1.1.1" mathvariant="normal" xref="S6.T10.50.50.50.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.50.50.50.7.m1.1b"><ci id="S6.T10.50.50.50.7.m1.1.1.cmml" xref="S6.T10.50.50.50.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.50.50.50.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.50.50.50.7.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.55.55.55">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.55.55.55.6">ERNIE-3.0 TITAN</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.55.55.55.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.55.55.55.7.1">Same as ERNIE 3.0 and ERNIE 3.0 adversarial dataset, ERNIE 3.0 controllable dataset</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.51.51.51.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.51.51.51.1.m1.1"><semantics id="S6.T10.51.51.51.1.m1.1a"><mi id="S6.T10.51.51.51.1.m1.1.1" mathvariant="normal" xref="S6.T10.51.51.51.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.51.51.51.1.m1.1b"><ci id="S6.T10.51.51.51.1.m1.1.1.cmml" xref="S6.T10.51.51.51.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.51.51.51.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.51.51.51.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.52.52.52.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.52.52.52.2.m1.1"><semantics id="S6.T10.52.52.52.2.m1.1a"><mi id="S6.T10.52.52.52.2.m1.1.1" mathvariant="normal" xref="S6.T10.52.52.52.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.52.52.52.2.m1.1b"><ci id="S6.T10.52.52.52.2.m1.1.1.cmml" xref="S6.T10.52.52.52.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.52.52.52.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.52.52.52.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.53.53.53.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.53.53.53.3.m1.1"><semantics id="S6.T10.53.53.53.3.m1.1a"><mi id="S6.T10.53.53.53.3.m1.1.1" mathvariant="normal" xref="S6.T10.53.53.53.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.53.53.53.3.m1.1b"><ci id="S6.T10.53.53.53.3.m1.1.1.cmml" xref="S6.T10.53.53.53.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.53.53.53.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.53.53.53.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.54.54.54.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.54.54.54.4.m1.1"><semantics id="S6.T10.54.54.54.4.m1.1a"><mi id="S6.T10.54.54.54.4.m1.1.1" mathvariant="normal" xref="S6.T10.54.54.54.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.54.54.54.4.m1.1b"><ci id="S6.T10.54.54.54.4.m1.1.1.cmml" xref="S6.T10.54.54.54.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.54.54.54.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.54.54.54.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.55.55.55.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.55.55.55.5.m1.1"><semantics id="S6.T10.55.55.55.5.m1.1a"><mi id="S6.T10.55.55.55.5.m1.1.1" mathvariant="normal" xref="S6.T10.55.55.55.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.55.55.55.5.m1.1b"><ci id="S6.T10.55.55.55.5.m1.1.1.cmml" xref="S6.T10.55.55.55.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.55.55.55.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.55.55.55.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.55.55.55.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.55.55.55.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.61.61.61">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.61.61.61.7">GPT-NeoX-20B</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.61.61.61.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.61.61.61.8.1">Pile&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib301" title="">301</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.56.56.56.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.56.56.56.1.m1.1"><semantics id="S6.T10.56.56.56.1.m1.1a"><mi id="S6.T10.56.56.56.1.m1.1.1" mathvariant="normal" xref="S6.T10.56.56.56.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.56.56.56.1.m1.1b"><ci id="S6.T10.56.56.56.1.m1.1.1.cmml" xref="S6.T10.56.56.56.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.56.56.56.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.56.56.56.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.57.57.57.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.57.57.57.2.m1.1"><semantics id="S6.T10.57.57.57.2.m1.1a"><mi id="S6.T10.57.57.57.2.m1.1.1" mathvariant="normal" xref="S6.T10.57.57.57.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.57.57.57.2.m1.1b"><ci id="S6.T10.57.57.57.2.m1.1.1.cmml" xref="S6.T10.57.57.57.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.57.57.57.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.57.57.57.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.58.58.58.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.58.58.58.3.m1.1"><semantics id="S6.T10.58.58.58.3.m1.1a"><mi id="S6.T10.58.58.58.3.m1.1.1" mathvariant="normal" xref="S6.T10.58.58.58.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.58.58.58.3.m1.1b"><ci id="S6.T10.58.58.58.3.m1.1.1.cmml" xref="S6.T10.58.58.58.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.58.58.58.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.58.58.58.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.59.59.59.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.59.59.59.4.m1.1"><semantics id="S6.T10.59.59.59.4.m1.1a"><mi id="S6.T10.59.59.59.4.m1.1.1" mathvariant="normal" xref="S6.T10.59.59.59.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.59.59.59.4.m1.1b"><ci id="S6.T10.59.59.59.4.m1.1.1.cmml" xref="S6.T10.59.59.59.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.59.59.59.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.59.59.59.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.60.60.60.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.60.60.60.5.m1.1"><semantics id="S6.T10.60.60.60.5.m1.1a"><mi id="S6.T10.60.60.60.5.m1.1.1" mathvariant="normal" xref="S6.T10.60.60.60.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.60.60.60.5.m1.1b"><ci id="S6.T10.60.60.60.5.m1.1.1.cmml" xref="S6.T10.60.60.60.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.60.60.60.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.60.60.60.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.61.61.61.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.61.61.61.6.m1.1"><semantics id="S6.T10.61.61.61.6.m1.1a"><mi id="S6.T10.61.61.61.6.m1.1.1" mathvariant="normal" xref="S6.T10.61.61.61.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.61.61.61.6.m1.1b"><ci id="S6.T10.61.61.61.6.m1.1.1.cmml" xref="S6.T10.61.61.61.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.61.61.61.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.61.61.61.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.61.61.61.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.61.61.61.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.65.65.65">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.65.65.65.5">OPT</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.65.65.65.6" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.65.65.65.6.1">RoBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib424" title="">424</a>]</cite>, Pile&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib301" title="">301</a>]</cite>,
PushShift.io Reddit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib425" title="">425</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.62.62.62.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.62.62.62.1.m1.1"><semantics id="S6.T10.62.62.62.1.m1.1a"><mi id="S6.T10.62.62.62.1.m1.1.1" mathvariant="normal" xref="S6.T10.62.62.62.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.62.62.62.1.m1.1b"><ci id="S6.T10.62.62.62.1.m1.1.1.cmml" xref="S6.T10.62.62.62.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.62.62.62.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.62.62.62.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.63.63.63.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.63.63.63.2.m1.1"><semantics id="S6.T10.63.63.63.2.m1.1a"><mi id="S6.T10.63.63.63.2.m1.1.1" mathvariant="normal" xref="S6.T10.63.63.63.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.63.63.63.2.m1.1b"><ci id="S6.T10.63.63.63.2.m1.1.1.cmml" xref="S6.T10.63.63.63.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.63.63.63.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.63.63.63.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.64.64.64.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.64.64.64.3.m1.1"><semantics id="S6.T10.64.64.64.3.m1.1a"><mi id="S6.T10.64.64.64.3.m1.1.1" mathvariant="normal" xref="S6.T10.64.64.64.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.64.64.64.3.m1.1b"><ci id="S6.T10.64.64.64.3.m1.1.1.cmml" xref="S6.T10.64.64.64.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.64.64.64.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.64.64.64.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.65.65.65.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.65.65.65.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.65.65.65.4.m1.1"><semantics id="S6.T10.65.65.65.4.m1.1a"><mi id="S6.T10.65.65.65.4.m1.1.1" mathvariant="normal" xref="S6.T10.65.65.65.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.65.65.65.4.m1.1b"><ci id="S6.T10.65.65.65.4.m1.1.1.cmml" xref="S6.T10.65.65.65.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.65.65.65.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.65.65.65.4.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.71.71.71">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.71.71.71.7">BLOOM</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.71.71.71.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.71.71.71.8.1">ROOTs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.66.66.66.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.66.66.66.1.m1.1"><semantics id="S6.T10.66.66.66.1.m1.1a"><mi id="S6.T10.66.66.66.1.m1.1.1" mathvariant="normal" xref="S6.T10.66.66.66.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.66.66.66.1.m1.1b"><ci id="S6.T10.66.66.66.1.m1.1.1.cmml" xref="S6.T10.66.66.66.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.66.66.66.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.66.66.66.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.67.67.67.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.67.67.67.2.m1.1"><semantics id="S6.T10.67.67.67.2.m1.1a"><mi id="S6.T10.67.67.67.2.m1.1.1" mathvariant="normal" xref="S6.T10.67.67.67.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.67.67.67.2.m1.1b"><ci id="S6.T10.67.67.67.2.m1.1.1.cmml" xref="S6.T10.67.67.67.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.67.67.67.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.67.67.67.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.68.68.68.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.68.68.68.3.m1.1"><semantics id="S6.T10.68.68.68.3.m1.1a"><mi id="S6.T10.68.68.68.3.m1.1.1" mathvariant="normal" xref="S6.T10.68.68.68.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.68.68.68.3.m1.1b"><ci id="S6.T10.68.68.68.3.m1.1.1.cmml" xref="S6.T10.68.68.68.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.68.68.68.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.68.68.68.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.69.69.69.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.69.69.69.4.m1.1"><semantics id="S6.T10.69.69.69.4.m1.1a"><mi id="S6.T10.69.69.69.4.m1.1.1" mathvariant="normal" xref="S6.T10.69.69.69.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.69.69.69.4.m1.1b"><ci id="S6.T10.69.69.69.4.m1.1.1.cmml" xref="S6.T10.69.69.69.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.69.69.69.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.69.69.69.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.71.71.71.15"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.70.70.70.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.70.70.70.5.m1.1"><semantics id="S6.T10.70.70.70.5.m1.1a"><mi id="S6.T10.70.70.70.5.m1.1.1" mathvariant="normal" xref="S6.T10.70.70.70.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.70.70.70.5.m1.1b"><ci id="S6.T10.70.70.70.5.m1.1.1.cmml" xref="S6.T10.70.70.70.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.70.70.70.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.70.70.70.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.71.71.71.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.71.71.71.6.m1.1"><semantics id="S6.T10.71.71.71.6.m1.1a"><mi id="S6.T10.71.71.71.6.m1.1.1" mathvariant="normal" xref="S6.T10.71.71.71.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.71.71.71.6.m1.1b"><ci id="S6.T10.71.71.71.6.m1.1.1.cmml" xref="S6.T10.71.71.71.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.71.71.71.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.71.71.71.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.76.76.76">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.76.76.76.6">Galactica</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.76.76.76.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.76.76.76.7.1">arXiv, PMC, Semantic Scholar, Wikipedia, StackExchange, LibreText, Open Textbooks, RefSeq Genome, OEIS, LIPID MAPS, NASAExoplanet, Common Crawl, ScientificCC, AcademicCC, GitHub repositories
Khan Problems, GSM8K, OneSmallStep</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.72.72.72.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.72.72.72.1.m1.1"><semantics id="S6.T10.72.72.72.1.m1.1a"><mi id="S6.T10.72.72.72.1.m1.1.1" mathvariant="normal" xref="S6.T10.72.72.72.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.72.72.72.1.m1.1b"><ci id="S6.T10.72.72.72.1.m1.1.1.cmml" xref="S6.T10.72.72.72.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.72.72.72.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.72.72.72.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.73.73.73.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.73.73.73.2.m1.1"><semantics id="S6.T10.73.73.73.2.m1.1a"><mi id="S6.T10.73.73.73.2.m1.1.1" mathvariant="normal" xref="S6.T10.73.73.73.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.73.73.73.2.m1.1b"><ci id="S6.T10.73.73.73.2.m1.1.1.cmml" xref="S6.T10.73.73.73.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.73.73.73.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.73.73.73.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.74.74.74.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.74.74.74.3.m1.1"><semantics id="S6.T10.74.74.74.3.m1.1a"><mi id="S6.T10.74.74.74.3.m1.1.1" mathvariant="normal" xref="S6.T10.74.74.74.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.74.74.74.3.m1.1b"><ci id="S6.T10.74.74.74.3.m1.1.1.cmml" xref="S6.T10.74.74.74.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.74.74.74.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.74.74.74.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.75.75.75.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.75.75.75.4.m1.1"><semantics id="S6.T10.75.75.75.4.m1.1a"><mi id="S6.T10.75.75.75.4.m1.1.1" mathvariant="normal" xref="S6.T10.75.75.75.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.75.75.75.4.m1.1b"><ci id="S6.T10.75.75.75.4.m1.1.1.cmml" xref="S6.T10.75.75.75.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.75.75.75.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.75.75.75.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.76.76.76.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.76.76.76.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.76.76.76.5.m1.1"><semantics id="S6.T10.76.76.76.5.m1.1a"><mi id="S6.T10.76.76.76.5.m1.1.1" mathvariant="normal" xref="S6.T10.76.76.76.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.76.76.76.5.m1.1b"><ci id="S6.T10.76.76.76.5.m1.1.1.cmml" xref="S6.T10.76.76.76.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.76.76.76.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.76.76.76.5.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.81.81.81">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.81.81.81.6">GLaM</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.81.81.81.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.81.81.81.7.1">Filtered Webpages, Social media conversations Wikipedia, Forums, Books, News</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.77.77.77.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.77.77.77.1.m1.1"><semantics id="S6.T10.77.77.77.1.m1.1a"><mi id="S6.T10.77.77.77.1.m1.1.1" mathvariant="normal" xref="S6.T10.77.77.77.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.77.77.77.1.m1.1b"><ci id="S6.T10.77.77.77.1.m1.1.1.cmml" xref="S6.T10.77.77.77.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.77.77.77.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.77.77.77.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.78.78.78.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.78.78.78.2.m1.1"><semantics id="S6.T10.78.78.78.2.m1.1a"><mi id="S6.T10.78.78.78.2.m1.1.1" mathvariant="normal" xref="S6.T10.78.78.78.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.78.78.78.2.m1.1b"><ci id="S6.T10.78.78.78.2.m1.1.1.cmml" xref="S6.T10.78.78.78.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.78.78.78.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.78.78.78.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.79.79.79.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.79.79.79.3.m1.1"><semantics id="S6.T10.79.79.79.3.m1.1a"><mi id="S6.T10.79.79.79.3.m1.1.1" mathvariant="normal" xref="S6.T10.79.79.79.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.79.79.79.3.m1.1b"><ci id="S6.T10.79.79.79.3.m1.1.1.cmml" xref="S6.T10.79.79.79.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.79.79.79.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.79.79.79.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.80.80.80.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.80.80.80.4.m1.1"><semantics id="S6.T10.80.80.80.4.m1.1a"><mi id="S6.T10.80.80.80.4.m1.1.1" mathvariant="normal" xref="S6.T10.80.80.80.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.80.80.80.4.m1.1b"><ci id="S6.T10.80.80.80.4.m1.1.1.cmml" xref="S6.T10.80.80.80.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.80.80.80.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.80.80.80.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.81.81.81.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.81.81.81.5.m1.1"><semantics id="S6.T10.81.81.81.5.m1.1a"><mi id="S6.T10.81.81.81.5.m1.1.1" mathvariant="normal" xref="S6.T10.81.81.81.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.81.81.81.5.m1.1b"><ci id="S6.T10.81.81.81.5.m1.1.1.cmml" xref="S6.T10.81.81.81.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.81.81.81.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.81.81.81.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.81.81.81.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.81.81.81.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.82.82.82">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.82.82.82.2">LaMDA</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.82.82.82.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.82.82.82.3.1">Infiniset : Public documents, Dialogs, Utterances</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.82.82.82.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.82.82.82.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.82.82.82.1.m1.1"><semantics id="S6.T10.82.82.82.1.m1.1a"><mi id="S6.T10.82.82.82.1.m1.1.1" mathvariant="normal" xref="S6.T10.82.82.82.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.82.82.82.1.m1.1b"><ci id="S6.T10.82.82.82.1.m1.1.1.cmml" xref="S6.T10.82.82.82.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.82.82.82.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.82.82.82.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.87.87.87">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.87.87.87.6">MT-NLG</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.87.87.87.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.87.87.87.7.1">Two snapshots of Common Crawl and Books3, OpenWebText2, Stack Exchange, PubMed Abstracts, Wikipedia, PG-19 [242], BookCorpus2, NIH ExPorter, Pile, CC-Stories, RealNews</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.83.83.83.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.83.83.83.1.m1.1"><semantics id="S6.T10.83.83.83.1.m1.1a"><mi id="S6.T10.83.83.83.1.m1.1.1" mathvariant="normal" xref="S6.T10.83.83.83.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.83.83.83.1.m1.1b"><ci id="S6.T10.83.83.83.1.m1.1.1.cmml" xref="S6.T10.83.83.83.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.83.83.83.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.83.83.83.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.84.84.84.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.84.84.84.2.m1.1"><semantics id="S6.T10.84.84.84.2.m1.1a"><mi id="S6.T10.84.84.84.2.m1.1.1" mathvariant="normal" xref="S6.T10.84.84.84.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.84.84.84.2.m1.1b"><ci id="S6.T10.84.84.84.2.m1.1.1.cmml" xref="S6.T10.84.84.84.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.84.84.84.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.84.84.84.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.85.85.85.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.85.85.85.3.m1.1"><semantics id="S6.T10.85.85.85.3.m1.1a"><mi id="S6.T10.85.85.85.3.m1.1.1" mathvariant="normal" xref="S6.T10.85.85.85.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.85.85.85.3.m1.1b"><ci id="S6.T10.85.85.85.3.m1.1.1.cmml" xref="S6.T10.85.85.85.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.85.85.85.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.85.85.85.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.86.86.86.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.86.86.86.4.m1.1"><semantics id="S6.T10.86.86.86.4.m1.1a"><mi id="S6.T10.86.86.86.4.m1.1.1" mathvariant="normal" xref="S6.T10.86.86.86.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.86.86.86.4.m1.1b"><ci id="S6.T10.86.86.86.4.m1.1.1.cmml" xref="S6.T10.86.86.86.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.86.86.86.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.86.86.86.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.87.87.87.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.87.87.87.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.87.87.87.5.m1.1"><semantics id="S6.T10.87.87.87.5.m1.1a"><mi id="S6.T10.87.87.87.5.m1.1.1" mathvariant="normal" xref="S6.T10.87.87.87.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.87.87.87.5.m1.1b"><ci id="S6.T10.87.87.87.5.m1.1.1.cmml" xref="S6.T10.87.87.87.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.87.87.87.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.87.87.87.5.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.88.88.88">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.88.88.88.2">AlphaCode</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.88.88.88.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.88.88.88.3.1">Selected GitHub repositories, CodeContests: Codeforces, Description2Code, CodeNet</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.88.88.88.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.88.88.88.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.88.88.88.1.m1.1"><semantics id="S6.T10.88.88.88.1.m1.1a"><mi id="S6.T10.88.88.88.1.m1.1.1" mathvariant="normal" xref="S6.T10.88.88.88.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.88.88.88.1.m1.1b"><ci id="S6.T10.88.88.88.1.m1.1.1.cmml" xref="S6.T10.88.88.88.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.88.88.88.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.88.88.88.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.88.88.88.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.94.94.94">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.94.94.94.7">Chinchilla</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.94.94.94.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.94.94.94.8.1">MassiveWeb, MassiveText Books, C4, News, GitHub, Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.89.89.89.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.89.89.89.1.m1.1"><semantics id="S6.T10.89.89.89.1.m1.1a"><mi id="S6.T10.89.89.89.1.m1.1.1" mathvariant="normal" xref="S6.T10.89.89.89.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.89.89.89.1.m1.1b"><ci id="S6.T10.89.89.89.1.m1.1.1.cmml" xref="S6.T10.89.89.89.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.89.89.89.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.89.89.89.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.90.90.90.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.90.90.90.2.m1.1"><semantics id="S6.T10.90.90.90.2.m1.1a"><mi id="S6.T10.90.90.90.2.m1.1.1" mathvariant="normal" xref="S6.T10.90.90.90.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.90.90.90.2.m1.1b"><ci id="S6.T10.90.90.90.2.m1.1.1.cmml" xref="S6.T10.90.90.90.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.90.90.90.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.90.90.90.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.91.91.91.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.91.91.91.3.m1.1"><semantics id="S6.T10.91.91.91.3.m1.1a"><mi id="S6.T10.91.91.91.3.m1.1.1" mathvariant="normal" xref="S6.T10.91.91.91.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.91.91.91.3.m1.1b"><ci id="S6.T10.91.91.91.3.m1.1.1.cmml" xref="S6.T10.91.91.91.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.91.91.91.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.91.91.91.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.92.92.92.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.92.92.92.4.m1.1"><semantics id="S6.T10.92.92.92.4.m1.1a"><mi id="S6.T10.92.92.92.4.m1.1.1" mathvariant="normal" xref="S6.T10.92.92.92.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.92.92.92.4.m1.1b"><ci id="S6.T10.92.92.92.4.m1.1.1.cmml" xref="S6.T10.92.92.92.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.92.92.92.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.92.92.92.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.93.93.93.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.93.93.93.5.m1.1"><semantics id="S6.T10.93.93.93.5.m1.1a"><mi id="S6.T10.93.93.93.5.m1.1.1" mathvariant="normal" xref="S6.T10.93.93.93.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.93.93.93.5.m1.1b"><ci id="S6.T10.93.93.93.5.m1.1.1.cmml" xref="S6.T10.93.93.93.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.93.93.93.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.93.93.93.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.94.94.94.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.94.94.94.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.94.94.94.6.m1.1"><semantics id="S6.T10.94.94.94.6.m1.1a"><mi id="S6.T10.94.94.94.6.m1.1.1" mathvariant="normal" xref="S6.T10.94.94.94.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.94.94.94.6.m1.1b"><ci id="S6.T10.94.94.94.6.m1.1.1.cmml" xref="S6.T10.94.94.94.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.94.94.94.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.94.94.94.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.100.100.100">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.100.100.100.7">PaLM</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.100.100.100.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.100.100.100.8.1">webpages, books, Wikipedia, news, articles, source code, social media conversations</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.95.95.95.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.95.95.95.1.m1.1"><semantics id="S6.T10.95.95.95.1.m1.1a"><mi id="S6.T10.95.95.95.1.m1.1.1" mathvariant="normal" xref="S6.T10.95.95.95.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.95.95.95.1.m1.1b"><ci id="S6.T10.95.95.95.1.m1.1.1.cmml" xref="S6.T10.95.95.95.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.95.95.95.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.95.95.95.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.96.96.96.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.96.96.96.2.m1.1"><semantics id="S6.T10.96.96.96.2.m1.1a"><mi id="S6.T10.96.96.96.2.m1.1.1" mathvariant="normal" xref="S6.T10.96.96.96.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.96.96.96.2.m1.1b"><ci id="S6.T10.96.96.96.2.m1.1.1.cmml" xref="S6.T10.96.96.96.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.96.96.96.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.96.96.96.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.97.97.97.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.97.97.97.3.m1.1"><semantics id="S6.T10.97.97.97.3.m1.1a"><mi id="S6.T10.97.97.97.3.m1.1.1" mathvariant="normal" xref="S6.T10.97.97.97.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.97.97.97.3.m1.1b"><ci id="S6.T10.97.97.97.3.m1.1.1.cmml" xref="S6.T10.97.97.97.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.97.97.97.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.97.97.97.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.98.98.98.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.98.98.98.4.m1.1"><semantics id="S6.T10.98.98.98.4.m1.1a"><mi id="S6.T10.98.98.98.4.m1.1.1" mathvariant="normal" xref="S6.T10.98.98.98.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.98.98.98.4.m1.1b"><ci id="S6.T10.98.98.98.4.m1.1.1.cmml" xref="S6.T10.98.98.98.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.98.98.98.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.98.98.98.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.100.100.100.15"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.99.99.99.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.99.99.99.5.m1.1"><semantics id="S6.T10.99.99.99.5.m1.1a"><mi id="S6.T10.99.99.99.5.m1.1.1" mathvariant="normal" xref="S6.T10.99.99.99.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.99.99.99.5.m1.1b"><ci id="S6.T10.99.99.99.5.m1.1.1.cmml" xref="S6.T10.99.99.99.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.99.99.99.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.99.99.99.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.100.100.100.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.100.100.100.6.m1.1"><semantics id="S6.T10.100.100.100.6.m1.1a"><mi id="S6.T10.100.100.100.6.m1.1.1" mathvariant="normal" xref="S6.T10.100.100.100.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.100.100.100.6.m1.1b"><ci id="S6.T10.100.100.100.6.m1.1.1.cmml" xref="S6.T10.100.100.100.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.100.100.100.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.100.100.100.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.105.105.105">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.105.105.105.6">AlexaTM</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.105.105.105.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.105.105.105.7.1">Wikipedia, mC4</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.101.101.101.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.101.101.101.1.m1.1"><semantics id="S6.T10.101.101.101.1.m1.1a"><mi id="S6.T10.101.101.101.1.m1.1.1" mathvariant="normal" xref="S6.T10.101.101.101.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.101.101.101.1.m1.1b"><ci id="S6.T10.101.101.101.1.m1.1.1.cmml" xref="S6.T10.101.101.101.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.101.101.101.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.101.101.101.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.102.102.102.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.102.102.102.2.m1.1"><semantics id="S6.T10.102.102.102.2.m1.1a"><mi id="S6.T10.102.102.102.2.m1.1.1" mathvariant="normal" xref="S6.T10.102.102.102.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.102.102.102.2.m1.1b"><ci id="S6.T10.102.102.102.2.m1.1.1.cmml" xref="S6.T10.102.102.102.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.102.102.102.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.102.102.102.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.103.103.103.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.103.103.103.3.m1.1"><semantics id="S6.T10.103.103.103.3.m1.1a"><mi id="S6.T10.103.103.103.3.m1.1.1" mathvariant="normal" xref="S6.T10.103.103.103.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.103.103.103.3.m1.1b"><ci id="S6.T10.103.103.103.3.m1.1.1.cmml" xref="S6.T10.103.103.103.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.103.103.103.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.103.103.103.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.104.104.104.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.104.104.104.4.m1.1"><semantics id="S6.T10.104.104.104.4.m1.1a"><mi id="S6.T10.104.104.104.4.m1.1.1" mathvariant="normal" xref="S6.T10.104.104.104.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.104.104.104.4.m1.1b"><ci id="S6.T10.104.104.104.4.m1.1.1.cmml" xref="S6.T10.104.104.104.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.104.104.104.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.104.104.104.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.105.105.105.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.105.105.105.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.105.105.105.5.m1.1"><semantics id="S6.T10.105.105.105.5.m1.1a"><mi id="S6.T10.105.105.105.5.m1.1.1" mathvariant="normal" xref="S6.T10.105.105.105.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.105.105.105.5.m1.1b"><ci id="S6.T10.105.105.105.5.m1.1.1.cmml" xref="S6.T10.105.105.105.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.105.105.105.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.105.105.105.5.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.112.112.112">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.112.112.112.8">U-PaLM</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.112.112.112.9" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.112.112.112.9.1">Same as PaLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.106.106.106.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.106.106.106.1.m1.1"><semantics id="S6.T10.106.106.106.1.m1.1a"><mi id="S6.T10.106.106.106.1.m1.1.1" mathvariant="normal" xref="S6.T10.106.106.106.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.106.106.106.1.m1.1b"><ci id="S6.T10.106.106.106.1.m1.1.1.cmml" xref="S6.T10.106.106.106.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.106.106.106.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.106.106.106.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.112.112.112.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.107.107.107.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.107.107.107.2.m1.1"><semantics id="S6.T10.107.107.107.2.m1.1a"><mi id="S6.T10.107.107.107.2.m1.1.1" mathvariant="normal" xref="S6.T10.107.107.107.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.107.107.107.2.m1.1b"><ci id="S6.T10.107.107.107.2.m1.1.1.cmml" xref="S6.T10.107.107.107.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.107.107.107.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.107.107.107.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.108.108.108.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.108.108.108.3.m1.1"><semantics id="S6.T10.108.108.108.3.m1.1a"><mi id="S6.T10.108.108.108.3.m1.1.1" mathvariant="normal" xref="S6.T10.108.108.108.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.108.108.108.3.m1.1b"><ci id="S6.T10.108.108.108.3.m1.1.1.cmml" xref="S6.T10.108.108.108.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.108.108.108.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.108.108.108.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.112.112.112.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.109.109.109.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.109.109.109.4.m1.1"><semantics id="S6.T10.109.109.109.4.m1.1a"><mi id="S6.T10.109.109.109.4.m1.1.1" mathvariant="normal" xref="S6.T10.109.109.109.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.109.109.109.4.m1.1b"><ci id="S6.T10.109.109.109.4.m1.1.1.cmml" xref="S6.T10.109.109.109.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.109.109.109.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.109.109.109.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.112.112.112.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.110.110.110.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.110.110.110.5.m1.1"><semantics id="S6.T10.110.110.110.5.m1.1a"><mi id="S6.T10.110.110.110.5.m1.1.1" mathvariant="normal" xref="S6.T10.110.110.110.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.110.110.110.5.m1.1b"><ci id="S6.T10.110.110.110.5.m1.1.1.cmml" xref="S6.T10.110.110.110.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.110.110.110.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.110.110.110.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.111.111.111.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.111.111.111.6.m1.1"><semantics id="S6.T10.111.111.111.6.m1.1a"><mi id="S6.T10.111.111.111.6.m1.1.1" mathvariant="normal" xref="S6.T10.111.111.111.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.111.111.111.6.m1.1b"><ci id="S6.T10.111.111.111.6.m1.1.1.cmml" xref="S6.T10.111.111.111.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.111.111.111.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.111.111.111.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.112.112.112.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.112.112.112.7.m1.1"><semantics id="S6.T10.112.112.112.7.m1.1a"><mi id="S6.T10.112.112.112.7.m1.1.1" mathvariant="normal" xref="S6.T10.112.112.112.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.112.112.112.7.m1.1b"><ci id="S6.T10.112.112.112.7.m1.1.1.cmml" xref="S6.T10.112.112.112.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.112.112.112.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.112.112.112.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.112.112.112.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.112.112.112.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.112.112.112.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.118.118.118">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.118.118.118.7">UL2</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.118.118.118.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.118.118.118.8.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.113.113.113.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.113.113.113.1.m1.1"><semantics id="S6.T10.113.113.113.1.m1.1a"><mi id="S6.T10.113.113.113.1.m1.1.1" mathvariant="normal" xref="S6.T10.113.113.113.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.113.113.113.1.m1.1b"><ci id="S6.T10.113.113.113.1.m1.1.1.cmml" xref="S6.T10.113.113.113.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.113.113.113.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.113.113.113.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.114.114.114.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.114.114.114.2.m1.1"><semantics id="S6.T10.114.114.114.2.m1.1a"><mi id="S6.T10.114.114.114.2.m1.1.1" mathvariant="normal" xref="S6.T10.114.114.114.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.114.114.114.2.m1.1b"><ci id="S6.T10.114.114.114.2.m1.1.1.cmml" xref="S6.T10.114.114.114.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.114.114.114.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.114.114.114.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.115.115.115.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.115.115.115.3.m1.1"><semantics id="S6.T10.115.115.115.3.m1.1a"><mi id="S6.T10.115.115.115.3.m1.1.1" mathvariant="normal" xref="S6.T10.115.115.115.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.115.115.115.3.m1.1b"><ci id="S6.T10.115.115.115.3.m1.1.1.cmml" xref="S6.T10.115.115.115.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.115.115.115.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.115.115.115.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.116.116.116.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.116.116.116.4.m1.1"><semantics id="S6.T10.116.116.116.4.m1.1a"><mi id="S6.T10.116.116.116.4.m1.1.1" mathvariant="normal" xref="S6.T10.116.116.116.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.116.116.116.4.m1.1b"><ci id="S6.T10.116.116.116.4.m1.1.1.cmml" xref="S6.T10.116.116.116.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.116.116.116.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.116.116.116.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.117.117.117.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.117.117.117.5.m1.1"><semantics id="S6.T10.117.117.117.5.m1.1a"><mi id="S6.T10.117.117.117.5.m1.1.1" mathvariant="normal" xref="S6.T10.117.117.117.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.117.117.117.5.m1.1b"><ci id="S6.T10.117.117.117.5.m1.1.1.cmml" xref="S6.T10.117.117.117.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.117.117.117.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.117.117.117.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.118.118.118.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.118.118.118.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.118.118.118.6.m1.1"><semantics id="S6.T10.118.118.118.6.m1.1a"><mi id="S6.T10.118.118.118.6.m1.1.1" mathvariant="normal" xref="S6.T10.118.118.118.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.118.118.118.6.m1.1b"><ci id="S6.T10.118.118.118.6.m1.1.1.cmml" xref="S6.T10.118.118.118.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.118.118.118.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.118.118.118.6.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.121.121.121">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.121.121.121.4">GLM-130B</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.121.121.121.5" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.121.121.121.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.119.119.119.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.119.119.119.1.m1.1"><semantics id="S6.T10.119.119.119.1.m1.1a"><mi id="S6.T10.119.119.119.1.m1.1.1" mathvariant="normal" xref="S6.T10.119.119.119.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.119.119.119.1.m1.1b"><ci id="S6.T10.119.119.119.1.m1.1.1.cmml" xref="S6.T10.119.119.119.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.119.119.119.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.119.119.119.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.120.120.120.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.120.120.120.2.m1.1"><semantics id="S6.T10.120.120.120.2.m1.1a"><mi id="S6.T10.120.120.120.2.m1.1.1" mathvariant="normal" xref="S6.T10.120.120.120.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.120.120.120.2.m1.1b"><ci id="S6.T10.120.120.120.2.m1.1.1.cmml" xref="S6.T10.120.120.120.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.120.120.120.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.120.120.120.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.121.121.121.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.121.121.121.3.m1.1"><semantics id="S6.T10.121.121.121.3.m1.1a"><mi id="S6.T10.121.121.121.3.m1.1.1" mathvariant="normal" xref="S6.T10.121.121.121.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.121.121.121.3.m1.1b"><ci id="S6.T10.121.121.121.3.m1.1.1.cmml" xref="S6.T10.121.121.121.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.121.121.121.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.121.121.121.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.121.121.121.14"></td>
<td class="ltx_td ltx_border_t" id="S6.T10.121.121.121.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.122.122.122">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.122.122.122.2">CodeGen</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.122.122.122.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.122.122.122.3.1">Pile, BigQuery, BigPython</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.122.122.122.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.122.122.122.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.122.122.122.1.m1.1"><semantics id="S6.T10.122.122.122.1.m1.1a"><mi id="S6.T10.122.122.122.1.m1.1.1" mathvariant="normal" xref="S6.T10.122.122.122.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.122.122.122.1.m1.1b"><ci id="S6.T10.122.122.122.1.m1.1.1.cmml" xref="S6.T10.122.122.122.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.122.122.122.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.122.122.122.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.122.122.122.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.129.129.129">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.129.129.129.8">LLaMA</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.129.129.129.9" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.129.129.129.9.1">CommonCrawl, C4, Github, Wikipedia, Books, arXiv, StackExchange</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.123.123.123.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.123.123.123.1.m1.1"><semantics id="S6.T10.123.123.123.1.m1.1a"><mi id="S6.T10.123.123.123.1.m1.1.1" mathvariant="normal" xref="S6.T10.123.123.123.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.123.123.123.1.m1.1b"><ci id="S6.T10.123.123.123.1.m1.1.1.cmml" xref="S6.T10.123.123.123.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.123.123.123.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.123.123.123.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.124.124.124.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.124.124.124.2.m1.1"><semantics id="S6.T10.124.124.124.2.m1.1a"><mi id="S6.T10.124.124.124.2.m1.1.1" mathvariant="normal" xref="S6.T10.124.124.124.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.124.124.124.2.m1.1b"><ci id="S6.T10.124.124.124.2.m1.1.1.cmml" xref="S6.T10.124.124.124.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.124.124.124.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.124.124.124.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.129.129.129.15"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.125.125.125.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.125.125.125.3.m1.1"><semantics id="S6.T10.125.125.125.3.m1.1a"><mi id="S6.T10.125.125.125.3.m1.1.1" mathvariant="normal" xref="S6.T10.125.125.125.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.125.125.125.3.m1.1b"><ci id="S6.T10.125.125.125.3.m1.1.1.cmml" xref="S6.T10.125.125.125.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.125.125.125.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.125.125.125.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.126.126.126.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.126.126.126.4.m1.1"><semantics id="S6.T10.126.126.126.4.m1.1a"><mi id="S6.T10.126.126.126.4.m1.1.1" mathvariant="normal" xref="S6.T10.126.126.126.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.126.126.126.4.m1.1b"><ci id="S6.T10.126.126.126.4.m1.1.1.cmml" xref="S6.T10.126.126.126.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.126.126.126.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.126.126.126.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.127.127.127.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.127.127.127.5.m1.1"><semantics id="S6.T10.127.127.127.5.m1.1a"><mi id="S6.T10.127.127.127.5.m1.1.1" mathvariant="normal" xref="S6.T10.127.127.127.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.127.127.127.5.m1.1b"><ci id="S6.T10.127.127.127.5.m1.1.1.cmml" xref="S6.T10.127.127.127.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.127.127.127.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.127.127.127.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.128.128.128.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.128.128.128.6.m1.1"><semantics id="S6.T10.128.128.128.6.m1.1a"><mi id="S6.T10.128.128.128.6.m1.1.1" mathvariant="normal" xref="S6.T10.128.128.128.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.128.128.128.6.m1.1b"><ci id="S6.T10.128.128.128.6.m1.1.1.cmml" xref="S6.T10.128.128.128.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.128.128.128.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.128.128.128.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.129.129.129.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.129.129.129.7.m1.1"><semantics id="S6.T10.129.129.129.7.m1.1a"><mi id="S6.T10.129.129.129.7.m1.1.1" mathvariant="normal" xref="S6.T10.129.129.129.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.129.129.129.7.m1.1b"><ci id="S6.T10.129.129.129.7.m1.1.1.cmml" xref="S6.T10.129.129.129.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.129.129.129.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.129.129.129.7.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.136.136.136">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.130.130.130.1">PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S6.T10.130.130.130.1.m1.1"><semantics id="S6.T10.130.130.130.1.m1.1a"><mi id="S6.T10.130.130.130.1.m1.1.1" mathvariant="normal" xref="S6.T10.130.130.130.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S6.T10.130.130.130.1.m1.1b"><ci id="S6.T10.130.130.130.1.m1.1.1.cmml" xref="S6.T10.130.130.130.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.130.130.130.1.m1.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S6.T10.130.130.130.1.m1.1d">roman_Σ</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.136.136.136.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.136.136.136.8.1">WuDaoCorpora, CLUE, Pile, C4, Python code</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.131.131.131.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.131.131.131.2.m1.1"><semantics id="S6.T10.131.131.131.2.m1.1a"><mi id="S6.T10.131.131.131.2.m1.1.1" mathvariant="normal" xref="S6.T10.131.131.131.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.131.131.131.2.m1.1b"><ci id="S6.T10.131.131.131.2.m1.1.1.cmml" xref="S6.T10.131.131.131.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.131.131.131.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.131.131.131.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.132.132.132.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.132.132.132.3.m1.1"><semantics id="S6.T10.132.132.132.3.m1.1a"><mi id="S6.T10.132.132.132.3.m1.1.1" mathvariant="normal" xref="S6.T10.132.132.132.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.132.132.132.3.m1.1b"><ci id="S6.T10.132.132.132.3.m1.1.1.cmml" xref="S6.T10.132.132.132.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.132.132.132.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.132.132.132.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.133.133.133.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.133.133.133.4.m1.1"><semantics id="S6.T10.133.133.133.4.m1.1a"><mi id="S6.T10.133.133.133.4.m1.1.1" mathvariant="normal" xref="S6.T10.133.133.133.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.133.133.133.4.m1.1b"><ci id="S6.T10.133.133.133.4.m1.1.1.cmml" xref="S6.T10.133.133.133.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.133.133.133.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.133.133.133.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.134.134.134.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.134.134.134.5.m1.1"><semantics id="S6.T10.134.134.134.5.m1.1a"><mi id="S6.T10.134.134.134.5.m1.1.1" mathvariant="normal" xref="S6.T10.134.134.134.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.134.134.134.5.m1.1b"><ci id="S6.T10.134.134.134.5.m1.1.1.cmml" xref="S6.T10.134.134.134.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.134.134.134.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.134.134.134.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.135.135.135.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.135.135.135.6.m1.1"><semantics id="S6.T10.135.135.135.6.m1.1a"><mi id="S6.T10.135.135.135.6.m1.1.1" mathvariant="normal" xref="S6.T10.135.135.135.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.135.135.135.6.m1.1b"><ci id="S6.T10.135.135.135.6.m1.1.1.cmml" xref="S6.T10.135.135.135.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.135.135.135.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.135.135.135.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.136.136.136.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.136.136.136.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.136.136.136.7.m1.1"><semantics id="S6.T10.136.136.136.7.m1.1a"><mi id="S6.T10.136.136.136.7.m1.1.1" mathvariant="normal" xref="S6.T10.136.136.136.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.136.136.136.7.m1.1b"><ci id="S6.T10.136.136.136.7.m1.1.1.cmml" xref="S6.T10.136.136.136.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.136.136.136.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.136.136.136.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.136.136.136.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.143.143.143">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.143.143.143.8">BloombergGPT</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.143.143.143.9" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.143.143.143.9.1">inPile, Pile, C4, Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.137.137.137.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.137.137.137.1.m1.1"><semantics id="S6.T10.137.137.137.1.m1.1a"><mi id="S6.T10.137.137.137.1.m1.1.1" mathvariant="normal" xref="S6.T10.137.137.137.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.137.137.137.1.m1.1b"><ci id="S6.T10.137.137.137.1.m1.1.1.cmml" xref="S6.T10.137.137.137.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.137.137.137.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.137.137.137.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.138.138.138.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.138.138.138.2.m1.1"><semantics id="S6.T10.138.138.138.2.m1.1a"><mi id="S6.T10.138.138.138.2.m1.1.1" mathvariant="normal" xref="S6.T10.138.138.138.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.138.138.138.2.m1.1b"><ci id="S6.T10.138.138.138.2.m1.1.1.cmml" xref="S6.T10.138.138.138.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.138.138.138.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.138.138.138.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.139.139.139.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.139.139.139.3.m1.1"><semantics id="S6.T10.139.139.139.3.m1.1a"><mi id="S6.T10.139.139.139.3.m1.1.1" mathvariant="normal" xref="S6.T10.139.139.139.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.139.139.139.3.m1.1b"><ci id="S6.T10.139.139.139.3.m1.1.1.cmml" xref="S6.T10.139.139.139.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.139.139.139.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.139.139.139.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.140.140.140.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.140.140.140.4.m1.1"><semantics id="S6.T10.140.140.140.4.m1.1a"><mi id="S6.T10.140.140.140.4.m1.1.1" mathvariant="normal" xref="S6.T10.140.140.140.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.140.140.140.4.m1.1b"><ci id="S6.T10.140.140.140.4.m1.1.1.cmml" xref="S6.T10.140.140.140.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.140.140.140.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.140.140.140.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.141.141.141.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.141.141.141.5.m1.1"><semantics id="S6.T10.141.141.141.5.m1.1a"><mi id="S6.T10.141.141.141.5.m1.1.1" mathvariant="normal" xref="S6.T10.141.141.141.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.141.141.141.5.m1.1b"><ci id="S6.T10.141.141.141.5.m1.1.1.cmml" xref="S6.T10.141.141.141.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.141.141.141.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.141.141.141.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.142.142.142.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.142.142.142.6.m1.1"><semantics id="S6.T10.142.142.142.6.m1.1a"><mi id="S6.T10.142.142.142.6.m1.1.1" mathvariant="normal" xref="S6.T10.142.142.142.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.142.142.142.6.m1.1b"><ci id="S6.T10.142.142.142.6.m1.1.1.cmml" xref="S6.T10.142.142.142.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.142.142.142.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.142.142.142.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.143.143.143.15"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.143.143.143.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.143.143.143.7.m1.1"><semantics id="S6.T10.143.143.143.7.m1.1a"><mi id="S6.T10.143.143.143.7.m1.1.1" mathvariant="normal" xref="S6.T10.143.143.143.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.143.143.143.7.m1.1b"><ci id="S6.T10.143.143.143.7.m1.1.1.cmml" xref="S6.T10.143.143.143.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.143.143.143.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.143.143.143.7.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.145.145.145">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.145.145.145.3">CodeT5+</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.145.145.145.4" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.145.145.145.4.1">CodeSearchNet, Github Code</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.145.145.145.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.144.144.144.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.144.144.144.1.m1.1"><semantics id="S6.T10.144.144.144.1.m1.1a"><mi id="S6.T10.144.144.144.1.m1.1.1" mathvariant="normal" xref="S6.T10.144.144.144.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.144.144.144.1.m1.1b"><ci id="S6.T10.144.144.144.1.m1.1.1.cmml" xref="S6.T10.144.144.144.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.144.144.144.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.144.144.144.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.145.145.145.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.145.145.145.2.m1.1"><semantics id="S6.T10.145.145.145.2.m1.1a"><mi id="S6.T10.145.145.145.2.m1.1.1" mathvariant="normal" xref="S6.T10.145.145.145.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.145.145.145.2.m1.1b"><ci id="S6.T10.145.145.145.2.m1.1.1.cmml" xref="S6.T10.145.145.145.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.145.145.145.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.145.145.145.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.145.145.145.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.149.149.149">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.149.149.149.5">StarCoder</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.149.149.149.6" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.149.149.149.6.1">The Stack v1.2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.146.146.146.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.146.146.146.1.m1.1"><semantics id="S6.T10.146.146.146.1.m1.1a"><mi id="S6.T10.146.146.146.1.m1.1.1" mathvariant="normal" xref="S6.T10.146.146.146.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.146.146.146.1.m1.1b"><ci id="S6.T10.146.146.146.1.m1.1.1.cmml" xref="S6.T10.146.146.146.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.146.146.146.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.146.146.146.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.14"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.149.149.149.15"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.147.147.147.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.147.147.147.2.m1.1"><semantics id="S6.T10.147.147.147.2.m1.1a"><mi id="S6.T10.147.147.147.2.m1.1.1" mathvariant="normal" xref="S6.T10.147.147.147.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.147.147.147.2.m1.1b"><ci id="S6.T10.147.147.147.2.m1.1.1.cmml" xref="S6.T10.147.147.147.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.147.147.147.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.147.147.147.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.148.148.148.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.148.148.148.3.m1.1"><semantics id="S6.T10.148.148.148.3.m1.1a"><mi id="S6.T10.148.148.148.3.m1.1.1" mathvariant="normal" xref="S6.T10.148.148.148.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.148.148.148.3.m1.1b"><ci id="S6.T10.148.148.148.3.m1.1.1.cmml" xref="S6.T10.148.148.148.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.148.148.148.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.148.148.148.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T10.149.149.149.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.149.149.149.4.m1.1"><semantics id="S6.T10.149.149.149.4.m1.1a"><mi id="S6.T10.149.149.149.4.m1.1.1" mathvariant="normal" xref="S6.T10.149.149.149.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.149.149.149.4.m1.1b"><ci id="S6.T10.149.149.149.4.m1.1.1.cmml" xref="S6.T10.149.149.149.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.149.149.149.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.149.149.149.4.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T10.156.156.156">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T10.156.156.156.8">LLaMA-2</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T10.150.150.150.1" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.150.150.150.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.150.150.150.1.1.1.m1.1"><semantics id="S6.T10.150.150.150.1.1.1.m1.1a"><mi id="S6.T10.150.150.150.1.1.1.m1.1.1" mathvariant="normal" xref="S6.T10.150.150.150.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.150.150.150.1.1.1.m1.1b"><ci id="S6.T10.150.150.150.1.1.1.m1.1.1.cmml" xref="S6.T10.150.150.150.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.150.150.150.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.150.150.150.1.1.1.m1.1d">✓</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.151.151.151.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.151.151.151.2.m1.1"><semantics id="S6.T10.151.151.151.2.m1.1a"><mi id="S6.T10.151.151.151.2.m1.1.1" mathvariant="normal" xref="S6.T10.151.151.151.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.151.151.151.2.m1.1b"><ci id="S6.T10.151.151.151.2.m1.1.1.cmml" xref="S6.T10.151.151.151.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.151.151.151.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.151.151.151.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.152.152.152.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.152.152.152.3.m1.1"><semantics id="S6.T10.152.152.152.3.m1.1a"><mi id="S6.T10.152.152.152.3.m1.1.1" mathvariant="normal" xref="S6.T10.152.152.152.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.152.152.152.3.m1.1b"><ci id="S6.T10.152.152.152.3.m1.1.1.cmml" xref="S6.T10.152.152.152.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.152.152.152.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.152.152.152.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.156.156.156.14"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.153.153.153.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.153.153.153.4.m1.1"><semantics id="S6.T10.153.153.153.4.m1.1a"><mi id="S6.T10.153.153.153.4.m1.1.1" mathvariant="normal" xref="S6.T10.153.153.153.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.153.153.153.4.m1.1b"><ci id="S6.T10.153.153.153.4.m1.1.1.cmml" xref="S6.T10.153.153.153.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.153.153.153.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.153.153.153.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.154.154.154.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.154.154.154.5.m1.1"><semantics id="S6.T10.154.154.154.5.m1.1a"><mi id="S6.T10.154.154.154.5.m1.1.1" mathvariant="normal" xref="S6.T10.154.154.154.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.154.154.154.5.m1.1b"><ci id="S6.T10.154.154.154.5.m1.1.1.cmml" xref="S6.T10.154.154.154.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.154.154.154.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.154.154.154.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.155.155.155.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.155.155.155.6.m1.1"><semantics id="S6.T10.155.155.155.6.m1.1a"><mi id="S6.T10.155.155.155.6.m1.1.1" mathvariant="normal" xref="S6.T10.155.155.155.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.155.155.155.6.m1.1b"><ci id="S6.T10.155.155.155.6.m1.1.1.cmml" xref="S6.T10.155.155.155.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.155.155.155.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.155.155.155.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T10.156.156.156.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.156.156.156.7.m1.1"><semantics id="S6.T10.156.156.156.7.m1.1a"><mi id="S6.T10.156.156.156.7.m1.1.1" mathvariant="normal" xref="S6.T10.156.156.156.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.156.156.156.7.m1.1b"><ci id="S6.T10.156.156.156.7.m1.1.1.cmml" xref="S6.T10.156.156.156.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.156.156.156.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.156.156.156.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S6.T10.156.156.156.15"></td>
</tr>
<tr class="ltx_tr" id="S6.T10.167.167.167">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.167.167.167.12">PaLM-2</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.167.167.167.13" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T10.167.167.167.13.1">Web documents, Code, Books, Maths, Conversation</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.167.167.167.14"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.167.167.167.15"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.157.157.157.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.157.157.157.1.m1.1"><semantics id="S6.T10.157.157.157.1.m1.1a"><mi id="S6.T10.157.157.157.1.m1.1.1" mathvariant="normal" xref="S6.T10.157.157.157.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.157.157.157.1.m1.1b"><ci id="S6.T10.157.157.157.1.m1.1.1.cmml" xref="S6.T10.157.157.157.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.157.157.157.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.157.157.157.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.158.158.158.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.158.158.158.2.m1.1"><semantics id="S6.T10.158.158.158.2.m1.1a"><mi id="S6.T10.158.158.158.2.m1.1.1" mathvariant="normal" xref="S6.T10.158.158.158.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.158.158.158.2.m1.1b"><ci id="S6.T10.158.158.158.2.m1.1.1.cmml" xref="S6.T10.158.158.158.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.158.158.158.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.158.158.158.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.159.159.159.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.159.159.159.3.m1.1"><semantics id="S6.T10.159.159.159.3.m1.1a"><mi id="S6.T10.159.159.159.3.m1.1.1" mathvariant="normal" xref="S6.T10.159.159.159.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.159.159.159.3.m1.1b"><ci id="S6.T10.159.159.159.3.m1.1.1.cmml" xref="S6.T10.159.159.159.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.159.159.159.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.159.159.159.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.160.160.160.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.160.160.160.4.m1.1"><semantics id="S6.T10.160.160.160.4.m1.1a"><mi id="S6.T10.160.160.160.4.m1.1.1" mathvariant="normal" xref="S6.T10.160.160.160.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.160.160.160.4.m1.1b"><ci id="S6.T10.160.160.160.4.m1.1.1.cmml" xref="S6.T10.160.160.160.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.160.160.160.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.160.160.160.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.161.161.161.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.161.161.161.5.m1.1"><semantics id="S6.T10.161.161.161.5.m1.1a"><mi id="S6.T10.161.161.161.5.m1.1.1" mathvariant="normal" xref="S6.T10.161.161.161.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.161.161.161.5.m1.1b"><ci id="S6.T10.161.161.161.5.m1.1.1.cmml" xref="S6.T10.161.161.161.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.161.161.161.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.161.161.161.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.162.162.162.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.162.162.162.6.m1.1"><semantics id="S6.T10.162.162.162.6.m1.1a"><mi id="S6.T10.162.162.162.6.m1.1.1" mathvariant="normal" xref="S6.T10.162.162.162.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.162.162.162.6.m1.1b"><ci id="S6.T10.162.162.162.6.m1.1.1.cmml" xref="S6.T10.162.162.162.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.162.162.162.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.162.162.162.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.163.163.163.7"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.163.163.163.7.m1.1"><semantics id="S6.T10.163.163.163.7.m1.1a"><mi id="S6.T10.163.163.163.7.m1.1.1" mathvariant="normal" xref="S6.T10.163.163.163.7.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.163.163.163.7.m1.1b"><ci id="S6.T10.163.163.163.7.m1.1.1.cmml" xref="S6.T10.163.163.163.7.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.163.163.163.7.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.163.163.163.7.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.164.164.164.8"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.164.164.164.8.m1.1"><semantics id="S6.T10.164.164.164.8.m1.1a"><mi id="S6.T10.164.164.164.8.m1.1.1" mathvariant="normal" xref="S6.T10.164.164.164.8.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.164.164.164.8.m1.1b"><ci id="S6.T10.164.164.164.8.m1.1.1.cmml" xref="S6.T10.164.164.164.8.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.164.164.164.8.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.164.164.164.8.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.165.165.165.9"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.165.165.165.9.m1.1"><semantics id="S6.T10.165.165.165.9.m1.1a"><mi id="S6.T10.165.165.165.9.m1.1.1" mathvariant="normal" xref="S6.T10.165.165.165.9.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.165.165.165.9.m1.1b"><ci id="S6.T10.165.165.165.9.m1.1.1.cmml" xref="S6.T10.165.165.165.9.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.165.165.165.9.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.165.165.165.9.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.166.166.166.10"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.166.166.166.10.m1.1"><semantics id="S6.T10.166.166.166.10.m1.1a"><mi id="S6.T10.166.166.166.10.m1.1.1" mathvariant="normal" xref="S6.T10.166.166.166.10.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.166.166.166.10.m1.1b"><ci id="S6.T10.166.166.166.10.m1.1.1.cmml" xref="S6.T10.166.166.166.10.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.166.166.166.10.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.166.166.166.10.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T10.167.167.167.11"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T10.167.167.167.11.m1.1"><semantics id="S6.T10.167.167.167.11.m1.1a"><mi id="S6.T10.167.167.167.11.m1.1.1" mathvariant="normal" xref="S6.T10.167.167.167.11.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T10.167.167.167.11.m1.1b"><ci id="S6.T10.167.167.167.11.m1.1.1.cmml" xref="S6.T10.167.167.167.11.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T10.167.167.167.11.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T10.167.167.167.11.m1.1d">✓</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XI: </span>An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T11.16.1">“SNI”</span> is a short of Super-NaturalInsturctions.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T11.14" style="width:433.6pt;height:99.9pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-251.0pt,57.5pt) scale(0.463488156071234,0.463488156071234) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T11.14.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T11.14.14.15.1" style="background-color:#BFBFBF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.1.1" style="background-color:#BFBFBF;">Models</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.2" style="width:142.3pt;">
<table class="ltx_tabular ltx_align_top" id="S6.T11.14.14.15.1.2.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T11.14.14.15.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.2.1.1.1.1" style="background-color:#BFBFBF;">Training Dataset</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T11.14.14.15.1.3.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T11.14.14.15.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.3.1.1.1.1" style="background-color:#BFBFBF;">BIG-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T11.14.14.15.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.3.1.2.1.1">bench</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.4"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.4.1" style="background-color:#BFBFBF;">MMLU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.5"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.5.1" style="background-color:#BFBFBF;">BBH</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.6"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.6.1" style="background-color:#BFBFBF;">RAFT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.7"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.7.1" style="background-color:#BFBFBF;">FLAN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.8"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.8.1" style="background-color:#BFBFBF;">SNI</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.9"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.9.1" style="background-color:#BFBFBF;">PromptSource</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.10"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.10.1" style="background-color:#BFBFBF;">TyDiQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.11"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.11.1" style="background-color:#BFBFBF;">HumanEval</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T11.14.14.15.1.12"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.12.1" style="background-color:#BFBFBF;">MBPP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T11.14.14.15.1.13">
<table class="ltx_tabular ltx_align_middle" id="S6.T11.14.14.15.1.13.1" style="background-color:#BFBFBF;">
<tbody><tr class="ltx_tr" id="S6.T11.14.14.15.1.13.1.1">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.13.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.13.1.1.1.1" style="background-color:#BFBFBF;">Truthful/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T11.14.14.15.1.13.1.2">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.13.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.13.1.2.1.1">Bias/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T11.14.14.15.1.13.1.3">
<td class="ltx_td ltx_align_center" id="S6.T11.14.14.15.1.13.1.3.1"><span class="ltx_text ltx_font_bold" id="S6.T11.14.14.15.1.13.1.3.1.1">Toxicity</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T11.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.2">T0</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.1.1.1.3.1">Pool of Prompts</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.1.1.1.1.m1.1"><semantics id="S6.T11.1.1.1.1.m1.1a"><mi id="S6.T11.1.1.1.1.m1.1.1" mathvariant="normal" xref="S6.T11.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.1.1.1.1.m1.1b"><ci id="S6.T11.1.1.1.1.m1.1.1.cmml" xref="S6.T11.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.1.1.1.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.4"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.5"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.6"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.7"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.8"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.9"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.10"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.11"></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="S6.T11.1.1.1.12"></td>
<td class="ltx_td ltx_border_tt" id="S6.T11.1.1.1.13"></td>
</tr>
<tr class="ltx_tr" id="S6.T11.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.2.2.2.2">WebGPT</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T11.2.2.2.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.2.2.2.3.1">ELI5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib426" title="">426</a>]</cite>, ELI5 fact-check&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib157" title="">157</a>]</cite>, TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib341" title="">341</a>]</cite>, ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>, ARC-Easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>, Hand-written data, Demonstrations of humans, Comparisons between model-generated answers</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.2.2.2.13"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T11.2.2.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.2.2.2.1.m1.1"><semantics id="S6.T11.2.2.2.1.m1.1a"><mi id="S6.T11.2.2.2.1.m1.1.1" mathvariant="normal" xref="S6.T11.2.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.2.2.2.1.m1.1b"><ci id="S6.T11.2.2.2.1.m1.1.1.cmml" xref="S6.T11.2.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.2.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.2.2.2.1.m1.1d">✓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T11.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.3.3.3.2">Tk-INSTRUCT</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T11.3.3.3.3" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.3.3.3.3.1">SNI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.3.3.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.3.3.3.1.m1.1"><semantics id="S6.T11.3.3.3.1.m1.1a"><mi id="S6.T11.3.3.3.1.m1.1.1" mathvariant="normal" xref="S6.T11.3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.3.3.3.1.m1.1b"><ci id="S6.T11.3.3.3.1.m1.1.1.cmml" xref="S6.T11.3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.3.3.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.3.3.3.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.3.3.3.12"></td>
<td class="ltx_td ltx_border_t" id="S6.T11.3.3.3.13"></td>
</tr>
<tr class="ltx_tr" id="S6.T11.14.14.16.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.1">mT0</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.2" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.14.14.16.1.2.1">xP3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib145" title="">145</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.14.14.16.1.12"></td>
<td class="ltx_td ltx_border_t" id="S6.T11.14.14.16.1.13"></td>
</tr>
<tr class="ltx_tr" id="S6.T11.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.9.9.9.7">OPT-IML</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T11.9.9.9.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.9.9.9.8.1">PromptSource&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib17" title="">17</a>]</cite>, FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>, SNI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib427" title="">427</a>]</cite>, UnifiedSKG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib428" title="">428</a>]</cite>, CrossFit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib429" title="">429</a>]</cite>, ExMix&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib430" title="">430</a>]</cite>, T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite>, Reasoning</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.9.9.9.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.4.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.4.4.4.1.m1.1"><semantics id="S6.T11.4.4.4.1.m1.1a"><mi id="S6.T11.4.4.4.1.m1.1.1" mathvariant="normal" xref="S6.T11.4.4.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.4.4.4.1.m1.1b"><ci id="S6.T11.4.4.4.1.m1.1.1.cmml" xref="S6.T11.4.4.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.4.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.4.4.4.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.5.5.5.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.5.5.5.2.m1.1"><semantics id="S6.T11.5.5.5.2.m1.1a"><mi id="S6.T11.5.5.5.2.m1.1.1" mathvariant="normal" xref="S6.T11.5.5.5.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.5.5.5.2.m1.1b"><ci id="S6.T11.5.5.5.2.m1.1.1.cmml" xref="S6.T11.5.5.5.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.5.5.5.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.5.5.5.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.6.6.6.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.6.6.6.3.m1.1"><semantics id="S6.T11.6.6.6.3.m1.1a"><mi id="S6.T11.6.6.6.3.m1.1.1" mathvariant="normal" xref="S6.T11.6.6.6.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.6.6.6.3.m1.1b"><ci id="S6.T11.6.6.6.3.m1.1.1.cmml" xref="S6.T11.6.6.6.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.6.6.6.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.6.6.6.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.7.7.7.4"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.7.7.7.4.m1.1"><semantics id="S6.T11.7.7.7.4.m1.1a"><mi id="S6.T11.7.7.7.4.m1.1.1" mathvariant="normal" xref="S6.T11.7.7.7.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.7.7.7.4.m1.1b"><ci id="S6.T11.7.7.7.4.m1.1.1.cmml" xref="S6.T11.7.7.7.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.7.7.7.4.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.7.7.7.4.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.8.8.8.5"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.8.8.8.5.m1.1"><semantics id="S6.T11.8.8.8.5.m1.1a"><mi id="S6.T11.8.8.8.5.m1.1.1" mathvariant="normal" xref="S6.T11.8.8.8.5.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.8.8.8.5.m1.1b"><ci id="S6.T11.8.8.8.5.m1.1.1.cmml" xref="S6.T11.8.8.8.5.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.8.8.8.5.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.8.8.8.5.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.9.9.9.6"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.9.9.9.6.m1.1"><semantics id="S6.T11.9.9.9.6.m1.1a"><mi id="S6.T11.9.9.9.6.m1.1.1" mathvariant="normal" xref="S6.T11.9.9.9.6.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.9.9.9.6.m1.1b"><ci id="S6.T11.9.9.9.6.m1.1.1.cmml" xref="S6.T11.9.9.9.6.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.9.9.9.6.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.9.9.9.6.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.9.9.9.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.9.9.9.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.9.9.9.12"></td>
<td class="ltx_td ltx_border_t" id="S6.T11.9.9.9.13"></td>
</tr>
<tr class="ltx_tr" id="S6.T11.12.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T11.12.12.12.4">Flan</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T11.12.12.12.5" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.12.12.12.5.1">Muffin, T0-SF, NIv2, CoT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.10.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.10.10.10.1.m1.1"><semantics id="S6.T11.10.10.10.1.m1.1a"><mi id="S6.T11.10.10.10.1.m1.1.1" mathvariant="normal" xref="S6.T11.10.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.10.10.10.1.m1.1b"><ci id="S6.T11.10.10.10.1.m1.1.1.cmml" xref="S6.T11.10.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.10.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.10.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.11.11.11.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.11.11.11.2.m1.1"><semantics id="S6.T11.11.11.11.2.m1.1a"><mi id="S6.T11.11.11.11.2.m1.1.1" mathvariant="normal" xref="S6.T11.11.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.11.11.11.2.m1.1b"><ci id="S6.T11.11.11.11.2.m1.1.1.cmml" xref="S6.T11.11.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.11.11.11.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.11.11.11.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T11.12.12.12.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.12.12.12.3.m1.1"><semantics id="S6.T11.12.12.12.3.m1.1a"><mi id="S6.T11.12.12.12.3.m1.1.1" mathvariant="normal" xref="S6.T11.12.12.12.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.12.12.12.3.m1.1b"><ci id="S6.T11.12.12.12.3.m1.1.1.cmml" xref="S6.T11.12.12.12.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.12.12.12.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.12.12.12.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T11.12.12.12.12"></td>
<td class="ltx_td ltx_border_t" id="S6.T11.12.12.12.13"></td>
</tr>
<tr class="ltx_tr" id="S6.T11.14.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.3">WizardCoder</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.4" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S6.T11.14.14.14.4.1">Code Alpaca</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.5"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.6"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.7"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.8"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.9"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.10"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.11"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.12"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.13.13.13.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.13.13.13.1.m1.1"><semantics id="S6.T11.13.13.13.1.m1.1a"><mi id="S6.T11.13.13.13.1.m1.1.1" mathvariant="normal" xref="S6.T11.13.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.13.13.13.1.m1.1b"><ci id="S6.T11.13.13.13.1.m1.1.1.cmml" xref="S6.T11.13.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.13.13.13.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.13.13.13.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T11.14.14.14.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S6.T11.14.14.14.2.m1.1"><semantics id="S6.T11.14.14.14.2.m1.1a"><mi id="S6.T11.14.14.14.2.m1.1.1" mathvariant="normal" xref="S6.T11.14.14.14.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S6.T11.14.14.14.2.m1.1b"><ci id="S6.T11.14.14.14.2.m1.1.1.cmml" xref="S6.T11.14.14.14.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T11.14.14.14.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S6.T11.14.14.14.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S6.T11.14.14.14.13"></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS1.5.1.1">VI-B</span>1 </span>Multi-task</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib307" title="">307</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.Px1.p1.1">A benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 subjects, testing both world knowledge and problem-solving ability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">SuperGLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib2" title="">2</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS1.Px2.p1.1">A more challenging and diverse successor to the GLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib309" title="">309</a>]</cite> benchmark, SuperGLUE includes a variety of language understanding tasks, such as question answering, natural language inference, and coreference resolution. It is designed to provide a rigorous test of language understanding and requires significant progress in areas like sample-efficient, transfer, multitasking, and unsupervised or self-supervised learning. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">BIG-bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib308" title="">308</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS1.Px3.p1.1">The BIG-bench (Behavior of Intelligent Generative Models Benchmark) is a large-scale benchmark designed to test the abilities of LLMs across a wide range of tasks, including reasoning, creativity, ethics, and understanding of specific domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">GLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib309" title="">309</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS1.Px4.p1.1">The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. It includes a variety of tasks that test a wide range of linguistic phenomena, making it a comprehensive tool for evaluating language understanding in AI.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS2.5.1.1">VI-B</span>2 </span>Language Understanding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib354" title="">354</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px1.p1.1">A large-scale dataset inspired by the original Winograd&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib357" title="">357</a>]</cite> Schema Challenge tests models on their ability to resolve pronoun ambiguity and encourages the development of models that understand the broad context in natural language text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">CoQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib316" title="">316</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px2.p1.1">A conversational question-answering dataset, CoQA challenges models with questions that rely on conversation history and require free-form text answers. Its diverse content from seven domains makes it a rigorous test for models’ ability to handle a wide range of topics and conversational contexts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">WiC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib317" title="">317</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px3.p1.1">This dataset assesses a model’s ability to discern word meanings based on context, aiding in tasks related to Word Sense Disambiguation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Wikitext103&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib318" title="">318</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px4.p1.1">With over 100 million tokens from Wikipedia’s top articles, this dataset is a rich resource for tasks that require understanding long-term dependencies, such as language modeling and translation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px5">
<h5 class="ltx_title ltx_title_paragraph">PG19&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib319" title="">319</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px5.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px5.p1.1">This is a digital library of diverse books from Project Gutenberg. It’s specifically designed to facilitate research in unsupervised learning and language modeling, with a special focus on long-form content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px6">
<h5 class="ltx_title ltx_title_paragraph">C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px6.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px6.p1.1">A clean, multilingual dataset, C4 offers billions of tokens from web-crawled data. It’s a comprehensive resource for training advanced Transformer models on various languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS2.Px7">
<h5 class="ltx_title ltx_title_paragraph">LCQMC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib320" title="">320</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.Px7.p1">
<p class="ltx_p" id="S6.SS2.SSS2.Px7.p1.1">The Large-scale Chinese Question Matching Corpus (LCQMC) is a dataset for evaluating the performance of models in semantic matching tasks. It contains pairs of questions in Chinese and their matching status, making it a valuable resource for research in Chinese language understanding.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS3.5.1.1">VI-B</span>3 </span>Story Cloze and Sentence Completion</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">StoryCloze&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib334" title="">334</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS3.Px1.p1.1">It introduces a new <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.SS2.SSS3.Px1.p1.1.1">“StoryCloze Test”</span>, a commonsense reasoning framework for evaluating story understanding, generation, and script learning. It considers a model’s ability to understand and generate coherent and sensible stories.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib335" title="">335</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS3.Px2.p1.1">This dataset evaluates contextual text understanding through a word prediction task. Models must predict the last word of a passage, which is easy for humans when given the whole passage, but not when given only the last sentence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS4.5.1.1">VI-B</span>4 </span>Physical Knowledge and World Understanding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS4.Px1">
<h5 class="ltx_title ltx_title_paragraph">PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib340" title="">340</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS4.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS4.Px1.p1.1">A dataset that probes the physical knowledge of models, aiming to understand how well they are learning about the real world.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS4.Px2">
<h5 class="ltx_title ltx_title_paragraph">TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib341" title="">341</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS4.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS4.Px2.p1.1">A dataset that tests models on reading comprehension and open domain question answering (QA) tasks, with a focus on Information Retrieval (IR)-style QA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS4.Px3">
<h5 class="ltx_title ltx_title_paragraph">ARC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS4.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS4.Px3.p1.1">A larger version of the ARC-Challenge, this dataset contains both easy and challenging grade-school level, multiple-choice science questions. It’s a comprehensive test of a model’s ability to understand and answer complex questions.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS4.Px4">
<h5 class="ltx_title ltx_title_paragraph">ARC-Easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS4.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.Px4.p1.1">A subset of the ARC dataset, ARC-Easy, contains questions that are answered correctly by either a retrieval-based algorithm or a word co-occurrence algorithm. It’s a great starting point for models beginning to explore advanced question-answering.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS4.Px5">
<h5 class="ltx_title ltx_title_paragraph">ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib342" title="">342</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS4.Px5.p1">
<p class="ltx_p" id="S6.SS2.SSS4.Px5.p1.1">A rigorous question-answering dataset, ARC-Challenge includes complex, grade-school level questions that demand reasoning beyond simple retrieval, testing the true comprehension capabilities of models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS5.5.1.1">VI-B</span>5 </span>Contextual Language Understanding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS5.Px1">
<h5 class="ltx_title ltx_title_paragraph">RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS5.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS5.Px1.p1.1">The RACE is a reading comprehension dataset collected from English examinations in China, which benchmarks AI models for understanding and answering questions on long and complex passages, simulating the challenge of a real-world examination.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS5.Px2">
<h5 class="ltx_title ltx_title_paragraph">RACE-Middle&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS5.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS5.Px2.p1.1">Another subset of the RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite> dataset, RACE-Middle, contains middle school-level English exam questions. It offers a slightly less challenging but academically oriented evaluation of a model’s comprehension skills.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS5.Px3">
<h5 class="ltx_title ltx_title_paragraph">RACE-High&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS5.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS5.Px3.p1.1">A subset of the RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib347" title="">347</a>]</cite> dataset, RACE-High consists of high school-level English exam questions. It is designed to evaluate the comprehension ability of models in a more academic and challenging context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS5.Px4">
<h5 class="ltx_title ltx_title_paragraph">QuAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib348" title="">348</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS5.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS5.Px4.p1.1">This dataset simulates an information-seeking dialog between students and teachers using hidden Wikipedia text. It introduces unique challenges not found in machine comprehension datasets, making it a valuable resource for advancing dialog systems.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS6.5.1.1">VI-B</span>6 </span>Commonsense Reasoning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS6.Px1">
<h5 class="ltx_title ltx_title_paragraph">HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib355" title="">355</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS6.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS6.Px1.p1.1">A dataset that challenges models to pick the best ending to a context uses Adversarial Filtering to create a ‘Goldilocks’ zone of complexity, where generated text is absurd to humans but often misclassified by models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS6.Px2">
<h5 class="ltx_title ltx_title_paragraph">COPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib402" title="">402</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS6.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS6.Px2.p1.1">This dataset evaluates a model’s progress in open-domain commonsense causal reasoning. Each question comprises a premise and two alternatives, and the model must select the more plausible alternative, testing a model’s ability to understand and reason about cause and effect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS6.Px3">
<h5 class="ltx_title ltx_title_paragraph">WSC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib357" title="">357</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS6.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS6.Px3.p1.1">The Winograd Schema Challenge (WSC) is a reading comprehension task in which a system must resolve references in a text, often requiring world knowledge and reasoning about the text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS6.Px4">
<h5 class="ltx_title ltx_title_paragraph">CSQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib358" title="">358</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS6.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS6.Px4.p1.1">The CommonsenseQA is a question-answering dataset that requires commonsense knowledge to answer the ability of AI models to understand and answer questions that require commonsense reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS7.5.1.1">VI-B</span>7 </span>Reading Comprehension</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px1">
<h5 class="ltx_title ltx_title_paragraph">BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib363" title="">363</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px1.p1.1">A dataset derived from Google search queries, BoolQ challenges models to answer binary (yes/no) questions. The questions are naturally occurring and are paired with a paragraph from a Wikipedia article containing the answer. It’s a test of reading comprehension and reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px2">
<h5 class="ltx_title ltx_title_paragraph">SQUADv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib364" title="">364</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px2.p1.1">The Stanford Question Answering Dataset (SQuAD)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib362" title="">362</a>]</cite> is a collection of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text from the corresponding reading passage. SQuADv2 combines the original SQuAD1.1 dataset with over 50,000 unanswerable questions. The aim is to evaluate a model’s ability to understand and answer questions based on a given context and to determine when a question is unanswerable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px3">
<h5 class="ltx_title ltx_title_paragraph">DROP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib365" title="">365</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px3.p1.1">DROP, or Discrete Reasoning Over the content of Paragraphs, is designed to test a model’s ability to understand a wide variety of reading phenomena. It encourages comprehensive and reliable evaluation of reading comprehension capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px4">
<h5 class="ltx_title ltx_title_paragraph">RTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib366" title="">366</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px4.p1.1">The Recognizing Textual Entailment (RTE) datasets come from a series of annual competitions on textual entailment, predicting whether a given sentence logically follows from another and evaluating a model’s understanding of logical relationships in a text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px5">
<h5 class="ltx_title ltx_title_paragraph">WebQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib367" title="">367</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px5.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px5.p1.1">A dataset for open-domain question answering, WebQA offers a large collection of web-based question-answer pairs. It is designed to assess the ability of AI models to understand and answer questions based on web content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS7.Px6">
<h5 class="ltx_title ltx_title_paragraph">CMRC2018&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib369" title="">369</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS7.Px6.p1">
<p class="ltx_p" id="S6.SS2.SSS7.Px6.p1.1">This dataset is a test of Chinese language models’ ability to reason comprehensively and is designed with a challenging span-extraction format that pushes the boundaries of machine performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS8.5.1.1">VI-B</span>8 </span>Mathematical Reasoning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS8.Px1">
<h5 class="ltx_title ltx_title_paragraph">MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib382" title="">382</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS8.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS8.Px1.p1.1">This dataset is a platform for evaluating the mathematical problem-solving abilities of AI models. It contains a diverse set of math problems, ranging from arithmetic to calculus, and is designed to test the model’s ability to understand and solve complex mathematical problems.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS8.Px2">
<h5 class="ltx_title ltx_title_paragraph">Math23k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib383" title="">383</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS8.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS8.Px2.p1.1">This one challenges a model’s ability to understand and solve mathematical word problems. It contains 23,000 Chinese arithmetic word problems that require models to perform reasoning and computation based on the problem description.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS8.Px3">
<h5 class="ltx_title ltx_title_paragraph">GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib384" title="">384</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS8.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS8.Px3.p1.1">A dataset of diverse grade school math word problems, testing a model’s ability to perform multi-step mathematical reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS9.5.1.1">VI-B</span>9 </span>Problem Solving and Logical Reasoning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS9.Px1">
<h5 class="ltx_title ltx_title_paragraph">ANLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib394" title="">394</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS9.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS9.Px1.p1.1">A large-scale dataset designed to test the robustness of machine learning models in Natural Language Inference (NLI) is created through an iterative, adversarial process where humans try to generate examples that models cannot correctly classify.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS9.Px2">
<h5 class="ltx_title ltx_title_paragraph">HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib391" title="">391</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS9.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS9.Px2.p1.1">A dataset for the problem-solving ability of AI models, which includes a diverse set of tasks that require various cognitive abilities, makes it a comprehensive tool for assessing general intelligence in AI.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS9.Px3">
<h5 class="ltx_title ltx_title_paragraph">StrategyQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib349" title="">349</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS9.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS9.Px3.p1.1">A question-answering dataset that requires reasoning over multiple pieces of evidence to evaluate the strategic reasoning ability of AI models, pushing the boundaries of what machines can understand and answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS10">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS10.5.1.1">VI-B</span>10 </span>Cross-Lingual Understanding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS10.Px1">
<h5 class="ltx_title ltx_title_paragraph">XNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib399" title="">399</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS10.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS10.Px1.p1.1">A cross-lingual benchmark, XNLI extends the MultiNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib431" title="">431</a>]</cite> corpus to 15 languages, including low-resource ones like Urdu. It tests models on cross-lingual sentence understanding, with 112,500 annotated pairs across three categories: entailment, contradiction, and neutral.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS10.Px2">
<h5 class="ltx_title ltx_title_paragraph">PAWS-X&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib400" title="">400</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS10.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS10.Px2.p1.1">PAWS-X, or Cross-lingual Paraphrase Adversaries from Word Scrambling, is a multilingual version of the PAWS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib432" title="">432</a>]</cite> dataset for paraphrase identification. It includes examples in seven languages and is designed to evaluate the performance of cross-lingual paraphrase identification models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS11">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS11.5.1.1">VI-B</span>11 </span>Truthfulness</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS11.Px1">
<h5 class="ltx_title ltx_title_paragraph">Truthful-QA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib406" title="">406</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS11.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS11.Px1.p1.1">A unique benchmark that measures a language model’s truthfulness when generating answers. The dataset includes questions across various categories like health, law, and politics, some designed to test the model against common human misconceptions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS12">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS2.SSS12.5.1.1">VI-B</span>12 </span>Biases and Ethics in AI</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS12.Px1">
<h5 class="ltx_title ltx_title_paragraph">ETHOS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib409" title="">409</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS12.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS12.Px1.p1.1">ETHOS is a hate speech detection dataset built from YouTube and Reddit comments. It’s a tool in the fight against online hate speech, offering binary and multi-label variants for robust content moderation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS12.Px2">
<h5 class="ltx_title ltx_title_paragraph">StereoSet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib410" title="">410</a>]</cite>
</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS12.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS12.Px2.p1.1">StereoSet is a comprehensive dataset designed to measure and evaluate the presence of stereotypical biases in language models. It focuses on four key domains: gender, profession, race, and religion. Contrasting stereotypical bias against language modeling ability provides a valuable tool for understanding and mitigating biases in large language models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XII: </span>Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T12.8.1">“N-Shots”</span> indicate the number of example prompts provided to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T12.9.2">“f”</span> represents the fine-tuned version, and <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S6.T12.10.3">“B”</span> represents the benchmark.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T12.4" style="width:433.6pt;height:731.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.9pt,30.2pt) scale(0.923674777075699,0.923674777075699) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T12.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T12.4.4.5.1" style="background-color:#BFBFBF;">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.5.1.1"><span class="ltx_text" id="S6.T12.4.4.5.1.1.1" style="background-color:#BFBFBF;">Task</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.5.1.2"><span class="ltx_text" id="S6.T12.4.4.5.1.2.1" style="background-color:#BFBFBF;">Dataset/Benchmark</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.5.1.3"><span class="ltx_text" id="S6.T12.4.4.5.1.3.1" style="background-color:#BFBFBF;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.5.1.4"><span class="ltx_text" id="S6.T12.4.4.5.1.4.1" style="background-color:#BFBFBF;">Model Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.5.1.5"><span class="ltx_text" id="S6.T12.4.4.5.1.5.1" style="background-color:#BFBFBF;">N-Shots</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.5.1.6"><span class="ltx_text" id="S6.T12.4.4.5.1.6.1" style="background-color:#BFBFBF;">Score</span></td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.6.2.1" rowspan="2"><span class="ltx_text" id="S6.T12.4.4.6.2.1.1">Multi-Task</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.6.2.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.6.2.2.1">BIG-bench (B)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.6.2.3">Chinchilla</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.6.2.4">70B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.6.2.5">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.6.2.6">65.1</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.7.3.1">Gopher</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.7.3.2">280B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.7.3.3">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.7.3.4">53.97</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.8.4">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.8.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.8.4.2">PaLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.8.4.3">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.8.4.4">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.8.4.5">53.7</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.9.5">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.9.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.9.5.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.9.5.2.1">MMLU (B)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.9.5.3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.9.5.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.9.5.5">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.9.5.6">86.4</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.10.6">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.10.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.10.6.2">Gemini</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.10.6.3">Ultra</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.10.6.4">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.10.6.5">83.7</td>
</tr>
<tr class="ltx_tr" id="S6.T12.1.1.1">
<td class="ltx_td ltx_border_r" id="S6.T12.1.1.1.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.1.1.1.1">Flan-PaLM-2<math alttext="{}_{(f)}" class="ltx_Math" display="inline" id="S6.T12.1.1.1.1.m1.1"><semantics id="S6.T12.1.1.1.1.m1.1a"><msub id="S6.T12.1.1.1.1.m1.1.1" xref="S6.T12.1.1.1.1.m1.1.1.cmml"><mi id="S6.T12.1.1.1.1.m1.1.1a" xref="S6.T12.1.1.1.1.m1.1.1.cmml"></mi><mrow id="S6.T12.1.1.1.1.m1.1.1.1.3" xref="S6.T12.1.1.1.1.m1.1.1.cmml"><mo id="S6.T12.1.1.1.1.m1.1.1.1.3.1" stretchy="false" xref="S6.T12.1.1.1.1.m1.1.1.cmml">(</mo><mi id="S6.T12.1.1.1.1.m1.1.1.1.1" xref="S6.T12.1.1.1.1.m1.1.1.1.1.cmml">f</mi><mo id="S6.T12.1.1.1.1.m1.1.1.1.3.2" stretchy="false" xref="S6.T12.1.1.1.1.m1.1.1.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.T12.1.1.1.1.m1.1b"><apply id="S6.T12.1.1.1.1.m1.1.1.cmml" xref="S6.T12.1.1.1.1.m1.1.1"><ci id="S6.T12.1.1.1.1.m1.1.1.1.1.cmml" xref="S6.T12.1.1.1.1.m1.1.1.1.1">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.1.1.1.1.m1.1c">{}_{(f)}</annotation><annotation encoding="application/x-llamapun" id="S6.T12.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT ( italic_f ) end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.1.1.1.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.1.1.1.4">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.1.1.1.5">81.2</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.11.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.11.7.1"><span class="ltx_text" id="S6.T12.4.4.11.7.1.1">Language Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.11.7.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.11.7.2.1">SuperGLUE (B)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.11.7.3">ERNIE 3.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.11.7.4">12B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.11.7.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.11.7.6">90.6</td>
</tr>
<tr class="ltx_tr" id="S6.T12.2.2.2">
<td class="ltx_td ltx_border_r" id="S6.T12.2.2.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.2.2.2.1">PaLM<math alttext="{}_{(f)}" class="ltx_Math" display="inline" id="S6.T12.2.2.2.1.m1.1"><semantics id="S6.T12.2.2.2.1.m1.1a"><msub id="S6.T12.2.2.2.1.m1.1.1" xref="S6.T12.2.2.2.1.m1.1.1.cmml"><mi id="S6.T12.2.2.2.1.m1.1.1a" xref="S6.T12.2.2.2.1.m1.1.1.cmml"></mi><mrow id="S6.T12.2.2.2.1.m1.1.1.1.3" xref="S6.T12.2.2.2.1.m1.1.1.cmml"><mo id="S6.T12.2.2.2.1.m1.1.1.1.3.1" stretchy="false" xref="S6.T12.2.2.2.1.m1.1.1.cmml">(</mo><mi id="S6.T12.2.2.2.1.m1.1.1.1.1" xref="S6.T12.2.2.2.1.m1.1.1.1.1.cmml">f</mi><mo id="S6.T12.2.2.2.1.m1.1.1.1.3.2" stretchy="false" xref="S6.T12.2.2.2.1.m1.1.1.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.T12.2.2.2.1.m1.1b"><apply id="S6.T12.2.2.2.1.m1.1.1.cmml" xref="S6.T12.2.2.2.1.m1.1.1"><ci id="S6.T12.2.2.2.1.m1.1.1.1.1.cmml" xref="S6.T12.2.2.2.1.m1.1.1.1.1">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.2.2.2.1.m1.1c">{}_{(f)}</annotation><annotation encoding="application/x-llamapun" id="S6.T12.2.2.2.1.m1.1d">start_FLOATSUBSCRIPT ( italic_f ) end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.2.2.2.3">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.2.2.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.2.2.2.5">90.4</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.12.8">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.12.8.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.12.8.2">T5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.12.8.3">11B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.12.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.12.8.5">88.9</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.13.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.13.9.1" rowspan="2"><span class="ltx_text" id="S6.T12.4.4.13.9.1.1">Story Comprehension and Generation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.13.9.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.13.9.2.1">HellaSwag</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.13.9.3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.13.9.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.13.9.5">10-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.13.9.6">95.3</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.14.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.14.10.1">Gemini</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.14.10.2">Ultra</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.14.10.3">10-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.14.10.4">87.8</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.15.11">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.15.11.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.15.11.2">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.15.11.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.15.11.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.15.11.5">86.8</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.16.12">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.16.12.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.16.12.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.16.12.2.1">StoryCloze</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.16.12.3">GPT3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.16.12.4">175B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.16.12.5">few shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.16.12.6">87.7</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.17.13">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.17.13.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.17.13.2">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.17.13.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.17.13.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.17.13.5">87.4</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.18.14">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.18.14.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.18.14.2">OPT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.18.14.3">175B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.18.14.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.18.14.5">79.82</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.19.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.19.15.1" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.19.15.1.1">Physical Knowledge and World Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.19.15.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.19.15.2.1">PIQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.19.15.3">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.19.15.4">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.19.15.5">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.19.15.6">85.0</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.20.16">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.20.16.1">LLaMa</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.20.16.2">65B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.20.16.3">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.20.16.4">82.8</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.21.17">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.21.17.1">MT-NLG</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.21.17.2">530B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.21.17.3">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.21.17.4">81.99</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.22.18">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.22.18.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.22.18.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.22.18.2.1">TriviaQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.22.18.3">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.22.18.4">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.22.18.5">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.22.18.6">86.1</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.23.19">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.23.19.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.23.19.2">LLaMA-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.23.19.3">70B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.23.19.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.23.19.5">85.0</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.24.20">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.24.20.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.24.20.2">PaLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.24.20.3">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.24.20.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.24.20.5">81.4</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.25.21">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.25.21.1"><span class="ltx_text" id="S6.T12.4.4.25.21.1.1">Contextual Language Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.25.21.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.25.21.2.1">LAMBADA</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.25.21.3">PaLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.25.21.4">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.25.21.5">few shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.25.21.6">89.7</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.26.22">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.26.22.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.26.22.2">MT-NLG</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.26.22.3">530B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.26.22.4">few shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.26.22.5">87.15</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.27.23">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.27.23.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.27.23.2">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.27.23.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.27.23.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.27.23.5">86.9</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.28.24">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.28.24.1" rowspan="2"><span class="ltx_text" id="S6.T12.4.4.28.24.1.1">Commonsense Reasoning</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.28.24.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.28.24.2.1">WinoGrande</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.28.24.3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.28.24.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.28.24.5">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.28.24.6">87.5</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.29.25">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.29.25.1">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.29.25.2">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.29.25.3">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.29.25.4">83.0</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.30.26">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.30.26.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.30.26.2">PaLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.30.26.3">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.30.26.4">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.30.26.5">81.1</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.31.27">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.31.27.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.31.27.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.31.27.2.1">SIQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.31.27.3">LLaMA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.31.27.4">65B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.31.27.5">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.31.27.6">52.3</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.32.28">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.32.28.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.32.28.2">Chinchilla</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.32.28.3">70B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.32.28.4">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.32.28.5">51.3</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.33.29">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.33.29.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.33.29.2">Gopher</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.33.29.3">280B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.33.29.4">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.33.29.5">50.6</td>
</tr>
<tr class="ltx_tr" id="S6.T12.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.3.3.3.2"><span class="ltx_text" id="S6.T12.3.3.3.2.1">Reading Comprehension</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.3.3.3.3" rowspan="3"><span class="ltx_text" id="S6.T12.3.3.3.3.1">BoolQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.3.3.3.1">PaLM<math alttext="{}_{(f)}" class="ltx_Math" display="inline" id="S6.T12.3.3.3.1.m1.1"><semantics id="S6.T12.3.3.3.1.m1.1a"><msub id="S6.T12.3.3.3.1.m1.1.1" xref="S6.T12.3.3.3.1.m1.1.1.cmml"><mi id="S6.T12.3.3.3.1.m1.1.1a" xref="S6.T12.3.3.3.1.m1.1.1.cmml"></mi><mrow id="S6.T12.3.3.3.1.m1.1.1.1.3" xref="S6.T12.3.3.3.1.m1.1.1.cmml"><mo id="S6.T12.3.3.3.1.m1.1.1.1.3.1" stretchy="false" xref="S6.T12.3.3.3.1.m1.1.1.cmml">(</mo><mi id="S6.T12.3.3.3.1.m1.1.1.1.1" xref="S6.T12.3.3.3.1.m1.1.1.1.1.cmml">f</mi><mo id="S6.T12.3.3.3.1.m1.1.1.1.3.2" stretchy="false" xref="S6.T12.3.3.3.1.m1.1.1.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.T12.3.3.3.1.m1.1b"><apply id="S6.T12.3.3.3.1.m1.1.1.cmml" xref="S6.T12.3.3.3.1.m1.1.1"><ci id="S6.T12.3.3.3.1.m1.1.1.1.1.cmml" xref="S6.T12.3.3.3.1.m1.1.1.1.1">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.3.3.3.1.m1.1c">{}_{(f)}</annotation><annotation encoding="application/x-llamapun" id="S6.T12.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT ( italic_f ) end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.3.3.3.4">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.3.3.3.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.3.3.3.6">92.2</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.34.30">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.34.30.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.34.30.2">T5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.34.30.3">11B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.34.30.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.34.30.5">91.2</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.35.31">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.35.31.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.35.31.2">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.35.31.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.35.31.4">one shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.35.31.5">90.9</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.36.32">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.36.32.1"><span class="ltx_text" id="S6.T12.4.4.36.32.1.1">Truthfulness</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.36.32.2">Truthful-QA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.36.32.3">LLaMA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.36.32.4">65B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.36.32.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.36.32.6">57</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.37.33">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.37.33.1" rowspan="2"><span class="ltx_text" id="S6.T12.4.4.37.33.1.1">Mathematical Reasoning</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.37.33.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.37.33.2.1">MATH</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.37.33.3">Gemini</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.37.33.4">Ultra</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.37.33.5">4-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.37.33.6">53.2</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.38.34">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.38.34.1">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.38.34.2">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.38.34.3">4-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.38.34.4">34.3</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.39.35">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.39.35.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.39.35.2">LLaMa-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.39.35.3">65B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.39.35.4">4-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.39.35.5">13.5</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.40.36">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.40.36.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.40.36.2" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.40.36.2.1">GSM8K</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.40.36.3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.40.36.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.40.36.5">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.40.36.6">92.0</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.41.37">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.41.37.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.41.37.2">PaLM-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.41.37.3">Large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.41.37.4">8-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.41.37.5">80.7</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.42.38">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.42.38.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.42.38.2">U-PaLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.42.38.3">540B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.42.38.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.42.38.5">58.5</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.4.2"><span class="ltx_text" id="S6.T12.4.4.4.2.1">Problem Solving and Logical Reasoning</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T12.4.4.4.3" rowspan="3"><span class="ltx_text" id="S6.T12.4.4.4.3.1">HumanEval</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.4.1">Gemini<math alttext="{}_{(f)}" class="ltx_Math" display="inline" id="S6.T12.4.4.4.1.m1.1"><semantics id="S6.T12.4.4.4.1.m1.1a"><msub id="S6.T12.4.4.4.1.m1.1.1" xref="S6.T12.4.4.4.1.m1.1.1.cmml"><mi id="S6.T12.4.4.4.1.m1.1.1a" xref="S6.T12.4.4.4.1.m1.1.1.cmml"></mi><mrow id="S6.T12.4.4.4.1.m1.1.1.1.3" xref="S6.T12.4.4.4.1.m1.1.1.cmml"><mo id="S6.T12.4.4.4.1.m1.1.1.1.3.1" stretchy="false" xref="S6.T12.4.4.4.1.m1.1.1.cmml">(</mo><mi id="S6.T12.4.4.4.1.m1.1.1.1.1" xref="S6.T12.4.4.4.1.m1.1.1.1.1.cmml">f</mi><mo id="S6.T12.4.4.4.1.m1.1.1.1.3.2" stretchy="false" xref="S6.T12.4.4.4.1.m1.1.1.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.T12.4.4.4.1.m1.1b"><apply id="S6.T12.4.4.4.1.m1.1.1.cmml" xref="S6.T12.4.4.4.1.m1.1.1"><ci id="S6.T12.4.4.4.1.m1.1.1.1.1.cmml" xref="S6.T12.4.4.4.1.m1.1.1.1.1">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.4.4.4.1.m1.1c">{}_{(f)}</annotation><annotation encoding="application/x-llamapun" id="S6.T12.4.4.4.1.m1.1d">start_FLOATSUBSCRIPT ( italic_f ) end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.4.4">Ultra</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.4.5">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.4.6">74.4</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.43.39">
<td class="ltx_td ltx_border_r" id="S6.T12.4.4.43.39.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T12.4.4.43.39.2">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.43.39.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T12.4.4.43.39.4">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T12.4.4.43.39.5">67.0</td>
</tr>
<tr class="ltx_tr" id="S6.T12.4.4.44.40">
<td class="ltx_td ltx_border_b ltx_border_r" id="S6.T12.4.4.44.40.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T12.4.4.44.40.2">Code Llama</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T12.4.4.44.40.3">34B</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T12.4.4.44.40.4">zero shot</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S6.T12.4.4.44.40.5">48.8</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Applications</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The application of Large Language Models (LLMs) has become a hot topic in both AI-related research communities and industries, with many emerging uses being discovered and explored daily. These models, capable of understanding and generating human-like text, have found meaningful applications across a variety of fields. This section will provide an overview of LLMs’ applications in medicine, education, science, mathematics, law, finance, robotics, and coding. While each of these domains poses different challenges, LLMs have opportunities to make significant contributions given their wide-ranging scope of applicability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">General Purpose:</span>
One of the most significant applications of LLMs is as a general-purpose tool for a wide variety of tasks, even those that have not been specifically trained for&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib433" title="">433</a>]</cite>. This is due to their inherent ability to understand, generate, and manipulate human-like text in a contextually relevant manner. This allows them to perform tasks ranging from simple language translation and question-answering to more complex tasks like summarization, text generation, and even programming help&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib434" title="">434</a>]</cite>. The utility of LLMs is further enhanced by their ability to adapt to the specific style and tone of the text they are processing, making the outputs more user-friendly and context-aware. In everyday applications, LLMs can be used as personal assistants, helping users draft emails or schedule appointments&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib435" title="">435</a>]</cite>; they can also be deployed in customer service to handle common questions, freeing up human resources for more complex issues; or applied to generate content for digital platforms like websites, by creating human-like text based on given prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib436" title="">436</a>]</cite>. Moreover, LLMs play a crucial role in data analysis, where they can filter large volumes of text data, summarize key points, and find patterns that would take humans much longer time to identify&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib437" title="">437</a>]</cite>. Despite their wide-ranging applications, it is essential to remember that LLMs, like any AI system, are only as good as the data they have been trained on. We should always use LLMs carefully since they can unintentionally reproduce biases in their training data, leading to potentially unfair or inaccurate results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Medicine:</span>
The application of LLMs in the field of medicine is reshaping healthcare delivery and research. For example, LLMs are increasingly used in clinical decision support systems to provide physicians with evidence-based treatment recommendations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib438" title="">438</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib439" title="">439</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib440" title="">440</a>]</cite>. By analyzing patient data and medical literature, they can help identify potential diagnoses, suggest appropriate tests, and recommend optimal treatment strategies. Moreover, LLMs can also enhance patient interactions with healthcare systems; e.g., they can be used in chatbot applications&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib441" title="">441</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib442" title="">442</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib443" title="">443</a>]</cite> to answer patient queries about symptoms or medications, schedule appointments, and even provide essential health advice. For medical research, LLMs are used to extract and filter information from a considerable amount of medical literature, identify relevant studies, summarize findings, and even predict future research trends&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib444" title="">444</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib445" title="">445</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib446" title="">446</a>]</cite>. For medical education, LLMs can help create training materials, generate exam questions, provide detailed explanations of complex medical topics, and offer personalized feedback to students&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib447" title="">447</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib448" title="">448</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib449" title="">449</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib450" title="">450</a>]</cite>. They can also simulate patient interactions, enabling students to practice and improve their clinical skills. At a broader level, LLMs can assist in public health initiatives by analyzing media data to detect disease outbreaks, monitor public sentiment towards health policies, and disseminate health information in a clear and understandable manner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib451" title="">451</a>]</cite>. When employing LLMs to support public health initiatives, addressing related issues such as data privacy, the necessity for explainability, and the potential risk of propagating biases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib452" title="">452</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib453" title="">453</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">Education:</span>
The integration of LLMs into the educational sector offers opportunities to enhance learning experiences, teacher support, and educational content development. For students, by analyzing their learning styles, performance, and preferences, LLMs can provide customized study materials and practice questions to develop personalized learning experiences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib454" title="">454</a>]</cite>. For teachers, LLMs can help to create lesson plans and grade assignments and generate diverse and inclusive educational content, significantly saving more time for teaching and student interaction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib455" title="">455</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib456" title="">456</a>]</cite>. In language learning, LLMs serve as advanced conversational partners capable of simulating conversations in multiple languages, correcting grammar, enhancing vocabulary, and aiding pronunciation for the needs of fluency in practice&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib457" title="">457</a>]</cite>. Furthermore, LLMs improve accessibility in education by providing support for students with disabilities. They can generate real-time transcriptions for the hearing impaired, offer reading assistance for the visually impaired, and simplify complex texts for those with learning disabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib453" title="">453</a>]</cite>. As LLMs continue to evolve, their applications in education can benefit more students and teachers from different perspectives in practice.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Science:</span>
Similar to medical applications, LLMs can expedite the research process by quickly analyzing and summarizing scientific literature. By briefing comprehensible and accessible research summaries, LLMs can assist researchers in staying up-to-date with the latest findings, even in fields outside their area of expertise&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib458" title="">458</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib459" title="">459</a>]</cite>. In addition, LLMs can aid scientists in formulating new hypotheses and research questions since their ability to process large-scale datasets allows them to unveil insights that might not be immediately apparent to human researchers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib460" title="">460</a>]</cite>. Moreover, for scientific writing, LLMs can help researchers draft documents, suggest improvements, and ensure adherence to specific formatting guidelines&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib461" title="">461</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib462" title="">462</a>]</cite>. This not only saves time but also improves the clarity of scientific communication, enabling interdisciplinary teams to work together more effectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p6">
<p class="ltx_p" id="S7.p6.1"><span class="ltx_text ltx_font_bold" id="S7.p6.1.1">Maths:</span>
In addition to providing mathematical research and education support, LLMs can assist in solving mathematical problems by giving step-by-step explanations and guiding users through complex proofs and calculations. They can help identify errors in reasoning or computation and suggest corrections, serving as an invaluable tool for both learning and verification purposes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib463" title="">463</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib464" title="">464</a>]</cite>. LLMs can be employed to check the validity of mathematical proofs, offering a preliminary filter before human review. While they are not a substitute for the meticulous work of mathematicians, they can help simplify the process of proof verification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib465" title="">465</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib466" title="">466</a>]</cite>. Moreover, LLMs enhance accessibility to mathematics by translating complex concepts and findings into understandable language for non-specialists&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib467" title="">467</a>]</cite>, where the gap between theoretical mathematics and applied contexts such as physics, engineering, and economics can be bridged.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p7">
<p class="ltx_p" id="S7.p7.1"><span class="ltx_text ltx_font_bold" id="S7.p7.1.1">Law:</span>
LLMs can assist with the thematic analysis of legal documents, including generating initial coding for datasets, identifying themes, and classifying data according to these themes. This collaborative effort between legal experts and LLMs has proved to be effective in analyzing legal texts such as court opinions on theft, improving both the efficiency and quality of the research&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib468" title="">468</a>]</cite>. Additionally, LLMs have been evaluated for their ability to generate explanations of legal terms, focusing on improving factual accuracy and relevance by incorporating sentences from case law. By feeding relevant case law into the LLM, the augmented models can generate higher-quality explanations with less factually incorrect information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib469" title="">469</a>]</cite>. Moreover, LLMs can be trained with specialized domain knowledge to perform legal reasoning tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib470" title="">470</a>]</cite> and answer legal questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib471" title="">471</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p8">
<p class="ltx_p" id="S7.p8.1"><span class="ltx_text ltx_font_bold" id="S7.p8.1.1">Finance:</span>
LLMs like BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib141" title="">141</a>]</cite>, trained on extensive proprietary financial datasets, exhibit superior performance on financial tasks. This indicates the value of domain-specific training in creating LLMs that can more accurately understand and process industry-specific language and concepts. The introduction of FinGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib472" title="">472</a>]</cite> as an open-source model offers transparent and accessible resources to develop novel applications such as robo-advising, algorithmic trading, and low-code solutions, ultimately expanding the capabilities of financial services. Both BloombergGPT and FinGPT show the adaptability of LLMs to the financial domain, with the former showing the power of custom datasets and the latter emphasizing a data-centric approach and low-rank adaptation techniques for customization. Moreover, LLMs demonstrate an ability to break down complex financial tasks into actionable plans, enabling end-to-end solutions that were previously unfeasible with a single model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib473" title="">473</a>]</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p9">
<p class="ltx_p" id="S7.p9.1"><span class="ltx_text ltx_font_bold" id="S7.p9.1.1">Others:</span>
The application of LLM in coding is introduced in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS1.SSS2" title="III-A2 Coding ‣ III-A Pre-Trained LLMs ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>2</span></a>. For the utility of LLMs in robotics, please refer to Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S3.SS4" title="III-D Robotics ‣ III Large Language Models ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Summary and Discussion</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS1.5.1.1">VIII-A</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS1.6.2">Architecture</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">Due to the gigantic scale of LLMs, minor changes in architecture and training strategies have a big impact on performance and stability. Here, we summarize key architectural modules used in various LLMs, leading to better performance, reduced training time and memory, and better training stability. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.1">Layer Normalization</em> is found to have a significant effect on the performance and training stability of LLMs. Pre-norm, that is normalizing inputs rather than outputs, is more common among LLMs stabilizing the training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib104" title="">104</a>]</cite>. BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> and AlexaTM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite> utilize an additional layer normalization before embedding layer to stabilize the training of large-scale models, while the model’s zero-shot generalization ability can be negatively impacted&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite>. However, another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite> finds that pre-norm degrades fine-tuned model performance as compared to post-norm, and there are no stability benefits of pre-norm beyond the 100B scale. Therefore, GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite> used deep-norm which is a variant of post-norm for better downstream task performance after fine-tuning. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.2">Positional Encoding</em> effect performance and training stability of LLMs like other building blocks of a model. BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> finds ALiBi outperforming learned and rotary positional encodings. Contrary to this, GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite> identifies rotary positional encoding better than ALiBi. So, there is no conclusion in literature about the positional encodings yet. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.3">Parallel Attention</em> where attention and feed-forward layers are parallel to each other rather than sequential in transformer block has shown to reduce training time by 15%. There is no evidence of performance drop due to this change in literature and used by the models PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>, GPT-NeoX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib115" title="">115</a>]</cite>, and CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib130" title="">130</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.4">Multi-Query Attention</em> has shared key and value attention heads in a transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degradation has been observed with this change and makes the training efficient allowing larger batch sizes. Multi-query attention is used in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib132" title="">132</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.5">Mixture of Experts</em> allows easily scaling model to trillion of parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite>. Only a few experts are activated during the computation making them compute-efficient. The performance of MoE models is better than the dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to the dense models as discussed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite>. MoE architectures are less prone to catastrophic forgetting, therefore are more suited for continual learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite>. Extracting smaller sub-models for downstream tasks is possible without losing any performance, making MoE architecture hardware-friendly&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS1.p1.1.6">Sparse vs Dense Activated</em>
GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> uses sparse transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib63" title="">63</a>]</cite> whereas GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite> and PanGu-<math alttext="\sum" class="ltx_Math" display="inline" id="S8.SS1.p1.1.m1.1"><semantics id="S8.SS1.p1.1.m1.1a"><mo id="S8.SS1.p1.1.m1.1.1" xref="S8.SS1.p1.1.m1.1.1.cmml">∑</mo><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.1.m1.1b"><sum id="S8.SS1.p1.1.m1.1.1.cmml" xref="S8.SS1.p1.1.m1.1.1"></sum></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.1.m1.1c">\sum</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.1.m1.1d">∑</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite> use MoE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib119" title="">119</a>]</cite> architecture to lower computational costs and increase the model size and capacity. According to the literature, sparse modules do not degrade the model’s performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib63" title="">63</a>]</cite>. However, more experiments are required to verify this statement.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS2.5.1.1">VIII-B</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS2.6.2">Training Strategies</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.5">Training models at a huge scale require some tricks to reduce training costs, avoid loss divergence and achieve better performance. We summarize and discuss some of these key tricks used in different LLMs. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.1">Mixed Precision</em> is a famous method for LLMs to reduce memory usage and improve training efficiency. In mixed precision, forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib474" title="">474</a>]</cite>. A drawback associated with this format change is training instability due to a smaller value range resulting in loss spikes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite>. An alternative to FP16 is BF16 which has a comparatively larger range and performs some precision-sensitive operations like gradient accumulation and softmax in FP32&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite>. BF16 has better performance and training stability but uses more memory and is supported on specific hardware, for example, A100 GPUs. Therefore, its adoption in LLMs is limited. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.2">Training Instability</em> is a common issue in LLMs where loss divergence or spiking is observed multiple times during training. This happens in the presence of gradient clipping&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>. To mitigate this problem, many approaches suggest restarting training from an earlier checkpoint&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite>, skipping 200-500 earlier data batches at the point of divergence in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite> and re-shuffling batches in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib118" title="">118</a>]</cite>. The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>]</cite>. Another suggestion to improve training stability for larger models is not to use <span class="ltx_text ltx_font_bold" id="S8.SS2.p1.5.3">biases</span> in dense and norm layers as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.4">Weight Initialization</em> plays a significant role in model convergence and training stability. GPT-NeoX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib115" title="">115</a>]</cite> initializes feed-forward layers before residuals with <math alttext="\frac{2}{L\sqrt{d}}" class="ltx_Math" display="inline" id="S8.SS2.p1.1.m1.1"><semantics id="S8.SS2.p1.1.m1.1a"><mfrac id="S8.SS2.p1.1.m1.1.1" xref="S8.SS2.p1.1.m1.1.1.cmml"><mn id="S8.SS2.p1.1.m1.1.1.2" xref="S8.SS2.p1.1.m1.1.1.2.cmml">2</mn><mrow id="S8.SS2.p1.1.m1.1.1.3" xref="S8.SS2.p1.1.m1.1.1.3.cmml"><mi id="S8.SS2.p1.1.m1.1.1.3.2" xref="S8.SS2.p1.1.m1.1.1.3.2.cmml">L</mi><mo id="S8.SS2.p1.1.m1.1.1.3.1" xref="S8.SS2.p1.1.m1.1.1.3.1.cmml">⁢</mo><msqrt id="S8.SS2.p1.1.m1.1.1.3.3" xref="S8.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S8.SS2.p1.1.m1.1.1.3.3.2" xref="S8.SS2.p1.1.m1.1.1.3.3.2.cmml">d</mi></msqrt></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.1.m1.1b"><apply id="S8.SS2.p1.1.m1.1.1.cmml" xref="S8.SS2.p1.1.m1.1.1"><divide id="S8.SS2.p1.1.m1.1.1.1.cmml" xref="S8.SS2.p1.1.m1.1.1"></divide><cn id="S8.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S8.SS2.p1.1.m1.1.1.2">2</cn><apply id="S8.SS2.p1.1.m1.1.1.3.cmml" xref="S8.SS2.p1.1.m1.1.1.3"><times id="S8.SS2.p1.1.m1.1.1.3.1.cmml" xref="S8.SS2.p1.1.m1.1.1.3.1"></times><ci id="S8.SS2.p1.1.m1.1.1.3.2.cmml" xref="S8.SS2.p1.1.m1.1.1.3.2">𝐿</ci><apply id="S8.SS2.p1.1.m1.1.1.3.3.cmml" xref="S8.SS2.p1.1.m1.1.1.3.3"><root id="S8.SS2.p1.1.m1.1.1.3.3a.cmml" xref="S8.SS2.p1.1.m1.1.1.3.3"></root><ci id="S8.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S8.SS2.p1.1.m1.1.1.3.3.2">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.1.m1.1c">\frac{2}{L\sqrt{d}}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.1.m1.1d">divide start_ARG 2 end_ARG start_ARG italic_L square-root start_ARG italic_d end_ARG end_ARG</annotation></semantics></math> as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib144" title="">144</a>]</cite> and other layers with small initialization scheme&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib475" title="">475</a>]</cite>. This avoids activations growing exponentially with the increasing depth. MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>]</cite> found higher variance for weight initialization leads to unstable training, hence validating small initialization scheme&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib475" title="">475</a>]</cite>. Various models perform random weight initialization which can cause bad initialization, Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib138" title="">138</a>]</cite> suggests a longer warmup to negate the effect. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.5">Learning Rate</em> is important for stable training. It is suggested to use a lower value&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib124" title="">124</a>]</cite> with warmup and decay (cosine or linear). Usually, the learning rate is within the range <math alttext="1e^{-4}" class="ltx_Math" display="inline" id="S8.SS2.p1.2.m2.1"><semantics id="S8.SS2.p1.2.m2.1a"><mrow id="S8.SS2.p1.2.m2.1.1" xref="S8.SS2.p1.2.m2.1.1.cmml"><mn id="S8.SS2.p1.2.m2.1.1.2" xref="S8.SS2.p1.2.m2.1.1.2.cmml">1</mn><mo id="S8.SS2.p1.2.m2.1.1.1" xref="S8.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><msup id="S8.SS2.p1.2.m2.1.1.3" xref="S8.SS2.p1.2.m2.1.1.3.cmml"><mi id="S8.SS2.p1.2.m2.1.1.3.2" xref="S8.SS2.p1.2.m2.1.1.3.2.cmml">e</mi><mrow id="S8.SS2.p1.2.m2.1.1.3.3" xref="S8.SS2.p1.2.m2.1.1.3.3.cmml"><mo id="S8.SS2.p1.2.m2.1.1.3.3a" xref="S8.SS2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S8.SS2.p1.2.m2.1.1.3.3.2" xref="S8.SS2.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.2.m2.1b"><apply id="S8.SS2.p1.2.m2.1.1.cmml" xref="S8.SS2.p1.2.m2.1.1"><times id="S8.SS2.p1.2.m2.1.1.1.cmml" xref="S8.SS2.p1.2.m2.1.1.1"></times><cn id="S8.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S8.SS2.p1.2.m2.1.1.2">1</cn><apply id="S8.SS2.p1.2.m2.1.1.3.cmml" xref="S8.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S8.SS2.p1.2.m2.1.1.3.1.cmml" xref="S8.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S8.SS2.p1.2.m2.1.1.3.2.cmml" xref="S8.SS2.p1.2.m2.1.1.3.2">𝑒</ci><apply id="S8.SS2.p1.2.m2.1.1.3.3.cmml" xref="S8.SS2.p1.2.m2.1.1.3.3"><minus id="S8.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S8.SS2.p1.2.m2.1.1.3.3"></minus><cn id="S8.SS2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S8.SS2.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.2.m2.1c">1e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.2.m2.1d">1 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="8e^{-4}" class="ltx_Math" display="inline" id="S8.SS2.p1.3.m3.1"><semantics id="S8.SS2.p1.3.m3.1a"><mrow id="S8.SS2.p1.3.m3.1.1" xref="S8.SS2.p1.3.m3.1.1.cmml"><mn id="S8.SS2.p1.3.m3.1.1.2" xref="S8.SS2.p1.3.m3.1.1.2.cmml">8</mn><mo id="S8.SS2.p1.3.m3.1.1.1" xref="S8.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><msup id="S8.SS2.p1.3.m3.1.1.3" xref="S8.SS2.p1.3.m3.1.1.3.cmml"><mi id="S8.SS2.p1.3.m3.1.1.3.2" xref="S8.SS2.p1.3.m3.1.1.3.2.cmml">e</mi><mrow id="S8.SS2.p1.3.m3.1.1.3.3" xref="S8.SS2.p1.3.m3.1.1.3.3.cmml"><mo id="S8.SS2.p1.3.m3.1.1.3.3a" xref="S8.SS2.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="S8.SS2.p1.3.m3.1.1.3.3.2" xref="S8.SS2.p1.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.3.m3.1b"><apply id="S8.SS2.p1.3.m3.1.1.cmml" xref="S8.SS2.p1.3.m3.1.1"><times id="S8.SS2.p1.3.m3.1.1.1.cmml" xref="S8.SS2.p1.3.m3.1.1.1"></times><cn id="S8.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S8.SS2.p1.3.m3.1.1.2">8</cn><apply id="S8.SS2.p1.3.m3.1.1.3.cmml" xref="S8.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S8.SS2.p1.3.m3.1.1.3.1.cmml" xref="S8.SS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S8.SS2.p1.3.m3.1.1.3.2.cmml" xref="S8.SS2.p1.3.m3.1.1.3.2">𝑒</ci><apply id="S8.SS2.p1.3.m3.1.1.3.3.cmml" xref="S8.SS2.p1.3.m3.1.1.3.3"><minus id="S8.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S8.SS2.p1.3.m3.1.1.3.3"></minus><cn id="S8.SS2.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="S8.SS2.p1.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.3.m3.1c">8e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.3.m3.1d">8 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>. Moreover, MT-NLG (530B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>]</cite> and GPT-NeoX (20B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib115" title="">115</a>]</cite> suggest interpolating learning rates based on the model size using the GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> models ranging between 13B and 175B. This avoids tuning the learning rate hyperparameter. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.6">Training Parallelism</em> 3D parallelism, a combination of data, pipeline and tensor parallelism, is the most utilized training parallelism approach in LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib109" title="">109</a>]</cite>. In addition to the 3D parallelism, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> uses zero optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib37" title="">37</a>]</cite> to shard optimizer states. PanGu-<math alttext="\alpha" class="ltx_Math" display="inline" id="S8.SS2.p1.4.m4.1"><semantics id="S8.SS2.p1.4.m4.1a"><mi id="S8.SS2.p1.4.m4.1.1" xref="S8.SS2.p1.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.4.m4.1b"><ci id="S8.SS2.p1.4.m4.1.1.cmml" xref="S8.SS2.p1.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.4.m4.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.4.m4.1d">italic_α</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib104" title="">104</a>]</cite> and PanGu-<math alttext="\Sigma" class="ltx_Math" display="inline" id="S8.SS2.p1.5.m5.1"><semantics id="S8.SS2.p1.5.m5.1a"><mi id="S8.SS2.p1.5.m5.1.1" mathvariant="normal" xref="S8.SS2.p1.5.m5.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S8.SS2.p1.5.m5.1b"><ci id="S8.SS2.p1.5.m5.1.1.cmml" xref="S8.SS2.p1.5.m5.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS2.p1.5.m5.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S8.SS2.p1.5.m5.1d">roman_Σ</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib129" title="">129</a>]</cite> go beyond the 3D parallelism and apply 5D parallelism which additionally contains optimizer parallelism and rematerialization. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.7">Mode Switching</em> adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve the downstream task performance in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite>. During fine-tuning and inference, tokens are appended based on the downstream tasks. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S8.SS2.p1.5.8">Controllable Text Generation</em> Generating credible and controlled text from a pre-trained model is challenging. GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite> and other LLMs use in-context learning to control generated text. While in-context learning helps in controlling the generated text, ERNIE 3.0 Titan&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib35" title="">35</a>]</cite> suggests using adversarial loss to rank its generated text for credibility and soft prompts such as genre, topic, keywords, sentiment, and length for better control on generated text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS3.5.1.1">VIII-C</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS3.6.2">Pre-Training vs
Instruction Tuning</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p" id="S8.SS3.p1.1">While pre-training is important for the generalization of LLMs, instruction-tuning improves the performance of LLMs further and makes them useable. Therefore, it is suggested to perform instruction fine-tuning of pre-trained LLMs to use them effectively&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib157" title="">157</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS4.5.1.1">VIII-D</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS4.6.2">Supervised Models vs Generalized Models</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS4.p1">
<p class="ltx_p" id="S8.SS4.p1.1">Although generalized models are capable of performing diverse tasks with good performance they have not yet outperformed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by a large margin as shown in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib18" title="">18</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS5.5.1.1">VIII-E</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS5.6.2">Zero-Shot vs Few-Shot</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS5.p1">
<p class="ltx_p" id="S8.SS5.p1.1">LLMs perform well in zero-shot and few-shot settings. But the performance difference between zero-shot and few-shot is large for pre-trained models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>, naming LLMs as meta-learners&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>. LLMs zero-shot evaluations underperform unsupervised methods in neural machine translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>. The literature shows pre-training is not enough for good zero-shot performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite>. To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms baselines. Instruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model Flan-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib16" title="">16</a>]</cite> unlocks zero-shot reasoning with CoT training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS6.5.1.1">VIII-F</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS6.6.2">Encoder vs Decoder vs Encoder-Decoder</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS6.p1">
<p class="ltx_p" id="S8.SS6.p1.1">Traditionally, these architectures perform well for different tasks, for example, encoder-only for NLU tasks, decoder-only for NLG, and encoder-decoder for sequence2sequence modeling. Encoder-only models are famous for smaller models such as Bert&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib7" title="">7</a>]</cite>, RoBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib424" title="">424</a>]</cite>, etc, whereas LLMs are either decoder-only&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite> or encoder-decoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite>. While decoder-only models are good at NLG tasks, various LLMs, PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite>, OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib14" title="">14</a>]</cite>, GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>]</cite>, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib13" title="">13</a>]</cite>, LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib147" title="">147</a>]</cite>, are decoder-only models with significant performance gains on both NLU and NLG tasks. In contradiction to this, T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib10" title="">10</a>]</cite> and UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>]</cite> identify encoder-decoder models out-performing decoder-only models. In another study, PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib15" title="">15</a>]</cite> finds increasing the size of decoder-only models can reduce the performance gap between decoder-only and encoder-decoder architectures. 
<br class="ltx_break">Although decoder-only architectures have become a trend for LLMs, many recently proposed approaches&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib122" title="">122</a>]</cite> use mode-switching tokens in text with encoder-decoder architectures to enable task-specific modes. Similarly, CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib34" title="">34</a>]</cite> uses an encoder-decoder architecture with multiple training objectives for different tasks, activating the encoder, decoder, or both according to the tasks. These variations in architecture and training objectives allow a model to perform well in different settings. Because of this dynamic configuration, the future of LLMs can be attributed to encoder-decoder architectures.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Challenges and Future Directions</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">LLMs such as GPT-4 and its predecessors have significantly advanced natural language processing. Nevertheless, they also bring along a set of challenges. The computational cost, adversarial robustness, and interpretability are among the technical challenges that are intrinsic to these models. Furthermore, as these models are scaled up to handle more complex tasks or to operate in more complex or dynamic environments, new challenges in scalability, privacy, and real-time processing emerge. On the frontier of foundational research, integrating multi-modality and the effectiveness of transfer learning are being keenly explored. Additionally, the continuous learning aspect of these models, which aims to have models that can adapt to new information over time, presents a fresh set of challenges. These challenges not only underscore the technical intricacies involved but also highlight the broader impact and the future trajectory of LLMs in real-world applications. The following sections delve into these challenges, shedding light on the ongoing and potential efforts to address them. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.1">Computational Cost:</em>
Training LLMs requires extensive computational resources, which increases production costs and raises environmental concerns due to substantial energy consumption during large-scale training. Improved performance occurs as computational resources increase, but the rate of improvement gradually decreases when both the model and dataset size remain fixed, following the power law of diminishing returns&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib476" title="">476</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.2">Bias and Fairness:</em>
LLMs can inherit and amplify societal biases in their training data. These biases can manifest in the model’s outputs, leading to potential ethical and fairness issues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib477" title="">477</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.3">Overfitting:</em>
Although LLMs possess substantial learning capabilities, they are susceptible to overfitting noisy and peculiar patterns within their extensive training data. Consequently, this may cause them to generate illogical responses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib478" title="">478</a>]</cite>. The debate about Memorization vs. Generalization in LLMs is about finding the right balance. Memorization allows the model to remember specific details from its training data, ensuring it can provide accurate answers to precise questions. However, generalization enables the model to make inferences and produce responses for inputs it hasn’t seen before, which is essential for handling various real-world tasks. Striking the right balance is the challenge: too much memorization can lead to overfitting, making the model inflexible and struggling with new inputs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib479" title="">479</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.4">Economic and Research Inequality:</em>
The high cost of training and deploying LLMs may make their development concentrated within well-funded organizations, potentially worsening economic and research inequalities in AI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib480" title="">480</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.5">Reasoning and Planning:</em>
Some reasoning and planning tasks, even as seemingly simple as common-sense planning, which humans find easy, remain well beyond the current capabilities of LLMs evaluated using an assessment framework. This isn’t entirely unexpected, considering that LLMs primarily generate text completions based on likelihood and offer no solid guarantees in terms of reasoning abilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib481" title="">481</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p1.1.6">Hallucinations:</em>
LLMs exhibit "hallucinations," where they generate responses that, while sounding plausible, are incorrect or don’t align with the provided information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib482" title="">482</a>]</cite>. The hallucination can be categorized into three categories.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S9.I1">
<li class="ltx_item" id="S9.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i1.p1">
<p class="ltx_p" id="S9.I1.i1.p1.1">Input-conflicting hallucination, wherein LLMs produce content that diverges from the input given by users.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S9.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i2.p1">
<p class="ltx_p" id="S9.I1.i2.p1.1">Context-conflicting hallucination, where LLMs generate content that contradicts information they have generated earlier.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S9.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i3.p1">
<p class="ltx_p" id="S9.I1.i3.p1.1">Fact-conflicting hallucination involves LLM’s generation of content that does not align with established world knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S9.p2">
<p class="ltx_p" id="S9.p2.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.1">Prompt Engineering:</em>
Prompts serve as inputs to LLMs, and their syntax and semantics play a crucial role in determining the model’s output. The prompt variations, sometimes counter-intuitive to humans, can result in significant changes in model output and are addressed through prompt engineering, which involves designing natural language queries to guide LLMs responses effectively&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib483" title="">483</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib32" title="">32</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.2">Limited Knowledge:</em>
Information acquired during pretraining is limited and may become obsolete after some time. Re-training the model using updated data is costly. To generate factually accurate responses people use retrieval augmentation pipeline&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib243" title="">243</a>]</cite>. However, pre-trained models are not trained with retrieval augmentation generation (RAG)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib21" title="">21</a>]</cite>, hence, adapting the training pipeline is necessary&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib238" title="">238</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib25" title="">25</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.3">Safety and Controllability:</em>
Using LLMs comes with the risk of generating harmful, misleading, or inappropriate content, whether by accident or when given specific prompts. Ensuring these models are safely utilized is a significant concern&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib484" title="">484</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.4">Multi-Modality:</em>
Multi-modal learning, where LLMs are trained on diverse data like text, images, and videos, aims to create models with richer understanding but faces challenges in data alignment, fusion strategies, and higher computational demands. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.5">Catastrophic Forgetting:</em>
LLMs are often pre-trained on large datasets and then fine-tuned on domain-specific data, reducing training resources but facing issues like domain adaptation and catastrophic forgetting, which hinders the retention of original knowledge when learning new tasks. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.6">Adversarial Robustness:</em>
Large Language Models (LLMs) have shown great capabilities in various tasks but are vulnerable to adversarial attacks, where slight, deliberate input alterations can mislead them. Especially with models like BERT, adversarial fine-tuning can enhance robustness, although it sometimes compromises generalization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib485" title="">485</a>]</cite>. As LLMs integrate more into complex systems, examining their security properties becomes crucial, given the emerging field of adversarial attacks on LLMs within trustworthy ML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib486" title="">486</a>]</cite>. This vulnerability is notable in safety-critical domains, necessitating robust adversarial evaluation tools to ensure LLM reliability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib487" title="">487</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.7">Interpretability and Explainability:</em>
The "black-box" nature of LLMs poses challenges in understanding their decision-making, which is crucial for broader acceptance and trust, especially in sensitive domains. Despite their advanced capabilities, the lack of insight into their operation limits their effectiveness and trustworthiness&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib488" title="">488</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib489" title="">489</a>]</cite>. Efforts are being made to make LLMs more explainable to promote user trust and to ensure responsible AI usage. Understanding the logic behind LLMs’ responses is essential for fostering trust and ensuring they align with human values and legal standards. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.8">Privacy Concerns:</em>
Privacy concerns in Large Language Models (LLMs) have escalated with their growth in complexity and size, particularly around data sharing and potential misuse. There is a risk of malicious content creation, filter bypass, and data privacy issues, especially in e-commerce, where protecting customer privacy is crucial. If models are trained on private data, additional concerns arise if such models are made publicly available. LLMs tend to memorize phrases from their training sets, which an adversary could exploit to extract sensitive data, posing a threat to personal privacy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib490" title="">490</a>, <a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib491" title="">491</a>]</cite>. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.9">Real-Time Processing:</em>
Real-time processing in Large Language Models (LLMs) is pivotal for various applications, especially with the rising popularity of mobile AI applications and concerns regarding information security and privacy. However, LLMs often have hundreds of layers and millions of parameters, which impede real-time processing due to the high computational demands and limited weight storage on hardware platforms, particularly in edge computing environments&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib492" title="">492</a>]</cite>. While certain efforts like MobileBERT aim to reduce memory requirements, they still face substantial execution overhead due to the large number of model layers, leading to high inference latency. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.10">Long-Term Dependencies:</em>
Large Language Models (LLMs) have shown considerable progress in understanding and generating text, yet they often struggle with preserving context and handling long-term dependencies, particularly in complex, multi-turn conversations or long documents. This limitation can lead to incoherent or irrelevant responses. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.11">Hardware Acceleration:</em>
The growth of LLMs presents significant hardware challenges due to the increasing computational and memory demands associated with training and deploying these models. GPUs have played a crucial role in meeting the hardware requirements for training LLMs, with the networking industry also evolving to optimize hardware for training workloads. However, the growing size of LLMs, which has been outpacing hardware progress, makes model inference increasingly costly. Model quantization is a promising approach to bridge the widening gap between LLM size and hardware capacity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib493" title="">493</a>]</cite>. Although specialized hardware acceleration like GPUs or TPUs can significantly reduce the computational cost, making real-time applications more feasible, they may not fully resolve all limitations, necessitating further advancements in hardware technology. 
<br class="ltx_break"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S9.p2.1.12">Regulatory and Ethical Frameworks:</em>
The rapid advancements in artificial intelligence have given rise to sophisticated Large Language Models (LLMs) like OpenAI’s GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib148" title="">148</a>]</cite> and Google’s Bard. These developments underscore the imperative for regulatory oversight to manage the ethical and social challenges accompanying LLMs’ widespread use&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib494" title="">494</a>]</cite>. For instance, LLMs can generate content that can be used positively or negatively, emphasizing the need for proactive ethical frameworks and policy measures to guide their responsible use and assign accountability for their outputs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib495" title="">495</a>]</cite>. Auditing is identified as a promising governance mechanism to ensure that AI systems, including LLMs, are designed and deployed ethically, legally, and technically robust&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#bib.bib496" title="">496</a>]</cite>. 
<br class="ltx_break"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span class="ltx_text ltx_font_smallcaps" id="S10.1.1">Conclusion</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">This article has reviewed various LLMs, discussing their interesting aspects. It contributes to summarizing significant findings in the existing literature and provides a detailed analysis of the design aspects of LLMs, including architectures, datasets, and training pipelines. We identified crucial architectural components and training strategies employed by different LLMs. These aspects are presented as summaries and discussions throughout the article. Moreover, we have discussed the performance differences of LLMs in zero-shot and few-shot settings, explored the impact of fine-tuning, and compared supervised and generalized models and encoder vs decoder vs encoder-decoder architectures. A comprehensive review of LLMs in robotics, multi-modal LLMs, augmented LLMs, datasets, and evaluation is also provided. This article is anticipated to serve as a valuable resource for researchers, offering insights into the recent advancements in LLMs and providing fundamental concepts and details to develop better LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">XI </span><span class="ltx_text ltx_font_smallcaps" id="S11.1.1">Versioning</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">We keep track of the versions of this paper we release as the content updates.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p2">
<p class="ltx_p" id="S11.p2.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p2.1.1">Version 1.0:</em> We covered 30 pre-trained models and 6 instruction-tuned models, including their overview, findings, training, and evaluation datasets, and discussed important architectural and training tricks by various LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p3">
<p class="ltx_p" id="S11.p3.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p3.1.1">Version 2.0:</em> Further pre-trained LLMs added along with discussion on on self-instruct LLMs. Categorized LLMs according to the application, provided descriptions of widely used evaluation datasets, added a section on robotics, and extended discussion in section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2307.06435v7/#S8" title="VIII Summary and Discussion ‣ A Comprehensive Overview of Large Language Models"><span class="ltx_text ltx_ref_tag">VIII</span></a>. Tables have been updated.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p4">
<p class="ltx_p" id="S11.p4.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p4.1.1">Version 3.0:</em> Added sections on Alignment tuning and multimodal LLMs. A performance comparison table on various benchmarks and datasets. Added LLaMA-2 and PaLM-2.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p5">
<p class="ltx_p" id="S11.p5.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p5.1.1">Version 4.0:</em> Tables on training and evaluation datasets, a sub-section on increasing context window, and minor improvements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p6">
<p class="ltx_p" id="S11.p6.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p6.1.1">Version 5.0:</em> Added sections on augmented LLMs and challenges and future directions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p7">
<p class="ltx_p" id="S11.p7.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p7.1.1">Version 6.0:</em> Minor improvements in abstract, introduction, and conclusion.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p8">
<p class="ltx_p" id="S11.p8.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S11.p8.1.1">Version 7.0:</em> Sections on efficient LLMs and applications.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S11.p9">
<p class="ltx_p" id="S11.p9.1"><span class="ltx_text ltx_font_bold" id="S11.p9.1.1">Note:</span> If you find any mistakes, or have issues and conflicts with the writing in this paper, please email us. We welcome suggestions to improve this paper.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Chernyavskiy, D.&nbsp;Ilvovsky, and P.&nbsp;Nakov, “Transformers:“the end of
history” for natural language processing?” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Machine Learning and
Knowledge Discovery in Databases. Research Track: European Conference, ECML
PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III
21</em>.&nbsp;&nbsp;&nbsp;Springer, 2021, pp. 677–693.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Wang, Y.&nbsp;Pruksachatkun, N.&nbsp;Nangia, A.&nbsp;Singh, J.&nbsp;Michael, F.&nbsp;Hill, O.&nbsp;Levy,
and S.&nbsp;Bowman, “Superglue: A stickier benchmark for general-purpose language
understanding systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing
systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Adiwardana, M.-T. Luong, D.&nbsp;R. So, J.&nbsp;Hall, N.&nbsp;Fiedel, R.&nbsp;Thoppilan,
Z.&nbsp;Yang, A.&nbsp;Kulshreshtha, G.&nbsp;Nemade, Y.&nbsp;Lu <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et&nbsp;al.</em>, “Towards a
human-like open-domain chatbot,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">arXiv preprint arXiv:2001.09977</em>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;A. y&nbsp;Arcas, “Do large language models understand us?” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Daedalus</em>,
vol. 151, no.&nbsp;2, pp. 183–197, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;Wu, R.&nbsp;Child, D.&nbsp;Luan, D.&nbsp;Amodei, I.&nbsp;Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et&nbsp;al.</em>,
“Language models are unsupervised multitask learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">OpenAI blog</em>,
vol.&nbsp;1, no.&nbsp;8, p.&nbsp;9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Brown, B.&nbsp;Mann, N.&nbsp;Ryder, M.&nbsp;Subbiah, J.&nbsp;D. Kaplan, P.&nbsp;Dhariwal,
A.&nbsp;Neelakantan, P.&nbsp;Shyam, G.&nbsp;Sastry, A.&nbsp;Askell <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et&nbsp;al.</em>, “Language
models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">Advances in neural information
processing systems</em>, vol.&nbsp;33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Devlin, M.-W. Chang, K.&nbsp;Lee, and K.&nbsp;Toutanova, “Bert: Pre-training of deep
bidirectional transformers for language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint
arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;E. Peters, M.&nbsp;Neumann, M.&nbsp;Iyyer, M.&nbsp;Gardner, C.&nbsp;Clark, K.&nbsp;Lee, and
L.&nbsp;Zettlemoyer, “Deep contextualized word representations,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">NAACL-HLT</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2018, pp. 2227–2237.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Lewis, Y.&nbsp;Liu, N.&nbsp;Goyal, M.&nbsp;Ghazvininejad, A.&nbsp;Mohamed, O.&nbsp;Levy, V.&nbsp;Stoyanov,
and L.&nbsp;Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv
preprint arXiv:1910.13461</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Raffel, N.&nbsp;Shazeer, A.&nbsp;Roberts, K.&nbsp;Lee, S.&nbsp;Narang, M.&nbsp;Matena, Y.&nbsp;Zhou,
W.&nbsp;Li, and P.&nbsp;J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">The Journal of Machine Learning
Research</em>, vol.&nbsp;21, no.&nbsp;1, pp. 5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua,
and C.&nbsp;Raffel, “mt5: A massively multilingual pre-trained text-to-text
transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2010.11934</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, Y.&nbsp;Gu, X.&nbsp;Han, S.&nbsp;Chen, C.&nbsp;Xiao, Z.&nbsp;Sun, Y.&nbsp;Yao, F.&nbsp;Qi, J.&nbsp;Guan,
P.&nbsp;Ke <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">et&nbsp;al.</em>, “Cpm-2: Large-scale cost-effective pre-trained language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2">AI Open</em>, vol.&nbsp;2, pp. 216–224, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;L. Scao, A.&nbsp;Fan, C.&nbsp;Akiki, E.&nbsp;Pavlick, S.&nbsp;Ilić, D.&nbsp;Hesslow,
R.&nbsp;Castagné, A.&nbsp;S. Luccioni, F.&nbsp;Yvon, M.&nbsp;Gallé <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et&nbsp;al.</em>,
“Bloom: A 176b-parameter open-access multilingual language model,”
<em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, S.&nbsp;Roller, N.&nbsp;Goyal, M.&nbsp;Artetxe, M.&nbsp;Chen, S.&nbsp;Chen, C.&nbsp;Dewan, M.&nbsp;Diab,
X.&nbsp;Li, X.&nbsp;V. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et&nbsp;al.</em>, “Opt: Open pre-trained transformer language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Chowdhery, S.&nbsp;Narang, J.&nbsp;Devlin, M.&nbsp;Bosma, G.&nbsp;Mishra, A.&nbsp;Roberts, P.&nbsp;Barham,
H.&nbsp;W. Chung, C.&nbsp;Sutton, S.&nbsp;Gehrmann <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et&nbsp;al.</em>, “Palm: Scaling language
modeling with pathways,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;W. Chung, L.&nbsp;Hou, S.&nbsp;Longpre, B.&nbsp;Zoph, Y.&nbsp;Tay, W.&nbsp;Fedus, E.&nbsp;Li, X.&nbsp;Wang,
M.&nbsp;Dehghani, S.&nbsp;Brahma <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et&nbsp;al.</em>, “Scaling instruction-finetuned
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Sanh, A.&nbsp;Webson, C.&nbsp;Raffel, S.&nbsp;H. Bach, L.&nbsp;Sutawika, Z.&nbsp;Alyafeai,
A.&nbsp;Chaffin, A.&nbsp;Stiegler, T.&nbsp;L. Scao, A.&nbsp;Raja <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">et&nbsp;al.</em>, “Multitask
prompted training enables zero-shot task generalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">arXiv
preprint arXiv:2110.08207</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, S.&nbsp;Mishra, P.&nbsp;Alipoormolabashi, Y.&nbsp;Kordi, A.&nbsp;Mirzaei, A.&nbsp;Naik,
A.&nbsp;Ashok, A.&nbsp;S. Dhanasekaran, A.&nbsp;Arunkumar, D.&nbsp;Stap <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">et&nbsp;al.</em>,
“Super-naturalinstructions: Generalization via declarative instructions on
1600+ nlp tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>, 2022, pp. 5085–5109.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and
H.&nbsp;Hajishirzi, “Self-instruct: Aligning language model with self generated
instructions,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2212.10560</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang,
S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et&nbsp;al.</em>, “Training language models to
follow instructions with human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">Advances in Neural
Information Processing Systems</em>, vol.&nbsp;35, pp. 27 730–27 744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et&nbsp;al.</em>, “Llama 2:
Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">arXiv preprint
arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, Y.&nbsp;Tay, R.&nbsp;Bommasani, C.&nbsp;Raffel, B.&nbsp;Zoph, S.&nbsp;Borgeaud, D.&nbsp;Yogatama,
M.&nbsp;Bosma, D.&nbsp;Zhou, D.&nbsp;Metzler <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et&nbsp;al.</em>, “Emergent abilities of large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">arXiv preprint arXiv:2206.07682</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Webb, K.&nbsp;J. Holyoak, and H.&nbsp;Lu, “Emergent analogical reasoning in large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Nature Human Behaviour</em>, vol.&nbsp;7, no.&nbsp;9, pp.
1526–1541, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;A. Boiko, R.&nbsp;MacKnight, and G.&nbsp;Gomes, “Emergent autonomous scientific
research capabilities of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint
arXiv:2304.05332</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Izacard, P.&nbsp;Lewis, M.&nbsp;Lomeli, L.&nbsp;Hosseini, F.&nbsp;Petroni, T.&nbsp;Schick,
J.&nbsp;Dwivedi-Yu, A.&nbsp;Joulin, S.&nbsp;Riedel, and E.&nbsp;Grave, “Few-shot learning with
retrieval augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint
arXiv:2208.03299</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Driess, F.&nbsp;Xia, M.&nbsp;S. Sajjadi, C.&nbsp;Lynch, A.&nbsp;Chowdhery, B.&nbsp;Ichter, A.&nbsp;Wahid,
J.&nbsp;Tompson, Q.&nbsp;Vuong, T.&nbsp;Yu <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et&nbsp;al.</em>, “Palm-e: An embodied multimodal
language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">arXiv preprint arXiv:2303.03378</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Parisi, Y.&nbsp;Zhao, and N.&nbsp;Fiedel, “Talm: Tool augmented language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2205.12255</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Zhang and H.&nbsp;Soh, “Large language models as zero-shot human models for
human-robot interaction,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2303.03548</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Ye, H.&nbsp;Xu, G.&nbsp;Xu, J.&nbsp;Ye, M.&nbsp;Yan, Y.&nbsp;Zhou, J.&nbsp;Wang, A.&nbsp;Hu, P.&nbsp;Shi, Y.&nbsp;Shi
<em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et&nbsp;al.</em>, “mplug-owl: Modularization empowers large language models
with multimodality,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">arXiv preprint arXiv:2304.14178</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Wang, Z.&nbsp;Chen, X.&nbsp;Chen, J.&nbsp;Wu, X.&nbsp;Zhu, G.&nbsp;Zeng, P.&nbsp;Luo, T.&nbsp;Lu, J.&nbsp;Zhou,
Y.&nbsp;Qiao <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et&nbsp;al.</em>, “Visionllm: Large language model is also an
open-ended decoder for vision-centric tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">arXiv preprint
arXiv:2305.11175</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Yang, L.&nbsp;Song, Y.&nbsp;Li, S.&nbsp;Zhao, Y.&nbsp;Ge, X.&nbsp;Li, and Y.&nbsp;Shan, “Gpt4tools:
Teaching large language model to use tools via self-instruction,”
<em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2305.18752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Saravia, “Prompt Engineering Guide,”
<em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">https://github.com/dair-ai/Prompt-Engineering-Guide</em>, 12 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Zeng, X.&nbsp;Liu, Z.&nbsp;Du, Z.&nbsp;Wang, H.&nbsp;Lai, M.&nbsp;Ding, Z.&nbsp;Yang, Y.&nbsp;Xu, W.&nbsp;Zheng,
X.&nbsp;Xia <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">et&nbsp;al.</em>, “Glm-130b: An open bilingual pre-trained model,”
<em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">arXiv preprint arXiv:2210.02414</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, H.&nbsp;Le, A.&nbsp;D. Gotmare, N.&nbsp;D. Bui, J.&nbsp;Li, and S.&nbsp;C. Hoi, “Codet5+: Open
code large language models for code understanding and generation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2305.07922</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Wang, Y.&nbsp;Sun, Y.&nbsp;Xiang, Z.&nbsp;Wu, S.&nbsp;Ding, W.&nbsp;Gong, S.&nbsp;Feng, J.&nbsp;Shang, Y.&nbsp;Zhao,
C.&nbsp;Pang <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et&nbsp;al.</em>, “Ernie 3.0 titan: Exploring larger-scale knowledge
enhanced pre-training for language understanding and generation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">arXiv preprint arXiv:2112.12731</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Rasley, S.&nbsp;Rajbhandari, O.&nbsp;Ruwase, and Y.&nbsp;He, “Deepspeed: System
optimizations enable training deep learning models with over 100 billion
parameters,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery &amp; Data Mining</em>, 2020, pp. 3505–3506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Rajbhandari, J.&nbsp;Rasley, O.&nbsp;Ruwase, and Y.&nbsp;He, “Zero: Memory optimizations
toward training trillion parameter models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">SC20: International
Conference for High Performance Computing, Networking, Storage and
Analysis</em>.&nbsp;&nbsp;&nbsp;IEEE, 2020, pp. 1–16.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;He, C.&nbsp;Zhou, X.&nbsp;Ma, T.&nbsp;Berg-Kirkpatrick, and G.&nbsp;Neubig, “Towards a unified
view of parameter-efficient transfer learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint
arXiv:2110.04366</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Hu, Y.&nbsp;Lan, L.&nbsp;Wang, W.&nbsp;Xu, E.-P. Lim, R.&nbsp;K.-W. Lee, L.&nbsp;Bing, and S.&nbsp;Poria,
“Llm-adapters: An adapter family for parameter-efficient fine-tuning of
large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2304.01933</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Lester, R.&nbsp;Al-Rfou, and N.&nbsp;Constant, “The power of scale for
parameter-efficient prompt tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2104.08691</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;L. Li and P.&nbsp;Liang, “Prefix-tuning: Optimizing continuous prompts for
generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2101.00190</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Pal, D.&nbsp;Karkhanis, M.&nbsp;Roberts, S.&nbsp;Dooley, A.&nbsp;Sundararajan, and S.&nbsp;Naidu,
“Giraffe: Adventures in expanding context lengths in llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv
preprint arXiv:2308.10882</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Peng, J.&nbsp;Quesnelle, H.&nbsp;Fan, and E.&nbsp;Shippole, “Yarn: Efficient context
window extension of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint
arXiv:2309.00071</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Guo, J.&nbsp;Ainslie, D.&nbsp;Uthus, S.&nbsp;Ontanon, J.&nbsp;Ni, Y.-H. Sung, and Y.&nbsp;Yang,
“Longt5: Efficient text-to-text transformer for long sequences,”
<em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2112.07916</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Chen, S.&nbsp;Wong, L.&nbsp;Chen, and Y.&nbsp;Tian, “Extending context window of large
language models via positional interpolation,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint
arXiv:2306.15595</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, K.&nbsp;Zhou, J.&nbsp;Li, T.&nbsp;Tang, X.&nbsp;Wang, Y.&nbsp;Hou, Y.&nbsp;Min, B.&nbsp;Zhang,
J.&nbsp;Zhang, Z.&nbsp;Dong <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">et&nbsp;al.</em>, “A survey of large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
U.&nbsp;Naseem, I.&nbsp;Razzak, S.&nbsp;K. Khan, and M.&nbsp;Prasad, “A comprehensive survey on
word representation models: From classical to state-of-the-art word
representation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Transactions on Asian and
Low-Resource Language Information Processing</em>, vol.&nbsp;20, no.&nbsp;5, pp. 1–35,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Min, H.&nbsp;Ross, E.&nbsp;Sulem, A.&nbsp;P.&nbsp;B. Veyseh, T.&nbsp;H. Nguyen, O.&nbsp;Sainz, E.&nbsp;Agirre,
I.&nbsp;Heinz, and D.&nbsp;Roth, “Recent advances in natural language processing via
large pre-trained language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint
arXiv:2111.01243</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Zhou, Q.&nbsp;Li, C.&nbsp;Li, J.&nbsp;Yu, Y.&nbsp;Liu, G.&nbsp;Wang, K.&nbsp;Zhang, C.&nbsp;Ji, Q.&nbsp;Yan, L.&nbsp;He
<em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">et&nbsp;al.</em>, “A comprehensive survey on pretrained foundation models: A
history from bert to chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.2.2">arXiv preprint arXiv:2302.09419</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Dong, L.&nbsp;Li, D.&nbsp;Dai, C.&nbsp;Zheng, Z.&nbsp;Wu, B.&nbsp;Chang, X.&nbsp;Sun, J.&nbsp;Xu, and Z.&nbsp;Sui,
“A survey for in-context learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2301.00234</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Huang and K.&nbsp;C.-C. Chang, “Towards reasoning in large language models: A
survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2212.10403</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, W.&nbsp;Zhong, L.&nbsp;Li, F.&nbsp;Mi, X.&nbsp;Zeng, W.&nbsp;Huang, L.&nbsp;Shang, X.&nbsp;Jiang, and
Q.&nbsp;Liu, “Aligning large language models with human: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv
preprint arXiv:2307.12966</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Zhu, J.&nbsp;Li, Y.&nbsp;Liu, C.&nbsp;Ma, and W.&nbsp;Wang, “A survey on model compression for
large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2308.07633</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yin, C.&nbsp;Fu, S.&nbsp;Zhao, K.&nbsp;Li, X.&nbsp;Sun, T.&nbsp;Xu, and E.&nbsp;Chen, “A survey on
multimodal large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2306.13549</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;J. Webster and C.&nbsp;Kit, “Tokenization as the initial phase in nlp,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">COLING 1992 volume 4: The 14th international conference on
computational linguistics</em>, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Kudo, “Subword regularization: Improving neural network translation models
with multiple subword candidates,” in <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers)</em>, 2018, pp. 66–75.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Sennrich, B.&nbsp;Haddow, and A.&nbsp;Birch, “Neural machine translation of rare
words with subword units,” in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers)</em>,
2016, pp. 1715–1725.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;J. Mielke, Z.&nbsp;Alyafeai, E.&nbsp;Salesky, C.&nbsp;Raffel, M.&nbsp;Dey, M.&nbsp;Gallé,
A.&nbsp;Raja, C.&nbsp;Si, W.&nbsp;Y. Lee, B.&nbsp;Sagot <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">et&nbsp;al.</em>, “Between words and
characters: A brief history of open-vocabulary modeling and tokenization in
nlp,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.2">arXiv preprint arXiv:2112.10508</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Schuster and K.&nbsp;Nakajima, “Japanese and korean voice search,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">2012 IEEE international conference on acoustics, speech and signal
processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2012, pp.
5149–5152.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;W. Eriksen and J.&nbsp;E. Hoffman, “Some characteristics of selective attention
in visual perception determined by vocal reaction time,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Perception &amp;
Psychophysics</em>, vol.&nbsp;11, no.&nbsp;2, pp. 169–171, 1972.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Bahdanau, K.&nbsp;Cho, and Y.&nbsp;Bengio, “Neural machine translation by jointly
learning to align and translate,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:1409.0473</em>,
2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Vaswani, N.&nbsp;Shazeer, N.&nbsp;Parmar, J.&nbsp;Uszkoreit, L.&nbsp;Jones, A.&nbsp;N. Gomez,
Ł.&nbsp;Kaiser, and I.&nbsp;Polosukhin, “Attention is all you need,”
<em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Advances in neural information processing systems</em>, vol.&nbsp;30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, and I.&nbsp;Sutskever, “Generating long sequences
with sparse transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:1904.10509</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Dao, D.&nbsp;Fu, S.&nbsp;Ermon, A.&nbsp;Rudra, and C.&nbsp;Ré, “Flashattention: Fast and
memory-efficient exact attention with io-awareness,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Advances in
Neural Information Processing Systems</em>, vol.&nbsp;35, pp. 16 344–16 359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Press, N.&nbsp;Smith, and M.&nbsp;Lewis, “Train short, test long: Attention with
linear biases enables input length extrapolation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">International
Conference on Learning Representations</em>, 2022. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=R8sQPpGCv0" title="">https://openreview.net/forum?id=R8sQPpGCv0</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Su, Y.&nbsp;Lu, S.&nbsp;Pan, A.&nbsp;Murtadha, B.&nbsp;Wen, and Y.&nbsp;Liu, “Roformer: Enhanced
transformer with rotary position embedding,” <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint
arXiv:2104.09864</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Kazemnejad, I.&nbsp;Padhi, K.&nbsp;N. Ramamurthy, P.&nbsp;Das, and S.&nbsp;Reddy, “The impact
of positional encoding on length generalization in transformers,”
<em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2305.19466</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Hornik, M.&nbsp;Stinchcombe, and H.&nbsp;White, “Multilayer feedforward networks are
universal approximators,” <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Neural networks</em>, vol.&nbsp;2, no.&nbsp;5, pp.
359–366, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Nair and G.&nbsp;E. Hinton, “Rectified linear units improve restricted boltzmann
machines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 27th international conference on
machine learning (ICML-10)</em>, 2010, pp. 807–814.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks and K.&nbsp;Gimpel, “Gaussian error linear units (gelus),”
<em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:1606.08415</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Srivastava, G.&nbsp;Hinton, A.&nbsp;Krizhevsky, I.&nbsp;Sutskever, and R.&nbsp;Salakhutdinov,
“Dropout: a simple way to prevent neural networks from overfitting,”
<em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">The journal of machine learning research</em>, vol.&nbsp;15, no.&nbsp;1, pp.
1929–1958, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Krueger, T.&nbsp;Maharaj, J.&nbsp;Kramár, M.&nbsp;Pezeshki, N.&nbsp;Ballas, N.&nbsp;R. Ke,
A.&nbsp;Goyal, Y.&nbsp;Bengio, A.&nbsp;Courville, and C.&nbsp;Pal, “Zoneout: Regularizing rnns
by randomly preserving hidden activations,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint
arXiv:1606.01305</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, “Glu variants improve transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint
arXiv:2002.05202</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;N. Dauphin, A.&nbsp;Fan, M.&nbsp;Auli, and D.&nbsp;Grangier, “Language modeling with gated
convolutional networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">International conference on machine
learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2017, pp. 933–941.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Zhang and R.&nbsp;Sennrich, “Root mean square layer normalization,”
<em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Advances in Neural Information Processing Systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Baevski and M.&nbsp;Auli, “Adaptive input representations for neural language
modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:1809.10853</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Shleifer, J.&nbsp;Weston, and M.&nbsp;Ott, “Normformer: Improved transformer
pretraining with extra normalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint
arXiv:2110.09456</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Wang, S.&nbsp;Ma, L.&nbsp;Dong, S.&nbsp;Huang, D.&nbsp;Zhang, and F.&nbsp;Wei, “Deepnet: Scaling
transformers to 1,000 layers,” <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2203.00555</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Shoeybi, M.&nbsp;Patwary, R.&nbsp;Puri, P.&nbsp;LeGresley, J.&nbsp;Casper, and B.&nbsp;Catanzaro,
“Megatron-lm: Training multi-billion parameter language models using model
parallelism,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“"bmtrain: Efficient training for big models.".” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/BMTrain" title="">https://github.com/OpenBMB/BMTrain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Wolf, L.&nbsp;Debut, V.&nbsp;Sanh, J.&nbsp;Chaumond, C.&nbsp;Delangue, A.&nbsp;Moi, P.&nbsp;Cistac,
T.&nbsp;Rault, R.&nbsp;Louf, M.&nbsp;Funtowicz <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">et&nbsp;al.</em>, “Transformers:
State-of-the-art natural language processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib81.2.2">Proceedings of the
2020 conference on empirical methods in natural language processing: system
demonstrations</em>, 2020, pp. 38–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Bradbury, R.&nbsp;Frostig, P.&nbsp;Hawkins, M.&nbsp;J. Johnson, C.&nbsp;Leary, D.&nbsp;Maclaurin,
G.&nbsp;Necula, A.&nbsp;Paszke, J.&nbsp;VanderPlas, S.&nbsp;Wanderman-Milne <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">et&nbsp;al.</em>, “Jax:
composable transformations of python+ numpy programs,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Li, J.&nbsp;Fang, Z.&nbsp;Bian, H.&nbsp;Liu, Y.&nbsp;Liu, H.&nbsp;Huang, B.&nbsp;Wang, and Y.&nbsp;You,
“Colossal-ai: A unified deep learning system for large-scale parallel
training,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2110.14883</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;He, J.&nbsp;Qiu, A.&nbsp;Zeng, Z.&nbsp;Yang, J.&nbsp;Zhai, and J.&nbsp;Tang, “Fastmoe: A fast
mixture-of-expert training system,” <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2103.13262</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Huawei Technologies&nbsp;Co., “Huawei mindspore ai development framework,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Artificial Intelligence Technology</em>.&nbsp;&nbsp;&nbsp;Springer, 2022, pp. 137–162.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Paszke, S.&nbsp;Gross, F.&nbsp;Massa, A.&nbsp;Lerer, J.&nbsp;Bradbury, G.&nbsp;Chanan, T.&nbsp;Killeen,
Z.&nbsp;Lin, N.&nbsp;Gimelshein, L.&nbsp;Antiga <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">et&nbsp;al.</em>, “Pytorch: An imperative
style, high-performance deep learning library,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.2.2">Advances in neural
information processing systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Abadi, P.&nbsp;Barham, J.&nbsp;Chen, Z.&nbsp;Chen, A.&nbsp;Davis, J.&nbsp;Dean, M.&nbsp;Devin,
S.&nbsp;Ghemawat, G.&nbsp;Irving, M.&nbsp;Isard <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">et&nbsp;al.</em>, “Tensorflow: a system for
large-scale machine learning.” in <em class="ltx_emph ltx_font_italic" id="bib.bib87.2.2">Osdi</em>, vol.&nbsp;16, no. 2016.&nbsp;&nbsp;&nbsp;Savannah, GA, USA, 2016, pp. 265–283.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Chen, M.&nbsp;Li, Y.&nbsp;Li, M.&nbsp;Lin, N.&nbsp;Wang, M.&nbsp;Wang, T.&nbsp;Xiao, B.&nbsp;Xu, C.&nbsp;Zhang, and
Z.&nbsp;Zhang, “Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:1512.01274</em>,
2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, M.&nbsp;Dehghani, V.&nbsp;Q. Tran, X.&nbsp;Garcia, J.&nbsp;Wei, X.&nbsp;Wang, H.&nbsp;W. Chung,
D.&nbsp;Bahri, T.&nbsp;Schuster, S.&nbsp;Zheng <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">et&nbsp;al.</em>, “Ul2: Unifying language
learning paradigms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib89.2.2">The Eleventh International Conference on
Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;J. Liu*, M.&nbsp;Saleh*, E.&nbsp;Pot, B.&nbsp;Goodrich, R.&nbsp;Sepassi, L.&nbsp;Kaiser, and
N.&nbsp;Shazeer, “Generating wikipedia by summarizing long sequences,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">International Conference on Learning Representations</em>, 2018. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Hyg0vbWC-" title="">https://openreview.net/forum?id=Hyg0vbWC-</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Wang, A.&nbsp;Roberts, D.&nbsp;Hesslow, T.&nbsp;Le&nbsp;Scao, H.&nbsp;W. Chung, I.&nbsp;Beltagy,
J.&nbsp;Launay, and C.&nbsp;Raffel, “What language model architecture and pretraining
objective works best for zero-shot generalization?” in <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">International
Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR,
2022, pp. 22 964–22 984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Dong, N.&nbsp;Yang, W.&nbsp;Wang, F.&nbsp;Wei, X.&nbsp;Liu, Y.&nbsp;Wang, J.&nbsp;Gao, M.&nbsp;Zhou, and H.-W.
Hon, “Unified language model pre-training for natural language understanding
and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Advances in neural information processing systems</em>,
vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Iyer, X.&nbsp;V. Lin, R.&nbsp;Pasunuru, T.&nbsp;Mihaylov, D.&nbsp;Simig, P.&nbsp;Yu, K.&nbsp;Shuster,
T.&nbsp;Wang, Q.&nbsp;Liu, P.&nbsp;S. Koura <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">et&nbsp;al.</em>, “Opt-iml: Scaling language model
instruction meta learning through the lens of generalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib93.2.2">arXiv
preprint arXiv:2212.12017</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, Y.&nbsp;Shen, Q.&nbsp;Zhou, H.&nbsp;Zhang, Z.&nbsp;Chen, D.&nbsp;Cox, Y.&nbsp;Yang, and C.&nbsp;Gan,
“Principle-driven self-alignment of language models from scratch with
minimal human supervision,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arXiv:2305.03047</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">et&nbsp;al.</em>, “A general language assistant
as a laboratory for alignment,” <em class="ltx_emph ltx_font_italic" id="bib.bib95.2.2">arXiv preprint arXiv:2112.00861</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;M. Ziegler, N.&nbsp;Stiennon, J.&nbsp;Wu, T.&nbsp;B. Brown, A.&nbsp;Radford, D.&nbsp;Amodei,
P.&nbsp;Christiano, and G.&nbsp;Irving, “Fine-tuning language models from human
preferences,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:1909.08593</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Kim, S.&nbsp;J. Joo, D.&nbsp;Kim, J.&nbsp;Jang, S.&nbsp;Ye, J.&nbsp;Shin, and M.&nbsp;Seo, “The cot
collection: Improving zero-shot and few-shot learning of language models via
chain-of-thought fine-tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">arXiv preprint arXiv:2305.14045</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Liu, F.&nbsp;Zhou, Z.&nbsp;Jiang, L.&nbsp;Dou, and M.&nbsp;Lin, “From zero to hero: Examining
the power of symbolic tasks in instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint
arXiv:2304.07995</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, X.&nbsp;Wang, D.&nbsp;Schuurmans, M.&nbsp;Bosma, F.&nbsp;Xia, E.&nbsp;Chi, Q.&nbsp;V. Le, D.&nbsp;Zhou
<em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">et&nbsp;al.</em>, “Chain-of-thought prompting elicits reasoning in large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.2.2">Advances in Neural Information Processing Systems</em>,
vol.&nbsp;35, pp. 24 824–24 837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, S.&nbsp;Narang, A.&nbsp;Chowdhery, and
D.&nbsp;Zhou, “Self-consistency improves chain of thought reasoning in language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2203.11171</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yao, D.&nbsp;Yu, J.&nbsp;Zhao, I.&nbsp;Shafran, T.&nbsp;L. Griffiths, Y.&nbsp;Cao, and K.&nbsp;Narasimhan,
“Tree of thoughts: Deliberate problem solving with large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2305.10601</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Houlsby, A.&nbsp;Giurgiu, S.&nbsp;Jastrzebski, B.&nbsp;Morrone, Q.&nbsp;De&nbsp;Laroussilhe,
A.&nbsp;Gesmundo, M.&nbsp;Attariyan, and S.&nbsp;Gelly, “Parameter-efficient transfer
learning for nlp,” in <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">International Conference on Machine
Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2019, pp. 2790–2799.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;McCandlish, J.&nbsp;Kaplan, D.&nbsp;Amodei, and O.&nbsp;D. Team, “An empirical model of
large-batch training,” <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">arXiv preprint arXiv:1812.06162</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Zeng, X.&nbsp;Ren, T.&nbsp;Su, H.&nbsp;Wang, Y.&nbsp;Liao, Z.&nbsp;Wang, X.&nbsp;Jiang, Z.&nbsp;Yang, K.&nbsp;Wang,
X.&nbsp;Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib104.2.1">et&nbsp;al.</em>, “Pangu-<math alttext="\alpha" class="ltx_Math" display="inline" id="bib.bib104.1.m1.1"><semantics id="bib.bib104.1.m1.1a"><mi id="bib.bib104.1.m1.1.1" xref="bib.bib104.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib104.1.m1.1b"><ci id="bib.bib104.1.m1.1.1.cmml" xref="bib.bib104.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.1.m1.1d">italic_α</annotation></semantics></math> : Large-scale autoregressive
pretrained chinese language models with auto-parallel computation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib104.3.2">arXiv preprint arXiv:2104.12369</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yuan, H.&nbsp;Zhao, Z.&nbsp;Du, M.&nbsp;Ding, X.&nbsp;Liu, Y.&nbsp;Cen, X.&nbsp;Zou, Z.&nbsp;Yang, and J.&nbsp;Tang,
“Wudaocorpora: A super large-scale chinese corpora for pre-training language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">AI Open</em>, vol.&nbsp;2, pp. 65–68, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Lester, R.&nbsp;Al-Rfou, and N.&nbsp;Constant, “The power of scale for
parameter-efficient prompt tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2104.08691</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, S.&nbsp;Wang, S.&nbsp;Feng, S.&nbsp;Ding, C.&nbsp;Pang, J.&nbsp;Shang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Zhao,
Y.&nbsp;Lu <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">et&nbsp;al.</em>, “Ernie 3.0: Large-scale knowledge enhanced pre-training
for language understanding and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib107.2.2">arXiv preprint
arXiv:2107.02137</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Dai, Z.&nbsp;Yang, Y.&nbsp;Yang, J.&nbsp;Carbonell, Q.&nbsp;V. Le, and R.&nbsp;Salakhutdinov,
“Transformer-xl: Attentive language models beyond a fixed-length context,”
<em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:1901.02860</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Lieber, O.&nbsp;Sharir, B.&nbsp;Lenz, and Y.&nbsp;Shoham, “Jurassic-1: Technical details
and evaluation,” <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">White Paper. AI21 Labs</em>, vol.&nbsp;1, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Levine, N.&nbsp;Wies, O.&nbsp;Sharir, H.&nbsp;Bata, and A.&nbsp;Shashua, “Limits to depth
efficiencies of self-attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Advances in Neural Information
Processing Systems</em>, vol.&nbsp;33, pp. 22 640–22 651, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Kim, H.&nbsp;Kim, S.-W. Lee, G.&nbsp;Lee, D.&nbsp;Kwak, D.&nbsp;H. Jeon, S.&nbsp;Park, S.&nbsp;Kim,
S.&nbsp;Kim, D.&nbsp;Seo <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">et&nbsp;al.</em>, “What changes can large-scale language models
bring? intensive study on hyperclova: Billions-scale korean generative
pretrained transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib111.2.2">arXiv preprint arXiv:2109.04650</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Wu, X.&nbsp;Zhao, T.&nbsp;Yu, R.&nbsp;Zhang, C.&nbsp;Shen, H.&nbsp;Liu, F.&nbsp;Li, H.&nbsp;Zhu, J.&nbsp;Luo, L.&nbsp;Xu
<em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">et&nbsp;al.</em>, “Yuan 1.0: Large-scale pre-trained language model in
zero-shot and few-shot learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib112.2.2">arXiv preprint arXiv:2110.04725</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, F.&nbsp;Song,
J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">et&nbsp;al.</em>, “Scaling
language models: Methods, analysis &amp; insights from training gopher,”
<em class="ltx_emph ltx_font_italic" id="bib.bib113.2.2">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Smith, M.&nbsp;Patwary, B.&nbsp;Norick, P.&nbsp;LeGresley, S.&nbsp;Rajbhandari, J.&nbsp;Casper,
Z.&nbsp;Liu, S.&nbsp;Prabhumoye, G.&nbsp;Zerveas, V.&nbsp;Korthikanti <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">et&nbsp;al.</em>, “Using
deepspeed and megatron to train megatron-turing nlg 530b, a large-scale
generative language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.2.2">arXiv preprint arXiv:2201.11990</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Black, S.&nbsp;Biderman, E.&nbsp;Hallahan, Q.&nbsp;Anthony, L.&nbsp;Gao, L.&nbsp;Golding, H.&nbsp;He,
C.&nbsp;Leahy, K.&nbsp;McDonell, J.&nbsp;Phang <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">et&nbsp;al.</em>, “Gpt-neox-20b: An open-source
autoregressive language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib115.2.2">arXiv preprint arXiv:2204.06745</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Ben and K.&nbsp;Aran, “Gpt-j-6b: A 6 billion parameter autoregressive language
model,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Micikevicius, S.&nbsp;Narang, J.&nbsp;Alben, G.&nbsp;Diamos, E.&nbsp;Elsen, D.&nbsp;Garcia,
B.&nbsp;Ginsburg, M.&nbsp;Houston, O.&nbsp;Kuchaiev, G.&nbsp;Venkatesh <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">et&nbsp;al.</em>, “Mixed
precision training,” <em class="ltx_emph ltx_font_italic" id="bib.bib117.2.2">arXiv preprint arXiv:1710.03740</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Du, Y.&nbsp;Huang, A.&nbsp;M. Dai, S.&nbsp;Tong, D.&nbsp;Lepikhin, Y.&nbsp;Xu, M.&nbsp;Krikun, Y.&nbsp;Zhou,
A.&nbsp;W. Yu, O.&nbsp;Firat <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">et&nbsp;al.</em>, “Glam: Efficient scaling of language
models with mixture-of-experts,” in <em class="ltx_emph ltx_font_italic" id="bib.bib118.2.2">International Conference on
Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp.
5547–5569.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, A.&nbsp;Mirhoseini, K.&nbsp;Maziarz, A.&nbsp;Davis, Q.&nbsp;Le, G.&nbsp;Hinton, and J.&nbsp;Dean,
“Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer,” <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:1701.06538</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Fedus, B.&nbsp;Zoph, and N.&nbsp;Shazeer, “Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">The Journal of
Machine Learning Research</em>, vol.&nbsp;23, no.&nbsp;1, pp. 5232–5270, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;d.&nbsp;L. Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">et&nbsp;al.</em>, “Training
compute-optimal large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib121.2.2">arXiv preprint
arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Soltan, S.&nbsp;Ananthakrishnan, J.&nbsp;FitzGerald, R.&nbsp;Gupta, W.&nbsp;Hamza, H.&nbsp;Khan,
C.&nbsp;Peris, S.&nbsp;Rawls, A.&nbsp;Rosenbaum, A.&nbsp;Rumshisky <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">et&nbsp;al.</em>, “Alexatm 20b:
Few-shot learning using a large-scale multilingual seq2seq model,”
<em class="ltx_emph ltx_font_italic" id="bib.bib122.2.2">arXiv preprint arXiv:2208.01448</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Anil, A.&nbsp;M. Dai, O.&nbsp;Firat, M.&nbsp;Johnson, D.&nbsp;Lepikhin, A.&nbsp;Passos, S.&nbsp;Shakeri,
E.&nbsp;Taropa, P.&nbsp;Bailey, Z.&nbsp;Chen <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">et&nbsp;al.</em>, “Palm 2 technical report,”
<em class="ltx_emph ltx_font_italic" id="bib.bib123.2.2">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, J.&nbsp;Wei, H.&nbsp;W. Chung, V.&nbsp;Q. Tran, D.&nbsp;R. So, S.&nbsp;Shakeri, X.&nbsp;Garcia, H.&nbsp;S.
Zheng, J.&nbsp;Rao, A.&nbsp;Chowdhery <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">et&nbsp;al.</em>, “Transcending scaling laws with
0.1% extra compute,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.2.2">arXiv preprint arXiv:2210.11399</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Du, Y.&nbsp;Qian, X.&nbsp;Liu, M.&nbsp;Ding, J.&nbsp;Qiu, Z.&nbsp;Yang, and J.&nbsp;Tang, “Glm: General
language model pretraining with autoregressive blank infilling,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, 2022, pp. 320–335.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix,
B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">et&nbsp;al.</em>, “Llama: Open
and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib126.2.2">arXiv preprint
arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;N. Rabe and C.&nbsp;Staats, “Self-attention does not need o(n<math alttext="{}^{2}" class="ltx_Math" display="inline" id="bib.bib127.1.m1.1"><semantics id="bib.bib127.1.m1.1a"><msup id="bib.bib127.1.m1.1.1" xref="bib.bib127.1.m1.1.1.cmml"><mi id="bib.bib127.1.m1.1.1a" xref="bib.bib127.1.m1.1.1.cmml"></mi><mn id="bib.bib127.1.m1.1.1.1" xref="bib.bib127.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib127.1.m1.1b"><apply id="bib.bib127.1.m1.1.1.cmml" xref="bib.bib127.1.m1.1.1"><cn id="bib.bib127.1.m1.1.1.1.cmml" type="integer" xref="bib.bib127.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib127.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib127.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>) memory,”
<em class="ltx_emph ltx_font_italic" id="bib.bib127.2.1">arXiv preprint arXiv:2112.05682</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;A. Korthikanti, J.&nbsp;Casper, S.&nbsp;Lym, L.&nbsp;McAfee, M.&nbsp;Andersch, M.&nbsp;Shoeybi, and
B.&nbsp;Catanzaro, “Reducing activation recomputation in large transformer
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of Machine Learning and Systems</em>, vol.&nbsp;5, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Ren, P.&nbsp;Zhou, X.&nbsp;Meng, X.&nbsp;Huang, Y.&nbsp;Wang, W.&nbsp;Wang, P.&nbsp;Li, X.&nbsp;Zhang,
A.&nbsp;Podolskiy, G.&nbsp;Arshinov <em class="ltx_emph ltx_font_italic" id="bib.bib129.2.1">et&nbsp;al.</em>, “Pangu-<math alttext="\sum" class="ltx_Math" display="inline" id="bib.bib129.1.m1.1"><semantics id="bib.bib129.1.m1.1a"><mo id="bib.bib129.1.m1.1.1" xref="bib.bib129.1.m1.1.1.cmml">∑</mo><annotation-xml encoding="MathML-Content" id="bib.bib129.1.m1.1b"><sum id="bib.bib129.1.m1.1.1.cmml" xref="bib.bib129.1.m1.1.1"></sum></annotation-xml><annotation encoding="application/x-tex" id="bib.bib129.1.m1.1c">\sum</annotation><annotation encoding="application/x-llamapun" id="bib.bib129.1.m1.1d">∑</annotation></semantics></math>: Towards trillion
parameter language model with sparse heterogeneous computing,” <em class="ltx_emph ltx_font_italic" id="bib.bib129.3.2">arXiv
preprint arXiv:2303.10845</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Nijkamp, B.&nbsp;Pang, H.&nbsp;Hayashi, L.&nbsp;Tu, H.&nbsp;Wang, Y.&nbsp;Zhou, S.&nbsp;Savarese, and
C.&nbsp;Xiong, “Codegen: An open large language model for code with multi-turn
program synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:2203.13474</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Chen, J.&nbsp;Tworek, H.&nbsp;Jun, Q.&nbsp;Yuan, H.&nbsp;P. d.&nbsp;O. Pinto, J.&nbsp;Kaplan, H.&nbsp;Edwards,
Y.&nbsp;Burda, N.&nbsp;Joseph, G.&nbsp;Brockman <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">et&nbsp;al.</em>, “Evaluating large language
models trained on code,” <em class="ltx_emph ltx_font_italic" id="bib.bib131.2.2">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Li, D.&nbsp;Choi, J.&nbsp;Chung, N.&nbsp;Kushman, J.&nbsp;Schrittwieser, R.&nbsp;Leblond, T.&nbsp;Eccles,
J.&nbsp;Keeling, F.&nbsp;Gimeno, A.&nbsp;Dal&nbsp;Lago <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">et&nbsp;al.</em>, “Competition-level code
generation with alphacode,” <em class="ltx_emph ltx_font_italic" id="bib.bib132.2.2">Science</em>, vol. 378, no. 6624, pp.
1092–1097, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, “Fast transformer decoding: One write-head is all you need,”
<em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:1911.02150</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Y. Pang and H.&nbsp;He, “Text generation by learning from demonstrations,”
<em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">arXiv preprint arXiv:2009.07839</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Dabre and A.&nbsp;Fujita, “Softmax tempering for training neural machine
translation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2009.09372</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, W.&nbsp;Wang, S.&nbsp;Joty, and S.&nbsp;C. Hoi, “Codet5: Identifier-aware unified
pre-trained encoder-decoder models for code understanding and generation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">arXiv preprint arXiv:2109.00859</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Li, L.&nbsp;B. Allal, Y.&nbsp;Zi, N.&nbsp;Muennighoff, D.&nbsp;Kocetkov, C.&nbsp;Mou, M.&nbsp;Marone,
C.&nbsp;Akiki, J.&nbsp;Li, J.&nbsp;Chim <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">et&nbsp;al.</em>, “Starcoder: may the source be with
you!” <em class="ltx_emph ltx_font_italic" id="bib.bib137.2.2">arXiv preprint arXiv:2305.06161</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia,
A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic, “Galactica: A large language model
for science,” <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2211.09085</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
FairScale authors, “Fairscale: A general purpose modular pytorch library for
high performance and large scale training,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairscale" title="">https://github.com/facebookresearch/fairscale</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng,
A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">et&nbsp;al.</em>, “Lamda: Language models for
dialog applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib140.2.2">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Wu, O.&nbsp;Irsoy, S.&nbsp;Lu, V.&nbsp;Dabravolski, M.&nbsp;Dredze, S.&nbsp;Gehrmann, P.&nbsp;Kambadur,
D.&nbsp;Rosenberg, and G.&nbsp;Mann, “Bloomberggpt: A large language model for
finance,” <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">arXiv preprint arXiv:2303.17564</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Levine, N.&nbsp;Wies, O.&nbsp;Sharir, H.&nbsp;Bata, and A.&nbsp;Shashua, “Limits to depth
efficiencies of self-attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">Advances in Neural Information
Processing Systems</em>, vol.&nbsp;33, pp. 22 640–22 651, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Zhang, Q.&nbsp;Yang, and D.&nbsp;Xu, “Xuanyuan 2.0: A large chinese financial chat
model with hundreds of billions parameters,” <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">arXiv preprint
arXiv:2305.12002</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Ben, “Mesh-transformer-jax: Model-parallel implementation of transformer
language model with jax,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, T.&nbsp;Wang, L.&nbsp;Sutawika, A.&nbsp;Roberts, S.&nbsp;Biderman, T.&nbsp;L. Scao,
M.&nbsp;S. Bari, S.&nbsp;Shen, Z.-X. Yong, H.&nbsp;Schoelkopf <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">et&nbsp;al.</em>, “Crosslingual
generalization through multitask finetuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib145.2.2">arXiv preprint
arXiv:2211.01786</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Yin, X.&nbsp;Liu, F.&nbsp;Yin, M.&nbsp;Zhong, H.&nbsp;Bansal, J.&nbsp;Han, and K.-W. Chang,
“Dynosaur: A dynamic growth paradigm for instruction-tuning data curation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">arXiv preprint arXiv:2305.14327</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Gao, J.&nbsp;Han, R.&nbsp;Zhang, Z.&nbsp;Lin, S.&nbsp;Geng, A.&nbsp;Zhou, W.&nbsp;Zhang, P.&nbsp;Lu, C.&nbsp;He,
X.&nbsp;Yue <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">et&nbsp;al.</em>, “Llama-adapter v2: Parameter-efficient visual
instruction model,” <em class="ltx_emph ltx_font_italic" id="bib.bib147.2.2">arXiv preprint arXiv:2304.15010</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“Openai. gpt-4 technical report,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto, “Stanford alpaca: An instruction-following llama model,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.-L. Chiang, Z.&nbsp;Li, Z.&nbsp;Lin, Y.&nbsp;Sheng, Z.&nbsp;Wu, H.&nbsp;Zhang, L.&nbsp;Zheng, S.&nbsp;Zhuang,
Y.&nbsp;Zhuang, J.&nbsp;E. Gonzalez, I.&nbsp;Stoica, and E.&nbsp;P. Xing, “Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” March
2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">https://lmsys.org/blog/2023-03-30-vicuna/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Peng, C.&nbsp;Li, P.&nbsp;He, M.&nbsp;Galley, and J.&nbsp;Gao, “Instruction tuning with
gpt-4,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">arXiv preprint arXiv:2304.03277</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Liu and B.&nbsp;K.&nbsp;H. Low, “Goat: Fine-tuned llama outperforms gpt-4 on
arithmetic tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">arXiv preprint arXiv:2305.14201</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Wang, C.&nbsp;Liu, N.&nbsp;Xi, Z.&nbsp;Qiang, S.&nbsp;Zhao, B.&nbsp;Qin, and T.&nbsp;Liu, “Huatuo: Tuning
llama model with chinese medical knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">arXiv preprint
arXiv:2304.06975</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xu, Q.&nbsp;Sun, K.&nbsp;Zheng, X.&nbsp;Geng, P.&nbsp;Zhao, J.&nbsp;Feng, C.&nbsp;Tao, and D.&nbsp;Jiang,
“Wizardlm: Empowering large language models to follow complex
instructions,” <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">arXiv preprint arXiv:2304.12244</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Luo, C.&nbsp;Xu, P.&nbsp;Zhao, Q.&nbsp;Sun, X.&nbsp;Geng, W.&nbsp;Hu, C.&nbsp;Tao, J.&nbsp;Ma, Q.&nbsp;Lin, and
D.&nbsp;Jiang, “Wizardcoder: Empowering code large language models with
evol-instruct,” <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2306.08568</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Menick, M.&nbsp;Trebacz, V.&nbsp;Mikulik, J.&nbsp;Aslanides, F.&nbsp;Song, M.&nbsp;Chadwick,
M.&nbsp;Glaese, S.&nbsp;Young, L.&nbsp;Campbell-Gillingham, G.&nbsp;Irving <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">et&nbsp;al.</em>,
“Teaching language models to support answers with verified quotes,”
<em class="ltx_emph ltx_font_italic" id="bib.bib156.2.2">arXiv preprint arXiv:2203.11147</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Nakano, J.&nbsp;Hilton, S.&nbsp;Balaji, J.&nbsp;Wu, L.&nbsp;Ouyang, C.&nbsp;Kim, C.&nbsp;Hesse, S.&nbsp;Jain,
V.&nbsp;Kosaraju, W.&nbsp;Saunders <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">et&nbsp;al.</em>, “Webgpt: Browser-assisted
question-answering with human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib157.2.2">arXiv preprint
arXiv:2112.09332</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Glaese, N.&nbsp;McAleese, M.&nbsp;Trębacz, J.&nbsp;Aslanides, V.&nbsp;Firoiu, T.&nbsp;Ewalds,
M.&nbsp;Rauh, L.&nbsp;Weidinger, M.&nbsp;Chadwick, P.&nbsp;Thacker <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">et&nbsp;al.</em>, “Improving
alignment of dialogue agents via targeted human judgements,” <em class="ltx_emph ltx_font_italic" id="bib.bib158.2.2">arXiv
preprint arXiv:2209.14375</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn,
“Direct preference optimization: Your language model is secretly a reward
model,” <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Dong, W.&nbsp;Xiong, D.&nbsp;Goyal, R.&nbsp;Pan, S.&nbsp;Diao, J.&nbsp;Zhang, K.&nbsp;Shum, and T.&nbsp;Zhang,
“Raft: Reward ranked finetuning for generative foundation model alignment,”
<em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">arXiv preprint arXiv:2304.06767</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Tan, W.&nbsp;Wang, S.&nbsp;Huang, and F.&nbsp;Huang, “Rrhf: Rank
responses to align language models with human feedback without tears,”
<em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">arXiv preprint arXiv:2304.05302</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Song, B.&nbsp;Yu, M.&nbsp;Li, H.&nbsp;Yu, F.&nbsp;Huang, Y.&nbsp;Li, and H.&nbsp;Wang, “Preference
ranking optimization for human alignment,” <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">arXiv preprint
arXiv:2306.17492</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Sferrazza, and P.&nbsp;Abbeel, “Languages are rewards: Hindsight
finetuning using human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2302.02676</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, S.&nbsp;Kadavath, S.&nbsp;Kundu, A.&nbsp;Askell, J.&nbsp;Kernion, A.&nbsp;Jones, A.&nbsp;Chen,
A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;McKinnon <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">et&nbsp;al.</em>, “Constitutional ai:
Harmlessness from ai feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib164.2.2">arXiv preprint arXiv:2212.08073</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Dubois, X.&nbsp;Li, R.&nbsp;Taori, T.&nbsp;Zhang, I.&nbsp;Gulrajani, J.&nbsp;Ba, C.&nbsp;Guestrin,
P.&nbsp;Liang, and T.&nbsp;B. Hashimoto, “Alpacafarm: A simulation framework for
methods that learn from human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">arXiv preprint
arXiv:2305.14387</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Si, Z.&nbsp;Gan, Z.&nbsp;Yang, S.&nbsp;Wang, J.&nbsp;Wang, J.&nbsp;Boyd-Graber, and L.&nbsp;Wang,
“Prompting gpt-3 to be reliable,” <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">arXiv preprint arXiv:2210.09150</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, A.&nbsp;Askell, N.&nbsp;Schiefer, T.&nbsp;Liao, K.&nbsp;Lukošiūtė,
A.&nbsp;Chen, A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;Olsson, D.&nbsp;Hernandez <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">et&nbsp;al.</em>,
“The capacity for moral self-correction in large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib167.2.2">arXiv preprint arXiv:2302.07459</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Wei, N.&nbsp;Haghtalab, and J.&nbsp;Steinhardt, “Jailbroken: How does llm safety
training fail?” <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">arXiv preprint arXiv:2307.02483</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, L.&nbsp;Lovitt, J.&nbsp;Kernion, A.&nbsp;Askell, Y.&nbsp;Bai, S.&nbsp;Kadavath, B.&nbsp;Mann,
E.&nbsp;Perez, N.&nbsp;Schiefer, K.&nbsp;Ndousse <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">et&nbsp;al.</em>, “Red teaming language
models to reduce harms: Methods, scaling behaviors, and lessons learned,”
<em class="ltx_emph ltx_font_italic" id="bib.bib169.2.2">arXiv preprint arXiv:2209.07858</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Casper, J.&nbsp;Lin, J.&nbsp;Kwon, G.&nbsp;Culp, and D.&nbsp;Hadfield-Menell, “Explore,
establish, exploit: Red teaming language models from scratch,” <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">arXiv
preprint arXiv:2306.09442</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Huang, F.&nbsp;Song, T.&nbsp;Cai, R.&nbsp;Ring, J.&nbsp;Aslanides, A.&nbsp;Glaese,
N.&nbsp;McAleese, and G.&nbsp;Irving, “Red teaming language models with language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2202.03286</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Scialom, T.&nbsp;Chakrabarty, and S.&nbsp;Muresan, “Fine-tuned language models are
continual learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing</em>, 2022, pp. 6107–6122.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Shi and A.&nbsp;Lipani, “Don’t stop pretraining? make prompt-based fine-tuning
powerful learner,” <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">arXiv preprint arXiv:2305.01711</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Gupta, S.&nbsp;A. Sawant, S.&nbsp;Mishra, M.&nbsp;Nakamura, A.&nbsp;Mitra, S.&nbsp;Mashetty, and
C.&nbsp;Baral, “Instruction tuned models are quick learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">arXiv
preprint arXiv:2306.05539</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Chen, Y.&nbsp;Zhang, Q.&nbsp;Zhang, H.&nbsp;Yang, X.&nbsp;Hu, X.&nbsp;Ma, Y.&nbsp;Yanggong, and J.&nbsp;Zhao,
“Maybe only 0.5% data is needed: A preliminary exploration of low training
data instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2305.09246</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Zhou, P.&nbsp;Liu, P.&nbsp;Xu, S.&nbsp;Iyer, J.&nbsp;Sun, Y.&nbsp;Mao, X.&nbsp;Ma, A.&nbsp;Efrat, P.&nbsp;Yu, L.&nbsp;Yu
<em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">et&nbsp;al.</em>, “Lima: Less is more for alignment,” <em class="ltx_emph ltx_font_italic" id="bib.bib176.2.2">arXiv preprint
arXiv:2305.11206</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Han, Q.&nbsp;Wang, W.&nbsp;Xiong, Y.&nbsp;Chen, H.&nbsp;Ji, and S.&nbsp;Wang, “Lm-infinite: Simple
on-the-fly length generalization for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">arXiv
preprint arXiv:2308.16137</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ainslie, T.&nbsp;Lei, M.&nbsp;de&nbsp;Jong, S.&nbsp;Ontañón, S.&nbsp;Brahma, Y.&nbsp;Zemlyanskiy,
D.&nbsp;Uthus, M.&nbsp;Guo, J.&nbsp;Lee-Thorp, Y.&nbsp;Tay <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">et&nbsp;al.</em>, “Colt5: Faster
long-range transformers with conditional computation,” <em class="ltx_emph ltx_font_italic" id="bib.bib178.2.2">arXiv preprint
arXiv:2303.09752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ding, S.&nbsp;Ma, L.&nbsp;Dong, X.&nbsp;Zhang, S.&nbsp;Huang, W.&nbsp;Wang, and F.&nbsp;Wei, “Longnet:
Scaling transformers to 1,000,000,000 tokens,” <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">arXiv preprint
arXiv:2307.02486</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Chen, S.&nbsp;Qian, H.&nbsp;Tang, X.&nbsp;Lai, Z.&nbsp;Liu, S.&nbsp;Han, and J.&nbsp;Jia, “Longlora:
Efficient fine-tuning of long-context large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">arXiv
preprint arXiv:2309.12307</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Ratner, Y.&nbsp;Levine, Y.&nbsp;Belinkov, O.&nbsp;Ram, I.&nbsp;Magar, O.&nbsp;Abend, E.&nbsp;Karpas,
A.&nbsp;Shashua, K.&nbsp;Leyton-Brown, and Y.&nbsp;Shoham, “Parallel context windows for
large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2023,
pp. 6383–6402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Lykov and D.&nbsp;Tsetserukou, “Llm-brain: Ai-driven fast generation of robot
behaviour tree based on large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">arXiv preprint
arXiv:2305.19352</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Billing, J.&nbsp;Rosén, and M.&nbsp;Lamb, “Language models for human-robot
interaction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">ACM/IEEE International Conference on Human-Robot
Interaction, March 13–16, 2023, Stockholm, Sweden</em>.&nbsp;&nbsp;&nbsp;ACM Digital Library, 2023, pp. 905–906.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Ye, H.&nbsp;You, and J.&nbsp;Du, “Improved trust in human-robot collaboration with
chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">IEEE Access</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Singh, V.&nbsp;Blukis, A.&nbsp;Mousavian, A.&nbsp;Goyal, D.&nbsp;Xu, J.&nbsp;Tremblay, D.&nbsp;Fox,
J.&nbsp;Thomason, and A.&nbsp;Garg, “Progprompt: Generating situated robot task plans
using large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">2023 IEEE International Conference on
Robotics and Automation (ICRA)</em>.&nbsp;&nbsp;&nbsp;IEEE,
2023, pp. 11 523–11 530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhen, S.&nbsp;Bi, L.&nbsp;Xing-tong, P.&nbsp;Wei-qin, S.&nbsp;Hai-peng, C.&nbsp;Zi-rui, and
F.&nbsp;Yi-shu, “Robot task planning based on large language model representing
knowledge with directed graph structures,” <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">arXiv preprint
arXiv:2306.05171</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Huang, P.&nbsp;Abbeel, D.&nbsp;Pathak, and I.&nbsp;Mordatch, “Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 9118–9147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Ding, X.&nbsp;Zhang, C.&nbsp;Paxton, and S.&nbsp;Zhang, “Task and motion planning with
large language models for object rearrangement,” <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">arXiv preprint
arXiv:2303.06247</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Leveraging commonsense knowledge from large language models for task
and motion planning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">RSS 2023 Workshop on Learning for Task and
Motion Planning</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Ge, W.&nbsp;Hua, J.&nbsp;Ji, J.&nbsp;Tan, S.&nbsp;Xu, and Y.&nbsp;Zhang, “Openagi: When llm meets
domain experts,” <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">arXiv preprint arXiv:2304.04370</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Zhong, Y.&nbsp;Wei, L.&nbsp;Yang, Z.&nbsp;Wu, Z.&nbsp;Liu, X.&nbsp;Wei, W.&nbsp;Li, J.&nbsp;Yao, C.&nbsp;Ma, X.&nbsp;Li
<em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">et&nbsp;al.</em>, “Chatabl: Abductive learning via natural language interaction
with chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib191.2.2">arXiv preprint arXiv:2304.11107</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wu, R.&nbsp;Antonova, A.&nbsp;Kan, M.&nbsp;Lepert, A.&nbsp;Zeng, S.&nbsp;Song, J.&nbsp;Bohg,
S.&nbsp;Rusinkiewicz, and T.&nbsp;Funkhouser, “Tidybot: Personalized robot assistance
with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">arXiv preprint arXiv:2305.05658</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Huang, F.&nbsp;Xia, T.&nbsp;Xiao, H.&nbsp;Chan, J.&nbsp;Liang, P.&nbsp;Florence, A.&nbsp;Zeng, J.&nbsp;Tompson,
I.&nbsp;Mordatch, Y.&nbsp;Chebotar, P.&nbsp;Sermanet, T.&nbsp;Jackson, N.&nbsp;Brown, L.&nbsp;Luu,
S.&nbsp;Levine, K.&nbsp;Hausman, and brian ichter, “Inner monologue: Embodied
reasoning through planning with language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">6th Annual
Conference on Robot Learning</em>, 2022. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=3R3Pz5i0tye" title="">https://openreview.net/forum?id=3R3Pz5i0tye</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;S. Kannan, V.&nbsp;L. Venkatesh, and B.-C. Min, “Smart-llm: Smart multi-agent
robot task planning using large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">arXiv preprint
arXiv:2309.10062</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Singh, V.&nbsp;Blukis, A.&nbsp;Mousavian, A.&nbsp;Goyal, D.&nbsp;Xu, J.&nbsp;Tremblay, D.&nbsp;Fox,
J.&nbsp;Thomason, and A.&nbsp;Garg, “Progprompt: program generation for situated robot
task planning using large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">Autonomous Robots</em>, pp.
1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Jin, W.&nbsp;Tan, J.&nbsp;Yang, B.&nbsp;Liu, R.&nbsp;Song, L.&nbsp;Wang, and J.&nbsp;Fu, “Alphablock:
Embodied finetuning for vision-language reasoning in robot manipulation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">arXiv preprint arXiv:2305.18898</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Chalvatzaki, A.&nbsp;Younes, D.&nbsp;Nandha, A.&nbsp;T. Le, L.&nbsp;F. Ribeiro, and I.&nbsp;Gurevych,
“Learning to reason over scene graphs: a case study of finetuning gpt-2 into
a robot language model for grounded task planning,” <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">Frontiers in
Robotics and AI</em>, vol.&nbsp;10, p. 1221739, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Ha, P.&nbsp;Florence, and S.&nbsp;Song, “Scaling up and distilling down:
Language-guided robot skill acquisition,” <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">arXiv preprint
arXiv:2307.14535</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Mandi, S.&nbsp;Jain, and S.&nbsp;Song, “Roco: Dialectic multi-robot collaboration
with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">arXiv preprint arXiv:2307.04738</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Rajvanshi, K.&nbsp;Sikka, X.&nbsp;Lin, B.&nbsp;Lee, H.-P. Chiu, and A.&nbsp;Velasquez, “Saynav:
Grounding large language models for dynamic planning to navigation in new
environments,” <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">arXiv preprint arXiv:2309.04077</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;H. Song, J.&nbsp;Wu, C.&nbsp;Washington, B.&nbsp;M. Sadler, W.-L. Chao, and Y.&nbsp;Su,
“Llm-planner: Few-shot grounded planning for embodied agents with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">arXiv preprint arXiv:2212.04088</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;S. Dorbala, J.&nbsp;F. Mullen&nbsp;Jr, and D.&nbsp;Manocha, “Can an embodied agent find
your" cat-shaped mug"? llm-based zero-shot object navigation,” <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">arXiv
preprint arXiv:2303.03480</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Huang, O.&nbsp;Mees, A.&nbsp;Zeng, and W.&nbsp;Burgard, “Visual language maps for robot
navigation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2023, pp.
10 608–10 615.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.-B. Alayrac, J.&nbsp;Donahue, P.&nbsp;Luc, A.&nbsp;Miech, I.&nbsp;Barr, Y.&nbsp;Hasson, K.&nbsp;Lenc,
A.&nbsp;Mensch, K.&nbsp;Millican, M.&nbsp;Reynolds <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">et&nbsp;al.</em>, “Flamingo: a visual
language model for few-shot learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib204.2.2">Advances in Neural Information
Processing Systems</em>, vol.&nbsp;35, pp. 23 716–23 736, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Li, D.&nbsp;Li, S.&nbsp;Savarese, and S.&nbsp;Hoi, “Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">arXiv preprint arXiv:2301.12597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee, “Visual instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">arXiv
preprint arXiv:2304.08485</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Li, Y.&nbsp;He, Y.&nbsp;Wang, Y.&nbsp;Li, W.&nbsp;Wang, P.&nbsp;Luo, Y.&nbsp;Wang, L.&nbsp;Wang, and Y.&nbsp;Qiao,
“Videochat: Chat-centric video understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">arXiv preprint
arXiv:2305.06355</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Maaz, H.&nbsp;Rasheed, S.&nbsp;Khan, and F.&nbsp;S. Khan, “Video-chatgpt: Towards detailed
video understanding via large vision and language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">arXiv
preprint arXiv:2306.05424</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Zhang, X.&nbsp;Li, and L.&nbsp;Bing, “Video-llama: An instruction-tuned audio-visual
language model for video understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">arXiv preprint
arXiv:2306.02858</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Mei, C.&nbsp;Meng, H.&nbsp;Liu, Q.&nbsp;Kong, T.&nbsp;Ko, C.&nbsp;Zhao, M.&nbsp;D. Plumbley, Y.&nbsp;Zou, and
W.&nbsp;Wang, “Wavcaps: A chatgpt-assisted weakly-labelled audio captioning
dataset for audio-language multimodal research,” <em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">arXiv preprint
arXiv:2303.17395</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Lyu, M.&nbsp;Wu, L.&nbsp;Wang, X.&nbsp;Huang, B.&nbsp;Liu, Z.&nbsp;Du, S.&nbsp;Shi, and Z.&nbsp;Tu,
“Macaw-llm: Multi-modal language modeling with image, audio, video, and text
integration,” <em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">arXiv preprint arXiv:2306.09093</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Zhu, J.&nbsp;Chen, X.&nbsp;Shen, X.&nbsp;Li, and M.&nbsp;Elhoseiny, “Minigpt-4: Enhancing
vision-language understanding with advanced large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">arXiv preprint arXiv:2304.10592</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Dosovitskiy, L.&nbsp;Beyer, A.&nbsp;Kolesnikov, D.&nbsp;Weissenborn, X.&nbsp;Zhai,
T.&nbsp;Unterthiner, M.&nbsp;Dehghani, M.&nbsp;Minderer, G.&nbsp;Heigold, S.&nbsp;Gelly <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">et&nbsp;al.</em>,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib213.2.2">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Dai, J.&nbsp;Li, D.&nbsp;Li, A.&nbsp;M.&nbsp;H. Tiong, J.&nbsp;Zhao, W.&nbsp;Wang, B.&nbsp;Li, P.&nbsp;Fung, and
S.&nbsp;Hoi, “Instructblip: Towards general-purpose vision-language models with
instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">arXiv preprint arXiv:2305.06500</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Xu, Y.&nbsp;Shen, and L.&nbsp;Huang, “Multiinstruct: Improving multi-modal zero-shot
learning via instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">arXiv preprint arXiv:2212.10773</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yin, C.&nbsp;Fu, S.&nbsp;Zhao, K.&nbsp;Li, X.&nbsp;Sun, T.&nbsp;Xu, and E.&nbsp;Chen, “A survey on
multimodal large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">arXiv preprint arXiv:2306.13549</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zhao, L.&nbsp;Guo, T.&nbsp;Yue, S.&nbsp;Chen, S.&nbsp;Shao, X.&nbsp;Zhu, Z.&nbsp;Yuan, and J.&nbsp;Liu,
“Chatbridge: Bridging modalities with large language model as a language
catalyst,” <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">arXiv preprint arXiv:2305.16103</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Li, Y.&nbsp;Yin, S.&nbsp;Li, L.&nbsp;Chen, P.&nbsp;Wang, S.&nbsp;Ren, M.&nbsp;Li, Y.&nbsp;Yang, J.&nbsp;Xu, X.&nbsp;Sun
<em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">et&nbsp;al.</em>, “M3 it: A large-scale dataset towards multi-modal
multilingual instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib218.2.2">arXiv preprint arXiv:2306.04387</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Pi, J.&nbsp;Gao, S.&nbsp;Diao, R.&nbsp;Pan, H.&nbsp;Dong, J.&nbsp;Zhang, L.&nbsp;Yao, J.&nbsp;Han, H.&nbsp;Xu, and
L.&nbsp;K.&nbsp;T. Zhang, “Detgpt: Detect what you need via reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">arXiv
preprint arXiv:2305.14167</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Luo, Y.&nbsp;Zhou, T.&nbsp;Ren, S.&nbsp;Chen, X.&nbsp;Sun, and R.&nbsp;Ji, “Cheap and quick:
Efficient vision-language instruction tuning for large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">arXiv preprint arXiv:2305.15023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Zhang, J.&nbsp;Han, A.&nbsp;Zhou, X.&nbsp;Hu, S.&nbsp;Yan, P.&nbsp;Lu, H.&nbsp;Li, P.&nbsp;Gao, and Y.&nbsp;Qiao,
“Llama-adapter: Efficient fine-tuning of language models with zero-init
attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">arXiv preprint arXiv:2303.16199</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;W. Kim, T.&nbsp;Xu, G.&nbsp;Brockman, C.&nbsp;McLeavey, and I.&nbsp;Sutskever,
“Robust speech recognition via large-scale weak supervision,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, A.&nbsp;Zhang, M.&nbsp;Li, H.&nbsp;Zhao, G.&nbsp;Karypis, and A.&nbsp;Smola, “Multimodal
chain-of-thought reasoning in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">arXiv preprint
arXiv:2302.00923</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ge, H.&nbsp;Luo, S.&nbsp;Qian, Y.&nbsp;Gan, J.&nbsp;Fu, and S.&nbsp;Zhan, “Chain of thought prompt
tuning in vision language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib224.1.1">arXiv preprint arXiv:2304.07919</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Wu, S.&nbsp;Yin, W.&nbsp;Qi, X.&nbsp;Wang, Z.&nbsp;Tang, and N.&nbsp;Duan, “Visual chatgpt: Talking,
drawing and editing with visual foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">arXiv preprint
arXiv:2303.04671</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yang, L.&nbsp;Li, J.&nbsp;Wang, K.&nbsp;Lin, E.&nbsp;Azarnasab, F.&nbsp;Ahmed, Z.&nbsp;Liu, C.&nbsp;Liu,
M.&nbsp;Zeng, and L.&nbsp;Wang, “Mm-react: Prompting chatgpt for multimodal reasoning
and action,” <em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">arXiv preprint arXiv:2303.11381</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Wang, J.&nbsp;Zhang, J.&nbsp;Fei, Y.&nbsp;Ge, H.&nbsp;Zheng, Y.&nbsp;Tang, Z.&nbsp;Li, M.&nbsp;Gao, S.&nbsp;Zhao,
Y.&nbsp;Shan <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">et&nbsp;al.</em>, “Caption anything: Interactive image description with
diverse multimodal controls,” <em class="ltx_emph ltx_font_italic" id="bib.bib227.2.2">arXiv preprint arXiv:2305.02677</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Zhu, R.&nbsp;Zhang, B.&nbsp;He, Z.&nbsp;Zeng, S.&nbsp;Zhang, and P.&nbsp;Gao, “Pointclip v2:
Adapting clip for powerful 3d open-world learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">arXiv preprint
arXiv:2211.11682</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Lu, B.&nbsp;Peng, H.&nbsp;Cheng, M.&nbsp;Galley, K.-W. Chang, Y.&nbsp;N. Wu, S.-C. Zhu, and
J.&nbsp;Gao, “Chameleon: Plug-and-play compositional reasoning with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">arXiv preprint arXiv:2304.09842</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Gupta and A.&nbsp;Kembhavi, “Visual programming: Compositional visual reasoning
without training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib230.1.1">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2023, pp. 14 953–14 962.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Gao, Z.&nbsp;Jiang, H.&nbsp;You, P.&nbsp;Lu, S.&nbsp;C. Hoi, X.&nbsp;Wang, and H.&nbsp;Li, “Dynamic
fusion with intra-and inter-modality attention flow for visual question
answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib231.1.1">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2019, pp. 6639–6648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yu, J.&nbsp;Yu, Y.&nbsp;Cui, D.&nbsp;Tao, and Q.&nbsp;Tian, “Deep modular co-attention networks
for visual question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib232.1.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2019, pp. 6281–6290.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;J. Hu, Y.&nbsp;Shen, P.&nbsp;Wallis, Z.&nbsp;Allen-Zhu, Y.&nbsp;Li, S.&nbsp;Wang, L.&nbsp;Wang, and
W.&nbsp;Chen, “Lora: Low-rank adaptation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib233.1.1">arXiv
preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;You, R.&nbsp;Sun, Z.&nbsp;Wang, L.&nbsp;Chen, G.&nbsp;Wang, H.&nbsp;A. Ayyubi, K.-W. Chang, and S.-F.
Chang, “Idealgpt: Iteratively decomposing vision and language reasoning via
large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib234.1.1">arXiv preprint arXiv:2305.14985</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Zhang, X.&nbsp;Hu, B.&nbsp;Li, S.&nbsp;Huang, H.&nbsp;Deng, Y.&nbsp;Qiao, P.&nbsp;Gao, and H.&nbsp;Li,
“Prompt, generate, then cache: Cascade of foundation models makes strong
few-shot learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib235.1.1">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2023, pp. 15 211–15 222.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Wang, L.&nbsp;Dong, H.&nbsp;Cheng, X.&nbsp;Liu, X.&nbsp;Yan, J.&nbsp;Gao, and F.&nbsp;Wei, “Augmenting
language models with long-term memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">arXiv preprint
arXiv:2306.07174</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Xu, Z.&nbsp;Gou, W.&nbsp;Wu, Z.-Y. Niu, H.&nbsp;Wu, H.&nbsp;Wang, and S.&nbsp;Wang, “Long time no
see! open-domain conversation with long-term persona memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib237.1.1">arXiv
preprint arXiv:2203.05797</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican, G.&nbsp;B.
Van Den&nbsp;Driessche, J.-B. Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark <em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">et&nbsp;al.</em>,
“Improving language models by retrieving from trillions of tokens,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib238.2.2">International conference on machine learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 2206–2240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Zhong, L.&nbsp;Guo, Q.&nbsp;Gao, and Y.&nbsp;Wang, “Memorybank: Enhancing large language
models with long-term memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">arXiv preprint arXiv:2305.10250</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, B.&nbsp;Labash, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao,
“Reflexion: Language agents with verbal reinforcement learning,”
<em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">arXiv preprint arXiv:2303.11366</em>, vol.&nbsp;14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Hu, J.&nbsp;Fu, C.&nbsp;Du, S.&nbsp;Luo, J.&nbsp;Zhao, and H.&nbsp;Zhao, “Chatdb: Augmenting llms
with databases as their symbolic memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">arXiv preprint
arXiv:2306.03901</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Jiang, F.&nbsp;F. Xu, L.&nbsp;Gao, Z.&nbsp;Sun, Q.&nbsp;Liu, J.&nbsp;Dwivedi-Yu, Y.&nbsp;Yang, J.&nbsp;Callan,
and G.&nbsp;Neubig, “Active retrieval augmented generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">arXiv
preprint arXiv:2305.06983</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Ram, Y.&nbsp;Levine, I.&nbsp;Dalmedigos, D.&nbsp;Muhlgay, A.&nbsp;Shashua, K.&nbsp;Leyton-Brown, and
Y.&nbsp;Shoham, “In-context retrieval-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">arXiv
preprint arXiv:2302.00083</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_tag_bibitem">[244]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Li and X.&nbsp;Qiu, “Mot: Pre-thinking and recalling enable chatgpt to
self-improve with memory-of-thoughts,” <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">arXiv preprint
arXiv:2305.05181</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_tag_bibitem">[245]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Schuurmans, “Memory augmented large language models are computationally
universal,” <em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">arXiv preprint arXiv:2301.04589</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_tag_bibitem">[246]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Modarressi, A.&nbsp;Imani, M.&nbsp;Fayyaz, and H.&nbsp;Schütze, “Ret-llm: Towards a
general read-write memory for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib246.1.1">arXiv preprint
arXiv:2305.14322</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_tag_bibitem">[247]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Robertson, H.&nbsp;Zaragoza <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">et&nbsp;al.</em>, “The probabilistic relevance
framework: Bm25 and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib247.2.2">Foundations and Trends®
in Information Retrieval</em>, vol.&nbsp;3, no.&nbsp;4, pp. 333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_tag_bibitem">[248]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, and D.&nbsp;Zhou,
“Rationale-augmented ensembles in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">arXiv preprint
arXiv:2207.00747</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_tag_bibitem">[249]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Zhang, B.&nbsp;Chen, Y.&nbsp;Zhang, J.&nbsp;Liu, D.&nbsp;Zan, Y.&nbsp;Mao, J.-G. Lou, and W.&nbsp;Chen,
“Repocoder: Repository-level code completion through iterative retrieval and
generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">arXiv preprint arXiv:2303.12570</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_tag_bibitem">[250]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Wang, W.&nbsp;Ping, P.&nbsp;Xu, L.&nbsp;McAfee, Z.&nbsp;Liu, M.&nbsp;Shoeybi, Y.&nbsp;Dong, O.&nbsp;Kuchaiev,
B.&nbsp;Li, C.&nbsp;Xiao <em class="ltx_emph ltx_font_italic" id="bib.bib250.1.1">et&nbsp;al.</em>, “Shall we pretrain autoregressive language
models with retrieval? a comprehensive study,” <em class="ltx_emph ltx_font_italic" id="bib.bib250.2.2">arXiv preprint
arXiv:2304.06762</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_tag_bibitem">[251]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Wang, N.&nbsp;Yang, and F.&nbsp;Wei, “Learning to retrieve in-context examples for
large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib251.1.1">arXiv preprint arXiv:2307.07164</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_tag_bibitem">[252]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Liu, D.&nbsp;Shen, Y.&nbsp;Zhang, B.&nbsp;Dolan, L.&nbsp;Carin, and W.&nbsp;Chen, “What makes good
in-context examples for gpt-<math alttext="3" class="ltx_Math" display="inline" id="bib.bib252.1.m1.1"><semantics id="bib.bib252.1.m1.1a"><mn id="bib.bib252.1.m1.1.1" xref="bib.bib252.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="bib.bib252.1.m1.1b"><cn id="bib.bib252.1.m1.1.1.cmml" type="integer" xref="bib.bib252.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="bib.bib252.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="bib.bib252.1.m1.1d">3</annotation></semantics></math>?” <em class="ltx_emph ltx_font_italic" id="bib.bib252.2.1">arXiv preprint arXiv:2101.06804</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_tag_bibitem">[253]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Rubin, J.&nbsp;Herzig, and J.&nbsp;Berant, “Learning to retrieve prompts for
in-context learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib253.1.1">arXiv preprint arXiv:2112.08633</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_tag_bibitem">[254]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Shi, S.&nbsp;Min, M.&nbsp;Yasunaga, M.&nbsp;Seo, R.&nbsp;James, M.&nbsp;Lewis, L.&nbsp;Zettlemoyer, and
W.-t. Yih, “Replug: Retrieval-augmented black-box language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib254.1.1">arXiv preprint arXiv:2301.12652</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_tag_bibitem">[255]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Rubin and J.&nbsp;Berant, “Long-range language modeling with self-retrieval,”
<em class="ltx_emph ltx_font_italic" id="bib.bib255.1.1">arXiv preprint arXiv:2306.13421</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_tag_bibitem">[256]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Guu, K.&nbsp;Lee, Z.&nbsp;Tung, P.&nbsp;Pasupat, and M.&nbsp;Chang, “Retrieval augmented
language model pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib256.1.1">International conference on machine
learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2020, pp. 3929–3938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_tag_bibitem">[257]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Hofstätter, J.&nbsp;Chen, K.&nbsp;Raman, and H.&nbsp;Zamani, “Fid-light: Efficient and
effective retrieval-augmented text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib257.1.1">Proceedings of the
46th International ACM SIGIR Conference on Research and Development in
Information Retrieval</em>, 2023, pp. 1437–1447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_tag_bibitem">[258]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Komeili, K.&nbsp;Shuster, and J.&nbsp;Weston, “Internet-augmented dialogue
generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib258.1.1">arXiv preprint arXiv:2107.07566</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_tag_bibitem">[259]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Lazaridou, E.&nbsp;Gribovskaya, W.&nbsp;Stokowiec, and N.&nbsp;Grigorev,
“Internet-augmented language models through few-shot prompting for
open-domain question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib259.1.1">arXiv preprint arXiv:2203.05115</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_tag_bibitem">[260]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Gao, L.&nbsp;Ji, L.&nbsp;Zhou, K.&nbsp;Q. Lin, J.&nbsp;Chen, Z.&nbsp;Fan, and M.&nbsp;Z. Shou,
“Assistgpt: A general multi-modal assistant that can plan, execute, inspect,
and learn,” <em class="ltx_emph ltx_font_italic" id="bib.bib260.1.1">arXiv preprint arXiv:2306.08640</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_tag_bibitem">[261]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Lu, B.&nbsp;Peng, H.&nbsp;Cheng, M.&nbsp;Galley, K.-W. Chang, Y.&nbsp;N. Wu, S.-C. Zhu, and
J.&nbsp;Gao, “Chameleon: Plug-and-play compositional reasoning with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib261.1.1">arXiv preprint arXiv:2304.09842</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_tag_bibitem">[262]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Paranjape, S.&nbsp;Lundberg, S.&nbsp;Singh, H.&nbsp;Hajishirzi, L.&nbsp;Zettlemoyer, and M.&nbsp;T.
Ribeiro, “Art: Automatic multi-step reasoning and tool-use for large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib262.1.1">arXiv preprint arXiv:2303.09014</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_tag_bibitem">[263]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y.&nbsp;Fujii, A.&nbsp;Ratner, C.-Y. Lee, R.&nbsp;Krishna,
and T.&nbsp;Pfister, “Tool documentation enables zero-shot tool-usage with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib263.1.1">arXiv preprint arXiv:2308.00675</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_tag_bibitem">[264]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Song, W.&nbsp;Xiong, D.&nbsp;Zhu, C.&nbsp;Li, K.&nbsp;Wang, Y.&nbsp;Tian, and S.&nbsp;Li, “Restgpt:
Connecting large language models with real-world applications via restful
apis,” <em class="ltx_emph ltx_font_italic" id="bib.bib264.1.1">arXiv preprint arXiv:2306.06624</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_tag_bibitem">[265]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Hao, T.&nbsp;Liu, Z.&nbsp;Wang, and Z.&nbsp;Hu, “Toolkengpt: Augmenting frozen language
models with massive tools via tool embeddings,” <em class="ltx_emph ltx_font_italic" id="bib.bib265.1.1">arXiv preprint
arXiv:2305.11554</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_tag_bibitem">[266]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;G. Patil, T.&nbsp;Zhang, X.&nbsp;Wang, and J.&nbsp;E. Gonzalez, “Gorilla: Large language
model connected with massive apis,” <em class="ltx_emph ltx_font_italic" id="bib.bib266.1.1">arXiv preprint arXiv:2305.15334</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_tag_bibitem">[267]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Xu, F.&nbsp;Hong, B.&nbsp;Li, C.&nbsp;Hu, Z.&nbsp;Chen, and J.&nbsp;Zhang, “On the tool manipulation
capability of open-source large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib267.1.1">arXiv preprint
arXiv:2305.16504</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_tag_bibitem">[268]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Qin, S.&nbsp;Liang, Y.&nbsp;Ye, K.&nbsp;Zhu, L.&nbsp;Yan, Y.&nbsp;Lu, Y.&nbsp;Lin, X.&nbsp;Cong, X.&nbsp;Tang,
B.&nbsp;Qian <em class="ltx_emph ltx_font_italic" id="bib.bib268.1.1">et&nbsp;al.</em>, “Toolllm: Facilitating large language models to
master 16000+ real-world apis,” <em class="ltx_emph ltx_font_italic" id="bib.bib268.2.2">arXiv preprint arXiv:2307.16789</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_tag_bibitem">[269]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Shen, K.&nbsp;Song, X.&nbsp;Tan, D.&nbsp;Li, W.&nbsp;Lu, and Y.&nbsp;Zhuang, “Hugginggpt: Solving ai
tasks with chatgpt and its friends in huggingface,” <em class="ltx_emph ltx_font_italic" id="bib.bib269.1.1">arXiv preprint
arXiv:2303.17580</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_tag_bibitem">[270]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Yang, L.&nbsp;Song, Y.&nbsp;Li, S.&nbsp;Zhao, Y.&nbsp;Ge, X.&nbsp;Li, and Y.&nbsp;Shan, “Gpt4tools:
Teaching large language model to use tools via self-instruction,”
<em class="ltx_emph ltx_font_italic" id="bib.bib270.1.1">arXiv preprint arXiv:2305.18752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_tag_bibitem">[271]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Liang, C.&nbsp;Wu, T.&nbsp;Song, W.&nbsp;Wu, Y.&nbsp;Xia, Y.&nbsp;Liu, Y.&nbsp;Ou, S.&nbsp;Lu, L.&nbsp;Ji, S.&nbsp;Mao
<em class="ltx_emph ltx_font_italic" id="bib.bib271.1.1">et&nbsp;al.</em>, “Taskmatrix. ai: Completing tasks by connecting foundation
models with millions of apis,” <em class="ltx_emph ltx_font_italic" id="bib.bib271.2.2">arXiv preprint arXiv:2303.16434</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_tag_bibitem">[272]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Surís, S.&nbsp;Menon, and C.&nbsp;Vondrick, “Vipergpt: Visual inference via
python execution for reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib272.1.1">arXiv preprint arXiv:2303.08128</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_tag_bibitem">[273]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Liu, Y.&nbsp;Zheng, Z.&nbsp;Du, M.&nbsp;Ding, Y.&nbsp;Qian, Z.&nbsp;Yang, and J.&nbsp;Tang, “Gpt
understands, too,” <em class="ltx_emph ltx_font_italic" id="bib.bib273.1.1">arXiv preprint arXiv:2103.10385</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_tag_bibitem">[274]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Chen, F.&nbsp;Liu, Z.&nbsp;Meng, and S.&nbsp;Liang, “Revisiting parameter-efficient
tuning: Are we really there yet?” <em class="ltx_emph ltx_font_italic" id="bib.bib274.1.1">arXiv preprint arXiv:2202.07962</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_tag_bibitem">[275]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;He, C.&nbsp;Zhou, X.&nbsp;Ma, T.&nbsp;Berg-Kirkpatrick, and G.&nbsp;Neubig, “Towards a unified
view of parameter-efficient transfer learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib275.1.1">arXiv preprint
arXiv:2110.04366</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_tag_bibitem">[276]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, S.&nbsp;Mukherjee, X.&nbsp;Liu, J.&nbsp;Gao, A.&nbsp;H. Awadallah, and J.&nbsp;Gao, “Adamix:
Mixture-of-adapter for parameter-efficient tuning of large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib276.1.1">arXiv preprint arXiv:2205.12410</em>, vol.&nbsp;1, no.&nbsp;2, p.&nbsp;4, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_tag_bibitem">[277]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Liu, K.&nbsp;Ji, Y.&nbsp;Fu, W.&nbsp;Tam, Z.&nbsp;Du, Z.&nbsp;Yang, and J.&nbsp;Tang, “P-tuning: Prompt
tuning can be comparable to fine-tuning across scales and tasks,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib277.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers)</em>, 2022, pp. 61–68.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_tag_bibitem">[278]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Razdaibiedina, Y.&nbsp;Mao, R.&nbsp;Hou, M.&nbsp;Khabsa, M.&nbsp;Lewis, and A.&nbsp;Almahairi,
“Progressive prompts: Continual learning for language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib278.1.1">arXiv
preprint arXiv:2301.12314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_tag_bibitem">[279]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.-R. Zhang, C.&nbsp;Tan, H.&nbsp;Xu, C.&nbsp;Wang, J.&nbsp;Huang, and S.&nbsp;Huang, “Towards adaptive
prefix tuning for parameter-efficient language model fine-tuning,”
<em class="ltx_emph ltx_font_italic" id="bib.bib279.1.1">arXiv preprint arXiv:2305.15212</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_tag_bibitem">[280]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;B. Zaken, S.&nbsp;Ravfogel, and Y.&nbsp;Goldberg, “Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-models,” <em class="ltx_emph ltx_font_italic" id="bib.bib280.1.1">arXiv
preprint arXiv:2106.10199</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_tag_bibitem">[281]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Xiao, J.&nbsp;Lin, M.&nbsp;Seznec, H.&nbsp;Wu, J.&nbsp;Demouth, and S.&nbsp;Han, “Smoothquant:
Accurate and efficient post-training quantization for large language
models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib281.1.1">ICML</em>, ser. Proceedings of Machine Learning Research,
vol. 202.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp.
38 087–38 099.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_tag_bibitem">[282]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, M.&nbsp;Lewis, Y.&nbsp;Belkada, and L.&nbsp;Zettlemoyer, “Llm. int8 (): 8-bit
matrix multiplication for transformers at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib282.1.1">arXiv preprint
arXiv:2208.07339</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_tag_bibitem">[283]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Frantar, S.&nbsp;Ashkboos, T.&nbsp;Hoefler, and D.&nbsp;Alistarh, “Gptq: Accurate
post-training quantization for generative pre-trained transformers,”
<em class="ltx_emph ltx_font_italic" id="bib.bib283.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_tag_bibitem">[284]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Tao, L.&nbsp;Hou, W.&nbsp;Zhang, L.&nbsp;Shang, X.&nbsp;Jiang, Q.&nbsp;Liu, P.&nbsp;Luo, and N.&nbsp;Wong,
“Compression of generative pre-trained language models via quantization,”
<em class="ltx_emph ltx_font_italic" id="bib.bib284.1.1">arXiv preprint arXiv:2203.10705</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_tag_bibitem">[285]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wei, Y.&nbsp;Zhang, Y.&nbsp;Li, X.&nbsp;Zhang, R.&nbsp;Gong, J.&nbsp;Guo, and X.&nbsp;Liu, “Outlier
suppression+: Accurate quantization of large language models by equivalent
and optimal shifting and scaling,” <em class="ltx_emph ltx_font_italic" id="bib.bib285.1.1">arXiv preprint arXiv:2304.09145</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_tag_bibitem">[286]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Frantar and D.&nbsp;Alistarh, “Optimal brain compression: A framework for
accurate post-training quantization and pruning,” <em class="ltx_emph ltx_font_italic" id="bib.bib286.1.1">Advances in Neural
Information Processing Systems</em>, vol.&nbsp;35, pp. 4475–4488, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_tag_bibitem">[287]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Lee, J.&nbsp;Jin, T.&nbsp;Kim, H.&nbsp;Kim, and E.&nbsp;Park, “Owq: Lessons learned from
activation outliers for weight quantization in large language models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib287.1.1">arXiv preprint arXiv:2306.02272</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_tag_bibitem">[288]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;J. Kwon, J.&nbsp;Kim, J.&nbsp;Bae, K.&nbsp;M. Yoo, J.-H. Kim, B.&nbsp;Park, B.&nbsp;Kim, J.-W. Ha,
N.&nbsp;Sung, and D.&nbsp;Lee, “Alphatuning: Quantization-aware parameter-efficient
adaptation of large-scale pre-trained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib288.1.1">arXiv preprint
arXiv:2210.03858</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_tag_bibitem">[289]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, A.&nbsp;Pagnoni, A.&nbsp;Holtzman, and L.&nbsp;Zettlemoyer, “Qlora: Efficient
finetuning of quantized llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib289.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_tag_bibitem">[290]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Liu, B.&nbsp;Oguz, C.&nbsp;Zhao, E.&nbsp;Chang, P.&nbsp;Stock, Y.&nbsp;Mehdad, Y.&nbsp;Shi,
R.&nbsp;Krishnamoorthi, and V.&nbsp;Chandra, “Llm-qat: Data-free quantization aware
training for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib290.1.1">arXiv preprint arXiv:2305.17888</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_tag_bibitem">[291]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Guo, A.&nbsp;Yao, H.&nbsp;Zhao, and Y.&nbsp;Chen, “Network sketching: Exploiting binary
structure in deep cnns,” in <em class="ltx_emph ltx_font_italic" id="bib.bib291.1.1">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em>, 2017, pp. 5955–5963.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_tag_bibitem">[292]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kim, J.&nbsp;H. Lee, S.&nbsp;Kim, J.&nbsp;Park, K.&nbsp;M. Yoo, S.&nbsp;J. Kwon, and D.&nbsp;Lee,
“Memory-efficient fine-tuning of compressed large language models via
sub-4-bit integer quantization,” <em class="ltx_emph ltx_font_italic" id="bib.bib292.1.1">arXiv preprint arXiv:2305.14152</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_tag_bibitem">[293]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Sun, Z.&nbsp;Liu, A.&nbsp;Bair, and J.&nbsp;Z. Kolter, “A simple and effective pruning
approach for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib293.1.1">arXiv preprint arXiv:2306.11695</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_tag_bibitem">[294]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Ma, G.&nbsp;Fang, and X.&nbsp;Wang, “Llm-pruner: On the structural pruning of large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib294.1.1">arXiv preprint arXiv:2305.11627</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_tag_bibitem">[295]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, J.&nbsp;Wohlwend, and T.&nbsp;Lei, “Structured pruning of large language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib295.1.1">arXiv preprint arXiv:1910.04732</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_tag_bibitem">[296]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Yin, Y.&nbsp;Wu, Z.&nbsp;Zhang, C.-Y. Hsieh, Y.&nbsp;Wang, Y.&nbsp;Jia, M.&nbsp;Pechenizkiy,
Y.&nbsp;Liang, Z.&nbsp;Wang, and S.&nbsp;Liu, “Outlier weighed layerwise sparsity (owl): A
missing secret sauce for pruning llms to high sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib296.1.1">arXiv
preprint arXiv:2310.05175</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_tag_bibitem">[297]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Xu, F.&nbsp;Luo, C.&nbsp;Wang, B.&nbsp;Chang, J.&nbsp;Huang, S.&nbsp;Huang, and F.&nbsp;Huang, “From
dense to sparse: Contrastive pruning for better pre-trained language model
compression,” in <em class="ltx_emph ltx_font_italic" id="bib.bib297.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, vol.&nbsp;36, no.&nbsp;10, 2022, pp. 11 547–11 555.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_tag_bibitem">[298]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Tao, L.&nbsp;Hou, H.&nbsp;Bai, J.&nbsp;Wei, X.&nbsp;Jiang, Q.&nbsp;Liu, P.&nbsp;Luo, and N.&nbsp;Wong,
“Structured pruning for efficient generative pre-trained language models,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib298.1.1">Findings of the Association for Computational Linguistics: ACL
2023</em>, 2023, pp. 10 880–10 895.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_tag_bibitem">[299]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Black, S.&nbsp;Biderman, E.&nbsp;Hallahan, Q.&nbsp;Anthony, L.&nbsp;Gao, L.&nbsp;Golding, H.&nbsp;He,
C.&nbsp;Leahy, K.&nbsp;McDonell, J.&nbsp;Phang <em class="ltx_emph ltx_font_italic" id="bib.bib299.1.1">et&nbsp;al.</em>, “Gpt-neox-20b: An open-source
autoregressive language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib299.2.2">arXiv preprint arXiv:2204.06745</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_tag_bibitem">[300]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Geng, A.&nbsp;Gudibande, H.&nbsp;Liu, E.&nbsp;Wallace, P.&nbsp;Abbeel, S.&nbsp;Levine, and D.&nbsp;Song,
“Koala: A dialogue model for academic research,” Blog post, April 2023.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bair.berkeley.edu/blog/2023/04/03/koala/" title="">https://bair.berkeley.edu/blog/2023/04/03/koala/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_tag_bibitem">[301]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, S.&nbsp;Biderman, S.&nbsp;Black, L.&nbsp;Golding, T.&nbsp;Hoppe, C.&nbsp;Foster, J.&nbsp;Phang,
H.&nbsp;He, A.&nbsp;Thite, N.&nbsp;Nabeshima <em class="ltx_emph ltx_font_italic" id="bib.bib301.1.1">et&nbsp;al.</em>, “The pile: An 800gb dataset of
diverse text for language modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib301.2.2">arXiv preprint arXiv:2101.00027</em>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_tag_bibitem">[302]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Laurençon, L.&nbsp;Saulnier, T.&nbsp;Wang, C.&nbsp;Akiki, A.&nbsp;Villanova&nbsp;del Moral,
T.&nbsp;Le&nbsp;Scao, L.&nbsp;Von&nbsp;Werra, C.&nbsp;Mou, E.&nbsp;González&nbsp;Ponferrada, H.&nbsp;Nguyen
<em class="ltx_emph ltx_font_italic" id="bib.bib302.1.1">et&nbsp;al.</em>, “The bigscience roots corpus: A 1.6 tb composite multilingual
dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib302.2.2">Advances in Neural Information Processing Systems</em>, vol.&nbsp;35,
pp. 31 809–31 826, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_tag_bibitem">[303]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
“Wikipedia.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Main_Page" title="">https://en.wikipedia.org/wiki/Main_Page</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_tag_bibitem">[304]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer, “Redpajama: An open source recipe to reproduce llama
training dataset,” Apr. 2023. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" title="">https://github.com/togethercomputer/RedPajama-Data</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_tag_bibitem">[305]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Honovich, T.&nbsp;Scialom, O.&nbsp;Levy, and T.&nbsp;Schick, “Unnatural instructions:
Tuning language models with (almost) no human labor,” <em class="ltx_emph ltx_font_italic" id="bib.bib305.1.1">arXiv preprint
arXiv:2212.09689</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_tag_bibitem">[306]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, A.&nbsp;Jones, K.&nbsp;Ndousse, A.&nbsp;Askell, A.&nbsp;Chen, N.&nbsp;DasSarma, D.&nbsp;Drain,
S.&nbsp;Fort, D.&nbsp;Ganguli, T.&nbsp;Henighan <em class="ltx_emph ltx_font_italic" id="bib.bib306.1.1">et&nbsp;al.</em>, “Training a helpful and
harmless assistant with reinforcement learning from human feedback,”
<em class="ltx_emph ltx_font_italic" id="bib.bib306.2.2">arXiv preprint arXiv:2204.05862</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_tag_bibitem">[307]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, C.&nbsp;Burns, S.&nbsp;Basart, A.&nbsp;Zou, M.&nbsp;Mazeika, D.&nbsp;Song, and
J.&nbsp;Steinhardt, “Measuring massive multitask language understanding,”
<em class="ltx_emph ltx_font_italic" id="bib.bib307.1.1">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_tag_bibitem">[308]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Srivastava, A.&nbsp;Rastogi, A.&nbsp;Rao, A.&nbsp;A.&nbsp;M. Shoeb, A.&nbsp;Abid, A.&nbsp;Fisch, A.&nbsp;R.
Brown, A.&nbsp;Santoro, A.&nbsp;Gupta, A.&nbsp;Garriga-Alonso <em class="ltx_emph ltx_font_italic" id="bib.bib308.1.1">et&nbsp;al.</em>, “Beyond the
imitation game: Quantifying and extrapolating the capabilities of language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib308.2.2">arXiv preprint arXiv:2206.04615</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_tag_bibitem">[309]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Wang, A.&nbsp;Singh, J.&nbsp;Michael, F.&nbsp;Hill, O.&nbsp;Levy, and S.&nbsp;R. Bowman, “Glue: A
multi-task benchmark and analysis platform for natural language
understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib309.1.1">arXiv preprint arXiv:1804.07461</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_tag_bibitem">[310]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Yao, Q.&nbsp;Dong, J.&nbsp;Guan, B.&nbsp;Cao, Z.&nbsp;Zhang, C.&nbsp;Xiao, X.&nbsp;Wang, F.&nbsp;Qi, J.&nbsp;Bao,
J.&nbsp;Nie <em class="ltx_emph ltx_font_italic" id="bib.bib310.1.1">et&nbsp;al.</em>, “Cuge: A chinese language understanding and generation
evaluation benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib310.2.2">arXiv preprint arXiv:2112.13610</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_tag_bibitem">[311]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xu, H.&nbsp;Hu, X.&nbsp;Zhang, L.&nbsp;Li, C.&nbsp;Cao, Y.&nbsp;Li, Y.&nbsp;Xu, K.&nbsp;Sun, D.&nbsp;Yu, C.&nbsp;Yu
<em class="ltx_emph ltx_font_italic" id="bib.bib311.1.1">et&nbsp;al.</em>, “Clue: A chinese language understanding evaluation
benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib311.2.2">arXiv preprint arXiv:2004.05986</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_tag_bibitem">[312]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xu, X.&nbsp;Lu, C.&nbsp;Yuan, X.&nbsp;Zhang, H.&nbsp;Xu, H.&nbsp;Yuan, G.&nbsp;Wei, X.&nbsp;Pan, X.&nbsp;Tian,
L.&nbsp;Qin <em class="ltx_emph ltx_font_italic" id="bib.bib312.1.1">et&nbsp;al.</em>, “Fewclue: A chinese few-shot learning evaluation
benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib312.2.2">arXiv preprint arXiv:2107.07498</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_tag_bibitem">[313]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;M. Smith, M.&nbsp;Williamson, K.&nbsp;Shuster, J.&nbsp;Weston, and Y.-L. Boureau, “Can you
put it all together: Evaluating conversational agents’ ability to blend
skills,” <em class="ltx_emph ltx_font_italic" id="bib.bib313.1.1">arXiv preprint arXiv:2004.08449</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_tag_bibitem">[314]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang,
D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar <em class="ltx_emph ltx_font_italic" id="bib.bib314.1.1">et&nbsp;al.</em>, “Holistic evaluation of
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib314.2.2">arXiv preprint arXiv:2211.09110</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_tag_bibitem">[315]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Park, J.&nbsp;Moon, S.&nbsp;Kim, W.&nbsp;I. Cho, J.&nbsp;Han, J.&nbsp;Park, C.&nbsp;Song, J.&nbsp;Kim, Y.&nbsp;Song,
T.&nbsp;Oh <em class="ltx_emph ltx_font_italic" id="bib.bib315.1.1">et&nbsp;al.</em>, “Klue: Korean language understanding evaluation,”
<em class="ltx_emph ltx_font_italic" id="bib.bib315.2.2">arXiv preprint arXiv:2105.09680</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_tag_bibitem">[316]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Reddy, D.&nbsp;Chen, and C.&nbsp;D. Manning, “Coqa: A conversational question
answering challenge,” <em class="ltx_emph ltx_font_italic" id="bib.bib316.1.1">Transactions of the Association for
Computational Linguistics</em>, vol.&nbsp;7, pp. 249–266, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_tag_bibitem">[317]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;T. Pilehvar and J.&nbsp;Camacho-Collados, “Wic: 10,000 example pairs for
evaluating context-sensitive representations,” <em class="ltx_emph ltx_font_italic" id="bib.bib317.1.1">arXiv preprint
arXiv:1808.09121</em>, vol.&nbsp;6, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_tag_bibitem">[318]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Merity, C.&nbsp;Xiong, J.&nbsp;Bradbury, and R.&nbsp;Socher, “Pointer sentinel mixture
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib318.1.1">arXiv preprint arXiv:1609.07843</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_tag_bibitem">[319]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, A.&nbsp;Potapenko, S.&nbsp;M. Jayakumar, and T.&nbsp;P. Lillicrap, “Compressive
transformers for long-range sequence modelling,” <em class="ltx_emph ltx_font_italic" id="bib.bib319.1.1">arXiv preprint
arXiv:1911.05507</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_tag_bibitem">[320]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Liu, Q.&nbsp;Chen, C.&nbsp;Deng, H.&nbsp;Zeng, J.&nbsp;Chen, D.&nbsp;Li, and B.&nbsp;Tang, “Lcqmc: A
large-scale chinese question matching corpus,” in <em class="ltx_emph ltx_font_italic" id="bib.bib320.1.1">Proceedings of the
27th international conference on computational linguistics</em>, 2018, pp.
1952–1962.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_tag_bibitem">[321]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Iyer, N.&nbsp;Dandekar, and K.&nbsp;Csernai, “First quora dataset release: Question
pairs,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs" title="">https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_tag_bibitem">[322]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rudinger, J.&nbsp;Naradowsky, B.&nbsp;Leonard, and B.&nbsp;Van&nbsp;Durme, “Gender bias in
coreference resolution,” <em class="ltx_emph ltx_font_italic" id="bib.bib322.1.1">arXiv preprint arXiv:1804.09301</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_tag_bibitem">[323]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.-C. De&nbsp;Marneffe, M.&nbsp;Simons, and J.&nbsp;Tonhauser, “The commitmentbank:
Investigating projection in naturally occurring discourse,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib323.1.1">proceedings of Sinn und Bedeutung</em>, vol.&nbsp;23, no.&nbsp;2, 2019, pp. 107–124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_tag_bibitem">[324]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Li, N.&nbsp;Ding, Z.&nbsp;Liu, H.&nbsp;Zheng, and Y.&nbsp;Shen, “Chinese relation extraction
with multi-grained information and external linguistic knowledge,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib324.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, 2019, pp. 4377–4386.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_tag_bibitem">[325]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Xu, J.&nbsp;Wen, X.&nbsp;Sun, and Q.&nbsp;Su, “A discourse-level named entity recognition
and relation extraction dataset for chinese literature text,” <em class="ltx_emph ltx_font_italic" id="bib.bib325.1.1">arXiv
preprint arXiv:1711.07010</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_tag_bibitem">[326]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Chen, Q.&nbsp;Chen, X.&nbsp;Liu, H.&nbsp;Yang, D.&nbsp;Lu, and B.&nbsp;Tang, “The bq corpus: A
large-scale domain-specific chinese corpus for sentence semantic equivalence
identification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib326.1.1">Proceedings of the 2018 conference on empirical
methods in natural language processing</em>, 2018, pp. 4946–4951.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_tag_bibitem">[327]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Liu, D.&nbsp;Niu, H.&nbsp;Wei, J.&nbsp;Lin, Y.&nbsp;He, K.&nbsp;Lai, and Y.&nbsp;Xu, “Matching article
pairs with graphical decomposition and convolutions,” <em class="ltx_emph ltx_font_italic" id="bib.bib327.1.1">arXiv preprint
arXiv:1802.07459</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_tag_bibitem">[328]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Li, W.&nbsp;Li, Z.&nbsp;He, X.&nbsp;Wang, Y.&nbsp;Cao, J.&nbsp;Zhou, and W.&nbsp;Xu, “Dataset and neural
recurrent sequence labeling model for open-domain factoid question
answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib328.1.1">arXiv preprint arXiv:1607.06275</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_tag_bibitem">[329]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Peng and M.&nbsp;Dredze, “Named entity recognition for chinese social media with
jointly trained embeddings,” in <em class="ltx_emph ltx_font_italic" id="bib.bib329.1.1">Proceedings of the 2015 conference on
empirical methods in natural language processing</em>, 2015, pp. 548–554.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_tag_bibitem">[330]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Ling, D.&nbsp;Yogatama, C.&nbsp;Dyer, and P.&nbsp;Blunsom, “Program induction by rationale
generation: Learning to solve and explain algebraic word problems,”
<em class="ltx_emph ltx_font_italic" id="bib.bib330.1.1">arXiv preprint arXiv:1705.04146</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_tag_bibitem">[331]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Weischedel, S.&nbsp;Pradhan, L.&nbsp;Ramshaw, M.&nbsp;Palmer, N.&nbsp;Xue, M.&nbsp;Marcus, A.&nbsp;Taylor,
C.&nbsp;Greenberg, E.&nbsp;Hovy, R.&nbsp;Belvin <em class="ltx_emph ltx_font_italic" id="bib.bib331.1.1">et&nbsp;al.</em>, “Ontonotes release 4.0,”
<em class="ltx_emph ltx_font_italic" id="bib.bib331.2.2">LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium</em>, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_tag_bibitem">[332]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Vilares and C.&nbsp;Gómez-Rodríguez, “Head-qa: A healthcare dataset for
complex reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib332.1.1">arXiv preprint arXiv:1906.04701</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_tag_bibitem">[333]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;L. Blodgett, L.&nbsp;Green, and B.&nbsp;O’Connor, “Demographic dialectal variation in
social media: A case study of african-american english,” <em class="ltx_emph ltx_font_italic" id="bib.bib333.1.1">arXiv
preprint arXiv:1608.08868</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_tag_bibitem">[334]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Mostafazadeh, N.&nbsp;Chambers, X.&nbsp;He, D.&nbsp;Parikh, D.&nbsp;Batra, L.&nbsp;Vanderwende,
P.&nbsp;Kohli, and J.&nbsp;Allen, “A corpus and evaluation framework for deeper
understanding of commonsense stories,” <em class="ltx_emph ltx_font_italic" id="bib.bib334.1.1">arXiv preprint
arXiv:1604.01696</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_tag_bibitem">[335]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Paperno, G.&nbsp;Kruszewski, A.&nbsp;Lazaridou, Q.&nbsp;N. Pham, R.&nbsp;Bernardi, S.&nbsp;Pezzelle,
M.&nbsp;Baroni, G.&nbsp;Boleda, and R.&nbsp;Fernández, “The lambada dataset: Word
prediction requiring a broad discourse context,” <em class="ltx_emph ltx_font_italic" id="bib.bib335.1.1">arXiv preprint
arXiv:1606.06031</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_tag_bibitem">[336]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Hu, Q.&nbsp;Chen, and F.&nbsp;Zhu, “Lcsts: A large scale chinese short text
summarization dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib336.1.1">arXiv preprint arXiv:1506.05865</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_tag_bibitem">[337]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Shao, M.&nbsp;Huang, J.&nbsp;Wen, W.&nbsp;Xu, and X.&nbsp;Zhu, “Long and diverse text
generation with planning-based hierarchical variational model,” <em class="ltx_emph ltx_font_italic" id="bib.bib337.1.1">arXiv
preprint arXiv:1908.06605</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_tag_bibitem">[338]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Novikova, O.&nbsp;Dušek, and V.&nbsp;Rieser, “The e2e dataset: New challenges
for end-to-end generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib338.1.1">arXiv preprint arXiv:1706.09254</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib339">
<span class="ltx_tag ltx_tag_bibitem">[339]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Zheng, M.&nbsp;Huang, and A.&nbsp;Sun, “Chid: A large-scale chinese idiom dataset for
cloze test,” <em class="ltx_emph ltx_font_italic" id="bib.bib339.1.1">arXiv preprint arXiv:1906.01265</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib340">
<span class="ltx_tag ltx_tag_bibitem">[340]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bisk, R.&nbsp;Zellers, J.&nbsp;Gao, Y.&nbsp;Choi <em class="ltx_emph ltx_font_italic" id="bib.bib340.1.1">et&nbsp;al.</em>, “Piqa: Reasoning about
physical commonsense in natural language,” in <em class="ltx_emph ltx_font_italic" id="bib.bib340.2.2">Proceedings of the AAAI
conference on artificial intelligence</em>, vol.&nbsp;34, no.&nbsp;05, 2020, pp.
7432–7439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib341">
<span class="ltx_tag ltx_tag_bibitem">[341]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Joshi, E.&nbsp;Choi, D.&nbsp;S. Weld, and L.&nbsp;Zettlemoyer, “Triviaqa: A large scale
distantly supervised challenge dataset for reading comprehension,”
<em class="ltx_emph ltx_font_italic" id="bib.bib341.1.1">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib342">
<span class="ltx_tag ltx_tag_bibitem">[342]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Clark, I.&nbsp;Cowhey, O.&nbsp;Etzioni, T.&nbsp;Khot, A.&nbsp;Sabharwal, C.&nbsp;Schoenick, and
O.&nbsp;Tafjord, “Think you have solved question answering? try arc, the ai2
reasoning challenge,” <em class="ltx_emph ltx_font_italic" id="bib.bib342.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib343">
<span class="ltx_tag ltx_tag_bibitem">[343]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Aroca-Ouellette, C.&nbsp;Paik, A.&nbsp;Roncone, and K.&nbsp;Kann, “Prost: Physical
reasoning of objects through space and time,” <em class="ltx_emph ltx_font_italic" id="bib.bib343.1.1">arXiv preprint
arXiv:2106.03634</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib344">
<span class="ltx_tag ltx_tag_bibitem">[344]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Mihaylov, P.&nbsp;Clark, T.&nbsp;Khot, and A.&nbsp;Sabharwal, “Can a suit of armor conduct
electricity? a new dataset for open book question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib344.1.1">arXiv
preprint arXiv:1809.02789</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib345">
<span class="ltx_tag ltx_tag_bibitem">[345]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;C. Ferreira, C.&nbsp;Gardent, N.&nbsp;Ilinykh, C.&nbsp;Van Der&nbsp;Lee, S.&nbsp;Mille,
D.&nbsp;Moussallem, and A.&nbsp;Shimorina, “The 2020 bilingual, bi-directional webnlg+
shared task overview and evaluation results (webnlg+ 2020),” in
<em class="ltx_emph ltx_font_italic" id="bib.bib345.1.1">Proceedings of the 3rd International Workshop on Natural Language
Generation from the Semantic Web (WebNLG+)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib346">
<span class="ltx_tag ltx_tag_bibitem">[346]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xu, W.&nbsp;Zhou, T.&nbsp;Ge, K.&nbsp;Xu, J.&nbsp;McAuley, and F.&nbsp;Wei, “Blow the dog whistle: A
chinese dataset for cant understanding with common sense and world
knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib346.1.1">arXiv preprint arXiv:2104.02704</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib347">
<span class="ltx_tag ltx_tag_bibitem">[347]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Lai, Q.&nbsp;Xie, H.&nbsp;Liu, Y.&nbsp;Yang, and E.&nbsp;Hovy, “Race: Large-scale reading
comprehension dataset from examinations,” <em class="ltx_emph ltx_font_italic" id="bib.bib347.1.1">arXiv preprint
arXiv:1704.04683</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib348">
<span class="ltx_tag ltx_tag_bibitem">[348]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Choi, H.&nbsp;He, M.&nbsp;Iyyer, M.&nbsp;Yatskar, W.-t. Yih, Y.&nbsp;Choi, P.&nbsp;Liang, and
L.&nbsp;Zettlemoyer, “Quac: Question answering in context,” <em class="ltx_emph ltx_font_italic" id="bib.bib348.1.1">arXiv preprint
arXiv:1808.07036</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib349">
<span class="ltx_tag ltx_tag_bibitem">[349]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Geva, D.&nbsp;Khashabi, E.&nbsp;Segal, T.&nbsp;Khot, D.&nbsp;Roth, and J.&nbsp;Berant, “Did
aristotle use a laptop? a question answering benchmark with implicit
reasoning strategies,” <em class="ltx_emph ltx_font_italic" id="bib.bib349.1.1">Transactions of the Association for
Computational Linguistics</em>, vol.&nbsp;9, pp. 346–361, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib350">
<span class="ltx_tag ltx_tag_bibitem">[350]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Boyd-Graber, B.&nbsp;Satinoff, H.&nbsp;He, and H.&nbsp;Daumé&nbsp;III, “Besting the quiz
master: Crowdsourcing incremental classification games,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib350.1.1">Proceedings of the 2012 joint conference on empirical methods in
natural language processing and computational natural language learning</em>,
2012, pp. 1290–1301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib351">
<span class="ltx_tag ltx_tag_bibitem">[351]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, X.&nbsp;Zhang, H.&nbsp;Wang, J.&nbsp;Cheng, P.&nbsp;Li, and Z.&nbsp;Ding, “Chinese medical
question answer matching using end-to-end character-level multi-scale cnns,”
<em class="ltx_emph ltx_font_italic" id="bib.bib351.1.1">Applied Sciences</em>, vol.&nbsp;7, no.&nbsp;8, p. 767, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib352">
<span class="ltx_tag ltx_tag_bibitem">[352]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, X.&nbsp;Zhang, H.&nbsp;Wang, L.&nbsp;Guo, and S.&nbsp;Liu, “Multi-scale attentive
interaction networks for chinese medical question answer selection,”
<em class="ltx_emph ltx_font_italic" id="bib.bib352.1.1">IEEE Access</em>, vol.&nbsp;6, pp. 74 061–74 071, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib353">
<span class="ltx_tag ltx_tag_bibitem">[353]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xu, J.&nbsp;Pei, H.&nbsp;Wu, Y.&nbsp;Liu, and C.&nbsp;Li, “Matinf: A jointly labeled
large-scale dataset for classification, question answering and
summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib353.1.1">arXiv preprint arXiv:2004.12302</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib354">
<span class="ltx_tag ltx_tag_bibitem">[354]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Sakaguchi, R.&nbsp;L. Bras, C.&nbsp;Bhagavatula, and Y.&nbsp;Choi, “Winogrande: An
adversarial winograd schema challenge at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib354.1.1">Communications of the
ACM</em>, vol.&nbsp;64, no.&nbsp;9, pp. 99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib355">
<span class="ltx_tag ltx_tag_bibitem">[355]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Zellers, A.&nbsp;Holtzman, Y.&nbsp;Bisk, A.&nbsp;Farhadi, and Y.&nbsp;Choi, “Hellaswag: Can a
machine really finish your sentence?” <em class="ltx_emph ltx_font_italic" id="bib.bib355.1.1">arXiv preprint
arXiv:1905.07830</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib356">
<span class="ltx_tag ltx_tag_bibitem">[356]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Roemmele, C.&nbsp;A. Bejan, and A.&nbsp;S. Gordon, “Choice of plausible alternatives:
An evaluation of commonsense causal reasoning.” in <em class="ltx_emph ltx_font_italic" id="bib.bib356.1.1">AAAI spring
symposium: logical formalizations of commonsense reasoning</em>, 2011, pp.
90–95.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib357">
<span class="ltx_tag ltx_tag_bibitem">[357]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Levesque, E.&nbsp;Davis, and L.&nbsp;Morgenstern, “The winograd schema challenge,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib357.1.1">Thirteenth international conference on the principles of knowledge
representation and reasoning</em>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib358">
<span class="ltx_tag ltx_tag_bibitem">[358]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Talmor, J.&nbsp;Herzig, N.&nbsp;Lourie, and J.&nbsp;Berant, “Commonsenseqa: A question
answering challenge targeting commonsense knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib358.1.1">arXiv preprint
arXiv:1811.00937</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib359">
<span class="ltx_tag ltx_tag_bibitem">[359]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Sap, H.&nbsp;Rashkin, D.&nbsp;Chen, R.&nbsp;LeBras, and Y.&nbsp;Choi, “Socialiqa: Commonsense
reasoning about social interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib359.1.1">arXiv preprint
arXiv:1904.09728</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib360">
<span class="ltx_tag ltx_tag_bibitem">[360]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Sun, D.&nbsp;Yu, D.&nbsp;Yu, and C.&nbsp;Cardie, “Investigating prior knowledge for
challenging chinese machine reading comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib360.1.1">Transactions of
the Association for Computational Linguistics</em>, vol.&nbsp;8, pp. 141–155, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib361">
<span class="ltx_tag ltx_tag_bibitem">[361]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, X.&nbsp;Liu, J.&nbsp;Liu, J.&nbsp;Gao, K.&nbsp;Duh, and B.&nbsp;Van&nbsp;Durme, “Record: Bridging
the gap between human and machine commonsense reading comprehension,”
<em class="ltx_emph ltx_font_italic" id="bib.bib361.1.1">arXiv preprint arXiv:1810.12885</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib362">
<span class="ltx_tag ltx_tag_bibitem">[362]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Rajpurkar, J.&nbsp;Zhang, K.&nbsp;Lopyrev, and P.&nbsp;Liang, “Squad: 100,000+ questions
for machine comprehension of text,” <em class="ltx_emph ltx_font_italic" id="bib.bib362.1.1">arXiv preprint arXiv:1606.05250</em>,
2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib363">
<span class="ltx_tag ltx_tag_bibitem">[363]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Clark, K.&nbsp;Lee, M.-W. Chang, T.&nbsp;Kwiatkowski, M.&nbsp;Collins, and K.&nbsp;Toutanova,
“Boolq: Exploring the surprising difficulty of natural yes/no questions,”
<em class="ltx_emph ltx_font_italic" id="bib.bib363.1.1">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib364">
<span class="ltx_tag ltx_tag_bibitem">[364]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Rajpurkar, R.&nbsp;Jia, and P.&nbsp;Liang, “Know what you don’t know: Unanswerable
questions for squad,” <em class="ltx_emph ltx_font_italic" id="bib.bib364.1.1">arXiv preprint arXiv:1806.03822</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib365">
<span class="ltx_tag ltx_tag_bibitem">[365]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Dua, Y.&nbsp;Wang, P.&nbsp;Dasigi, G.&nbsp;Stanovsky, S.&nbsp;Singh, and M.&nbsp;Gardner, “Drop: A
reading comprehension benchmark requiring discrete reasoning over
paragraphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib365.1.1">arXiv preprint arXiv:1903.00161</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib366">
<span class="ltx_tag ltx_tag_bibitem">[366]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Dagan, O.&nbsp;Glickman, and B.&nbsp;Magnini, “The pascal recognising textual
entailment challenge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib366.1.1">Machine learning challenges workshop</em>.&nbsp;&nbsp;&nbsp;Springer, 2005, pp. 177–190.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib367">
<span class="ltx_tag ltx_tag_bibitem">[367]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Chang, M.&nbsp;Narang, H.&nbsp;Suzuki, G.&nbsp;Cao, J.&nbsp;Gao, and Y.&nbsp;Bisk, “Webqa: Multihop
and multimodal qa,” in <em class="ltx_emph ltx_font_italic" id="bib.bib367.1.1">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2022, pp. 16 495–16 504.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib368">
<span class="ltx_tag ltx_tag_bibitem">[368]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Cui, T.&nbsp;Liu, Z.&nbsp;Chen, W.&nbsp;Ma, S.&nbsp;Wang, and G.&nbsp;Hu, “Dataset for the first
evaluation on chinese machine reading comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib368.1.1">arXiv preprint
arXiv:1709.08299</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib369">
<span class="ltx_tag ltx_tag_bibitem">[369]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Cui, T.&nbsp;Liu, W.&nbsp;Che, L.&nbsp;Xiao, Z.&nbsp;Chen, W.&nbsp;Ma, S.&nbsp;Wang, and G.&nbsp;Hu, “A
span-extraction dataset for chinese machine reading comprehension,”
<em class="ltx_emph ltx_font_italic" id="bib.bib369.1.1">arXiv preprint arXiv:1810.07366</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib370">
<span class="ltx_tag ltx_tag_bibitem">[370]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Cui, T.&nbsp;Liu, Z.&nbsp;Yang, Z.&nbsp;Chen, W.&nbsp;Ma, W.&nbsp;Che, S.&nbsp;Wang, and G.&nbsp;Hu, “A
sentence cloze dataset for chinese machine reading comprehension,”
<em class="ltx_emph ltx_font_italic" id="bib.bib370.1.1">arXiv preprint arXiv:2004.03116</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib371">
<span class="ltx_tag ltx_tag_bibitem">[371]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Li, T.&nbsp;Liu, D.&nbsp;Li, Q.&nbsp;Li, J.&nbsp;Shi, and Y.&nbsp;Wang, “Character-based bilstm-crf
incorporating pos and dictionaries for chinese opinion target extraction,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib371.1.1">Asian Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2018, pp. 518–533.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib372">
<span class="ltx_tag ltx_tag_bibitem">[372]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Khashabi, S.&nbsp;Chaturvedi, M.&nbsp;Roth, S.&nbsp;Upadhyay, and D.&nbsp;Roth, “Looking beyond
the surface: A challenge set for reading comprehension over multiple
sentences,” in <em class="ltx_emph ltx_font_italic" id="bib.bib372.1.1">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers)</em>, 2018, pp. 252–262.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib373">
<span class="ltx_tag ltx_tag_bibitem">[373]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Kwiatkowski, J.&nbsp;Palomaki, O.&nbsp;Redfield, M.&nbsp;Collins, A.&nbsp;Parikh, C.&nbsp;Alberti,
D.&nbsp;Epstein, I.&nbsp;Polosukhin, J.&nbsp;Devlin, K.&nbsp;Lee <em class="ltx_emph ltx_font_italic" id="bib.bib373.1.1">et&nbsp;al.</em>, “Natural
questions: a benchmark for question answering research,” <em class="ltx_emph ltx_font_italic" id="bib.bib373.2.2">Transactions
of the Association for Computational Linguistics</em>, vol.&nbsp;7, pp. 453–466,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib374">
<span class="ltx_tag ltx_tag_bibitem">[374]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;C. Shao, T.&nbsp;Liu, Y.&nbsp;Lai, Y.&nbsp;Tseng, and S.&nbsp;Tsai, “Drcd: A chinese machine
reading comprehension dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib374.1.1">arXiv preprint arXiv:1806.00920</em>,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib375">
<span class="ltx_tag ltx_tag_bibitem">[375]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;He, K.&nbsp;Liu, J.&nbsp;Liu, Y.&nbsp;Lyu, S.&nbsp;Zhao, X.&nbsp;Xiao, Y.&nbsp;Liu, Y.&nbsp;Wang, H.&nbsp;Wu, Q.&nbsp;She
<em class="ltx_emph ltx_font_italic" id="bib.bib375.1.1">et&nbsp;al.</em>, “Dureader: a chinese machine reading comprehension dataset
from real-world applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib375.2.2">arXiv preprint arXiv:1711.05073</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib376">
<span class="ltx_tag ltx_tag_bibitem">[376]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Tang, J.&nbsp;Liu, H.&nbsp;Li, Y.&nbsp;Hong, H.&nbsp;Wu, and H.&nbsp;Wang, “Dureaderrobust: A
chinese dataset towards evaluating the robustness of machine reading
comprehension models,” <em class="ltx_emph ltx_font_italic" id="bib.bib376.1.1">arXiv preprint arXiv:2004.11142</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib377">
<span class="ltx_tag ltx_tag_bibitem">[377]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Welbl, N.&nbsp;F. Liu, and M.&nbsp;Gardner, “Crowdsourcing multiple choice science
questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib377.1.1">arXiv preprint arXiv:1707.06209</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib378">
<span class="ltx_tag ltx_tag_bibitem">[378]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xiong, Z.&nbsp;Dai, J.&nbsp;Callan, Z.&nbsp;Liu, and R.&nbsp;Power, “End-to-end neural ad-hoc
ranking with kernel pooling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib378.1.1">Proceedings of the 40th International
ACM SIGIR conference on research and development in information retrieval</em>,
2017, pp. 55–64.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib379">
<span class="ltx_tag ltx_tag_bibitem">[379]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Peñas, E.&nbsp;Hovy, P.&nbsp;Forner, Á.&nbsp;Rodrigo, R.&nbsp;Sutcliffe, and R.&nbsp;Morante,
“Qa4mre 2011-2013: Overview of question answering for machine reading
evaluation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib379.1.1">Information Access Evaluation. Multilinguality,
Multimodality, and Visualization: 4th International Conference of the CLEF
Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings
4</em>.&nbsp;&nbsp;&nbsp;Springer, 2013, pp. 303–320.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib380">
<span class="ltx_tag ltx_tag_bibitem">[380]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lim, M.&nbsp;Kim, and J.&nbsp;Lee, “Korquad1. 0: Korean qa dataset for machine
reading comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib380.1.1">arXiv preprint arXiv:1909.07005</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib381">
<span class="ltx_tag ltx_tag_bibitem">[381]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xiao, H.&nbsp;Zhong, Z.&nbsp;Guo, C.&nbsp;Tu, Z.&nbsp;Liu, M.&nbsp;Sun, Y.&nbsp;Feng, X.&nbsp;Han, Z.&nbsp;Hu,
H.&nbsp;Wang <em class="ltx_emph ltx_font_italic" id="bib.bib381.1.1">et&nbsp;al.</em>, “Cail2018: A large-scale legal dataset for judgment
prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib381.2.2">arXiv preprint arXiv:1807.02478</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib382">
<span class="ltx_tag ltx_tag_bibitem">[382]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, S.&nbsp;Basart, S.&nbsp;Kadavath, M.&nbsp;Mazeika, A.&nbsp;Arora, E.&nbsp;Guo, C.&nbsp;Burns,
S.&nbsp;Puranik, H.&nbsp;He, D.&nbsp;Song <em class="ltx_emph ltx_font_italic" id="bib.bib382.1.1">et&nbsp;al.</em>, “Measuring coding challenge
competence with apps,” <em class="ltx_emph ltx_font_italic" id="bib.bib382.2.2">arXiv preprint arXiv:2105.09938</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib383">
<span class="ltx_tag ltx_tag_bibitem">[383]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, X.&nbsp;Liu, and S.&nbsp;Shi, “Deep neural solver for math word problems,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib383.1.1">Proceedings of the 2017 conference on empirical methods in natural
language processing</em>, 2017, pp. 845–854.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib384">
<span class="ltx_tag ltx_tag_bibitem">[384]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Cobbe, V.&nbsp;Kosaraju, M.&nbsp;Bavarian, M.&nbsp;Chen, H.&nbsp;Jun, L.&nbsp;Kaiser, M.&nbsp;Plappert,
J.&nbsp;Tworek, J.&nbsp;Hilton, R.&nbsp;Nakano <em class="ltx_emph ltx_font_italic" id="bib.bib384.1.1">et&nbsp;al.</em>, “Training verifiers to solve
math word problems,” <em class="ltx_emph ltx_font_italic" id="bib.bib384.2.2">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib385">
<span class="ltx_tag ltx_tag_bibitem">[385]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Austin, A.&nbsp;Odena, M.&nbsp;I. Nye, M.&nbsp;Bosma, H.&nbsp;Michalewski, D.&nbsp;Dohan, E.&nbsp;Jiang,
C.&nbsp;J. Cai, M.&nbsp;Terry, Q.&nbsp;V. Le, and C.&nbsp;Sutton, “Program synthesis with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib385.1.1">CoRR</em>, vol. abs/2108.07732, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib386">
<span class="ltx_tag ltx_tag_bibitem">[386]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Shi, M.&nbsp;Suzgun, M.&nbsp;Freitag, X.&nbsp;Wang, S.&nbsp;Srivats, S.&nbsp;Vosoughi, H.&nbsp;W. Chung,
Y.&nbsp;Tay, S.&nbsp;Ruder, D.&nbsp;Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib386.1.1">et&nbsp;al.</em>, “Language models are multilingual
chain-of-thought reasoners,” <em class="ltx_emph ltx_font_italic" id="bib.bib386.2.2">arXiv preprint arXiv:2210.03057</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib387">
<span class="ltx_tag ltx_tag_bibitem">[387]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Roy and D.&nbsp;Roth, “Solving general arithmetic word problems,” <em class="ltx_emph ltx_font_italic" id="bib.bib387.1.1">arXiv
preprint arXiv:1608.01413</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib388">
<span class="ltx_tag ltx_tag_bibitem">[388]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.-Y. Miao, C.-C. Liang, and K.-Y. Su, “A diverse corpus for evaluating and
developing english math word problem solvers,” <em class="ltx_emph ltx_font_italic" id="bib.bib388.1.1">arXiv preprint
arXiv:2106.15772</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib389">
<span class="ltx_tag ltx_tag_bibitem">[389]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Koncel-Kedziorski, S.&nbsp;Roy, A.&nbsp;Amini, N.&nbsp;Kushman, and H.&nbsp;Hajishirzi, “Mawps:
A math word problem repository,” in <em class="ltx_emph ltx_font_italic" id="bib.bib389.1.1">Proceedings of the 2016 conference
of the north american chapter of the association for computational
linguistics: human language technologies</em>, 2016, pp. 1152–1157.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib390">
<span class="ltx_tag ltx_tag_bibitem">[390]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Patel, S.&nbsp;Bhattamishra, and N.&nbsp;Goyal, “Are nlp models really able to solve
simple math word problems?” <em class="ltx_emph ltx_font_italic" id="bib.bib390.1.1">arXiv preprint arXiv:2103.07191</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib391">
<span class="ltx_tag ltx_tag_bibitem">[391]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Chen, J.&nbsp;Tworek, H.&nbsp;Jun, Q.&nbsp;Yuan, H.&nbsp;P. d.&nbsp;O. Pinto, J.&nbsp;Kaplan, H.&nbsp;Edwards,
Y.&nbsp;Burda, N.&nbsp;Joseph, G.&nbsp;Brockman <em class="ltx_emph ltx_font_italic" id="bib.bib391.1.1">et&nbsp;al.</em>, “Evaluating large language
models trained on code,” <em class="ltx_emph ltx_font_italic" id="bib.bib391.2.2">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib392">
<span class="ltx_tag ltx_tag_bibitem">[392]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Lai, C.&nbsp;Li, Y.&nbsp;Wang, T.&nbsp;Zhang, R.&nbsp;Zhong, L.&nbsp;Zettlemoyer, W.-t. Yih,
D.&nbsp;Fried, S.&nbsp;Wang, and T.&nbsp;Yu, “Ds-1000: A natural and reliable benchmark for
data science code generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib392.1.1">International Conference on Machine
Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp.
18 319–18 345.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib393">
<span class="ltx_tag ltx_tag_bibitem">[393]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Austin, A.&nbsp;Odena, M.&nbsp;Nye, M.&nbsp;Bosma, H.&nbsp;Michalewski, D.&nbsp;Dohan, E.&nbsp;Jiang,
C.&nbsp;Cai, M.&nbsp;Terry, Q.&nbsp;Le <em class="ltx_emph ltx_font_italic" id="bib.bib393.1.1">et&nbsp;al.</em>, “Program synthesis with large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib393.2.2">arXiv preprint arXiv:2108.07732</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib394">
<span class="ltx_tag ltx_tag_bibitem">[394]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Nie, A.&nbsp;Williams, E.&nbsp;Dinan, M.&nbsp;Bansal, J.&nbsp;Weston, and D.&nbsp;Kiela,
“Adversarial nli: A new benchmark for natural language understanding,”
<em class="ltx_emph ltx_font_italic" id="bib.bib394.1.1">arXiv preprint arXiv:1910.14599</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib395">
<span class="ltx_tag ltx_tag_bibitem">[395]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Williams, N.&nbsp;Nangia, and S.&nbsp;R. Bowman, “A broad-coverage challenge corpus
for sentence understanding through inference,” <em class="ltx_emph ltx_font_italic" id="bib.bib395.1.1">arXiv preprint
arXiv:1704.05426</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib396">
<span class="ltx_tag ltx_tag_bibitem">[396]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;T. McCoy, E.&nbsp;Pavlick, and T.&nbsp;Linzen, “Right for the wrong reasons:
Diagnosing syntactic heuristics in natural language inference,” <em class="ltx_emph ltx_font_italic" id="bib.bib396.1.1">arXiv
preprint arXiv:1902.01007</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib397">
<span class="ltx_tag ltx_tag_bibitem">[397]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Liu, L.&nbsp;Cui, H.&nbsp;Liu, D.&nbsp;Huang, Y.&nbsp;Wang, and Y.&nbsp;Zhang, “Logiqa: A challenge
dataset for machine reading comprehension with logical reasoning,”
<em class="ltx_emph ltx_font_italic" id="bib.bib397.1.1">arXiv preprint arXiv:2007.08124</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib398">
<span class="ltx_tag ltx_tag_bibitem">[398]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Lewis, B.&nbsp;Oğuz, R.&nbsp;Rinott, S.&nbsp;Riedel, and H.&nbsp;Schwenk, “Mlqa:
Evaluating cross-lingual extractive question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib398.1.1">arXiv
preprint arXiv:1910.07475</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib399">
<span class="ltx_tag ltx_tag_bibitem">[399]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Conneau, G.&nbsp;Lample, R.&nbsp;Rinott, A.&nbsp;Williams, S.&nbsp;R. Bowman, H.&nbsp;Schwenk, and
V.&nbsp;Stoyanov, “Xnli: Evaluating cross-lingual sentence representations,”
<em class="ltx_emph ltx_font_italic" id="bib.bib399.1.1">arXiv preprint arXiv:1809.05053</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib400">
<span class="ltx_tag ltx_tag_bibitem">[400]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Yang, Y.&nbsp;Zhang, C.&nbsp;Tar, and J.&nbsp;Baldridge, “Paws-x: A cross-lingual
adversarial dataset for paraphrase identification,” <em class="ltx_emph ltx_font_italic" id="bib.bib400.1.1">arXiv preprint
arXiv:1908.11828</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib401">
<span class="ltx_tag ltx_tag_bibitem">[401]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Narayan, S.&nbsp;B. Cohen, and M.&nbsp;Lapata, “Don’t give me the details, just the
summary!” <em class="ltx_emph ltx_font_italic" id="bib.bib401.1.1">Topic-Aware Convolutional Neural Networks for Extreme
Summarization. ArXiv, abs</em>, 1808.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib402">
<span class="ltx_tag ltx_tag_bibitem">[402]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;M. Ponti, G.&nbsp;Glavaš, O.&nbsp;Majewska, Q.&nbsp;Liu, I.&nbsp;Vulić, and
A.&nbsp;Korhonen, “Xcopa: A multilingual dataset for causal commonsense
reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib402.1.1">arXiv preprint arXiv:2005.00333</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib403">
<span class="ltx_tag ltx_tag_bibitem">[403]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Tikhonov and M.&nbsp;Ryabinin, “It’s all in the heads: Using attention heads as
a baseline for cross-lingual transfer in commonsense reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib403.1.1">arXiv
preprint arXiv:2106.12066</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib404">
<span class="ltx_tag ltx_tag_bibitem">[404]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;H. Clark, E.&nbsp;Choi, M.&nbsp;Collins, D.&nbsp;Garrette, T.&nbsp;Kwiatkowski, V.&nbsp;Nikolaev, and
J.&nbsp;Palomaki, “Tydi qa: A benchmark for information-seeking question
answering in typologically diverse languages,” <em class="ltx_emph ltx_font_italic" id="bib.bib404.1.1">Transactions of the
Association for Computational Linguistics</em>, vol.&nbsp;8, pp. 454–470, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib405">
<span class="ltx_tag ltx_tag_bibitem">[405]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Scialom, P.-A. Dray, S.&nbsp;Lamprier, B.&nbsp;Piwowarski, and J.&nbsp;Staiano, “Mlsum:
The multilingual summarization corpus,” <em class="ltx_emph ltx_font_italic" id="bib.bib405.1.1">arXiv preprint
arXiv:2004.14900</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib406">
<span class="ltx_tag ltx_tag_bibitem">[406]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans, “Truthfulqa: Measuring how models mimic human
falsehoods,” <em class="ltx_emph ltx_font_italic" id="bib.bib406.1.1">arXiv preprint arXiv:2109.07958</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib407">
<span class="ltx_tag ltx_tag_bibitem">[407]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Augenstein, C.&nbsp;Lioma, D.&nbsp;Wang, L.&nbsp;C. Lima, C.&nbsp;Hansen, C.&nbsp;Hansen, and J.&nbsp;G.
Simonsen, “Multifc: A real-world multi-domain dataset for evidence-based
fact checking of claims,” <em class="ltx_emph ltx_font_italic" id="bib.bib407.1.1">arXiv preprint arXiv:1909.03242</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib408">
<span class="ltx_tag ltx_tag_bibitem">[408]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Thorne, A.&nbsp;Vlachos, C.&nbsp;Christodoulopoulos, and A.&nbsp;Mittal, “Fever: a
large-scale dataset for fact extraction and verification,” <em class="ltx_emph ltx_font_italic" id="bib.bib408.1.1">arXiv
preprint arXiv:1803.05355</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib409">
<span class="ltx_tag ltx_tag_bibitem">[409]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Mollas, Z.&nbsp;Chrysopoulou, S.&nbsp;Karlos, and G.&nbsp;Tsoumakas, “Ethos: an online
hate speech detection dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib409.1.1">arXiv preprint arXiv:2006.08328</em>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib410">
<span class="ltx_tag ltx_tag_bibitem">[410]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Nadeem, A.&nbsp;Bethke, and S.&nbsp;Reddy, “Stereoset: Measuring stereotypical bias
in pretrained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib410.1.1">arXiv preprint arXiv:2004.09456</em>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib411">
<span class="ltx_tag ltx_tag_bibitem">[411]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Parrish, A.&nbsp;Chen, N.&nbsp;Nangia, V.&nbsp;Padmakumar, J.&nbsp;Phang, J.&nbsp;Thompson, P.&nbsp;M.
Htut, and S.&nbsp;R. Bowman, “Bbq: A hand-built bias benchmark for question
answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib411.1.1">arXiv preprint arXiv:2110.08193</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib412">
<span class="ltx_tag ltx_tag_bibitem">[412]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhao, T.&nbsp;Wang, M.&nbsp;Yatskar, V.&nbsp;Ordonez, and K.-W. Chang, “Gender bias in
coreference resolution: Evaluation and debiasing methods,” <em class="ltx_emph ltx_font_italic" id="bib.bib412.1.1">arXiv
preprint arXiv:1804.06876</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib413">
<span class="ltx_tag ltx_tag_bibitem">[413]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Nangia, C.&nbsp;Vania, R.&nbsp;Bhalerao, and S.&nbsp;R. Bowman, “Crows-pairs: A challenge
dataset for measuring social biases in masked language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib413.1.1">arXiv
preprint arXiv:2010.00133</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib414">
<span class="ltx_tag ltx_tag_bibitem">[414]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Gehman, S.&nbsp;Gururangan, M.&nbsp;Sap, Y.&nbsp;Choi, and N.&nbsp;A. Smith,
“Realtoxicityprompts: Evaluating neural toxic degeneration in language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib414.1.1">arXiv preprint arXiv:2009.11462</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib415">
<span class="ltx_tag ltx_tag_bibitem">[415]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Borkan, L.&nbsp;Dixon, J.&nbsp;Sorensen, N.&nbsp;Thain, and L.&nbsp;Vasserman, “Nuanced metrics
for measuring unintended bias with real data for text classification,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib415.1.1">Companion proceedings of the 2019 world wide web conference</em>, 2019, pp.
491–500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib416">
<span class="ltx_tag ltx_tag_bibitem">[416]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Bojar, R.&nbsp;Chatterjee, C.&nbsp;Federmann, Y.&nbsp;Graham, B.&nbsp;Haddow, M.&nbsp;Huck, A.&nbsp;J.
Yepes, P.&nbsp;Koehn, V.&nbsp;Logacheva, C.&nbsp;Monz <em class="ltx_emph ltx_font_italic" id="bib.bib416.1.1">et&nbsp;al.</em>, “Findings of the 2016
conference on machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib416.2.2">Proceedings of the First
Conference on Machine Translation: Volume 2, Shared Task Papers</em>, 2016, pp.
131–198.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib417">
<span class="ltx_tag ltx_tag_bibitem">[417]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Loïc, B.&nbsp;Magdalena, B.&nbsp;Ondřej, F.&nbsp;Christian, G.&nbsp;Yvette, G.&nbsp;Roman,
H.&nbsp;Barry, H.&nbsp;Matthias, J.&nbsp;Eric, K.&nbsp;Tom <em class="ltx_emph ltx_font_italic" id="bib.bib417.1.1">et&nbsp;al.</em>, “Findings of the 2020
conference on machine translation (wmt20),” in <em class="ltx_emph ltx_font_italic" id="bib.bib417.2.2">Proceedings of the
Fifth Conference on Machine Translation</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics,, 2020, pp. 1–55.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib418">
<span class="ltx_tag ltx_tag_bibitem">[418]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Li, F.&nbsp;Qi, M.&nbsp;Sun, X.&nbsp;Yi, and J.&nbsp;Zhang, “Ccpm: A chinese classical poetry
matching dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib418.1.1">arXiv preprint arXiv:2106.01979</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib419">
<span class="ltx_tag ltx_tag_bibitem">[419]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Dinan, S.&nbsp;Roller, K.&nbsp;Shuster, A.&nbsp;Fan, M.&nbsp;Auli, and J.&nbsp;Weston, “Wizard of
wikipedia: Knowledge-powered conversational agents,” <em class="ltx_emph ltx_font_italic" id="bib.bib419.1.1">arXiv preprint
arXiv:1811.01241</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib420">
<span class="ltx_tag ltx_tag_bibitem">[420]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Rashkin, E.&nbsp;M. Smith, M.&nbsp;Li, and Y.-L. Boureau, “Towards empathetic
open-domain conversation models: A new benchmark and dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib420.1.1">arXiv
preprint arXiv:1811.00207</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib421">
<span class="ltx_tag ltx_tag_bibitem">[421]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Dinan, V.&nbsp;Logacheva, V.&nbsp;Malykh, A.&nbsp;Miller, K.&nbsp;Shuster, J.&nbsp;Urbanek, D.&nbsp;Kiela,
A.&nbsp;Szlam, I.&nbsp;Serban, R.&nbsp;Lowe <em class="ltx_emph ltx_font_italic" id="bib.bib421.1.1">et&nbsp;al.</em>, “The second conversational
intelligence challenge (convai2),” in <em class="ltx_emph ltx_font_italic" id="bib.bib421.2.2">The NeurIPS’18 Competition: From
Machine Learning to Intelligent Conversations</em>.&nbsp;&nbsp;&nbsp;Springer, 2020, pp. 187–208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib422">
<span class="ltx_tag ltx_tag_bibitem">[422]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Zhou, C.&nbsp;Zheng, K.&nbsp;Huang, M.&nbsp;Huang, and X.&nbsp;Zhu, “Kdconv: A chinese
multi-domain dialogue dataset towards multi-turn knowledge-driven
conversation,” <em class="ltx_emph ltx_font_italic" id="bib.bib422.1.1">arXiv preprint arXiv:2004.04100</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib423">
<span class="ltx_tag ltx_tag_bibitem">[423]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;CO, “Iflytek: a multiple categories chinese text classifier. competition
official website,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib424">
<span class="ltx_tag ltx_tag_bibitem">[424]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, M.&nbsp;Ott, N.&nbsp;Goyal, J.&nbsp;Du, M.&nbsp;Joshi, D.&nbsp;Chen, O.&nbsp;Levy, M.&nbsp;Lewis,
L.&nbsp;Zettlemoyer, and V.&nbsp;Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib424.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib425">
<span class="ltx_tag ltx_tag_bibitem">[425]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Baumgartner, S.&nbsp;Zannettou, B.&nbsp;Keegan, M.&nbsp;Squire, and J.&nbsp;Blackburn, “The
pushshift reddit dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib425.1.1">Proceedings of the international AAAI
conference on web and social media</em>, vol.&nbsp;14, 2020, pp. 830–839.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib426">
<span class="ltx_tag ltx_tag_bibitem">[426]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Fan, Y.&nbsp;Jernite, E.&nbsp;Perez, D.&nbsp;Grangier, J.&nbsp;Weston, and M.&nbsp;Auli, “Eli5: Long
form question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib426.1.1">arXiv preprint arXiv:1907.09190</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib427">
<span class="ltx_tag ltx_tag_bibitem">[427]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, S.&nbsp;Mishra, P.&nbsp;Alipoormolabashi, Y.&nbsp;Kordi, A.&nbsp;Mirzaei, A.&nbsp;Arunkumar,
A.&nbsp;Ashok, A.&nbsp;S. Dhanasekaran, A.&nbsp;Naik, D.&nbsp;Stap <em class="ltx_emph ltx_font_italic" id="bib.bib427.1.1">et&nbsp;al.</em>, “Benchmarking
generalization via in-context instructions on 1,600+ language tasks,”
<em class="ltx_emph ltx_font_italic" id="bib.bib427.2.2">arXiv preprint arXiv:2204.07705</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib428">
<span class="ltx_tag ltx_tag_bibitem">[428]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Xie, C.&nbsp;H. Wu, P.&nbsp;Shi, R.&nbsp;Zhong, T.&nbsp;Scholak, M.&nbsp;Yasunaga, C.-S. Wu,
M.&nbsp;Zhong, P.&nbsp;Yin, S.&nbsp;I. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib428.1.1">et&nbsp;al.</em>, “Unifiedskg: Unifying and
multi-tasking structured knowledge grounding with text-to-text language
models,” <em class="ltx_emph ltx_font_italic" id="bib.bib428.2.2">arXiv preprint arXiv:2201.05966</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib429">
<span class="ltx_tag ltx_tag_bibitem">[429]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Ye, B.&nbsp;Y. Lin, and X.&nbsp;Ren, “Crossfit: A few-shot learning challenge for
cross-task generalization in nlp,” <em class="ltx_emph ltx_font_italic" id="bib.bib429.1.1">arXiv preprint arXiv:2104.08835</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib430">
<span class="ltx_tag ltx_tag_bibitem">[430]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Aribandi, Y.&nbsp;Tay, T.&nbsp;Schuster, J.&nbsp;Rao, H.&nbsp;S. Zheng, S.&nbsp;V. Mehta, H.&nbsp;Zhuang,
V.&nbsp;Q. Tran, D.&nbsp;Bahri, J.&nbsp;Ni <em class="ltx_emph ltx_font_italic" id="bib.bib430.1.1">et&nbsp;al.</em>, “Ext5: Towards extreme multi-task
scaling for transfer learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib430.2.2">arXiv preprint arXiv:2111.10952</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib431">
<span class="ltx_tag ltx_tag_bibitem">[431]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Williams, N.&nbsp;Nangia, and S.&nbsp;Bowman, “A broad-coverage challenge corpus for
sentence understanding through inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib431.1.1">Proceedings of the 2018
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers)</em>.&nbsp;&nbsp;&nbsp;New Orleans, Louisiana:
Association for Computational Linguistics, Jun. 2018, pp. 1112–1122.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N18-1101" title="">https://aclanthology.org/N18-1101</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib432">
<span class="ltx_tag ltx_tag_bibitem">[432]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, J.&nbsp;Baldridge, and L.&nbsp;He, “PAWS: Paraphrase adversaries from word
scrambling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib432.1.1">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers)</em>.&nbsp;&nbsp;&nbsp;Minneapolis, Minnesota: Association for Computational
Linguistics, Jun. 2019, pp. 1298–1308. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1131" title="">https://aclanthology.org/N19-1131</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib433">
<span class="ltx_tag ltx_tag_bibitem">[433]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Qin, A.&nbsp;Zhang, Z.&nbsp;Zhang, J.&nbsp;Chen, M.&nbsp;Yasunaga, and D.&nbsp;Yang, “Is chatGPT a
general-purpose natural language processing task solver?” in <em class="ltx_emph ltx_font_italic" id="bib.bib433.1.1">The 2023
Conference on Empirical Methods in Natural Language Processing</em>, 2023.
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=u03xn1COsO" title="">https://openreview.net/forum?id=u03xn1COsO</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib434">
<span class="ltx_tag ltx_tag_bibitem">[434]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;U. Hadi, R.&nbsp;Qureshi, A.&nbsp;Shah, M.&nbsp;Irfan, A.&nbsp;Zafar, M.&nbsp;B. Shaikh, N.&nbsp;Akhtar,
J.&nbsp;Wu, S.&nbsp;Mirjalili <em class="ltx_emph ltx_font_italic" id="bib.bib434.1.1">et&nbsp;al.</em>, “Large language models: a comprehensive
survey of its applications, challenges, limitations, and future prospects,”
<em class="ltx_emph ltx_font_italic" id="bib.bib434.2.2">TechRxiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib435">
<span class="ltx_tag ltx_tag_bibitem">[435]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;L. Dong, S.&nbsp;Moon, Y.&nbsp;E. Xu, K.&nbsp;Malik, and Z.&nbsp;Yu, “Towards next-generation
intelligent assistants leveraging llm techniques,” in <em class="ltx_emph ltx_font_italic" id="bib.bib435.1.1">Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2023,
pp. 5792–5793.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib436">
<span class="ltx_tag ltx_tag_bibitem">[436]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Pandya and M.&nbsp;Holia, “Automating customer service using langchain: Building
custom open-source gpt chatbot for organizations,” <em class="ltx_emph ltx_font_italic" id="bib.bib436.1.1">arXiv preprint
arXiv:2310.05421</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib437">
<span class="ltx_tag ltx_tag_bibitem">[437]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Li, B.&nbsp;Hui, G.&nbsp;Qu, B.&nbsp;Li, J.&nbsp;Yang, B.&nbsp;Li, B.&nbsp;Wang, B.&nbsp;Qin, R.&nbsp;Cao, R.&nbsp;Geng
<em class="ltx_emph ltx_font_italic" id="bib.bib437.1.1">et&nbsp;al.</em>, “Can llm already serve as a database interface? a big bench
for large-scale database grounded text-to-sqls,” <em class="ltx_emph ltx_font_italic" id="bib.bib437.2.2">arXiv preprint
arXiv:2305.03111</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib438">
<span class="ltx_tag ltx_tag_bibitem">[438]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Rao, J.&nbsp;Kim, M.&nbsp;Kamineni, M.&nbsp;Pang, W.&nbsp;Lie, and M.&nbsp;D. Succi, “Evaluating
chatgpt as an adjunct for radiologic decision-making,” <em class="ltx_emph ltx_font_italic" id="bib.bib438.1.1">medRxiv</em>, pp.
2023–02, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib439">
<span class="ltx_tag ltx_tag_bibitem">[439]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Benary, X.&nbsp;D. Wang, M.&nbsp;Schmidt, D.&nbsp;Soll, G.&nbsp;Hilfenhaus, M.&nbsp;Nassir,
C.&nbsp;Sigler, M.&nbsp;Knödler, U.&nbsp;Keller, D.&nbsp;Beule <em class="ltx_emph ltx_font_italic" id="bib.bib439.1.1">et&nbsp;al.</em>, “Leveraging
large language models for decision support in personalized oncology,”
<em class="ltx_emph ltx_font_italic" id="bib.bib439.2.2">JAMA Network Open</em>, vol.&nbsp;6, no.&nbsp;11, pp. e2 343 689–e2 343 689,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib440">
<span class="ltx_tag ltx_tag_bibitem">[440]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;M. Chiesa-Estomba, J.&nbsp;R. Lechien, L.&nbsp;A. Vaira, A.&nbsp;Brunet, G.&nbsp;Cammaroto,
M.&nbsp;Mayo-Yanez, A.&nbsp;Sanchez-Barrueco, and C.&nbsp;Saga-Gutierrez, “Exploring the
potential of chat-gpt as a supportive tool for sialendoscopy clinical
decision making and patient information support,” <em class="ltx_emph ltx_font_italic" id="bib.bib440.1.1">European Archives of
Oto-Rhino-Laryngology</em>, pp. 1–6, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib441">
<span class="ltx_tag ltx_tag_bibitem">[441]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Montagna, S.&nbsp;Ferretti, L.&nbsp;C. Klopfenstein, A.&nbsp;Florio, and M.&nbsp;F. Pengo,
“Data decentralisation of llm-based chatbot systems in chronic disease
self-management,” in <em class="ltx_emph ltx_font_italic" id="bib.bib441.1.1">Proceedings of the 2023 ACM Conference on
Information Technology for Social Good</em>, 2023, pp. 205–212.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib442">
<span class="ltx_tag ltx_tag_bibitem">[442]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Bill and T.&nbsp;Eriksson, “Fine-tuning a llm using reinforcement learning from
human feedback for a therapy chatbot application,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib443">
<span class="ltx_tag ltx_tag_bibitem">[443]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Abbasian, I.&nbsp;Azimi, A.&nbsp;M. Rahmani, and R.&nbsp;Jain, “Conversational health
agents: A personalized llm-powered agent framework,” <em class="ltx_emph ltx_font_italic" id="bib.bib443.1.1">arXiv preprint
arXiv:2310.02374</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib444">
<span class="ltx_tag ltx_tag_bibitem">[444]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;V. Lemley, “Does chatgpt help us understand the medical literature?”
<em class="ltx_emph ltx_font_italic" id="bib.bib444.1.1">Journal of the American Society of Nephrology</em>, pp. 10–1681, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib445">
<span class="ltx_tag ltx_tag_bibitem">[445]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Pal, M.&nbsp;Bhattacharya, S.-S. Lee, and C.&nbsp;Chakraborty, “A domain-specific
next-generation large language model (llm) or chatgpt is required for
biomedical engineering and research,” <em class="ltx_emph ltx_font_italic" id="bib.bib445.1.1">Annals of Biomedical
Engineering</em>, pp. 1–4, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib446">
<span class="ltx_tag ltx_tag_bibitem">[446]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Du, S.&nbsp;Zhao, Y.&nbsp;Chen, R.&nbsp;Bai, J.&nbsp;Liu, H.&nbsp;Wu, H.&nbsp;Wang, and B.&nbsp;Qin, “The
calla dataset: Probing llms’ interactive knowledge acquisition from chinese
medical literature,” <em class="ltx_emph ltx_font_italic" id="bib.bib446.1.1">arXiv preprint arXiv:2309.04198</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib447">
<span class="ltx_tag ltx_tag_bibitem">[447]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Abd-Alrazaq, R.&nbsp;AlSaad, D.&nbsp;Alhuwail, A.&nbsp;Ahmed, P.&nbsp;M. Healy, S.&nbsp;Latifi,
S.&nbsp;Aziz, R.&nbsp;Damseh, S.&nbsp;A. Alrazak, J.&nbsp;Sheikh <em class="ltx_emph ltx_font_italic" id="bib.bib447.1.1">et&nbsp;al.</em>, “Large language
models in medical education: Opportunities, challenges, and future
directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib447.2.2">JMIR Medical Education</em>, vol.&nbsp;9, no.&nbsp;1, p. e48291, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib448">
<span class="ltx_tag ltx_tag_bibitem">[448]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;B. Mbakwe, I.&nbsp;Lourentzou, L.&nbsp;A. Celi, O.&nbsp;J. Mechanic, and A.&nbsp;Dagan,
“Chatgpt passing usmle shines a spotlight on the flaws of medical
education,” p. e0000205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib449">
<span class="ltx_tag ltx_tag_bibitem">[449]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Ahn, “The impending impacts of large language models on medical
education,” <em class="ltx_emph ltx_font_italic" id="bib.bib449.1.1">Korean Journal of Medical Education</em>, vol.&nbsp;35, no.&nbsp;1, p.
103, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib450">
<span class="ltx_tag ltx_tag_bibitem">[450]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Waisberg, J.&nbsp;Ong, M.&nbsp;Masalkhi, and A.&nbsp;G. Lee, “Large language model
(llm)-driven chatbots for neuro-ophthalmic medical education,” <em class="ltx_emph ltx_font_italic" id="bib.bib450.1.1">Eye</em>,
pp. 1–3, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib451">
<span class="ltx_tag ltx_tag_bibitem">[451]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Deiana, M.&nbsp;Dettori, A.&nbsp;Arghittu, A.&nbsp;Azara, G.&nbsp;Gabutti, and P.&nbsp;Castiglia,
“Artificial intelligence and public health: Evaluating chatgpt responses to
vaccination myths and misconceptions,” <em class="ltx_emph ltx_font_italic" id="bib.bib451.1.1">Vaccines</em>, vol.&nbsp;11, no.&nbsp;7, p.
1217, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib452">
<span class="ltx_tag ltx_tag_bibitem">[452]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;De&nbsp;Angelis, F.&nbsp;Baglivo, G.&nbsp;Arzilli, G.&nbsp;P. Privitera, P.&nbsp;Ferragina, A.&nbsp;E.
Tozzi, and C.&nbsp;Rizzo, “Chatgpt and the rise of large language models: the new
ai-driven infodemic threat in public health,” <em class="ltx_emph ltx_font_italic" id="bib.bib452.1.1">Frontiers in Public
Health</em>, vol.&nbsp;11, p. 1166120, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib453">
<span class="ltx_tag ltx_tag_bibitem">[453]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;L. Rane, A.&nbsp;Tawde, S.&nbsp;P. Choudhary, and J.&nbsp;Rane, “Contribution and
performance of chatgpt and other large language models (llm) for scientific
and research advancements: a double-edged sword,” <em class="ltx_emph ltx_font_italic" id="bib.bib453.1.1">International
Research Journal of Modernization in Engineering Technology and Science</em>,
vol.&nbsp;5, no.&nbsp;10, pp. 875–899, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib454">
<span class="ltx_tag ltx_tag_bibitem">[454]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Dai, J.&nbsp;Lin, H.&nbsp;Jin, T.&nbsp;Li, Y.-S. Tsai, D.&nbsp;Gašević, and G.&nbsp;Chen,
“Can large language models provide feedback to students? a case study on
chatgpt,” in <em class="ltx_emph ltx_font_italic" id="bib.bib454.1.1">2023 IEEE International Conference on Advanced Learning
Technologies (ICALT)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2023, pp.
323–325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib455">
<span class="ltx_tag ltx_tag_bibitem">[455]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Kasneci, K.&nbsp;Seßler, S.&nbsp;Küchemann, M.&nbsp;Bannert, D.&nbsp;Dementieva,
F.&nbsp;Fischer, U.&nbsp;Gasser, G.&nbsp;Groh, S.&nbsp;Günnemann, E.&nbsp;Hüllermeier
<em class="ltx_emph ltx_font_italic" id="bib.bib455.1.1">et&nbsp;al.</em>, “Chatgpt for good? on opportunities and challenges of large
language models for education,” <em class="ltx_emph ltx_font_italic" id="bib.bib455.2.2">Learning and individual differences</em>,
vol. 103, p. 102274, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib456">
<span class="ltx_tag ltx_tag_bibitem">[456]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Rane, “Enhancing the quality of teaching and learning through chatgpt and
similar large language models: Challenges, future prospects, and ethical
considerations in education,” <em class="ltx_emph ltx_font_italic" id="bib.bib456.1.1">Future Prospects, and Ethical
Considerations in Education (September 15, 2023)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib457">
<span class="ltx_tag ltx_tag_bibitem">[457]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;C. Young and M.&nbsp;Shishido, “Investigating openai’s chatgpt potentials in
generating chatbot’s dialogue for english as a foreign language learning,”
<em class="ltx_emph ltx_font_italic" id="bib.bib457.1.1">International Journal of Advanced Computer Science and Applications</em>,
vol.&nbsp;14, no.&nbsp;6, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib458">
<span class="ltx_tag ltx_tag_bibitem">[458]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Irons, C.&nbsp;Mason, P.&nbsp;Cooper, S.&nbsp;Sidra, A.&nbsp;Reeson, and C.&nbsp;Paris, “Exploring
the impacts of chatgpt on future scientific work,” <em class="ltx_emph ltx_font_italic" id="bib.bib458.1.1">SocArXiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib459">
<span class="ltx_tag ltx_tag_bibitem">[459]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;G. Schmidt and A.&nbsp;J. Meir, “Using generative ai for literature searches and
scholarly writing: Is the integrity of the scientific discourse in
jeopardy?” <em class="ltx_emph ltx_font_italic" id="bib.bib459.1.1">arXiv preprint arXiv:2311.06981</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib460">
<span class="ltx_tag ltx_tag_bibitem">[460]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zheng, H.&nbsp;Y. Koh, J.&nbsp;Ju, A.&nbsp;T. Nguyen, L.&nbsp;T. May, G.&nbsp;I. Webb, and S.&nbsp;Pan,
“Large language models for scientific synthesis, inference and
explanation,” <em class="ltx_emph ltx_font_italic" id="bib.bib460.1.1">arXiv preprint arXiv:2310.07984</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib461">
<span class="ltx_tag ltx_tag_bibitem">[461]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Aczel and E.-J. Wagenmakers, “Transparency guidance for chatgpt usage in
scientific writing,” <em class="ltx_emph ltx_font_italic" id="bib.bib461.1.1">PsyArXiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib462">
<span class="ltx_tag ltx_tag_bibitem">[462]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Altmäe, A.&nbsp;Sola-Leyva, and A.&nbsp;Salumets, “Artificial intelligence in
scientific writing: a friend or a foe?” <em class="ltx_emph ltx_font_italic" id="bib.bib462.1.1">Reproductive BioMedicine
Online</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib463">
<span class="ltx_tag ltx_tag_bibitem">[463]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Imani, L.&nbsp;Du, and H.&nbsp;Shrivastava, “Mathprompter: Mathematical reasoning
using large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib463.1.1">arXiv preprint arXiv:2303.05398</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib464">
<span class="ltx_tag ltx_tag_bibitem">[464]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Li, G.&nbsp;Dong, C.&nbsp;Tan, and C.&nbsp;Zhou, “Scaling relationship
on learning mathematical reasoning with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib464.1.1">arXiv
preprint arXiv:2308.01825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib465">
<span class="ltx_tag ltx_tag_bibitem">[465]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Yang, A.&nbsp;M. Swope, A.&nbsp;Gu, R.&nbsp;Chalamala, P.&nbsp;Song, S.&nbsp;Yu, S.&nbsp;Godil,
R.&nbsp;Prenger, and A.&nbsp;Anandkumar, “Leandojo: Theorem proving with
retrieval-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib465.1.1">arXiv preprint
arXiv:2306.15626</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib466">
<span class="ltx_tag ltx_tag_bibitem">[466]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;M. Collins, A.&nbsp;Q. Jiang, S.&nbsp;Frieder, L.&nbsp;Wong, M.&nbsp;Zilka, U.&nbsp;Bhatt,
T.&nbsp;Lukasiewicz, Y.&nbsp;Wu, J.&nbsp;B. Tenenbaum, W.&nbsp;Hart <em class="ltx_emph ltx_font_italic" id="bib.bib466.1.1">et&nbsp;al.</em>, “Evaluating
language models for mathematics through interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib466.2.2">arXiv preprint
arXiv:2306.01694</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib467">
<span class="ltx_tag ltx_tag_bibitem">[467]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, T.&nbsp;Han, S.&nbsp;Ma, J.&nbsp;Zhang, Y.&nbsp;Yang, J.&nbsp;Tian, H.&nbsp;He, A.&nbsp;Li, M.&nbsp;He, Z.&nbsp;Liu
<em class="ltx_emph ltx_font_italic" id="bib.bib467.1.1">et&nbsp;al.</em>, “Summary of chatgpt-related research and perspective towards
the future of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib467.2.2">Meta-Radiology</em>, p. 100017,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib468">
<span class="ltx_tag ltx_tag_bibitem">[468]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Drápal, H.&nbsp;Westermann, and J.&nbsp;Savelka, “Using large language models to
support thematic analysis in empirical legal studies,” <em class="ltx_emph ltx_font_italic" id="bib.bib468.1.1">arXiv preprint
arXiv:2310.18729</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib469">
<span class="ltx_tag ltx_tag_bibitem">[469]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Savelka, K.&nbsp;D. Ashley, M.&nbsp;A. Gray, H.&nbsp;Westermann, and H.&nbsp;Xu, “Explaining
legal concepts with augmented large language models (gpt-4),” <em class="ltx_emph ltx_font_italic" id="bib.bib469.1.1">arXiv
preprint arXiv:2306.09525</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib470">
<span class="ltx_tag ltx_tag_bibitem">[470]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Guha, J.&nbsp;Nyarko, D.&nbsp;E. Ho, C.&nbsp;Ré, A.&nbsp;Chilton, A.&nbsp;Narayana,
A.&nbsp;Chohlas-Wood, A.&nbsp;Peters, B.&nbsp;Waldon, D.&nbsp;N. Rockmore <em class="ltx_emph ltx_font_italic" id="bib.bib470.1.1">et&nbsp;al.</em>,
“Legalbench: A collaboratively built benchmark for measuring legal reasoning
in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib470.2.2">arXiv preprint arXiv:2308.11462</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib471">
<span class="ltx_tag ltx_tag_bibitem">[471]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Cui, Z.&nbsp;Li, Y.&nbsp;Yan, B.&nbsp;Chen, and L.&nbsp;Yuan, “Chatlaw: Open-source legal large
language model with integrated external knowledge bases,” <em class="ltx_emph ltx_font_italic" id="bib.bib471.1.1">arXiv
preprint arXiv:2306.16092</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib472">
<span class="ltx_tag ltx_tag_bibitem">[472]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Yang, X.-Y. Liu, and C.&nbsp;D. Wang, “Fingpt: Open-source financial large
language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib472.1.1">arXiv preprint arXiv:2306.06031</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib473">
<span class="ltx_tag ltx_tag_bibitem">[473]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Li, S.&nbsp;Wang, H.&nbsp;Ding, and H.&nbsp;Chen, “Large language models in finance: A
survey,” in <em class="ltx_emph ltx_font_italic" id="bib.bib473.1.1">Proceedings of the Fourth ACM International Conference on
AI in Finance</em>, 2023, pp. 374–382.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib474">
<span class="ltx_tag ltx_tag_bibitem">[474]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Micikevicius, S.&nbsp;Narang, J.&nbsp;Alben, G.&nbsp;Diamos, E.&nbsp;Elsen, D.&nbsp;Garcia,
B.&nbsp;Ginsburg, M.&nbsp;Houston, O.&nbsp;Kuchaiev, G.&nbsp;Venkatesh <em class="ltx_emph ltx_font_italic" id="bib.bib474.1.1">et&nbsp;al.</em>, “Mixed
precision training,” <em class="ltx_emph ltx_font_italic" id="bib.bib474.2.2">arXiv preprint arXiv:1710.03740</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib475">
<span class="ltx_tag ltx_tag_bibitem">[475]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Q. Nguyen and J.&nbsp;Salazar, “Transformers without tears: Improving the
normalization of self-attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib475.1.1">CoRR</em>, vol. abs/1910.05895, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib476">
<span class="ltx_tag ltx_tag_bibitem">[476]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Strubell, A.&nbsp;Ganesh, and A.&nbsp;McCallum, “Energy and policy considerations for
deep learning in nlp,” <em class="ltx_emph ltx_font_italic" id="bib.bib476.1.1">arXiv preprint arXiv:1906.02243</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib477">
<span class="ltx_tag ltx_tag_bibitem">[477]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;M. Bender, T.&nbsp;Gebru, A.&nbsp;McMillan-Major, and S.&nbsp;Shmitchell, “On the dangers
of stochastic parrots: Can language models be too big?” in <em class="ltx_emph ltx_font_italic" id="bib.bib477.1.1">Proceedings
of the 2021 ACM conference on fairness, accountability, and transparency</em>,
2021, pp. 610–623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib478">
<span class="ltx_tag ltx_tag_bibitem">[478]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Zhang, S.&nbsp;Bengio, M.&nbsp;Hardt, B.&nbsp;Recht, and O.&nbsp;Vinyals, “Understanding deep
learning (still) requires rethinking generalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib478.1.1">Communications
of the ACM</em>, vol.&nbsp;64, no.&nbsp;3, pp. 107–115, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib479">
<span class="ltx_tag ltx_tag_bibitem">[479]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Tänzer, S.&nbsp;Ruder, and M.&nbsp;Rei, “Memorisation versus generalisation in
pre-trained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib479.1.1">arXiv preprint arXiv:2105.00828</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib480">
<span class="ltx_tag ltx_tag_bibitem">[480]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;M. West, M.&nbsp;Whittaker, and K.&nbsp;Crawford, “Discriminating systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib480.1.1">AI
Now</em>, pp. 1–33, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib481">
<span class="ltx_tag ltx_tag_bibitem">[481]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Valmeekam, A.&nbsp;Olmo, S.&nbsp;Sreedharan, and S.&nbsp;Kambhampati, “Large language
models still can’t plan (a benchmark for llms on planning and reasoning about
change),” <em class="ltx_emph ltx_font_italic" id="bib.bib481.1.1">arXiv preprint arXiv:2206.10498</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib482">
<span class="ltx_tag ltx_tag_bibitem">[482]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang,
Y.&nbsp;Chen <em class="ltx_emph ltx_font_italic" id="bib.bib482.1.1">et&nbsp;al.</em>, “Siren’s song in the ai ocean: A survey on
hallucination in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib482.2.2">arXiv preprint
arXiv:2309.01219</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib483">
<span class="ltx_tag ltx_tag_bibitem">[483]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Webson and E.&nbsp;Pavlick, “Do prompt-based models really understand the
meaning of their prompts?” <em class="ltx_emph ltx_font_italic" id="bib.bib483.1.1">arXiv preprint arXiv:2109.01247</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib484">
<span class="ltx_tag ltx_tag_bibitem">[484]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Shaikh, H.&nbsp;Zhang, W.&nbsp;Held, M.&nbsp;Bernstein, and D.&nbsp;Yang, “On second thought,
let’s not think step by step! bias and toxicity in zero-shot reasoning,”
<em class="ltx_emph ltx_font_italic" id="bib.bib484.1.1">arXiv preprint arXiv:2212.08061</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib485">
<span class="ltx_tag ltx_tag_bibitem">[485]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Liu, H.&nbsp;Cheng, P.&nbsp;He, W.&nbsp;Chen, Y.&nbsp;Wang, H.&nbsp;Poon, and J.&nbsp;Gao, “Adversarial
training for large neural language models,” ArXiv, April 2020. [Online].
Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/adversarial-training-for-large-neural-language-models/" title="">https://www.microsoft.com/en-us/research/publication/adversarial-training-for-large-neural-language-models/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib486">
<span class="ltx_tag ltx_tag_bibitem">[486]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Shayegani, M.&nbsp;A.&nbsp;A. Mamun, Y.&nbsp;Fu, P.&nbsp;Zaree, Y.&nbsp;Dong, and N.&nbsp;Abu-Ghazaleh,
“Survey of vulnerabilities in large language models revealed by adversarial
attacks,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib487">
<span class="ltx_tag ltx_tag_bibitem">[487]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Xu, K.&nbsp;Kong, N.&nbsp;Liu, L.&nbsp;Cui, D.&nbsp;Wang, J.&nbsp;Zhang, and M.&nbsp;Kankanhalli, “An llm
can fool itself: A prompt-based adversarial attack,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib488">
<span class="ltx_tag ltx_tag_bibitem">[488]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Zhao, H.&nbsp;Chen, F.&nbsp;Yang, N.&nbsp;Liu, H.&nbsp;Deng, H.&nbsp;Cai, S.&nbsp;Wang, D.&nbsp;Yin, and M.&nbsp;Du,
“Explainability for large language models: A survey,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib489">
<span class="ltx_tag ltx_tag_bibitem">[489]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Huang, S.&nbsp;Mamidanna, S.&nbsp;Jangam, Y.&nbsp;Zhou, and L.&nbsp;H. Gilpin, “Can large
language models explain themselves? a study of llm-generated
self-explanations,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib490">
<span class="ltx_tag ltx_tag_bibitem">[490]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Brown, K.&nbsp;Lee, F.&nbsp;Mireshghallah, R.&nbsp;Shokri, and F.&nbsp;Tramèr, “What does
it mean for a language model to preserve privacy?” in <em class="ltx_emph ltx_font_italic" id="bib.bib490.1.1">Proceedings of
the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022,
pp. 2280–2292.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib491">
<span class="ltx_tag ltx_tag_bibitem">[491]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Plant, V.&nbsp;Giuffrida, and D.&nbsp;Gkatzia, “You are what you write: Preserving
privacy in the era of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib491.1.1">arXiv preprint
arXiv:2204.09391</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib492">
<span class="ltx_tag ltx_tag_bibitem">[492]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Niu, Z.&nbsp;Kong, G.&nbsp;Yuan, W.&nbsp;Jiang, J.&nbsp;Guan, C.&nbsp;Ding, P.&nbsp;Zhao, S.&nbsp;Liu, B.&nbsp;Ren,
and Y.&nbsp;Wang, “Real-time execution of large-scale language models on
mobile,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib493">
<span class="ltx_tag ltx_tag_bibitem">[493]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Guo, J.&nbsp;Tang, W.&nbsp;Hu, J.&nbsp;Leng, C.&nbsp;Zhang, F.&nbsp;Yang, Y.&nbsp;Liu, M.&nbsp;Guo, and Y.&nbsp;Zhu,
“Olive: Accelerating large language models via hardware-friendly
outlier-victim pair quantization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib493.1.1">Proceedings of the 50th Annual
International Symposium on Computer Architecture</em>, 2023, pp. 1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib494">
<span class="ltx_tag ltx_tag_bibitem">[494]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Meskó and E.&nbsp;J. Topol, “The imperative for regulatory oversight of
large language models (or generative ai) in healthcare,” <em class="ltx_emph ltx_font_italic" id="bib.bib494.1.1">npj Digital
Medicine</em>, vol.&nbsp;6, no.&nbsp;1, p. 120, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib495">
<span class="ltx_tag ltx_tag_bibitem">[495]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, X.&nbsp;Ji, Z.&nbsp;Zhao, X.&nbsp;Hei, and K.-K.&nbsp;R. Choo, “Ethical considerations
and policy implications for large language models: Guiding responsible
development and deployment,” <em class="ltx_emph ltx_font_italic" id="bib.bib495.1.1">arXiv preprint arXiv:2308.02678</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib496">
<span class="ltx_tag ltx_tag_bibitem">[496]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Mökander, J.&nbsp;Schuett, H.&nbsp;R. Kirk, and L.&nbsp;Floridi, “Auditing large
language models: a three-layered approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib496.1.1">AI and Ethics</em>, pp. 1–31,
2023.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none; left: 1013.2px; top: -7506.5px; transform: translate(-50%, -100%);">Report Issue for Selection</button></body></html>