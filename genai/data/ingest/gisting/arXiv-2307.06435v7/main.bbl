% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{y2022large}
B.~A. y~Arcas, ``Do large language models understand us?'' \emph{Daedalus},
  vol. 151, no.~2, pp. 183--197, 2022.

\bibitem{chernyavskiy2021transformers}
A.~Chernyavskiy, D.~Ilvovsky, and P.~Nakov, ``Transformers:“the end of
  history” for natural language processing?'' in \emph{Machine Learning and
  Knowledge Discovery in Databases. Research Track: European Conference, ECML
  PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III
  21}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2021, pp. 677--693.

\bibitem{wang2019superglue}
A.~Wang, Y.~Pruksachatkun, N.~Nangia, A.~Singh, J.~Michael, F.~Hill, O.~Levy,
  and S.~Bowman, ``Superglue: A stickier benchmark for general-purpose language
  understanding systems,'' \emph{Advances in neural information processing
  systems}, vol.~32, 2019.

\bibitem{adiwardana2020towards}
D.~Adiwardana, M.-T. Luong, D.~R. So, J.~Hall, N.~Fiedel, R.~Thoppilan,
  Z.~Yang, A.~Kulshreshtha, G.~Nemade, Y.~Lu \emph{et~al.}, ``Towards a
  human-like open-domain chatbot,'' \emph{arXiv preprint arXiv:2001.09977},
  2020.

\bibitem{Bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{ELMO}
M.~E. Peters, M.~Neumann, M.~Iyyer, M.~Gardner, C.~Clark, K.~Lee, and
  L.~Zettlemoyer, ``Deep contextualized word representations,'' in
  \emph{{NAACL-HLT}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for
  Computational Linguistics, 2018, pp. 2227--2237.

\bibitem{BART}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``Bart: Denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' \emph{arXiv
  preprint arXiv:1910.13461}, 2019.

\bibitem{GPT-3}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{BLOOM}
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ili{\'c}, D.~Hesslow,
  R.~Castagn{\'e}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'e} \emph{et~al.},
  ``Bloom: A 176b-parameter open-access multilingual language model,''
  \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{OPT}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin \emph{et~al.}, ``Opt: Open pre-trained transformer language
  models,'' \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{T5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, pp. 5485--5551, 2020.

\bibitem{mT5}
L.~Xue, N.~Constant, A.~Roberts, M.~Kale, R.~Al-Rfou, A.~Siddhant, A.~Barua,
  and C.~Raffel, ``mt5: A massively multilingual pre-trained text-to-text
  transformer,'' \emph{arXiv preprint arXiv:2010.11934}, 2020.

\bibitem{CPM-2}
Z.~Zhang, Y.~Gu, X.~Han, S.~Chen, C.~Xiao, Z.~Sun, Y.~Yao, F.~Qi, J.~Guan,
  P.~Ke \emph{et~al.}, ``Cpm-2: Large-scale cost-effective pre-trained language
  models,'' \emph{AI Open}, vol.~2, pp. 216--224, 2021.

\bibitem{PaLM}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann \emph{et~al.}, ``Palm: Scaling language
  modeling with pathways,'' \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{UL2}
Y.~Tay, M.~Dehghani, V.~Q. Tran, X.~Garcia, J.~Wei, X.~Wang, H.~W. Chung,
  D.~Bahri, T.~Schuster, S.~Zheng \emph{et~al.}, ``Ul2: Unifying language
  learning paradigms,'' in \emph{The Eleventh International Conference on
  Learning Representations}, 2022.

\bibitem{Common_Crawl}
\BIBentryALTinterwordspacing
``Common crawl.'' [Online]. Available: \url{https://commoncrawl.org/}
\BIBentrySTDinterwordspacing

\bibitem{Wikipedia}
\BIBentryALTinterwordspacing
``Wikipedia.'' [Online]. Available:
  \url{https://en.wikipedia.org/wiki/Main_Page}
\BIBentrySTDinterwordspacing

\bibitem{Openwebtext_Dataset}
\BIBentryALTinterwordspacing
``Openwebtext corpus.'' [Online]. Available:
  \url{http://Skylion007.github.io/OpenWebTextCorpus}
\BIBentrySTDinterwordspacing

\bibitem{BQ_Dataset}
\BIBentryALTinterwordspacing
``Bigquery dataset.'' [Online]. Available:
  \url{https://cloud.google.com/bigquery?hl=zh-cn}
\BIBentrySTDinterwordspacing

\bibitem{U-PaLM}
Y.~Tay, J.~Wei, H.~W. Chung, V.~Q. Tran, D.~R. So, S.~Shakeri, X.~Garcia, H.~S.
  Zheng, J.~Rao, A.~Chowdhery \emph{et~al.}, ``Transcending scaling laws with
  0.1\% extra compute,'' \emph{arXiv preprint arXiv:2210.11399}, 2022.

\bibitem{mtnlg}
S.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper,
  Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti \emph{et~al.}, ``Using
  deepspeed and megatron to train megatron-turing nlg 530b, a large-scale
  generative language model,'' \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{T0}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja \emph{et~al.}, ``Multitask
  prompted training enables zero-shot task generalization,'' \emph{arXiv
  preprint arXiv:2110.08207}, 2021.

\bibitem{mT0andBLOOMZ}
N.~Muennighoff, T.~Wang, L.~Sutawika, A.~Roberts, S.~Biderman, T.~L. Scao,
  M.~S. Bari, S.~Shen, Z.-X. Yong, H.~Schoelkopf \emph{et~al.}, ``Crosslingual
  generalization through multitask finetuning,'' \emph{arXiv preprint
  arXiv:2211.01786}, 2022.

\bibitem{OPT_IML}
S.~Iyer, X.~V. Lin, R.~Pasunuru, T.~Mihaylov, D.~Simig, P.~Yu, K.~Shuster,
  T.~Wang, Q.~Liu, P.~S. Koura \emph{et~al.}, ``Opt-iml: Scaling language model
  instruction meta learning through the lens of generalization,'' \emph{arXiv
  preprint arXiv:2212.12017}, 2022.

\bibitem{Flan}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma \emph{et~al.}, ``Scaling instruction-finetuned
  language models,'' \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{Tk-INSTRUCT}
Y.~Wang, S.~Mishra, P.~Alipoormolabashi, Y.~Kordi, A.~Mirzaei, A.~Naik,
  A.~Ashok, A.~S. Dhanasekaran, A.~Arunkumar, D.~Stap \emph{et~al.},
  ``Super-naturalinstructions: Generalization via declarative instructions on
  1600+ nlp tasks,'' in \emph{Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing}, 2022, pp. 5085--5109.

\bibitem{LMAdapted}
B.~Lester, R.~Al-Rfou, and N.~Constant, ``The power of scale for
  parameter-efficient prompt tuning,'' \emph{arXiv preprint arXiv:2104.08691},
  2021.

\bibitem{LMAdapter_2}
J.~He, C.~Zhou, X.~Ma, T.~Berg-Kirkpatrick, and G.~Neubig, ``Towards a unified
  view of parameter-efficient transfer learning,'' \emph{arXiv preprint
  arXiv:2110.04366}, 2021.

\bibitem{LMAdapter_3}
Z.~Hu, Y.~Lan, L.~Wang, W.~Xu, E.-P. Lim, R.~K.-W. Lee, L.~Bing, and S.~Poria,
  ``Llm-adapters: An adapter family for parameter-efficient fine-tuning of
  large language models,'' \emph{arXiv preprint arXiv:2304.01933}, 2023.

\bibitem{Prompt_Tuning}
B.~Lester, R.~Al-Rfou, and N.~Constant, ``The power of scale for
  parameter-efficient prompt tuning,'' \emph{arXiv preprint arXiv:2104.08691},
  2021.

\bibitem{Prefix_Tuning}
X.~L. Li and P.~Liang, ``Prefix-tuning: Optimizing continuous prompts for
  generation,'' \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{survey_1}
C.~Zhou, Q.~Li, C.~Li, J.~Yu, Y.~Liu, G.~Wang, K.~Zhang, C.~Ji, Q.~Yan, L.~He
  \emph{et~al.}, ``A comprehensive survey on pretrained foundation models: A
  history from bert to chatgpt,'' \emph{arXiv preprint arXiv:2302.09419}, 2023.

\bibitem{Survey_LLM}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang,
  J.~Zhang, Z.~Dong \emph{et~al.}, ``A survey of large language models,''
  \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem{survey_2}
G.~Mialon, R.~Dess{\`\i}, M.~Lomeli, C.~Nalmpantis, R.~Pasunuru, R.~Raileanu,
  B.~Rozi{\`e}re, T.~Schick, J.~Dwivedi-Yu, A.~Celikyilmaz \emph{et~al.},
  ``Augmented language models: a survey,'' \emph{arXiv preprint
  arXiv:2302.07842}, 2023.

\bibitem{survey_smaller_LLMs_1}
U.~Naseem, I.~Razzak, S.~K. Khan, and M.~Prasad, ``A comprehensive survey on
  word representation models: From classical to state-of-the-art word
  representation language models,'' \emph{Transactions on Asian and
  Low-Resource Language Information Processing}, vol.~20, no.~5, pp. 1--35,
  2021.

\bibitem{survey_smaller_LLMs_2}
B.~Min, H.~Ross, E.~Sulem, A.~P.~B. Veyseh, T.~H. Nguyen, O.~Sainz, E.~Agirre,
  I.~Heinz, and D.~Roth, ``Recent advances in natural language processing via
  large pre-trained language models: A survey,'' \emph{arXiv preprint
  arXiv:2111.01243}, 2021.

\bibitem{webster1992tokenization}
J.~J. Webster and C.~Kit, ``Tokenization as the initial phase in nlp,'' in
  \emph{COLING 1992 volume 4: The 14th international conference on
  computational linguistics}, 1992.

\bibitem{unigramLM}
T.~Kudo, ``Subword regularization: Improving neural network translation models
  with multiple subword candidates,'' in \emph{Proceedings of the 56th Annual
  Meeting of the Association for Computational Linguistics (Volume 1: Long
  Papers)}, 2018, pp. 66--75.

\bibitem{bpe}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' in \emph{Proceedings of the 54th Annual Meeting
  of the Association for Computational Linguistics (Volume 1: Long Papers)},
  2016, pp. 1715--1725.

\bibitem{tokenizationsurvey}
S.~J. Mielke, Z.~Alyafeai, E.~Salesky, C.~Raffel, M.~Dey, M.~Gall{\'e},
  A.~Raja, C.~Si, W.~Y. Lee, B.~Sagot \emph{et~al.}, ``Between words and
  characters: A brief history of open-vocabulary modeling and tokenization in
  nlp,'' \emph{arXiv preprint arXiv:2112.10508}, 2021.

\bibitem{wordpiece}
M.~Schuster and K.~Nakajima, ``Japanese and korean voice search,'' in
  \emph{2012 IEEE international conference on acoustics, speech and signal
  processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2012, pp.
  5149--5152.

\bibitem{selectiveattention}
C.~W. Eriksen and J.~E. Hoffman, ``Some characteristics of selective attention
  in visual perception determined by vocal reaction time,'' \emph{Perception \&
  Psychophysics}, vol.~11, no.~2, pp. 169--171, 1972.

\bibitem{attention}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly
  learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473},
  2014.

\bibitem{Transformers}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{sparse_transformer}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever, ``Generating long sequences
  with sparse transformers,'' \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}, ``Flashattention: Fast and
  memory-efficient exact attention with io-awareness,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~35, pp. 16\,344--16\,359, 2022.

\bibitem{alibi}
\BIBentryALTinterwordspacing
O.~Press, N.~Smith, and M.~Lewis, ``Train short, test long: Attention with
  linear biases enables input length extrapolation,'' in \emph{International
  Conference on Learning Representations}, 2022. [Online]. Available:
  \url{https://openreview.net/forum?id=R8sQPpGCv0}
\BIBentrySTDinterwordspacing

\bibitem{su2021roformer}
J.~Su, Y.~Lu, S.~Pan, A.~Murtadha, B.~Wen, and Y.~Liu, ``Roformer: Enhanced
  transformer with rotary position embedding,'' \emph{arXiv preprint
  arXiv:2104.09864}, 2021.

\bibitem{NoPE}
A.~Kazemnejad, I.~Padhi, K.~N. Ramamurthy, P.~Das, and S.~Reddy, ``The impact
  of positional encoding on length generalization in transformers,''
  \emph{arXiv preprint arXiv:2305.19466}, 2023.

\bibitem{activationfunction}
K.~Hornik, M.~Stinchcombe, and H.~White, ``Multilayer feedforward networks are
  universal approximators,'' \emph{Neural networks}, vol.~2, no.~5, pp.
  359--366, 1989.

\bibitem{relu}
V.~Nair and G.~E. Hinton, ``Rectified linear units improve restricted boltzmann
  machines,'' in \emph{Proceedings of the 27th international conference on
  machine learning (ICML-10)}, 2010, pp. 807--814.

\bibitem{gelu}
D.~Hendrycks and K.~Gimpel, ``Gaussian error linear units (gelus),''
  \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting,''
  \emph{The journal of machine learning research}, vol.~15, no.~1, pp.
  1929--1958, 2014.

\bibitem{krueger2016zoneout}
D.~Krueger, T.~Maharaj, J.~Kram{\'a}r, M.~Pezeshki, N.~Ballas, N.~R. Ke,
  A.~Goyal, Y.~Bengio, A.~Courville, and C.~Pal, ``Zoneout: Regularizing rnns
  by randomly preserving hidden activations,'' \emph{arXiv preprint
  arXiv:1606.01305}, 2016.

\bibitem{shazeer2020glu}
N.~Shazeer, ``Glu variants improve transformer,'' \emph{arXiv preprint
  arXiv:2002.05202}, 2020.

\bibitem{glu}
Y.~N. Dauphin, A.~Fan, M.~Auli, and D.~Grangier, ``Language modeling with gated
  convolutional networks,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 933--941.

\bibitem{rmsnorm}
B.~Zhang and R.~Sennrich, ``Root mean square layer normalization,''
  \emph{Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{preLN}
A.~Baevski and M.~Auli, ``Adaptive input representations for neural language
  modeling,'' \emph{arXiv preprint arXiv:1809.10853}, 2018.

\bibitem{shleifer2021normformer}
S.~Shleifer, J.~Weston, and M.~Ott, ``Normformer: Improved transformer
  pretraining with extra normalization,'' \emph{arXiv preprint
  arXiv:2110.09456}, 2021.

\bibitem{deepnorm}
H.~Wang, S.~Ma, L.~Dong, S.~Huang, D.~Zhang, and F.~Wei, ``Deepnet: Scaling
  transformers to 1,000 layers,'' \emph{arXiv preprint arXiv:2203.00555}, 2022.

\bibitem{ZeroOpt}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He, ``Zero: Memory optimizations
  toward training trillion parameter models,'' in \emph{SC20: International
  Conference for High Performance Computing, Networking, Storage and
  Analysis}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1--16.

\bibitem{Lib_Transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz \emph{et~al.}, ``Transformers:
  State-of-the-art natural language processing,'' in \emph{Proceedings of the
  2020 conference on empirical methods in natural language processing: system
  demonstrations}, 2020, pp. 38--45.

\bibitem{Lib_DeepSpeed}
J.~Rasley, S.~Rajbhandari, O.~Ruwase, and Y.~He, ``Deepspeed: System
  optimizations enable training deep learning models with over 100 billion
  parameters,'' in \emph{Proceedings of the 26th ACM SIGKDD International
  Conference on Knowledge Discovery \& Data Mining}, 2020, pp. 3505--3506.

\bibitem{Lib_Megatron}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro,
  ``Megatron-lm: Training multi-billion parameter language models using model
  parallelism,'' \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{Lib_Jax}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~VanderPlas, S.~Wanderman-Milne \emph{et~al.}, ``Jax:
  composable transformations of python+ numpy programs,'' 2018.

\bibitem{Lib_Colossal}
S.~Li, J.~Fang, Z.~Bian, H.~Liu, Y.~Liu, H.~Huang, B.~Wang, and Y.~You,
  ``Colossal-ai: A unified deep learning system for large-scale parallel
  training,'' \emph{arXiv preprint arXiv:2110.14883}, 2021.

\bibitem{Lib_Bmtrain}
\BIBentryALTinterwordspacing
``"bmtrain: Efficient training for big models.".'' [Online]. Available:
  \url{https://github.com/OpenBMB/BMTrain}
\BIBentrySTDinterwordspacing

\bibitem{Lib_Fastmoe}
J.~He, J.~Qiu, A.~Zeng, Z.~Yang, J.~Zhai, and J.~Tang, ``Fastmoe: A fast
  mixture-of-expert training system,'' \emph{arXiv preprint arXiv:2103.13262},
  2021.

\bibitem{Lib_Mindspore}
L.~Huawei Technologies~Co., ``Huawei mindspore ai development framework,'' in
  \emph{Artificial Intelligence Technology}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2022, pp. 137--162.

\bibitem{Lib_Pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{Lib_Tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard \emph{et~al.}, ``Tensorflow: a system for
  large-scale machine learning.'' in \emph{Osdi}, vol.~16, no. 2016.\hskip 1em
  plus 0.5em minus 0.4em\relax Savannah, GA, USA, 2016, pp. 265--283.

\bibitem{Lib_Mxnet}
T.~Chen, M.~Li, Y.~Li, M.~Lin, N.~Wang, M.~Wang, T.~Xiao, B.~Xu, C.~Zhang, and
  Z.~Zhang, ``Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems,'' \emph{arXiv preprint arXiv:1512.01274},
  2015.

\bibitem{decoderonly}
\BIBentryALTinterwordspacing
P.~J. Liu*, M.~Saleh*, E.~Pot, B.~Goodrich, R.~Sepassi, L.~Kaiser, and
  N.~Shazeer, ``Generating wikipedia by summarizing long sequences,'' in
  \emph{International Conference on Learning Representations}, 2018. [Online].
  Available: \url{https://openreview.net/forum?id=Hyg0vbWC-}
\BIBentrySTDinterwordspacing

\bibitem{LLM_Objectives}
T.~Wang, A.~Roberts, D.~Hesslow, T.~Le~Scao, H.~W. Chung, I.~Beltagy,
  J.~Launay, and C.~Raffel, ``What language model architecture and pretraining
  objective works best for zero-shot generalization?'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2022, pp. 22\,964--22\,984.

\bibitem{Unified_LM}
L.~Dong, N.~Yang, W.~Wang, F.~Wei, X.~Liu, Y.~Wang, J.~Gao, M.~Zhou, and H.-W.
  Hon, ``Unified language model pre-training for natural language understanding
  and generation,'' \emph{Advances in neural information processing systems},
  vol.~32, 2019.

\bibitem{Prompt_Tuning_2}
X.~Liu, Y.~Zheng, Z.~Du, M.~Ding, Y.~Qian, Z.~Yang, and J.~Tang, ``Gpt
  understands, too,'' \emph{arXiv preprint arXiv:2103.10385}, 2021.

\bibitem{LMAdapter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe,
  A.~Gesmundo, M.~Attariyan, and S.~Gelly, ``Parameter-efficient transfer
  learning for nlp,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 2790--2799.

\bibitem{PanGU_alpha}
W.~Zeng, X.~Ren, T.~Su, H.~Wang, Y.~Liao, Z.~Wang, X.~Jiang, Z.~Yang, K.~Wang,
  X.~Zhang \emph{et~al.}, ``Pangu-$\alpha$ : Large-scale autoregressive
  pretrained chinese language models with auto-parallel computation,''
  \emph{arXiv preprint arXiv:2104.12369}, 2021.

\bibitem{WuDaoCorpus}
S.~Yuan, H.~Zhao, Z.~Du, M.~Ding, X.~Liu, Y.~Cen, X.~Zou, Z.~Yang, and J.~Tang,
  ``Wudaocorpora: A super large-scale chinese corpora for pre-training language
  models,'' \emph{AI Open}, vol.~2, pp. 65--68, 2021.

\bibitem{CodeGen}
E.~Nijkamp, B.~Pang, H.~Hayashi, L.~Tu, H.~Wang, Y.~Zhou, S.~Savarese, and
  C.~Xiong, ``Codegen: An open large language model for code with multi-turn
  program synthesis,'' \emph{arXiv preprint arXiv:2203.13474}, 2022.

\bibitem{GPT_NeoX}
S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He,
  C.~Leahy, K.~McDonell, J.~Phang \emph{et~al.}, ``Gpt-neox-20b: An open-source
  autoregressive language model,'' \emph{arXiv preprint arXiv:2204.06745},
  2022.

\bibitem{GPT_J_6B}
W.~Ben and K.~Aran, ``Gpt-j-6b: A 6 billion parameter autoregressive language
  model,'' 2021.

\bibitem{S_mixed_precision}
P.~Micikevicius, S.~Narang, J.~Alben, G.~Diamos, E.~Elsen, D.~Garcia,
  B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh \emph{et~al.}, ``Mixed
  precision training,'' \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{GLM-130B}
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng,
  X.~Xia \emph{et~al.}, ``Glm-130b: An open bilingual pre-trained model,''
  \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{GLM}
Z.~Du, Y.~Qian, X.~Liu, M.~Ding, J.~Qiu, Z.~Yang, and J.~Tang, ``Glm: General
  language model pretraining with autoregressive blank infilling,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2022, pp. 320--335.

\bibitem{galactica}
R.~Taylor, M.~Kardas, G.~Cucurull, T.~Scialom, A.~Hartshorn, E.~Saravia,
  A.~Poulton, V.~Kerkez, and R.~Stojnic, ``Galactica: A large language model
  for science,'' \emph{arXiv preprint arXiv:2211.09085}, 2022.

\bibitem{fairscale}
{FairScale authors}, ``Fairscale: A general purpose modular pytorch library for
  high performance and large scale training,''
  \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem{GPT-2}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{batch_size_selec}
S.~McCandlish, J.~Kaplan, D.~Amodei, and O.~D. Team, ``An empirical model of
  large-batch training,'' \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem{codex}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,
  Y.~Burda, N.~Joseph, G.~Brockman \emph{et~al.}, ``Evaluating large language
  models trained on code,'' \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{ernie3}
Y.~Sun, S.~Wang, S.~Feng, S.~Ding, C.~Pang, J.~Shang, J.~Liu, X.~Chen, Y.~Zhao,
  Y.~Lu \emph{et~al.}, ``Ernie 3.0: Large-scale knowledge enhanced pre-training
  for language understanding and generation,'' \emph{arXiv preprint
  arXiv:2107.02137}, 2021.

\bibitem{dai2019transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~V. Le, and R.~Salakhutdinov,
  ``Transformer-xl: Attentive language models beyond a fixed-length context,''
  \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{lieber2021jurassic}
O.~Lieber, O.~Sharir, B.~Lenz, and Y.~Shoham, ``Jurassic-1: Technical details
  and evaluation,'' \emph{White Paper. AI21 Labs}, vol.~1, 2021.

\bibitem{levine2020limits}
Y.~Levine, N.~Wies, O.~Sharir, H.~Bata, and A.~Shashua, ``Limits to depth
  efficiencies of self-attention,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 22\,640--22\,651, 2020.

\bibitem{hyperclova}
B.~Kim, H.~Kim, S.-W. Lee, G.~Lee, D.~Kwak, D.~H. Jeon, S.~Park, S.~Kim,
  S.~Kim, D.~Seo \emph{et~al.}, ``What changes can large-scale language models
  bring? intensive study on hyperclova: Billions-scale korean generative
  pretrained transformers,'' \emph{arXiv preprint arXiv:2109.04650}, 2021.

\bibitem{wu2021yuan}
S.~Wu, X.~Zhao, T.~Yu, R.~Zhang, C.~Shen, H.~Liu, F.~Li, H.~Zhu, J.~Luo, L.~Xu
  \emph{et~al.}, ``Yuan 1.0: Large-scale pre-trained language model in
  zero-shot and few-shot learning,'' \emph{arXiv preprint arXiv:2110.04725},
  2021.

\bibitem{gopher}
J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song,
  J.~Aslanides, S.~Henderson, R.~Ring, S.~Young \emph{et~al.}, ``Scaling
  language models: Methods, analysis \& insights from training gopher,''
  \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{ernie3titan}
S.~Wang, Y.~Sun, Y.~Xiang, Z.~Wu, S.~Ding, W.~Gong, S.~Feng, J.~Shang, Y.~Zhao,
  C.~Pang \emph{et~al.}, ``Ernie 3.0 titan: Exploring larger-scale knowledge
  enhanced pre-training for language understanding and generation,''
  \emph{arXiv preprint arXiv:2112.12731}, 2021.

\bibitem{du2022glam}
N.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou,
  A.~W. Yu, O.~Firat \emph{et~al.}, ``Glam: Efficient scaling of language
  models with mixture-of-experts,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp.
  5547--5569.

\bibitem{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean,
  ``Outrageously large neural networks: The sparsely-gated mixture-of-experts
  layer,'' \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{fedus2022switch}
W.~Fedus, B.~Zoph, and N.~Shazeer, ``Switch transformers: Scaling to trillion
  parameter models with simple and efficient sparsity,'' \emph{The Journal of
  Machine Learning Research}, vol.~23, no.~1, pp. 5232--5270, 2022.

\bibitem{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du \emph{et~al.}, ``Lamda: Language models for
  dialog applications,'' \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{li2022competition}
Y.~Li, D.~Choi, J.~Chung, N.~Kushman, J.~Schrittwieser, R.~Leblond, T.~Eccles,
  J.~Keeling, F.~Gimeno, A.~Dal~Lago \emph{et~al.}, ``Competition-level code
  generation with alphacode,'' \emph{Science}, vol. 378, no. 6624, pp.
  1092--1097, 2022.

\bibitem{shazeer2019fast}
N.~Shazeer, ``Fast transformer decoding: One write-head is all you need,''
  \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem{pang2020text}
R.~Y. Pang and H.~He, ``Text generation by learning from demonstrations,''
  \emph{arXiv preprint arXiv:2009.07839}, 2020.

\bibitem{dabre2020softmax}
R.~Dabre and A.~Fujita, ``Softmax tempering for training neural machine
  translation models,'' \emph{arXiv preprint arXiv:2009.09372}, 2020.

\bibitem{chinchilla}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark \emph{et~al.}, ``Training
  compute-optimal large language models,'' \emph{arXiv preprint
  arXiv:2203.15556}, 2022.

\bibitem{soltan2022alexatm}
S.~Soltan, S.~Ananthakrishnan, J.~FitzGerald, R.~Gupta, W.~Hamza, H.~Khan,
  C.~Peris, S.~Rawls, A.~Rosenbaum, A.~Rumshisky \emph{et~al.}, ``Alexatm 20b:
  Few-shot learning using a large-scale multilingual seq2seq model,''
  \emph{arXiv preprint arXiv:2208.01448}, 2022.

\bibitem{glaese2022improving}
A.~Glaese, N.~McAleese, M.~Tr{\k{e}}bacz, J.~Aslanides, V.~Firoiu, T.~Ewalds,
  M.~Rauh, L.~Weidinger, M.~Chadwick, P.~Thacker \emph{et~al.}, ``Improving
  alignment of dialogue agents via targeted human judgements,'' \emph{arXiv
  preprint arXiv:2209.14375}, 2022.

\bibitem{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu, ``Asynchronous methods for deep reinforcement learning,''
  in \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2016, pp. 1928--1937.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar \emph{et~al.}, ``Llama: Open
  and efficient foundation language models,'' \emph{arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{wenzek2019ccnet}
G.~Wenzek, M.-A. Lachaux, A.~Conneau, V.~Chaudhary, F.~Guzm{\'a}n, A.~Joulin,
  and E.~Grave, ``Ccnet: Extracting high quality monolingual datasets from web
  crawl data,'' \emph{arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{PanGu_sigma}
X.~Ren, P.~Zhou, X.~Meng, X.~Huang, Y.~Wang, W.~Wang, P.~Li, X.~Zhang,
  A.~Podolskiy, G.~Arshinov \emph{et~al.}, ``Pangu-$\sum$: Towards trillion
  parameter language model with sparse heterogeneous computing,'' \emph{arXiv
  preprint arXiv:2303.10845}, 2023.

\bibitem{Mesh_Transformer_JAX}
W.~Ben, ``Mesh-transformer-jax: Model-parallel implementation of transformer
  language model with jax,'' 2021.

\bibitem{black2022gpt}
S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He,
  C.~Leahy, K.~McDonell, J.~Phang \emph{et~al.}, ``Gpt-neox-20b: An open-source
  autoregressive language model,'' \emph{arXiv preprint arXiv:2204.06745},
  2022.

\bibitem{nakano2021webgpt}
R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,
  V.~Kosaraju, W.~Saunders \emph{et~al.}, ``Webgpt: Browser-assisted
  question-answering with human feedback,'' \emph{arXiv preprint
  arXiv:2112.09332}, 2021.

\bibitem{mT0}
N.~Muennighoff, T.~Wang, L.~Sutawika, A.~Roberts, S.~Biderman, T.~L. Scao,
  M.~S. Bari, S.~Shen, Z.-X. Yong, H.~Schoelkopf \emph{et~al.}, ``Crosslingual
  generalization through multitask finetuning,'' \emph{arXiv preprint
  arXiv:2211.01786}, 2022.

\bibitem{fan2019eli5}
A.~Fan, Y.~Jernite, E.~Perez, D.~Grangier, J.~Weston, and M.~Auli, ``Eli5: Long
  form question answering,'' \emph{arXiv preprint arXiv:1907.09190}, 2019.

\bibitem{lin2021truthfulqa}
S.~Lin, J.~Hilton, and O.~Evans, ``Truthfulqa: Measuring how models mimic human
  falsehoods,'' \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman, ``Glue: A
  multi-task benchmark and analysis platform for natural language
  understanding,'' \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{rajpurkar2016squad}
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang, ``Squad: 100,000+ questions
  for machine comprehension of text,'' \emph{arXiv preprint arXiv:1606.05250},
  2016.

\bibitem{QQP}
S.~Iyer, N.~Dandekar, and K.~Csernai, ``First quora dataset release: Question
  pairs,''
  \url{https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}.

\bibitem{williams2017broad}
A.~Williams, N.~Nangia, and S.~R. Bowman, ``A broad-coverage challenge corpus
  for sentence understanding through inference,'' \emph{arXiv preprint
  arXiv:1704.05426}, 2017.

\bibitem{levesque2012winograd}
H.~Levesque, E.~Davis, and L.~Morgenstern, ``The winograd schema challenge,''
  in \emph{Thirteenth international conference on the principles of knowledge
  representation and reasoning}, 2012.

\bibitem{de2019commitmentbank}
M.-C. De~Marneffe, M.~Simons, and J.~Tonhauser, ``The commitmentbank:
  Investigating projection in naturally occurring discourse,'' in
  \emph{proceedings of Sinn und Bedeutung}, vol.~23, no.~2, 2019, pp. 107--124.

\bibitem{pilehvar2018wic}
M.~T. Pilehvar and J.~Camacho-Collados, ``Wic: 10,000 example pairs for
  evaluating context-sensitive representations,'' \emph{arXiv preprint
  arXiv:1808.09121}, vol.~6, 2018.

\bibitem{bojar2016findings}
O.~Bojar, R.~Chatterjee, C.~Federmann, Y.~Graham, B.~Haddow, M.~Huck, A.~J.
  Yepes, P.~Koehn, V.~Logacheva, C.~Monz \emph{et~al.}, ``Findings of the 2016
  conference on machine translation,'' in \emph{Proceedings of the First
  Conference on Machine Translation: Volume 2, Shared Task Papers}, 2016, pp.
  131--198.

\bibitem{conneau2018xnli}
A.~Conneau, G.~Lample, R.~Rinott, A.~Williams, S.~R. Bowman, H.~Schwenk, and
  V.~Stoyanov, ``Xnli: Evaluating cross-lingual sentence representations,''
  \emph{arXiv preprint arXiv:1809.05053}, 2018.

\bibitem{yang2019paws}
Y.~Yang, Y.~Zhang, C.~Tar, and J.~Baldridge, ``Paws-x: A cross-lingual
  adversarial dataset for paraphrase identification,'' \emph{arXiv preprint
  arXiv:1908.11828}, 2019.

\bibitem{pan2017cross}
X.~Pan, B.~Zhang, J.~May, J.~Nothman, K.~Knight, and H.~Ji, ``Cross-lingual
  name tagging and linking for 282 languages,'' in \emph{Proceedings of the
  55th Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers)}, 2017, pp. 1946--1958.

\bibitem{lewis2019mlqa}
P.~Lewis, B.~O{\u{g}}uz, R.~Rinott, S.~Riedel, and H.~Schwenk, ``Mlqa:
  Evaluating cross-lingual extractive question answering,'' \emph{arXiv
  preprint arXiv:1910.07475}, 2019.

\bibitem{clark2020tydi}
J.~H. Clark, E.~Choi, M.~Collins, D.~Garrette, T.~Kwiatkowski, V.~Nikolaev, and
  J.~Palomaki, ``Tydi qa: A benchmark for information-seeking question
  answering in typologically diverse languages,'' \emph{Transactions of the
  Association for Computational Linguistics}, vol.~8, pp. 454--470, 2020.

\bibitem{li2021ccpm}
W.~Li, F.~Qi, M.~Sun, X.~Yi, and J.~Zhang, ``Ccpm: A chinese classical poetry
  matching dataset,'' \emph{arXiv preprint arXiv:2106.01979}, 2021.

\bibitem{sun2020investigating}
K.~Sun, D.~Yu, D.~Yu, and C.~Cardie, ``Investigating prior knowledge for
  challenging chinese machine reading comprehension,'' \emph{Transactions of
  the Association for Computational Linguistics}, vol.~8, pp. 141--155, 2020.

\bibitem{loic2020findings}
B.~Lo{\"\i}c, B.~Magdalena, B.~Ond{\v{r}}ej, F.~Christian, G.~Yvette, G.~Roman,
  H.~Barry, H.~Matthias, J.~Eric, K.~Tom \emph{et~al.}, ``Findings of the 2020
  conference on machine translation (wmt20),'' in \emph{Proceedings of the
  Fifth Conference on Machine Translation}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics,, 2020, pp. 1--55.

\bibitem{wang2017deep}
Y.~Wang, X.~Liu, and S.~Shi, ``Deep neural solver for math word problems,'' in
  \emph{Proceedings of the 2017 conference on empirical methods in natural
  language processing}, 2017, pp. 845--854.

\bibitem{hu2015lcsts}
B.~Hu, Q.~Chen, and F.~Zhu, ``Lcsts: A large scale chinese short text
  summarization dataset,'' \emph{arXiv preprint arXiv:1506.05865}, 2015.

\bibitem{liu2018lcqmc}
X.~Liu, Q.~Chen, C.~Deng, H.~Zeng, J.~Chen, D.~Li, and B.~Tang, ``Lcqmc: A
  large-scale chinese question matching corpus,'' in \emph{Proceedings of the
  27th international conference on computational linguistics}, 2018, pp.
  1952--1962.

\bibitem{shao2019long}
Z.~Shao, M.~Huang, J.~Wen, W.~Xu, and X.~Zhu, ``Long and diverse text
  generation with planning-based hierarchical variational model,'' \emph{arXiv
  preprint arXiv:1908.06605}, 2019.

\bibitem{yao2021cuge}
Y.~Yao, Q.~Dong, J.~Guan, B.~Cao, Z.~Zhang, C.~Xiao, X.~Wang, F.~Qi, J.~Bao,
  J.~Nie \emph{et~al.}, ``Cuge: A chinese language understanding and generation
  evaluation benchmark,'' \emph{arXiv preprint arXiv:2112.13610}, 2021.

\bibitem{gao2020pile}
L.~Gao, S.~Biderman, S.~Black, L.~Golding, T.~Hoppe, C.~Foster, J.~Phang,
  H.~He, A.~Thite, N.~Nabeshima \emph{et~al.}, ``The pile: An 800gb dataset of
  diverse text for language modeling,'' \emph{arXiv preprint arXiv:2101.00027},
  2020.

\bibitem{nie2019adversarial}
Y.~Nie, A.~Williams, E.~Dinan, M.~Bansal, J.~Weston, and D.~Kiela,
  ``Adversarial nli: A new benchmark for natural language understanding,''
  \emph{arXiv preprint arXiv:1910.14599}, 2019.

\bibitem{clark2018think}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and
  O.~Tafjord, ``Think you have solved question answering? try arc, the ai2
  reasoning challenge,'' \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{vilares2019head}
D.~Vilares and C.~G{\'o}mez-Rodr{\'\i}guez, ``Head-qa: A healthcare dataset for
  complex reasoning,'' \emph{arXiv preprint arXiv:1906.04701}, 2019.

\bibitem{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi, ``Hellaswag: Can a
  machine really finish your sentence?'' \emph{arXiv preprint
  arXiv:1905.07830}, 2019.

\bibitem{paperno2016lambada}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, Q.~N. Pham, R.~Bernardi, S.~Pezzelle,
  M.~Baroni, G.~Boleda, and R.~Fern{\'a}ndez, ``The lambada dataset: Word
  prediction requiring a broad discourse context,'' \emph{arXiv preprint
  arXiv:1606.06031}, 2016.

\bibitem{liu2020logiqa}
J.~Liu, L.~Cui, H.~Liu, D.~Huang, Y.~Wang, and Y.~Zhang, ``Logiqa: A challenge
  dataset for machine reading comprehension with logical reasoning,''
  \emph{arXiv preprint arXiv:2007.08124}, 2020.

\bibitem{mihaylov2018can}
T.~Mihaylov, P.~Clark, T.~Khot, and A.~Sabharwal, ``Can a suit of armor conduct
  electricity? a new dataset for open book question answering,'' \emph{arXiv
  preprint arXiv:1809.02789}, 2018.

\bibitem{bisk2020piqa}
Y.~Bisk, R.~Zellers, J.~Gao, Y.~Choi \emph{et~al.}, ``Piqa: Reasoning about
  physical commonsense in natural language,'' in \emph{Proceedings of the AAAI
  conference on artificial intelligence}, vol.~34, no.~05, 2020, pp.
  7432--7439.

\bibitem{aroca2021prost}
S.~Aroca-Ouellette, C.~Paik, A.~Roncone, and K.~Kann, ``Prost: Physical
  reasoning of objects through space and time,'' \emph{arXiv preprint
  arXiv:2106.03634}, 2021.

\bibitem{penas2013qa4mre}
A.~Pe{\~n}as, E.~Hovy, P.~Forner, {\'A}.~Rodrigo, R.~Sutcliffe, and R.~Morante,
  ``Qa4mre 2011-2013: Overview of question answering for machine reading
  evaluation,'' in \emph{Information Access Evaluation. Multilinguality,
  Multimodality, and Visualization: 4th International Conference of the CLEF
  Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings
  4}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2013, pp. 303--320.

\bibitem{welbl2017crowdsourcing}
J.~Welbl, N.~F. Liu, and M.~Gardner, ``Crowdsourcing multiple choice science
  questions,'' \emph{arXiv preprint arXiv:1707.06209}, 2017.

\bibitem{joshi2017triviaqa}
M.~Joshi, E.~Choi, D.~S. Weld, and L.~Zettlemoyer, ``Triviaqa: A large scale
  distantly supervised challenge dataset for reading comprehension,''
  \emph{arXiv preprint arXiv:1705.03551}, 2017.

\bibitem{sakaguchi2021winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi, ``Winogrande: An
  adversarial winograd schema challenge at scale,'' \emph{Communications of the
  ACM}, vol.~64, no.~9, pp. 99--106, 2021.

\bibitem{hendrycks2021measuring}
D.~Hendrycks, S.~Basart, S.~Kadavath, M.~Mazeika, A.~Arora, E.~Guo, C.~Burns,
  S.~Puranik, H.~He, D.~Song \emph{et~al.}, ``Measuring coding challenge
  competence with apps,'' \emph{arXiv preprint arXiv:2105.09938}, 2021.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{baumgartner2020pushshift}
J.~Baumgartner, S.~Zannettou, B.~Keegan, M.~Squire, and J.~Blackburn, ``The
  pushshift reddit dataset,'' in \emph{Proceedings of the international AAAI
  conference on web and social media}, vol.~14, 2020, pp. 830--839.

\bibitem{mostafazadeh2016corpus}
N.~Mostafazadeh, N.~Chambers, X.~He, D.~Parikh, D.~Batra, L.~Vanderwende,
  P.~Kohli, and J.~Allen, ``A corpus and evaluation framework for deeper
  understanding of commonsense stories,'' \emph{arXiv preprint
  arXiv:1604.01696}, 2016.

\bibitem{dinan2018wizard}
E.~Dinan, S.~Roller, K.~Shuster, A.~Fan, M.~Auli, and J.~Weston, ``Wizard of
  wikipedia: Knowledge-powered conversational agents,'' \emph{arXiv preprint
  arXiv:1811.01241}, 2018.

\bibitem{rashkin2018towards}
H.~Rashkin, E.~M. Smith, M.~Li, and Y.-L. Boureau, ``Towards empathetic
  open-domain conversation models: A new benchmark and dataset,'' \emph{arXiv
  preprint arXiv:1811.00207}, 2018.

\bibitem{dinan2020second}
E.~Dinan, V.~Logacheva, V.~Malykh, A.~Miller, K.~Shuster, J.~Urbanek, D.~Kiela,
  A.~Szlam, I.~Serban, R.~Lowe \emph{et~al.}, ``The second conversational
  intelligence challenge (convai2),'' in \emph{The NeurIPS'18 Competition: From
  Machine Learning to Intelligent Conversations}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2020, pp. 187--208.

\bibitem{smith2020can}
E.~M. Smith, M.~Williamson, K.~Shuster, J.~Weston, and Y.-L. Boureau, ``Can you
  put it all together: Evaluating conversational agents' ability to blend
  skills,'' \emph{arXiv preprint arXiv:2004.08449}, 2020.

\bibitem{komeili2021internet}
M.~Komeili, K.~Shuster, and J.~Weston, ``Internet-augmented dialogue
  generation,'' \emph{arXiv preprint arXiv:2107.07566}, 2021.

\bibitem{mollas2020ethos}
I.~Mollas, Z.~Chrysopoulou, S.~Karlos, and G.~Tsoumakas, ``Ethos: an online
  hate speech detection dataset,'' \emph{arXiv preprint arXiv:2006.08328},
  2020.

\bibitem{nangia2020crows}
N.~Nangia, C.~Vania, R.~Bhalerao, and S.~R. Bowman, ``Crows-pairs: A challenge
  dataset for measuring social biases in masked language models,'' \emph{arXiv
  preprint arXiv:2010.00133}, 2020.

\bibitem{nadeem2020stereoset}
M.~Nadeem, A.~Bethke, and S.~Reddy, ``Stereoset: Measuring stereotypical bias
  in pretrained language models,'' \emph{arXiv preprint arXiv:2004.09456},
  2020.

\bibitem{gehman2020realtoxicityprompts}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith,
  ``Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models,'' \emph{arXiv preprint arXiv:2009.11462}, 2020.

\bibitem{hendrycks2020measuring}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt, ``Measuring massive multitask language understanding,''
  \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{xu2020clue}
L.~Xu, H.~Hu, X.~Zhang, L.~Li, C.~Cao, Y.~Li, Y.~Xu, K.~Sun, D.~Yu, C.~Yu
  \emph{et~al.}, ``Clue: A chinese language understanding evaluation
  benchmark,'' \emph{arXiv preprint arXiv:2004.05986}, 2020.

\bibitem{laurenccon2022bigscience}
H.~Lauren{\c{c}}on, L.~Saulnier, T.~Wang, C.~Akiki, A.~Villanova~del Moral,
  T.~Le~Scao, L.~Von~Werra, C.~Mou, E.~Gonz{\'a}lez~Ponferrada, H.~Nguyen
  \emph{et~al.}, ``The bigscience roots corpus: A 1.6 tb composite multilingual
  dataset,'' \emph{Advances in Neural Information Processing Systems}, vol.~35,
  pp. 31\,809--31\,826, 2022.

\bibitem{hendrycks2021Math}
D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and
  J.~Steinhardt, ``Measuring mathematical problem solving with the math
  dataset,'' \emph{arXiv preprint arXiv:2103.03874}, 2021.

\bibitem{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano \emph{et~al.}, ``Training verifiers to solve
  math word problems,'' \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,
  Y.~Burda, N.~Joseph, G.~Brockman \emph{et~al.}, ``Evaluating large language
  models trained on code,'' \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{li2018character}
Y.~Li, T.~Liu, D.~Li, Q.~Li, J.~Shi, and Y.~Wang, ``Character-based bilstm-crf
  incorporating pos and dictionaries for chinese opinion target extraction,''
  in \emph{Asian Conference on Machine Learning}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2018, pp. 518--533.

\bibitem{li2019chinese}
Z.~Li, N.~Ding, Z.~Liu, H.~Zheng, and Y.~Shen, ``Chinese relation extraction
  with multi-grained information and external linguistic knowledge,'' in
  \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019, pp. 4377--4386.

\bibitem{xu2017discourse}
J.~Xu, J.~Wen, X.~Sun, and Q.~Su, ``A discourse-level named entity recognition
  and relation extraction dataset for chinese literature text,'' \emph{arXiv
  preprint arXiv:1711.07010}, 2017.

\bibitem{chen2018bq}
J.~Chen, Q.~Chen, X.~Liu, H.~Yang, D.~Lu, and B.~Tang, ``The bq corpus: A
  large-scale domain-specific chinese corpus for sentence semantic equivalence
  identification,'' in \emph{Proceedings of the 2018 conference on empirical
  methods in natural language processing}, 2018, pp. 4946--4951.

\bibitem{co2019iflytek}
L.~CO, ``Iflytek: a multiple categories chinese text classifier. competition
  official website,'' 2019.

\bibitem{liu2018matching}
B.~Liu, D.~Niu, H.~Wei, J.~Lin, Y.~He, K.~Lai, and Y.~Xu, ``Matching article
  pairs with graphical decomposition and convolutions,'' \emph{arXiv preprint
  arXiv:1802.07459}, 2018.

\bibitem{zhang2017chinese}
S.~Zhang, X.~Zhang, H.~Wang, J.~Cheng, P.~Li, and Z.~Ding, ``Chinese medical
  question answer matching using end-to-end character-level multi-scale cnns,''
  \emph{Applied Sciences}, vol.~7, no.~8, p. 767, 2017.

\bibitem{zhang2018multi}
S.~Zhang, X.~Zhang, H.~Wang, L.~Guo, and S.~Liu, ``Multi-scale attentive
  interaction networks for chinese medical question answer selection,''
  \emph{IEEE Access}, vol.~6, pp. 74\,061--74\,071, 2018.

\bibitem{li2016dataset}
P.~Li, W.~Li, Z.~He, X.~Wang, Y.~Cao, J.~Zhou, and W.~Xu, ``Dataset and neural
  recurrent sequence labeling model for open-domain factoid question
  answering,'' \emph{arXiv preprint arXiv:1607.06275}, 2016.

\bibitem{chang2022webqa}
Y.~Chang, M.~Narang, H.~Suzuki, G.~Cao, J.~Gao, and Y.~Bisk, ``Webqa: Multihop
  and multimodal qa,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2022, pp. 16\,495--16\,504.

\bibitem{peng2015named}
N.~Peng and M.~Dredze, ``Named entity recognition for chinese social media with
  jointly trained embeddings,'' in \emph{Proceedings of the 2015 conference on
  empirical methods in natural language processing}, 2015, pp. 548--554.

\bibitem{weischedel2011ontonotes}
R.~Weischedel, S.~Pradhan, L.~Ramshaw, M.~Palmer, N.~Xue, M.~Marcus, A.~Taylor,
  C.~Greenberg, E.~Hovy, R.~Belvin \emph{et~al.}, ``Ontonotes release 4.0,''
  \emph{LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium}, 2011.

\bibitem{cui2018span}
Y.~Cui, T.~Liu, W.~Che, L.~Xiao, Z.~Chen, W.~Ma, S.~Wang, and G.~Hu, ``A
  span-extraction dataset for chinese machine reading comprehension,''
  \emph{arXiv preprint arXiv:1810.07366}, 2018.

\bibitem{cui2020sentence}
Y.~Cui, T.~Liu, Z.~Yang, Z.~Chen, W.~Ma, W.~Che, S.~Wang, and G.~Hu, ``A
  sentence cloze dataset for chinese machine reading comprehension,''
  \emph{arXiv preprint arXiv:2004.03116}, 2020.

\bibitem{shao2018drcd}
C.~C. Shao, T.~Liu, Y.~Lai, Y.~Tseng, and S.~Tsai, ``Drcd: A chinese machine
  reading comprehension dataset,'' \emph{arXiv preprint arXiv:1806.00920},
  2018.

\bibitem{he2017dureader}
W.~He, K.~Liu, J.~Liu, Y.~Lyu, S.~Zhao, X.~Xiao, Y.~Liu, Y.~Wang, H.~Wu, Q.~She
  \emph{et~al.}, ``Dureader: a chinese machine reading comprehension dataset
  from real-world applications,'' \emph{arXiv preprint arXiv:1711.05073}, 2017.

\bibitem{tang2020dureaderrobust}
H.~Tang, J.~Liu, H.~Li, Y.~Hong, H.~Wu, and H.~Wang, ``Dureaderrobust: A
  chinese dataset towards evaluating the robustness of machine reading
  comprehension models,'' \emph{arXiv preprint arXiv:2004.11142}, 2020.

\bibitem{zheng2019chid}
C.~Zheng, M.~Huang, and A.~Sun, ``Chid: A large-scale chinese idiom dataset for
  cloze test,'' \emph{arXiv preprint arXiv:1906.01265}, 2019.

\bibitem{xiao2018cail2018}
C.~Xiao, H.~Zhong, Z.~Guo, C.~Tu, Z.~Liu, M.~Sun, Y.~Feng, X.~Han, Z.~Hu,
  H.~Wang \emph{et~al.}, ``Cail2018: A large-scale legal dataset for judgment
  prediction,'' \emph{arXiv preprint arXiv:1807.02478}, 2018.

\bibitem{xu2021blow}
C.~Xu, W.~Zhou, T.~Ge, K.~Xu, J.~McAuley, and F.~Wei, ``Blow the dog whistle: A
  chinese dataset for cant understanding with common sense and world
  knowledge,'' \emph{arXiv preprint arXiv:2104.02704}, 2021.

\bibitem{xiong2017end}
C.~Xiong, Z.~Dai, J.~Callan, Z.~Liu, and R.~Power, ``End-to-end neural ad-hoc
  ranking with kernel pooling,'' in \emph{Proceedings of the 40th International
  ACM SIGIR conference on research and development in information retrieval},
  2017, pp. 55--64.

\bibitem{xu2020matinf}
C.~Xu, J.~Pei, H.~Wu, Y.~Liu, and C.~Li, ``Matinf: A jointly labeled
  large-scale dataset for classification, question answering and
  summarization,'' \emph{arXiv preprint arXiv:2004.12302}, 2020.

\bibitem{zhou2020kdconv}
H.~Zhou, C.~Zheng, K.~Huang, M.~Huang, and X.~Zhu, ``Kdconv: A chinese
  multi-domain dialogue dataset towards multi-turn knowledge-driven
  conversation,'' \emph{arXiv preprint arXiv:2004.04100}, 2020.

\bibitem{clark2019boolq}
C.~Clark, K.~Lee, M.-W. Chang, T.~Kwiatkowski, M.~Collins, and K.~Toutanova,
  ``Boolq: Exploring the surprising difficulty of natural yes/no questions,''
  \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem{lai2017race}
G.~Lai, Q.~Xie, H.~Liu, Y.~Yang, and E.~Hovy, ``Race: Large-scale reading
  comprehension dataset from examinations,'' \emph{arXiv preprint
  arXiv:1704.04683}, 2017.

\bibitem{dagan2005pascal}
I.~Dagan, O.~Glickman, and B.~Magnini, ``The pascal recognising textual
  entailment challenge,'' in \emph{Machine learning challenges workshop}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2005, pp. 177--190.

\bibitem{lim2019korquad1}
S.~Lim, M.~Kim, and J.~Lee, ``Korquad1. 0: Korean qa dataset for machine
  reading comprehension,'' \emph{arXiv preprint arXiv:1909.07005}, 2019.

\bibitem{park2021klue}
S.~Park, J.~Moon, S.~Kim, W.~I. Cho, J.~Han, J.~Park, C.~Song, J.~Kim, Y.~Song,
  T.~Oh \emph{et~al.}, ``Klue: Korean language understanding evaluation,''
  \emph{arXiv preprint arXiv:2105.09680}, 2021.

\bibitem{xu2021fewclue}
L.~Xu, X.~Lu, C.~Yuan, X.~Zhang, H.~Xu, H.~Yuan, G.~Wei, X.~Pan, X.~Tian,
  L.~Qin \emph{et~al.}, ``Fewclue: A chinese few-shot learning evaluation
  benchmark,'' \emph{arXiv preprint arXiv:2107.07498}, 2021.

\bibitem{merity2016pointer}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher, ``Pointer sentinel mixture
  models,'' \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{rae2019compressive}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, and T.~P. Lillicrap, ``Compressive
  transformers for long-range sequence modelling,'' \emph{arXiv preprint
  arXiv:1911.05507}, 2019.

\bibitem{srivastava2022beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R.
  Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso \emph{et~al.}, ``Beyond the
  imitation game: Quantifying and extrapolating the capabilities of language
  models,'' \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{kwiatkowski2019natural}
T.~Kwiatkowski, J.~Palomaki, O.~Redfield, M.~Collins, A.~Parikh, C.~Alberti,
  D.~Epstein, I.~Polosukhin, J.~Devlin, K.~Lee \emph{et~al.}, ``Natural
  questions: a benchmark for question answering research,'' \emph{Transactions
  of the Association for Computational Linguistics}, vol.~7, pp. 453--466,
  2019.

\bibitem{thorne2018fever}
J.~Thorne, A.~Vlachos, C.~Christodoulopoulos, and A.~Mittal, ``Fever: a
  large-scale dataset for fact extraction and verification,'' \emph{arXiv
  preprint arXiv:1803.05355}, 2018.

\bibitem{augenstein2019multifc}
I.~Augenstein, C.~Lioma, D.~Wang, L.~C. Lima, C.~Hansen, C.~Hansen, and J.~G.
  Simonsen, ``Multifc: A real-world multi-domain dataset for evidence-based
  fact checking of claims,'' \emph{arXiv preprint arXiv:1909.03242}, 2019.

\bibitem{sap2019socialiqa}
M.~Sap, H.~Rashkin, D.~Chen, R.~LeBras, and Y.~Choi, ``Socialiqa: Commonsense
  reasoning about social interactions,'' \emph{arXiv preprint
  arXiv:1904.09728}, 2019.

\bibitem{blodgett2016demographic}
S.~L. Blodgett, L.~Green, and B.~O'Connor, ``Demographic dialectal variation in
  social media: A case study of african-american english,'' \emph{arXiv
  preprint arXiv:1608.08868}, 2016.

\bibitem{borkan2019nuanced}
D.~Borkan, L.~Dixon, J.~Sorensen, N.~Thain, and L.~Vasserman, ``Nuanced metrics
  for measuring unintended bias with real data for text classification,'' in
  \emph{Companion proceedings of the 2019 world wide web conference}, 2019, pp.
  491--500.

\bibitem{rajpurkar2018know}
P.~Rajpurkar, R.~Jia, and P.~Liang, ``Know what you don't know: Unanswerable
  questions for squad,'' \emph{arXiv preprint arXiv:1806.03822}, 2018.

\bibitem{dua2019drop}
D.~Dua, Y.~Wang, P.~Dasigi, G.~Stanovsky, S.~Singh, and M.~Gardner, ``Drop: A
  reading comprehension benchmark requiring discrete reasoning over
  paragraphs,'' \emph{arXiv preprint arXiv:1903.00161}, 2019.

\bibitem{choi2018quac}
E.~Choi, H.~He, M.~Iyyer, M.~Yatskar, W.-t. Yih, Y.~Choi, P.~Liang, and
  L.~Zettlemoyer, ``Quac: Question answering in context,'' \emph{arXiv preprint
  arXiv:1808.07036}, 2018.

\bibitem{reddy2019coqa}
S.~Reddy, D.~Chen, and C.~D. Manning, ``Coqa: A conversational question
  answering challenge,'' \emph{Transactions of the Association for
  Computational Linguistics}, vol.~7, pp. 249--266, 2019.

\bibitem{roemmele2011choice}
M.~Roemmele, C.~A. Bejan, and A.~S. Gordon, ``Choice of plausible alternatives:
  An evaluation of commonsense causal reasoning.'' in \emph{AAAI spring
  symposium: logical formalizations of commonsense reasoning}, 2011, pp.
  90--95.

\bibitem{khashabi2018looking}
D.~Khashabi, S.~Chaturvedi, M.~Roth, S.~Upadhyay, and D.~Roth, ``Looking beyond
  the surface: A challenge set for reading comprehension over multiple
  sentences,'' in \emph{Proceedings of the 2018 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, Volume 1 (Long Papers)}, 2018, pp. 252--262.

\bibitem{zhang2018record}
S.~Zhang, X.~Liu, J.~Liu, J.~Gao, K.~Duh, and B.~Van~Durme, ``Record: Bridging
  the gap between human and machine commonsense reading comprehension,''
  \emph{arXiv preprint arXiv:1810.12885}, 2018.

\bibitem{trinh2018simple}
T.~H. Trinh and Q.~V. Le, ``A simple method for commonsense reasoning,''
  \emph{arXiv preprint arXiv:1806.02847}, 2018.

\bibitem{zellers2019defending}
R.~Zellers, A.~Holtzman, H.~Rashkin, Y.~Bisk, A.~Farhadi, F.~Roesner, and
  Y.~Choi, ``Defending against neural fake news,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{mccoy2019right}
R.~T. McCoy, E.~Pavlick, and T.~Linzen, ``Right for the wrong reasons:
  Diagnosing syntactic heuristics in natural language inference,'' \emph{arXiv
  preprint arXiv:1902.01007}, 2019.

\bibitem{codeforce}
M.~Mirzayanov, ``Codeforces: Results of 2020,''
  \url{https://codeforces.com/blog/entry/89502}.

\bibitem{Caballero_Description2Code_Dataset_2016}
\BIBentryALTinterwordspacing
E.~Caballero, .~OpenAI, and I.~Sutskever, ``{Description2Code Dataset},'' 8
  2016. [Online]. Available:
  \url{https://github.com/ethancaballero/description2code}
\BIBentrySTDinterwordspacing

\bibitem{puri2021codenet}
R.~Puri, D.~S. Kung, G.~Janssen, W.~Zhang, G.~Domeniconi, V.~Zolotov, J.~Dolby,
  J.~Chen, M.~Choudhury, L.~Decker \emph{et~al.}, ``Codenet: A large-scale ai
  for code dataset for learning a diversity of coding tasks,'' \emph{arXiv
  preprint arXiv:2105.12655}, 2021.

\bibitem{berant2013semantic}
J.~Berant, A.~Chou, R.~Frostig, and P.~Liang, ``Semantic parsing on freebase
  from question-answer pairs,'' in \emph{Proceedings of the 2013 conference on
  empirical methods in natural language processing}, 2013, pp. 1533--1544.

\bibitem{geva2021did}
M.~Geva, D.~Khashabi, E.~Segal, T.~Khot, D.~Roth, and J.~Berant, ``Did
  aristotle use a laptop? a question answering benchmark with implicit
  reasoning strategies,'' \emph{Transactions of the Association for
  Computational Linguistics}, vol.~9, pp. 346--361, 2021.

\bibitem{talmor2018commonsenseqa}
A.~Talmor, J.~Herzig, N.~Lourie, and J.~Berant, ``Commonsenseqa: A question
  answering challenge targeting commonsense knowledge,'' \emph{arXiv preprint
  arXiv:1811.00937}, 2018.

\bibitem{patel2021nlp}
A.~Patel, S.~Bhattamishra, and N.~Goyal, ``Are nlp models really able to solve
  simple math word problems?'' \emph{arXiv preprint arXiv:2103.07191}, 2021.

\bibitem{koncel2016mawps}
R.~Koncel-Kedziorski, S.~Roy, A.~Amini, N.~Kushman, and H.~Hajishirzi, ``Mawps:
  A math word problem repository,'' in \emph{Proceedings of the 2016 conference
  of the north american chapter of the association for computational
  linguistics: human language technologies}, 2016, pp. 1152--1157.

\bibitem{ling2017program}
W.~Ling, D.~Yogatama, C.~Dyer, and P.~Blunsom, ``Program induction by rationale
  generation: Learning to solve and explain algebraic word problems,''
  \emph{arXiv preprint arXiv:1705.04146}, 2017.

\bibitem{scialom2020mlsum}
T.~Scialom, P.-A. Dray, S.~Lamprier, B.~Piwowarski, and J.~Staiano, ``Mlsum:
  The multilingual summarization corpus,'' \emph{arXiv preprint
  arXiv:2004.14900}, 2020.

\bibitem{narayan1808don}
S.~Narayan, S.~B. Cohen, and M.~Lapata, ``Don’t give me the details, just the
  summary!'' \emph{Topic-Aware Convolutional Neural Networks for Extreme
  Summarization. ArXiv, abs}, 1808.

\bibitem{novikova2017e2e}
J.~Novikova, O.~Du{\v{s}}ek, and V.~Rieser, ``The e2e dataset: New challenges
  for end-to-end generation,'' \emph{arXiv preprint arXiv:1706.09254}, 2017.

\bibitem{ferreira20202020}
T.~C. Ferreira, C.~Gardent, N.~Ilinykh, C.~Van Der~Lee, S.~Mille,
  D.~Moussallem, and A.~Shimorina, ``The 2020 bilingual, bi-directional webnlg+
  shared task overview and evaluation results (webnlg+ 2020),'' in
  \emph{Proceedings of the 3rd International Workshop on Natural Language
  Generation from the Semantic Web (WebNLG+)}, 2020.

\bibitem{goyal2022flores}
N.~Goyal, C.~Gao, V.~Chaudhary, P.-J. Chen, G.~Wenzek, D.~Ju, S.~Krishnan,
  M.~Ranzato, F.~Guzm{\'a}n, and A.~Fan, ``The flores-101 evaluation benchmark
  for low-resource and multilingual machine translation,'' \emph{Transactions
  of the Association for Computational Linguistics}, vol.~10, pp. 522--538,
  2022.

\bibitem{xia2019microsoft}
Y.~Xia, X.~Tan, F.~Tian, F.~Gao, W.~Chen, Y.~Fan, L.~Gong, Y.~Leng, R.~Luo,
  Y.~Wang \emph{et~al.}, ``Microsoft research asia's systems for wmt19,''
  \emph{arXiv preprint arXiv:1911.06191}, 2019.

\bibitem{ponti2020xcopa}
E.~M. Ponti, G.~Glava{\v{s}}, O.~Majewska, Q.~Liu, I.~Vuli{\'c}, and
  A.~Korhonen, ``Xcopa: A multilingual dataset for causal commonsense
  reasoning,'' \emph{arXiv preprint arXiv:2005.00333}, 2020.

\bibitem{tikhonov2021s}
A.~Tikhonov and M.~Ryabinin, ``It's all in the heads: Using attention heads as
  a baseline for cross-lingual transfer in commonsense reasoning,'' \emph{arXiv
  preprint arXiv:2106.12066}, 2021.

\bibitem{roy2016solving}
S.~Roy and D.~Roth, ``Solving general arithmetic word problems,'' \emph{arXiv
  preprint arXiv:1608.01413}, 2016.

\bibitem{menick2022teaching}
J.~Menick, M.~Trebacz, V.~Mikulik, J.~Aslanides, F.~Song, M.~Chadwick,
  M.~Glaese, S.~Young, L.~Campbell-Gillingham, G.~Irving \emph{et~al.},
  ``Teaching language models to support answers with verified quotes,''
  \emph{arXiv preprint arXiv:2203.11147}, 2022.

\bibitem{rudinger2018gender}
R.~Rudinger, J.~Naradowsky, B.~Leonard, and B.~Van~Durme, ``Gender bias in
  coreference resolution,'' \emph{arXiv preprint arXiv:1804.09301}, 2018.

\bibitem{zhao2018gender}
J.~Zhao, T.~Wang, M.~Yatskar, V.~Ordonez, and K.-W. Chang, ``Gender bias in
  coreference resolution: Evaluation and debiasing methods,'' \emph{arXiv
  preprint arXiv:1804.06876}, 2018.

\bibitem{parrish2021bbq}
A.~Parrish, A.~Chen, N.~Nangia, V.~Padmakumar, J.~Phang, J.~Thompson, P.~M.
  Htut, and S.~R. Bowman, ``Bbq: A hand-built bias benchmark for question
  answering,'' \emph{arXiv preprint arXiv:2110.08193}, 2021.

\bibitem{boyd2012besting}
J.~Boyd-Graber, B.~Satinoff, H.~He, and H.~Daum{\'e}~III, ``Besting the quiz
  master: Crowdsourcing incremental classification games,'' in
  \emph{Proceedings of the 2012 joint conference on empirical methods in
  natural language processing and computational natural language learning},
  2012, pp. 1290--1301.

\bibitem{shi2022language}
F.~Shi, M.~Suzgun, M.~Freitag, X.~Wang, S.~Srivats, S.~Vosoughi, H.~W. Chung,
  Y.~Tay, S.~Ruder, D.~Zhou \emph{et~al.}, ``Language models are multilingual
  chain-of-thought reasoners,'' \emph{arXiv preprint arXiv:2210.03057}, 2022.

\bibitem{austin2021program}
J.~Austin, A.~Odena, M.~Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang,
  C.~Cai, M.~Terry, Q.~Le \emph{et~al.}, ``Program synthesis with large
  language models,'' \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{wang2022benchmarking}
Y.~Wang, S.~Mishra, P.~Alipoormolabashi, Y.~Kordi, A.~Mirzaei, A.~Arunkumar,
  A.~Ashok, A.~S. Dhanasekaran, A.~Naik, D.~Stap \emph{et~al.}, ``Benchmarking
  generalization via in-context instructions on 1,600+ language tasks,''
  \emph{arXiv preprint arXiv:2204.07705}, 2022.

\bibitem{xie2022unifiedskg}
T.~Xie, C.~H. Wu, P.~Shi, R.~Zhong, T.~Scholak, M.~Yasunaga, C.-S. Wu,
  M.~Zhong, P.~Yin, S.~I. Wang \emph{et~al.}, ``Unifiedskg: Unifying and
  multi-tasking structured knowledge grounding with text-to-text language
  models,'' \emph{arXiv preprint arXiv:2201.05966}, 2022.

\bibitem{ye2021crossfit}
Q.~Ye, B.~Y. Lin, and X.~Ren, ``Crossfit: A few-shot learning challenge for
  cross-task generalization in nlp,'' \emph{arXiv preprint arXiv:2104.08835},
  2021.

\bibitem{aribandi2021ext5}
V.~Aribandi, Y.~Tay, T.~Schuster, J.~Rao, H.~S. Zheng, S.~V. Mehta, H.~Zhuang,
  V.~Q. Tran, D.~Bahri, J.~Ni \emph{et~al.}, ``Ext5: Towards extreme multi-task
  scaling for transfer learning,'' \emph{arXiv preprint arXiv:2111.10952},
  2021.

\bibitem{alex2021raft}
N.~Alex, E.~Lifland, L.~Tunstall, A.~Thakur, P.~Maham, C.~J. Riedel, E.~Hine,
  C.~Ashurst, P.~Sedille, A.~Carlier \emph{et~al.}, ``Raft: A real-world
  few-shot text classification benchmark,'' \emph{arXiv preprint
  arXiv:2109.14076}, 2021.

\bibitem{Mixed_Precision}
P.~Micikevicius, S.~Narang, J.~Alben, G.~Diamos, E.~Elsen, D.~Garcia,
  B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh \emph{et~al.}, ``Mixed
  precision training,'' \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{instructgpt}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray \emph{et~al.}, ``Training language models to
  follow instructions with human feedback,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~35, pp. 27\,730--27\,744, 2022.

\end{thebibliography}
