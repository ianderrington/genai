\subsection{Models assigned to Shi}
\subsubsection{LaMDA~\cite{thoppilan2022lamda}}
A family of Transformer-based neural language models for dialog ranging from 2B to 137B parameters. The model architecture of LaMDA follows a decoder-only Transformer~\cite{Transformers} language model, containing 64 layers, relative attention~\cite{T5}, and gated-GELU activation~\cite{shazeer2020glu}. To predict the next token in a text corpus, LaMDA is pre-trained on a total of 1.56T words, covering 1.12B public dialog data, 13.39B public dialog utterances, and 2.97B public web documents.
Particularly, more than 90\% of the pre-training data is in English. The pre-training of LaMDA consumes 1024 TPU v3 chips for about 57.7 days, where each batch of data consists of 256K tokens. LaMDA can be used as a general language model to perform multiple tasks, since it generates potential responses with key objectives of quality, safety, and groundedness. Moreover, discriminative and generative fine-tuning techniques can be applied to improve safety and quality for LaMDA; and LaMDA can be fine-tuned to learn to call different external information resources and tools.
\subsubsection{Jurassic-1~\cite{lieber2021jurassic}}
A pair of auto-regressive language models including a 7B-parameter J1-Large model and a 178B-parameter J1-Jumbo model. The main differences from the pioneering GPT-3~\cite{GPT-3} are about the size of vocabulary (50K for GPT-3, 256K for Jurassic-1), the number of layers in the neural net (\emph{e.g.}, 96 layers in the GPT-3 175B model, 76 layers in the J1-Jumbo model), the number of hidden dimensions in Transformer layer~\cite{Transformers} (\emph{e.g.}, 12288 hidden dimensions in the Transformer layer of the GPT-3 175B model, 13824 hidden dimensions in the Transformer layer of the J1-Jumbo model), and the number of dimensions in attention head~\cite{Transformers} (\emph{e.g.}, 128 dimensions in the attention head of the GPT-3 175B model, 144 dimensions in the attention head of the J1-Jumbo model). In particular, Jurassic-1 trains a T5~\cite{T5}-like SentencePiece (SP) tokenizer~\cite{kudo2018sentencepiece} with the 256K vocabulary items comprising word pieces, complete words, and
multi-word expressions without any word boundaries, while possible out-of-vocabulary instances are interpreted as unicode bytes. In practice, a combination of data, model, and pipeline parallelism strategies proposed in~\cite{rajbhandari2020zero} are adopted to train the Jurassic-1 models; and 300B tokens collected from public available resources (\emph{e.g.}, arXiv, Github, StackExchange, Youtube Subtitles) are formed in the GPT-3's data structure to train the Jurassic-1 models following the conventional self-supervised auto-regressive training objective. Compared to the GPT-3 counterparts, the Jurassic-1 models apply a more efficient architecture and tokenizer for a faster prediction based on broader resources, achieving a comparable performance in zero-shot learning tasks and a superior performance in zero-shot learning tasks given the ability of feeding more examples as a prompt.
\subsubsection{LLaMA~\cite{touvron2023llama}}
A set of foundation language models varying from 7B to 65B parameters. The overall architecture of LLaMA follows the Transformer~\cite{Transformers}, while a few subsequently proposed improvements are incorporated for better performances. To be specific, the RMSNorm normalizing function~\cite{rmsnorm} is adopted as the pre-normalization technique used in GPT-3~\cite{GPT-3}; the ReLU non-linearity is replaced by the SwiGLU activation function~\cite{shazeer2020glu}, which is suggest in PaLM~\cite{chowdhery2022palm}; and following the operations used in GPTNeo~\cite{black2022gpt}, the rotary positional embeddings (RoPE)~\cite{su2021roformer} are used as a substitute for the absolute positional embeddings. The smallest LLaMA model, LLaMA 7B, has 4096 hidden dimensions, 32 attention heads, and 32 layers; while the largest LLaMA model, LLaMA 65B, has 8192 hidden dimensions, 64 attention heads, and 80 layers. About 67\% of LLaMA's pre-training data is collected from English CommonCrawl following the CCNet method~\cite{wenzek2019ccnet}, the T5's C4 dataset~\cite{T5} contributes to about 15\% of the pre-training data, and the rest is collected from Github, Wikipedia, Books, arXiv, StackExchange. Practically, the pre-training data is tokenized with the bytepair encoding (BPE) algorithm~\cite{sennrich2015neural} implemented as in~\cite{kudo2018sentencepiece}. A total of 1.4T tokens are obtained to train the LLaMA models, where the smaller LLaMA 7B, 13B models are trained with 1.0T tokens and the larger LLaMA 33B, 65B models are trained with 1.4T tokens. All models are trained with the same batch size of 4M tokens, AdamW optimizer~\cite{loshchilov2017decoupled} with the hyper-parameters $\beta_{1}=0.9$ and $\beta_{2}=0.95$, a cosine learning rate schedule, 2000 warmup steps, and varying learning rates. LLaMA models demonstrate that smaller models can also realize good performances using more training data and time. Currently, LLaMA and the associated variants are widely used for parameter-efficient tuning, especially for instruction following tasks.
\subsubsection{Yuan 1.0~\cite{wu2021yuan}}
A large singleton language model with 245B parameters. The Yuan 1.0 is structured as a Transformer~\cite{Transformers}, where the sequence length, the hidden dimensions, the global batch size and the micro batch size in a node are scaled up for better performances. To accelerate the large-scale distributed training of Yuan 1.0, three parallel strategies are incorporated, including tensor parallelism, pipeline parallelism and data parallelism~\cite{shoeybi2019megatron}. A Chinese corpus with 5TB high-quality text is created to train Yuan 1.0 model, where the raw data is about 850TB collected from Internet resources like Common Crawl, Sougou News, Baidu Baike, Wikipedia. Particularly, a Massive Data Filtering System (MDFS) built on Spark is developed to process the raw data via coarse and fine filtering techniques. In practice, the Yuan 245B model takes 180B tokens and 2128 GPUs for training, and the performances are evaluated on the FewCLUE and ZeroCLUE datasets~\cite{xu2020clue} including text classification, Winograd Schema, natural language inference, and reading comprehension tasks.
\subsubsection{WebGPT~\cite{nakano2021webgpt}}
A set of fine-tuned GPT-3~\cite{GPT-3} models that can answer long-form questions in a text-based web-browsing environment. Since the task can be operated by humans to directly optimize answer quality, the models can be trained by imitation learning and answer quality is further improved with human feedback~\cite{stiennon2020learning}. During browsing, the models must collect references to support their answers, in order to exploit easier human evaluation. In addition to regular training data such as the answers and questions in the ELI5 dataset~\cite{fan2019eli5}, two more types of data, \emph{demonstrations} of humans and \emph{comparisons} between two model-generated answers, are additionally collected. By using human labelers' feedback on whether the retrieved information is useful to answer the given inputs, the data is then tested with four usages including behavior cloning, reward modeling, reinforcement learning, and rejection sampling. Practically, the combination of behavior cloning and rejection sampling contribute to the best performing model containing 175B parameters. To evaluate the performance of WebGPT model, the model's answers are compared with the human demonstrators' written ones, the highest-voted answers in ELI5~\cite{fan2019eli5}, and the adversarial short-form questions and answers in the TruthfulQA~\cite{lin2021truthfulqa} dataset.
\subsubsection{GLaM~\cite{du2022glam}}
Generalist Language Model (GLaM) represents a family of language models using a sparsely activated mixture-of-experts (MoE) structure~\cite{shazeer2017outrageously,fedus2022switch}. Specifically, the architecture of GLaM is derived from a Decoder-only Transformer~\cite{Transformers}, while the feed-forward component of each Transformer layer is replaced with an MoE module consisting of a set of independent feed-forward networks (\emph{i.e.}, the `experts'). To gain more model capacity while reducing computation, the experts are sparsely activated where only the best two experts are used to process each input token. Compared to original Transformer architecture~\cite{Transformers}, GLaM also utilizes per-layer relative positional bias~\cite{dai2019transformer} and the Gated Linear Unit~\cite{shazeer2020glu} to replace the original positional embedding and an activated linear layer, respectively. The largest GLaM model, GLaM (64B/64E), has 1.2T parameters (about 7 times larger than GPT-3~\cite{GPT-3}), while 96.6B parameters are activated per input token. The training data of GLaM comes from Webpages, Wikipedia, News, Forums, \emph{etc.}, contributing to a high-quality dataset of 1.6T tokens. To effectively compare with GPT-3, the evaluation of GLaM follows the similar zero, one and few-shot learning protocols as in GPT-3, where 8 natural language generative (NLG) and 21 natural language understanding (NLU) tasks are evaluated. Particularly, the largest GLaM (64B/64E) model achieves better overall results while consuming only one third of GPT-3's training energy.
\subsubsection{AlphaCode~\cite{li2022competition}}
A set of large language models designed for competition-level code generation task. Basically, the AlphaCode models follow an encoder-decoder transformer architecture~\cite{Transformers} ranging from 300M to 41B parameters. For higher effectiveness and efficiency, the architecture is asymmetric as the encoder is shallower than the decoder. Moreover, the multi-query attention~\cite{shazeer2019fast} is applied to reduce memory and cache cost, and both the encoder and the decoder use SentencePiece~\cite{kudo2018sentencepiece} as tokennizer. Since competitive programming problems highly require deep reasoning and understanding of complex natural language algorithms, the AlphaCode models are pre-trained on a total of 715.1GB filtered GitHub code in popular languages, and then fine-tuned on a new competitive programming dataset named CodeContests. The largest AlphaCode 41B model takes about 967B tokens for training. Particularly, the CodeContests dataset mainly contains problems, solutions and test cases collected from the Codeforces platform\footnote{\url{https://codeforces.com/}}, while some public competitive programming data like Description2Code~\cite{Caballero_Description2Code_Dataset_2016} and CodeNet~\cite{puri2021codenet} is also utilized. In practice, standard language modelling objectives are used for the pre-training on Github code data, while GOLD~\cite{pang2020text} with tempering~\cite{dabre2020softmax} serve as the training objective for the fine-tuning on CodeContests data. To evaluate the performance of AlphaCode, simulated programming competitions are hosted on the Codeforces platform: overall, AlphaCode ranks at the top 54.3\% among over 5000 competitors, where its Codeforces rating is within the top 28\% of recently participated users.
\subsubsection{AlexaTM~\cite{soltan2022alexatm}}
A 20B parameter multilingual sequence-to-sequence (seq2seq) model. The AlexaTM 20B model applies a 46-layer encoder and a 32-layer decoder to form a standard Transformer model architecture, while the layernorms are positioned at the beginning of each positional embedding layer in order to improve the stability of training large-scale models. The pre-training data is collected from Wikipedia (119B tokens) and mC4 dataset~\cite{mT5} (1.2T tokens) in 12 languages. To enable the AlexaTM 20B model to perform on both spoken and written cases, all data is converted into spoken format via a written to spoken formatter, and input data is tokenized by SentencePiece~\cite{kudo2018sentencepiece}. For the model training, a denoising objective together with 15\% of the input tokens being dropped are used to make model reconstruct the input. Moreover, an extra Causal Language Modeling (CLM) task to the 20B training for one fifth of the time is added, benefiting the model with a more efficient in-context learning. For a faster training, we initialized the encoder is initialized as a 10B pre-trained encoder used in~\cite{fitzgerald2022alexa}. To fine-tune the AlexaTM 20B model on generation tasks comparing with other LLMs, a special CLM token is attached to the beginning of the input sentence. Given comprehensive evaluations, the AlexaTM 20B model achieves better performances on 1-shot summarization tasks compared to a 540B PaLM decoder-only model~\cite{chowdhery2022palm}, and zero-shot settings on the SuperGLUE~\cite{wang2019superglue} and SQuADv2~\cite{rajpurkar2018know} datasets compared to the GPT3 175B model~\cite{GPT-3}. For the 1-shot machine translation task across almost all languages on Flores-101 dataset~\cite{goyal2022flores} and the multilingual tasks such as XNLI~\cite{conneau2018xnli}, XCOPA~\cite{ponti2020xcopa}, Paws-X~\cite{yang2019paws}, and XWinograd~\cite{tikhonov2021s}, the AlexaTM 20B model achieves state-of-the-art results.
\subsubsection{Sparrow~\cite{glaese2022improving}}
An information-seeking dialogue agent trained to gain more helpfulness and correctness with fewer harms. Basically, reinforcement learning from multi-objective human feedback is leveraged to train the Sparrow models, in order to maximize preference rates and minimize rule violations. Two additions are proposed to help human raters judge agent behaviour: the first addition is the specific natural language rules that need raters to rate separately; and the second addition is to make the agent show proof from sources that support factual claims when collecting opinions about the model's statements. The architecture of the Sparrow models is based on Dialogue Prompted Chinchila 70B~\cite{chinchilla}. Particularly, human data is collected for rule violations and per-turn response preferences, which mainly aims to train preference reward models (preference RMs) and a rule reward model (rule RM). In practice, reinforcement learning with advantage actor-critic (A2C)~\cite{mnih2016asynchronous} is used to train the initialized Chinchila model; the rule RM estimated rule violation rate and the preference RMs estimated per-turn response preferences are jointly optimized. Given experimental data, Sparrow's evidence can support the sampled response for factual questions 78\% of the time. Moreover, Sparrow is highly resistant to human adversarial probing, since it only violates the defined rules 8\% of the time when probed.



\\

\textbf{TABLES BACKUP}
\begin{table*}[!tbhp]
\caption{An example table of merging datasets and findings for each model. (To be updated.)}
\begin{tabular}{lcccl}
\hline \hline
Model & \begin{tabular}[c]{@{}c@{}}Training Dataset \end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation Dataset\end{tabular}  \\ \hline \hline
\textbf{LaMDA}~\cite{thoppilan2022lamda} & Infiniset~\cite{thoppilan2022lamda}: Public documents, Dialogs, Utterances      & \begin{tabular}[c]{@{}c@{}}Mini-Turing Benchmark (MTB)~\cite{adiwardana2020towards};\\
Self-collected dialogs with turns by asking \\crowdworkers to interact with LaMDA; \\Wizard of Wikipedia~\cite{dinan2018wizard}\end{tabular}  \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{\begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item The model can be fine-tuned to learn to call different external information resources and tools.
\end{itemize}}
\end{tabular}  } \\\hline\hline
\textbf{Jurassic-1}~\cite{lieber2021jurassic} & \begin{tabular}[c]{@{}c@{}}Wikipedia, OWT, Books, C4~\cite{T5}, \\PileCC~\cite{gao2020pile}, arXiv, GitHub\end{tabular}                                                                                                                                                                                & \begin{tabular}[c]{@{}c@{}}ARC-Challenge~\cite{clark2018think}, ARC-Easy~\cite{clark2018think}, BoolQ~\cite{clark2019boolq}, \\HellaSwag~\cite{zellers2019hellaswag}, PIQA~\cite{bisk2020piqa}, \\RACE-high~\cite{lai2017race}, RACE-middle~\cite{lai2017race}, \\RTE~\cite{dagan2005pascal}, StoryCloze~\cite{mostafazadeh2016corpus}, WinoGrande~\cite{sakaguchi2021winogrande}\end{tabular} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{\begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item The performance of an LLM is highly related to the network size.
\item To improve runtime performance, more operations can be performed
in parallel (width) rather than sequentially (depth).
\item To efficiently represent and fit more text in the same context length, the model uses a larger vocabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This tokenizer improvement can further benefit few-shot learning tasks.
\end{itemize}}
\end{tabular}  } \\ \hline\hline
\textbf{LLaMA}~\cite{touvron2023llama} & \begin{tabular}[c]{@{}c@{}}CommonCrawl, C4~\cite{T5}, Github, \\Wikipedia, Books, arXiv, StackExchange\end{tabular}                                                                                                                                                                 & \begin{tabular}[c]{@{}c@{}}CR: BoolQ~\cite{clark2019boolq}, PIQA~\cite{bisk2020piqa}, SIQA~\cite{sap2019socialiqa}, HellaSwag~\cite{zellers2019hellaswag},\\ WinoGrande~\cite{sakaguchi2021winogrande}, ARC-Challenge~\cite{clark2018think}, OpenBookQA~\cite{mihaylov2018can}; \\QA: TriviaQA~\cite{joshi2017triviaqa}, Natural Questions~\cite{kwiatkowski2019natural}; \\RC: RACE-middle~\cite{lai2017race}, RACE-high~\cite{lai2017race}; \\Mathematical Reasoning: MATH~\cite{hendrycks2021measuring}, GSM8K~\cite{cobbe2021training}; \\Code Generation: HumanEval~\cite{chen2021evaluating}, MBPP~\cite{austin2021program};\\MMLU~\cite{hendrycks2020measuring}, RealToxicityPrompts~\cite{gehman2020realtoxicityprompts},\\ CrowS-Pairs~\cite{nangia2020crows}, WinoGender~\cite{rudinger2018gender}, TruthfulQA~\cite{lin2021truthfulqa}\end{tabular}  \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item LLaMA is open-source and can be fine-tuned or continually pre-trained to develop new models or instruction-based tools.
\item A few optimizations are proposed to improve the training efficiency of LLaMA, such as efficient implementation of multi-head self-attention and a reduced amount of activations during back-propagation.
\item Training exclusively on public data can also achieve state-of-the-art performance.
\item A constant performance improvement is gained when scaling the model.
\item Smaller models can also realize good performances using more training data and time.  
\end{itemize}}
\end{tabular}  }\\ \hline\hline
\textbf{Yuan 1.0}~\cite{wu2021yuan} & \begin{tabular}[c]{@{}c@{}}Common Crawl, SogouT, Sogou News, \\Baidu Baike, Wikipedia, Books\end{tabular} & \begin{tabular}[c]{@{}c@{}}FewCLUE~\cite{xu2021fewclue}, ZeroCLUE~\cite{xu2020clue}, \\CMRC2018~\cite{cui2018span}, WebQA~\cite{chang2022webqa}\end{tabular} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning.
\end{itemize}}
\end{tabular}  }\\ \hline\hline
\textbf{WebGPT}~\cite{nakano2021webgpt} & \begin{tabular}[c]{@{}c@{}}ELI5~\cite{fan2019eli5}, ELI5 fact-check~\cite{nakano2021webgpt}, TriviaQA~\cite{joshi2017triviaqa}, \\ARC-Challenge~\cite{clark2018think}, ARC-Easy~\cite{clark2018think},\\ Hand-written data, Demonstrations of humans, \\ Comparisons between model-generated answers\end{tabular} & ELI5~\cite{fan2019eli5}, TruthfulQA~\cite{lin2021truthfulqa}, TriviaQA~\cite{joshi2017triviaqa} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item The answer quality of LLMs can be further improved with human feedback. 
\item To aid the model in effectively filtering and utilizing relevant information, human labelers play a crucial role in answering questions regarding the usefulness of the retrieved documents.
\item Interacting a fine-tuned language model with a text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning.
\item Generating answers with references can make labelers easily judge the factual accuracy of answers.\textbf{}
\end{itemize}}
\end{tabular}  }\\ \hline\hline
\textbf{GLaM}~\cite{du2022glam} &\begin{tabular}[c]{@{}c@{}}Filtered Webpages, Social media conversations \\Wikipedia, Forums, Books, News\end{tabular} &\begin{tabular}[c]{@{}c@{}}NLG: TriviaQA~\cite{joshi2017triviaqa}, NQS, WebQS, SQuADv2~\cite{rajpurkar2018know}, \\LAMBADA~\cite{paperno2016lambada}, DROP~\cite{dua2019drop}, QuAC~\cite{choi2018quac}, CoQA~\cite{reddy2019coqa}; \\NLU: HellaSwag~\cite{zellers2019hellaswag}, StoryCloze~\cite{mostafazadeh2016corpus}, WinoGrad~\cite{levesque2012winograd},\\ WinoGrande~\cite{sakaguchi2021winogrande}, RACE-middle~\cite{lai2017race}, RACE-high~\cite{lai2017race}, PIQA~\cite{bisk2020piqa},\\ ARC-Challenge~\cite{clark2018think}, ARC-Easy~\cite{clark2018think}, OpenbookQA~\cite{mihaylov2018can}, \\ BoolQ~\cite{clark2019boolq}, COPA~\cite{roemmele2011choice}, RTE~\cite{dagan2005pascal}, WiC~\cite{pilehvar2018wic}, \\MultiRC~\cite{khashabi2018looking}, WSC~\cite{levesque2012winograd}, ReCoRD~\cite{zhang2018record}, CB~\cite{de2019commitmentbank},\\ ANLI R1~\cite{nie2019adversarial}, ANLI R2~\cite{nie2019adversarial}, ANLI R3~\cite{nie2019adversarial}\end{tabular}   \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item The feed-forward component of each Transformer layer can be replaced with a mixture-of-experts (MoE) module consisting of a set of independent feed-forward networks (\emph{i.e.}, the `experts'). By sparsely activating these experts, the model capacity can be maintained while much computation is saved. 
\item By leveraging sparsity, we can make significant strides towards developing high-quality NLP models while simultaneously reducing energy consumption. Consequently, MoE emerges as a robust candidate for future scaling endeavors.
\item The model trained on filtered data shows consistent better performances on both NLG and NLU tasks, where the effect of filtering is more significant on the former tasks.
\item Filtered pretraining corpora plays a crucial role in the generation capability of LLMs, especially for the downstream tasks.
\item The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given a fixed budget of computation, more experts contribute to better predictions.
\end{itemize}}
\end{tabular}   } \\ \hline\hline
\end{tabular}%
\label{datasets}
\end{table*}

\begin{table*}[!tbhp]
\ContinuedFloat
\caption{An example table of merging datasets and findings for each model. \textbf{(Continued.)} (To be updated.)}
\begin{tabular}{lcccl}
\hline \hline
Model & \begin{tabular}[c]{@{}c@{}}Training Dataset \end{tabular} & \begin{tabular}[c]{@{}c@{}}Evaluation Dataset\end{tabular}  \\ \hline \hline
\textbf{AlphaCode}~\cite{li2022competition} & \begin{tabular}[c]{@{}c@{}}Selected GitHub repositories \\CodeContests~\cite{li2022competition}: Codeforces~\cite{codeforce},\\ Description2Code~\cite{Caballero_Description2Code_Dataset_2016}, CodeNet~\cite{puri2021codenet}\end{tabular} & Codeforces competitions, CodeContests~\cite{li2022competition}, APPS~\cite{hendrycks2021measuring} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed with a shallower encoder and a deeper decoder.
\item To achieve better performances, it is necessary to employ strategies such as massively scaling up sampling, followed by the filtering and clustering of samples into a compact set. 
\item The utilization of novel sampling-efficient transformer architectures designed to facilitate large-scale sampling is crucial.
\item Simplifying problem descriptions can effectively improve the model’s performance.
\end{itemize}}
\end{tabular}   }\\ \hline\hline
\textbf{AlexaTM}~\cite{soltan2022alexatm} & Wikipedia, mC4~\cite{mT5} & \begin{tabular}[c]{@{}c@{}}NLG: MLSum~\cite{scialom2020mlsum}, XSum~\cite{narayan1808don}, E2E~\cite{novikova2017e2e}, WebNLG~\cite{ferreira20202020}; \\Machine Translation: Flores-101~\cite{goyal2022flores}, English-German WMT'16,\\ English-French WMT'14, German-French WMT'19~\cite{xia2019microsoft};\\NLP: XNLI~\cite{conneau2018xnli}, XCOPA~\cite{ponti2020xcopa}, PAWS-X~\cite{yang2019paws}, XWinograd~\cite{tikhonov2021s},\\ SuperGLUE~\cite{wang2019superglue}, SQUADv2~\cite{rajpurkar2018know}, MultiArith~\cite{roy2016solving}\end{tabular} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item Compared to commonly used Decoder-only Transformer models, seq2seq architecture is more suitable for training generative LLMs given stronger bidirectional attention to the context. 
\item An extra Causal Language Modeling (CLM) task can be added to benefit the model with a more efficient in-context learning, especially for few-shot learning tasks. 
\item The key to training powerful seq2seq-based LLMs lies in mixed pre-training, rather than additional multitask training.
\item Placing layernorms at the beginning of each transformer layer can improve the training stability of large models.
\end{itemize}}
\end{tabular}   }   \\ \hline\hline
\textbf{Sparrow}~\cite{glaese2022improving} & \begin{tabular}[c]{@{}c@{}}Human data for rule violations\\ and per-turn response preferences,\\Self-play data accumulated through training,\\ GopherCite FilteredELI5~\cite{menick2022teaching}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Per-turn response preference and adversarial probing, \\Multi-turn dialogues, Information-seeking dialogues, \\Chinchilla-generated~\cite{chinchilla} conversational questions, \\ GopherCite~\cite{menick2022teaching} human evaluation interface,\\ FilteredELI5~\cite{menick2022teaching} “Free” dialogues, DPC-generated~\cite{chinchilla} dialogues \\WinoGender~\cite{rudinger2018gender}, Winobias~\cite{zhao2018gender}, BBQ~\cite{parrish2021bbq},\\ Natural Questions~\cite{kwiatkowski2019natural}, Quiz Bowl~\cite{boyd2012besting}, TriviaQA~\cite{joshi2017triviaqa}\end{tabular} \\\hline
\emph{Our Findings}: & \multicolumn{2}{c}{ \begin{tabular}{c}
\multicolumn{1}{p{15cm}}{\begin{itemize}
\item Reinforcement learning from multi-objective human feedback can be leveraged to train the models, in order to maximize preference rates and minimize rule violations.
\item The judgments of labelers and the alignments with defined rules can help the model generate better responses.
\item Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters.
\item Constructing useful and reliable agents from generative models requires a combination of width and depth: the width aspect enables addressing the complexities of goals and topics; while the depth aspect ensures accurate handling of them.
\item The combination of reinforcement learning (RL) with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing.
\end{itemize}}
\end{tabular}  }   \\ \hline\hline

\end{tabular}%
\label{datasets}
\end{table*}