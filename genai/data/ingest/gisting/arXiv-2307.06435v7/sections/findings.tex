\section{Findings}
\label{sec:Findings}
Training a billion-scale model is difficult as compared to a smaller model. LLMs are prone to various instabilities during training, such as hardware failure and instability. Other than this, LLMs exhibit different behaviors such as emergent abilities, improved zero-shot, few-shot, and reasoning abilities. Researchers report these essential details in their papers for results reproduction and field progress. We identify critical information in Table~\ref{tab:pre_trained_findings}~and~\ref{tab:instruction_tuned_findings} such as architecture, training strategies, and pipelines that improve LLMs' performance or other abilities acquired because of changes mentioned in section~\ref{sec_review}.  