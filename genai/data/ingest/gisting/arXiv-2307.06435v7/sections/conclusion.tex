\section{Conclusion}
% In this paper, we have reviewed LLMs (at least 10B parameters), discussing the pros and cons of every model. We have discussed important findings and provided a detailed overview of the design of every LLM, including architecture, datasets, and training pipeline. The paper identifies important architectural components and training strategies of various LLMs and summarizes and discusses them. Additionally to that, the paper compares LLMs abilities in zero-shot and few-shot settings, the effect on performance with fine-tuning, and performance comparison between a supervised and generalized model. We aim to regularly update this paper and hope that this paper will help researchers in reviewing the recent development in LLMs and acquiring key concepts and details to build better LLMs.   
In this paper, we have reviewed LLMs (with at least 10B parameters), discussing the pros and cons of every model. Our review concluded significant findings and provided a detailed analysis of the design aspects of each LLM, including architecture, datasets, and training pipelines. We have identified crucial architectural components and training strategies employed by different LLMs and presented a summary and discussion. Moreover, we have compared the performance of LLMs in zero-shot and few-shot settings, explored the impact of fine-tuning, and compared supervised and generalized models. This paper will serve as a valuable resource for researchers, offering insights into the recent advancements in LLMs and providing fundamental concepts and details to develop improved LLMs.

%We intend to update this paper regularly to keep up with the rapid development in the field. \SA{This sentence should be removed, we have already included versioning}