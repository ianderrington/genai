\section{Summary and Discussion}
\subsection{Architecture}
Due to the gigantic scale of LLMs, minor changes in architecture and training strategies have a big impact on performance and stability. Here, we summarize key architectural modules used in various LLMs, leading to better performance, reduced training time and memory, and better training stability. \\
\emph{\textbf{Layer Normalization}} is found to have a significant effect on the performance and training stability of LLMs. Pre-norm, that is normalizing inputs rather than outputs, is more common among LLMs stabilizing the training~\cite{GPT-3, touvron2023llama, PanGU_alpha}. BLOOM~\cite{BLOOM} and AlexaTM~\cite{soltan2022alexatm} utilize an additional layer normalization before embedding layer to stabilize the training of large-scale models, while the model's zero-shot generalization ability can be negatively impacted~\cite{BLOOM}. However, another study~\cite{GLM-130B} finds that pre-norm degrades fine-tuned model performance as compared to post-norm, and there are no stability benefits of pre-norm beyond the 100B scale. Therefore, GLM-130B~\cite{GLM-130B} used deep-norm which is a variant of post-norm for better downstream task performance after fine-tuning.   \\
\emph{\textbf{Positional Encoding}} effect performance and training stability of LLMs like other building blocks of a model. BLOOM~\cite{BLOOM} finds ALiBi outperforming learned and rotary positional encodings. Contrary to this, GLM-130B~\cite{GLM-130B} identifies rotary positional encoding better than ALiBi. So, there is no conclusion in literature about the positional encodings yet. \\
\emph{\textbf{Parallel Attention}} where attention and feed-forward layers are parallel to each other rather than sequential in transformer block has shown to reduce training time by 15\%. There is no evidence of performance drop due to this change in literature and used by the models PaLM~\cite{PaLM}, GPT-NeoX~\cite{GPT_NeoX}, and CodeGen~\cite{CodeGen}.  \\
\emph{\textbf{Multi-Query Attention}} has shared key and value attention heads in a transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degradation has been observed with this change and makes the training efficient allowing larger batch sizes. Multi-query attention is used in~\cite{PaLM, li2022competition}.    \\
\emph{\textbf{Mixture of Experts}} allows scaling model to trillion of parameters easily~\cite{PanGu_sigma, du2022glam}. Only a few experts are activated during the computation making them compute-efficient. The performance of MoE models is better than the dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to the dense models as discussed in~\cite{du2022glam}. MoE architectures are less prone to catastrophic forgetting, therefore are more suited for continual learning~\cite{PanGu_sigma}. Extracting smaller sub-models for downstream tasks is possible without losing any performance, making MoE architecture hardware-friendly~\cite {PanGu_sigma}.    \\
\emph{\textbf{Sparse vs Dense Activated}}
GPT-3~\cite{GPT-3} uses sparse transformers~\cite{sparse_transformer} whereas GLaM~\cite{du2022glam} and PanGu-$\sum$~\cite{PanGu_sigma} use MoE~\cite{shazeer2017outrageously} architecture to lower computational costs and increase the model size and capacity. According to the literature, sparse modules do not degrade the model's performance~\cite{sparse_transformer}. However, more experiments are required to verify this statement.  \\

\subsection{Training Strategies}
Training models at a huge scale require some tricks to reduce training costs, avoid loss divergence and achieve better performance. We summarize and discuss some of these key tricks used in different LLMs. \\
\emph{\textbf{Mixed Precision}} is a famous method for LLMs to reduce memory usage and improve training efficiency. In mixed precision, forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format~\cite{Mixed_Precision}. A drawback associated with this format change is training instability due to a smaller value range resulting in loss spikes~\cite{GLM-130B}. An alternative to FP16 is BF16 which has a comparatively larger range and performs some precision-sensitive operations like gradient accumulation and softmax in FP32~\cite{BLOOM}. BF16 has better performance and training stability but uses more memory and is supported on specific hardware, for example, A100 GPUs. Therefore, its adoption in LLMs is limited. \\ %\textcolor{blue}{for instance, the GLaM~\cite{du2022glam} model adopts this method for training, where FP32 is used for model weights and BF16 is used for activations.}  \\
\emph{\textbf{Training Instability}} is a common issue in LLMs where loss divergence or spiking is observed multiple times during training. This happens in the presence of gradient clipping~\cite{PaLM}. To mitigate this problem, many approaches suggest restarting training from an earlier checkpoint~\cite{PaLM, GLM-130B, du2022glam}, skipping 200-500 earlier data batches at the point of divergence in~\cite{PaLM} and re-shuffling batches in~\cite{du2022glam}. The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers~\cite{GLM-130B}. Another suggestion to improve training stability for larger models is not to use \textbf{biases} in dense and norm layers as in~\cite{PaLM}.    \\
\emph{\textbf{Training Parallelism}} 3D parallelism, a combination of data, pipeline and tensor parallelism, is the most utilized training parallelism approach in LLMs~\cite{GLM-130B, PaLM, OPT, BLOOM, mtnlg, wu2021yuan,lieber2021jurassic}. In addition to the 3D parallelism, BLOOM~\cite{BLOOM} uses zero optimizer~\cite{ZeroOpt} to shard optimizer states. PanGu-$\alpha$~\cite{PanGU_alpha} and PanGu-$\Sigma$~\cite{PanGu_sigma} go beyond the 3D parallelism and apply 5D parallelism which additionally contains optimizer parallelism and rematerialization.     \\
\emph{\textbf{Mode Switching}} adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve the downstream task performance in~\cite{UL2, U-PaLM, soltan2022alexatm}. During fine-tuning and inference, tokens are appended based on the downstream tasks.  \\
\subsection{Pre-Training vs 
Instruction Tuning}
While pre-training is important for the generalization of LLMs, instruction-tuning improves the performance of LLMs further and makes them useable. Therefore, it is suggested to perform instruction fine-tuning of pre-trained LLMs to use them effectively~\cite{Flan, Tk-INSTRUCT, instructgpt, OPT_IML, nakano2021webgpt}. 
\subsection{Supervised Models vs Generalized Models}
Although generalized models are capable of performing diverse tasks with good performance they have not yet outperformed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by a large margin as shown in~\cite{GPT-3, PaLM, Tk-INSTRUCT}.   
\subsection{Zero-Shot vs Few-Shot}
LLMs perform well in zero-shot and few-shot settings. But the performance difference between zero-shot and few-shot is large for pre-trained models~\cite{GPT-3, PaLM}, naming LLMs as meta-learners~\cite{GPT-3}. LLMs zero-shot evaluations underperform unsupervised methods in neural machine translation~\cite{GPT-3}. The literature shows pre-training is not enough for good zero-shot performance~\cite{PaLM, Flan}. To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms baselines. Instruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model Flan-PaLM~\cite{Flan} unlocks zero-shot reasoning with CoT training. 

