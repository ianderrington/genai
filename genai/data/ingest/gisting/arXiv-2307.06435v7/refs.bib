@article{T5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{Transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{LMAdapter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{LMAdapter_2,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}
@article{LMAdapter_3,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}
@article{mT5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}
@article{mtnlg,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}
@article{Flan,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{LMAdapted,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{mT0andBLOOMZ,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}
@article{gopher,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}
@article{WuDaoCorpus,
  title={Wudaocorpora: A super large-scale chinese corpora for pre-training language models},
  author={Yuan, Sha and Zhao, Hanyu and Du, Zhengxiao and Ding, Ming and Liu, Xiao and Cen, Yukuo and Zou, Xu and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  volume={2},
  pages={65--68},
  year={2021},
  publisher={Elsevier}
}
@article{CPM-2,
  title={Cpm-2: Large-scale cost-effective pre-trained language models},
  author={Zhang, Zhengyan and Gu, Yuxian and Han, Xu and Chen, Shengqi and Xiao, Chaojun and Sun, Zhenbo and Yao, Yuan and Qi, Fanchao and Guan, Jian and Ke, Pei and others},
  journal={AI Open},
  volume={2},
  pages={216--224},
  year={2021},
  publisher={Elsevier}
}
@Misc{fairscale,
  author =       {{FairScale authors}},
  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}
@article{hyperclova,
  title={What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers},
  author={Kim, Boseop and Kim, HyoungSeok and Lee, Sang-Woo and Lee, Gichang and Kwak, Donghyun and Jeon, Dong Hyeon and Park, Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and others},
  journal={arXiv preprint arXiv:2109.04650},
  year={2021}
}
@article{codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{ernie3,
  title={Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation},
  author={Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and others},
  journal={arXiv preprint arXiv:2107.02137},
  year={2021}
}
@article{ernie3titan,
  title={Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation},
  author={Wang, Shuohuan and Sun, Yu and Xiang, Yang and Wu, Zhihua and Ding, Siyu and Gong, Weibao and Feng, Shikun and Shang, Junyuan and Zhao, Yanbin and Pang, Chao and others},
  journal={arXiv preprint arXiv:2112.12731},
  year={2021}
}

@article{instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{Tk-INSTRUCT,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}
@article{PanGU_alpha,
  title={PanGu-$\alpha$ : Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
  author={Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
  journal={arXiv preprint arXiv:2104.12369},
  year={2021}
}
@article{PanGu_sigma,
  title={PanGu-$\sum$: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},
  author={Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and others},
  journal={arXiv preprint arXiv:2303.10845},
  year={2023}
}
@article{S_mixed_precision,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}
@article{OPT,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  volume={1},
  year={2021}
}
@article{GPT-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{OpenAI-OpenAI-2023-GPT-4,
  author    = {OpenAI},
  title     = {GPT-4 Technical Report},
  journal   = {OpenAI},
  year      = {2023}
}
@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}
@inproceedings{alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}
@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}
@article{wenzek2019ccnet,
  title={CCNet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{wu2021yuan,
  title={Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning},
  author={Wu, Shaohua and Zhao, Xudong and Yu, Tong and Zhang, Rongguo and Shen, Chong and Liu, Hongli and Li, Feng and Zhu, Hong and Luo, Jiangang and Xu, Liang and others},
  journal={arXiv preprint arXiv:2110.04725},
  year={2021}
}
@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
@article{xu2020clue,
  title={CLUE: A Chinese language understanding evaluation benchmark},
  author={Xu, Liang and Hu, Hai and Zhang, Xuanwei and Li, Lu and Cao, Chenjie and Li, Yudong and Xu, Yechen and Sun, Kai and Yu, Dian and Yu, Cong and others},
  journal={arXiv preprint arXiv:2004.05986},
  year={2020}
}
@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}
@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}
@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{fan2019eli5,
  title={ELI5: Long form question answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  journal={arXiv preprint arXiv:1907.09190},
  year={2019}
}
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}
@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}
@article{soltan2022alexatm,
  title={Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model},
  author={Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and others},
  journal={arXiv preprint arXiv:2208.01448},
  year={2022}
}
@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}
@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}
@misc{Caballero_Description2Code_Dataset_2016,
author = {Caballero, Ethan and OpenAI, . and Sutskever, Ilya},
doi = {10.5281/zenodo.5665051},
month = {8},
title = {{Description2Code Dataset}},
url = {https://github.com/ethancaballero/description2code},
year = {2016}
}
@article{puri2021codenet,
  title={CodeNet: A large-scale AI for code dataset for learning a diversity of coding tasks},
  author={Puri, Ruchir and Kung, David S and Janssen, Geert and Zhang, Wei and Domeniconi, Giacomo and Zolotov, Vladimir and Dolby, Julian and Chen, Jie and Choudhury, Mihir and Decker, Lindsey and others},
  journal={arXiv preprint arXiv:2105.12655},
  year={2021}
}
@article{pang2020text,
  title={Text generation by learning from demonstrations},
  author={Pang, Richard Yuanzhe and He, He},
  journal={arXiv preprint arXiv:2009.07839},
  year={2020}
}
@article{dabre2020softmax,
  title={Softmax Tempering for Training Neural Machine Translation Models},
  author={Dabre, Raj and Fujita, Atsushi},
  journal={arXiv preprint arXiv:2009.09372},
  year={2020}
}
@inproceedings{fitzgerald2022alexa,
  title={Alexa teacher model: Pretraining and distilling multi-billion-parameter encoders for natural language understanding systems},
  author={FitzGerald, Jack and Ananthakrishnan, Shankar and Arkoudas, Konstantine and Bernardi, Davide and Bhagia, Abhishek and Delli Bovi, Claudio and Cao, Jin and Chada, Rakesh and Chauhan, Amit and Chen, Luoxin and others},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2893--2902},
  year={2022}
}
@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}
@article{goyal2022flores,
  title={The flores-101 evaluation benchmark for low-resource and multilingual machine translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc’Aurelio and Guzm{\'a}n, Francisco and Fan, Angela},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={522--538},
  year={2022},
  publisher={MIT Press}
}
@article{conneau2018xnli,
  title={XNLI: Evaluating cross-lingual sentence representations},
  author={Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R and Schwenk, Holger and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1809.05053},
  year={2018}
}
@article{ponti2020xcopa,
  title={XCOPA: A multilingual dataset for causal commonsense reasoning},
  author={Ponti, Edoardo Maria and Glava{\v{s}}, Goran and Majewska, Olga and Liu, Qianchu and Vuli{\'c}, Ivan and Korhonen, Anna},
  journal={arXiv preprint arXiv:2005.00333},
  year={2020}
}
@article{yang2019paws,
  title={PAWS-X: A cross-lingual adversarial dataset for paraphrase identification},
  author={Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  journal={arXiv preprint arXiv:1908.11828},
  year={2019}
}
@article{tikhonov2021s,
  title={It's all in the heads: Using attention heads as a baseline for cross-lingual transfer in commonsense reasoning},
  author={Tikhonov, Alexey and Ryabinin, Max},
  journal={arXiv preprint arXiv:2106.12066},
  year={2021}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@inproceedings{batchnorm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@incollection{lecunwhitening,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--50},
  year={2002},
  publisher={Springer}
}

@article{layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{rmsnorm,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{deepnorm,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}

@article{shleifer2021normformer,
  title={Normformer: Improved transformer pretraining with extra normalization},
  author={Shleifer, Sam and Weston, Jason and Ott, Myle},
  journal={arXiv preprint arXiv:2110.09456},
  year={2021}
}

@article{preLN,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@article{activationfunction,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{relu,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}
@article{U-PaLM,
  title={Transcending scaling laws with 0.1\% extra compute},
  author={Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q and So, David R and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and others},
  journal={arXiv preprint arXiv:2210.11399},
  year={2022}
}
@inproceedings{UL2,
  title={Ul2: Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{gelu,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}
@article{batch_size_selec,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}
@article{sparse_transformer,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@article{GPT-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{dinan2018wizard,
  title={Wizard of wikipedia: Knowledge-powered conversational agents},
  author={Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
  journal={arXiv preprint arXiv:1811.01241},
  year={2018}
}
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}
@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}

@article{firstrelu,
  title={Self-Organizing Multilayered Neural Network},
  author={Fukushima, K},
  journal={The Transactions of Electronics and communication Engineers D},
  volume={58},
  number={9},
  pages={530},
  year={1975}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{krueger2016zoneout,
  title={Zoneout: Regularizing rnns by randomly preserving hidden activations},
  author={Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  journal={arXiv preprint arXiv:1606.01305},
  year={2016}
}

@inproceedings{glu,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@inproceedings{webster1992tokenization,
  title={Tokenization as the initial phase in NLP},
  author={Webster, Jonathan J and Kit, Chunyu},
  booktitle={COLING 1992 volume 4: The 14th international conference on computational linguistics},
  year={1992}
}

@article{tokenizationsurvey,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}

@article{attention,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{NoPE,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  journal={arXiv preprint arXiv:2305.19466},
  year={2023}
}

@article{BLOOM,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@article{PaLM,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{CodeGen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}
@article{GLM-130B,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
@inproceedings{GLM,
  title={GLM: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@inproceedings{bpe,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@inproceedings{wordpiece,
  title={Japanese and korean voice search},
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5149--5152},
  year={2012},
  organization={IEEE}
}

@inproceedings{unigramLM,
  title={Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  author={Kudo, Taku},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={66--75},
  year={2018}
}

@article{selectiveattention,
  title={Some characteristics of selective attention in visual perception determined by vocal reaction time},
  author={Eriksen, Charles W and Hoffman, James E},
  journal={Perception \& Psychophysics},
  volume={11},
  number={2},
  pages={169--171},
  year={1972},
  publisher={Springer}
}

@inproceedings{residualconnection,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{Survey_LLM,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}
@inproceedings{ZeroOpt,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}
@inproceedings{Lib_Transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}
@inproceedings{Lib_DeepSpeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}
@article{Lib_Megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
@misc{Lib_Jax,
  title={JAX: composable transformations of Python+ NumPy programs},
  author={Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and others},
  year={2018}
}
@article{Lib_Colossal,
  title={Colossal-AI: A unified deep learning system for large-scale parallel training},
  author={Li, Shenggui and Fang, Jiarui and Bian, Zhengda and Liu, Hongxin and Liu, Yuliang and Huang, Haichen and Wang, Boxiang and You, Yang},
  journal={arXiv preprint arXiv:2110.14883},
  year={2021}
}
@misc{Lib_Bmtrain,
  title={"Bmtrain: Efficient training for big models."},
  url={https://github.com/OpenBMB/BMTrain}
}
@article{Lib_Fastmoe,
  title={Fastmoe: A fast mixture-of-expert training system},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}
@article{Lib_Pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{Lib_Tensorflow,
  title={Tensorflow: a system for large-scale machine learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={Osdi},
  volume={16},
  number={2016},
  pages={265--283},
  year={2016},
  organization={Savannah, GA, USA}
}
@article{Lib_Mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}
@article{Lib_paddlepaddle,
  title={PaddlePaddle: An open-source deep learning platform from industrial practice},
  author={Ma, Yanjun and Yu, Dianhai and Wu, Tian and Wang, Haifeng},
  journal={Frontiers of Data and Domputing},
  volume={1},
  number={1},
  pages={105--115},
  year={2019}
}
@article{Lib_oneflow,
  title={Oneflow: Redesign the distributed deep learning framework from scratch},
  author={Yuan, Jinhui and Li, Xinqi and Cheng, Cheng and Liu, Juncheng and Guo, Ran and Cai, Shenghang and Yao, Chi and Yang, Fei and Yi, Xiaodong and Wu, Chuan and others},
  journal={arXiv preprint arXiv:2110.15032},
  year={2021}
}
@incollection{Lib_Mindspore,
  title={Huawei MindSpore AI Development Framework},
  author={Huawei Technologies Co., Ltd.},
  booktitle={Artificial Intelligence Technology},
  pages={137--162},
  year={2022},
  publisher={Springer}
}
@inproceedings{LLM_Objectives,
  title={What Language Model Architecture and Pretraining Objective Works Best for Zero-Shot Generalization?},
  author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={22964--22984},
  year={2022},
  organization={PMLR}
}
@article{Unified_LM,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{Prefix_Tuning,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{Prompt_Tuning,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{Prompt_Tuning_2,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@inproceedings{decoderonly,
title={Generating Wikipedia by Summarizing Long Sequences},
author={Peter J. Liu* and Mohammad Saleh* and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hyg0vbWC-},
}
@article{T0,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}
@article{mT0,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}
@article{OPT_IML,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}
@article{Bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{ELMO,
  author       = {Matthew E. Peters and
                  Mark Neumann and
                  Mohit Iyyer and
                  Matt Gardner and
                  Christopher Clark and
                  Kenton Lee and
                  Luke Zettlemoyer},
  title        = {Deep Contextualized Word Representations},
  booktitle    = {{NAACL-HLT}},
  pages        = {2227--2237},
  publisher    = {Association for Computational Linguistics},
  year         = {2018}
}
@article{BART,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{y2022large,
  title={Do Large Language Models Understand Us?},
  author={y Arcas, Blaise Ag{\"u}era},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={183--197},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{chernyavskiy2021transformers,
  title={Transformers:“the end of history” for natural language processing?},
  author={Chernyavskiy, Anton and Ilvovsky, Dmitry and Nakov, Preslav},
  booktitle={Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III 21},
  pages={677--693},
  year={2021},
  organization={Springer}
}

@article{adiwardana2020towards,
  title={Towards a human-like open-domain chatbot},
  author={Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and others},
  journal={arXiv preprint arXiv:2001.09977},
  year={2020}
}

@article{survey_1,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}
@article{survey_2,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}
@article{survey_smaller_LLMs_1,
  title={A comprehensive survey on word representation models: From classical to state-of-the-art word representation language models},
  author={Naseem, Usman and Razzak, Imran and Khan, Shah Khalid and Prasad, Mukesh},
  journal={Transactions on Asian and Low-Resource Language Information Processing},
  volume={20},
  number={5},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY}
}
@article{survey_smaller_LLMs_2,
  title={Recent advances in natural language processing via large pre-trained language models: A survey},
  author={Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
  journal={arXiv preprint arXiv:2111.01243},
  year={2021}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{baumgartner2020pushshift,
  title={The pushshift reddit dataset},
  author={Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={14},
  pages={830--839},
  year={2020}
}

@article{wang2022benchmarking,
  title={Benchmarking generalization via in-context instructions on 1,600+ language tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}
@article{xie2022unifiedskg,
  title={Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models},
  author={Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I and others},
  journal={arXiv preprint arXiv:2201.05966},
  year={2022}
}
@article{ye2021crossfit,
  title={Crossfit: A few-shot learning challenge for cross-task generalization in nlp},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  journal={arXiv preprint arXiv:2104.08835},
  year={2021}
}
@article{aribandi2021ext5,
  title={Ext5: Towards extreme multi-task scaling for transfer learning},
  author={Aribandi, Vamsi and Tay, Yi and Schuster, Tal and Rao, Jinfeng and Zheng, Huaixiu Steven and Mehta, Sanket Vaibhav and Zhuang, Honglei and Tran, Vinh Q and Bahri, Dara and Ni, Jianmo and others},
  journal={arXiv preprint arXiv:2111.10952},
  year={2021}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}
@article{alex2021raft,
  title={RAFT: A real-world few-shot text classification benchmark},
  author={Alex, Neel and Lifland, Eli and Tunstall, Lewis and Thakur, Abhishek and Maham, Pegah and Riedel, C Jess and Hine, Emmie and Ashurst, Carolyn and Sedille, Paul and Carlier, Alexis and others},
  journal={arXiv preprint arXiv:2109.14076},
  year={2021}
}
@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}
@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@misc{QQP,
  title = {First Quora Dataset Release: Question Pairs},
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornél},
  howpublished = {\url{https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}}
}
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}
@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}
@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}
@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}
@inproceedings{roemmele2011choice,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI spring symposium: logical formalizations of commonsense reasoning},
  pages={90--95},
  year={2011}
}
@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}
@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}
@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}
@article{pilehvar2018wic,
  title={Wic: 10,000 example pairs for evaluating context-sensitive representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jos{\'e}},
  journal={arXiv preprint arXiv:1808.09121},
  volume={6},
  year={2018}
}
@inproceedings{bojar2016findings,
  title={Findings of the 2016 conference on machine translation},
  author={Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and others},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  pages={131--198},
  year={2016}
}
@article{GPT_NeoX,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}
@article{lewis2019mlqa,
  title={MLQA: Evaluating cross-lingual extractive question answering},
  author={Lewis, Patrick and O{\u{g}}uz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
  journal={arXiv preprint arXiv:1910.07475},
  year={2019}
}
@article{clark2020tydi,
  title={TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages},
  author={Clark, Jonathan H and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={454--470},
  year={2020},
  publisher={MIT Press}
}
@inproceedings{pan2017cross,
  title={Cross-lingual name tagging and linking for 282 languages},
  author={Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1946--1958},
  year={2017}
}
@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}
@article{nie2019adversarial,
  title={Adversarial NLI: A new benchmark for natural language understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  journal={arXiv preprint arXiv:1910.14599},
  year={2019}
}
@article{zellers2019hellaswag,
  title={HellaSwag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}
@article{mostafazadeh2016corpus,
  title={A corpus and evaluation framework for deeper understanding of commonsense stories},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
  journal={arXiv preprint arXiv:1604.01696},
  year={2016}
}
@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{li2021ccpm,
  title={Ccpm: A chinese classical poetry matching dataset},
  author={Li, Wenhao and Qi, Fanchao and Sun, Maosong and Yi, Xiaoyuan and Zhang, Jiarui},
  journal={arXiv preprint arXiv:2106.01979},
  year={2021}
}
@article{sun2020investigating,
  title={Investigating prior knowledge for challenging chinese machine reading comprehension},
  author={Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={141--155},
  year={2020},
  publisher={MIT Press}
}
@inproceedings{loic2020findings,
  title={Findings of the 2020 conference on machine translation (wmt20)},
  author={Lo{\"\i}c, Barrault and Magdalena, Biesialska and Ond{\v{r}}ej, Bojar and Christian, Federmann and Yvette, Graham and Roman, Grundkiewicz and Barry, Haddow and Matthias, Huck and Eric, Joanis and Tom, Kocmi and others},
  booktitle={Proceedings of the Fifth Conference on Machine Translation},
  pages={1--55},
  year={2020},
  organization={Association for Computational Linguistics,}
}
@inproceedings{wang2017deep,
  title={Deep neural solver for math word problems},
  author={Wang, Yan and Liu, Xiaojiang and Shi, Shuming},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={845--854},
  year={2017}
}
@article{hu2015lcsts,
  title={Lcsts: A large scale chinese short text summarization dataset},
  author={Hu, Baotian and Chen, Qingcai and Zhu, Fangze},
  journal={arXiv preprint arXiv:1506.05865},
  year={2015}
}
@inproceedings{liu2018lcqmc,
  title={Lcqmc: A large-scale chinese question matching corpus},
  author={Liu, Xin and Chen, Qingcai and Deng, Chong and Zeng, Huajun and Chen, Jing and Li, Dongfang and Tang, Buzhou},
  booktitle={Proceedings of the 27th international conference on computational linguistics},
  pages={1952--1962},
  year={2018}
}
@article{shao2019long,
  title={Long and diverse text generation with planning-based hierarchical variational model},
  author={Shao, Zhihong and Huang, Minlie and Wen, Jiangtao and Xu, Wenfei and Zhu, Xiaoyan},
  journal={arXiv preprint arXiv:1908.06605},
  year={2019}
}
@article{yao2021cuge,
  title={Cuge: A chinese language understanding and generation evaluation benchmark},
  author={Yao, Yuan and Dong, Qingxiu and Guan, Jian and Cao, Boxi and Zhang, Zhengyan and Xiao, Chaojun and Wang, Xiaozhi and Qi, Fanchao and Bao, Junwei and Nie, Jinran and others},
  journal={arXiv preprint arXiv:2112.13610},
  year={2021}
}
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{vilares2019head,
  title={HEAD-QA: A healthcare dataset for complex reasoning},
  author={Vilares, David and G{\'o}mez-Rodr{\'\i}guez, Carlos},
  journal={arXiv preprint arXiv:1906.04701},
  year={2019}
}
@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}
@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}
@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}
@article{aroca2021prost,
  title={Prost: Physical reasoning of objects through space and time},
  author={Aroca-Ouellette, St{\'e}phane and Paik, Cory and Roncone, Alessandro and Kann, Katharina},
  journal={arXiv preprint arXiv:2106.03634},
  year={2021}
}
@inproceedings{penas2013qa4mre,
  title={QA4MRE 2011-2013: Overview of question answering for machine reading evaluation},
  author={Pe{\~n}as, Anselmo and Hovy, Eduard and Forner, Pamela and Rodrigo, {\'A}lvaro and Sutcliffe, Richard and Morante, Roser},
  booktitle={Information Access Evaluation. Multilinguality, Multimodality, and Visualization: 4th International Conference of the CLEF Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings 4},
  pages={303--320},
  year={2013},
  organization={Springer}
}
@article{welbl2017crowdsourcing,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}
@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}
@article{nangia2020crows,
  title={CrowS-pairs: A challenge dataset for measuring social biases in masked language models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.00133},
  year={2020}
}
@article{nadeem2020stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  journal={arXiv preprint arXiv:2004.09456},
  year={2020}
}
@article{mollas2020ethos,
  title={Ethos: an online hate speech detection dataset},
  author={Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal={arXiv preprint arXiv:2006.08328},
  year={2020}
}
@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}
@inproceedings{dinan2020second,
  title={The second conversational intelligence challenge (convai2)},
  author={Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and others},
  booktitle={The NeurIPS'18 Competition: From Machine Learning to Intelligent Conversations},
  pages={187--208},
  year={2020},
  organization={Springer}
}
@article{rashkin2018towards,
  title={Towards empathetic open-domain conversation models: A new benchmark and dataset},
  author={Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
  journal={arXiv preprint arXiv:1811.00207},
  year={2018}
}
@article{smith2020can,
  title={Can you put it all together: Evaluating conversational agents' ability to blend skills},
  author={Smith, Eric Michael and Williamson, Mary and Shuster, Kurt and Weston, Jason and Boureau, Y-Lan},
  journal={arXiv preprint arXiv:2004.08449},
  year={2020}
}
@article{komeili2021internet,
  title={Internet-augmented dialogue generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  journal={arXiv preprint arXiv:2107.07566},
  year={2021}
}
@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press}
}
@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1533--1544},
  year={2013}
}
@article{dua2019drop,
  title={DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1903.00161},
  year={2019}
}
@article{reddy2019coqa,
  title={Coqa: A conversational question answering challenge},
  author={Reddy, Siva and Chen, Danqi and Manning, Christopher D},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={249--266},
  year={2019},
  publisher={MIT Press}
}
@article{choi2018quac,
  title={QuAC: Question answering in context},
  author={Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1808.07036},
  year={2018}
}
@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press}
}
@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}
@article{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}
@inproceedings{koncel2016mawps,
  title={MAWPS: A math word problem repository},
  author={Koncel-Kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies},
  pages={1152--1157},
  year={2016}
}
@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}
@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}
@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={arXiv preprint arXiv:1902.01007},
  year={2019}
}
@article{thorne2018fever,
  title={Fever: a large-scale dataset for fact extraction and verification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  journal={arXiv preprint arXiv:1803.05355},
  year={2018}
}
@article{augenstein2019multifc,
  title={MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims},
  author={Augenstein, Isabelle and Lioma, Christina and Wang, Dongsheng and Lima, Lucas Chaves and Hansen, Casper and Hansen, Christian and Simonsen, Jakob Grue},
  journal={arXiv preprint arXiv:1909.03242},
  year={2019}
}
@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}
@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}
@article{blodgett2016demographic,
  title={Demographic dialectal variation in social media: A case study of African-American English},
  author={Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
  journal={arXiv preprint arXiv:1608.08868},
  year={2016}
}
@inproceedings{borkan2019nuanced,
  title={Nuanced metrics for measuring unintended bias with real data for text classification},
  author={Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  booktitle={Companion proceedings of the 2019 world wide web conference},
  pages={491--500},
  year={2019}
}
@article{hendrycks2021Math,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@article{rudinger2018gender,
  title={Gender bias in coreference resolution},
  author={Rudinger, Rachel and Naradowsky, Jason and Leonard, Brian and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1804.09301},
  year={2018}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@article{xu2021fewclue,
  title={Fewclue: A chinese few-shot learning evaluation benchmark},
  author={Xu, Liang and Lu, Xiaojing and Yuan, Chenyang and Zhang, Xuanwei and Xu, Huilin and Yuan, Hu and Wei, Guoao and Pan, Xiang and Tian, Xin and Qin, Libo and others},
  journal={arXiv preprint arXiv:2107.07498},
  year={2021}
}
@inproceedings{chang2022webqa,
  title={Webqa: Multihop and multimodal qa},
  author={Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16495--16504},
  year={2022}
}
@article{cui2018span,
  title={A span-extraction dataset for Chinese machine reading comprehension},
  author={Cui, Yiming and Liu, Ting and Che, Wanxiang and Xiao, Li and Chen, Zhipeng and Ma, Wentao and Wang, Shijin and Hu, Guoping},
  journal={arXiv preprint arXiv:1810.07366},
  year={2018}
}
@article{lim2019korquad1,
  title={Korquad1. 0: Korean qa dataset for machine reading comprehension},
  author={Lim, Seungyoung and Kim, Myungji and Lee, Jooyoul},
  journal={arXiv preprint arXiv:1909.07005},
  year={2019}
}
@article{park2021klue,
  title={Klue: Korean language understanding evaluation},
  author={Park, Sungjoon and Moon, Jihyung and Kim, Sungdong and Cho, Won Ik and Han, Jiyoon and Park, Jangwon and Song, Chisung and Kim, Junseong and Song, Yongsook and Oh, Taehwan and others},
  journal={arXiv preprint arXiv:2105.09680},
  year={2021}
}
@inproceedings{li2018character,
  title={Character-based bilstm-crf incorporating pos and dictionaries for chinese opinion target extraction},
  author={Li, Yanzeng and Liu, Tingwen and Li, Diying and Li, Quangang and Shi, Jinqiao and Wang, Yanqiu},
  booktitle={Asian Conference on Machine Learning},
  pages={518--533},
  year={2018},
  organization={PMLR}
}
@inproceedings{li2019chinese,
  title={Chinese relation extraction with multi-grained information and external linguistic knowledge},
  author={Li, Ziran and Ding, Ning and Liu, Zhiyuan and Zheng, Haitao and Shen, Ying},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4377--4386},
  year={2019}
}
@article{xu2017discourse,
  title={A discourse-level named entity recognition and relation extraction dataset for chinese literature text},
  author={Xu, Jingjing and Wen, Ji and Sun, Xu and Su, Qi},
  journal={arXiv preprint arXiv:1711.07010},
  year={2017}
}
@inproceedings{chen2018bq,
  title={The bq corpus: A large-scale domain-specific chinese corpus for sentence semantic equivalence identification},
  author={Chen, Jing and Chen, Qingcai and Liu, Xin and Yang, Haijun and Lu, Daohe and Tang, Buzhou},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  pages={4946--4951},
  year={2018}
}
@misc{co2019iflytek,
  title={Iflytek: a multiple categories chinese text classifier. competition official website},
  author={CO, LI},
  year={2019}
}
@article{liu2018matching,
  title={Matching article pairs with graphical decomposition and convolutions},
  author={Liu, Bang and Niu, Di and Wei, Haojie and Lin, Jinghong and He, Yancheng and Lai, Kunfeng and Xu, Yu},
  journal={arXiv preprint arXiv:1802.07459},
  year={2018}
}
@article{zhang2017chinese,
  title={Chinese medical question answer matching using end-to-end character-level multi-scale CNNs},
  author={Zhang, Sheng and Zhang, Xin and Wang, Hui and Cheng, Jiajun and Li, Pei and Ding, Zhaoyun},
  journal={Applied Sciences},
  volume={7},
  number={8},
  pages={767},
  year={2017},
  publisher={MDPI}
}
@article{zhang2018multi,
  title={Multi-scale attentive interaction networks for chinese medical question answer selection},
  author={Zhang, Sheng and Zhang, Xin and Wang, Hui and Guo, Lixiang and Liu, Shanshan},
  journal={IEEE Access},
  volume={6},
  pages={74061--74071},
  year={2018},
  publisher={IEEE}
}
@article{li2016dataset,
  title={Dataset and neural recurrent sequence labeling model for open-domain factoid question answering},
  author={Li, Peng and Li, Wei and He, Zhengyan and Wang, Xuguang and Cao, Ying and Zhou, Jie and Xu, Wei},
  journal={arXiv preprint arXiv:1607.06275},
  year={2016}
}
@inproceedings{peng2015named,
  title={Named entity recognition for chinese social media with jointly trained embeddings},
  author={Peng, Nanyun and Dredze, Mark},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={548--554},
  year={2015}
}
@article{weischedel2011ontonotes,
  title={Ontonotes release 4.0},
  author={Weischedel, Ralph and Pradhan, Sameer and Ramshaw, Lance and Palmer, Martha and Xue, Nianwen and Marcus, Mitchell and Taylor, Ann and Greenberg, Craig and Hovy, Eduard and Belvin, Robert and others},
  journal={LDC2011T03, Philadelphia, Penn.: Linguistic Data Consortium},
  year={2011}
}
@article{cui2020sentence,
  title={A sentence cloze dataset for Chinese machine reading comprehension},
  author={Cui, Yiming and Liu, Ting and Yang, Ziqing and Chen, Zhipeng and Ma, Wentao and Che, Wanxiang and Wang, Shijin and Hu, Guoping},
  journal={arXiv preprint arXiv:2004.03116},
  year={2020}
}
@article{shao2018drcd,
  title={DRCD: A Chinese machine reading comprehension dataset},
  author={Shao, Chih Chieh and Liu, Trois and Lai, Yuting and Tseng, Yiying and Tsai, Sam},
  journal={arXiv preprint arXiv:1806.00920},
  year={2018}
}
@article{he2017dureader,
  title={Dureader: a chinese machine reading comprehension dataset from real-world applications},
  author={He, Wei and Liu, Kai and Liu, Jing and Lyu, Yajuan and Zhao, Shiqi and Xiao, Xinyan and Liu, Yuan and Wang, Yizhong and Wu, Hua and She, Qiaoqiao and others},
  journal={arXiv preprint arXiv:1711.05073},
  year={2017}
}
@article{tang2020dureaderrobust,
  title={Dureaderrobust: A chinese dataset towards evaluating the robustness of machine reading comprehension models},
  author={Tang, Hongxuan and Liu, Jing and Li, Hongyu and Hong, Yu and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2004.11142},
  year={2020}
}
@article{zheng2019chid,
  title={ChID: A large-scale Chinese IDiom dataset for cloze test},
  author={Zheng, Chujie and Huang, Minlie and Sun, Aixin},
  journal={arXiv preprint arXiv:1906.01265},
  year={2019}
}
@article{xiao2018cail2018,
  title={Cail2018: A large-scale legal dataset for judgment prediction},
  author={Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and others},
  journal={arXiv preprint arXiv:1807.02478},
  year={2018}
}
@article{xu2021blow,
  title={Blow the dog whistle: A Chinese dataset for cant understanding with common sense and world knowledge},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Xu, Ke and McAuley, Julian and Wei, Furu},
  journal={arXiv preprint arXiv:2104.02704},
  year={2021}
}
@inproceedings{xiong2017end,
  title={End-to-end neural ad-hoc ranking with kernel pooling},
  author={Xiong, Chenyan and Dai, Zhuyun and Callan, Jamie and Liu, Zhiyuan and Power, Russell},
  booktitle={Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval},
  pages={55--64},
  year={2017}
}
@article{xu2020matinf,
  title={Matinf: A jointly labeled large-scale dataset for classification, question answering and summarization},
  author={Xu, Canwen and Pei, Jiaxin and Wu, Hongtao and Liu, Yiyu and Li, Chenliang},
  journal={arXiv preprint arXiv:2004.12302},
  year={2020}
}
@article{zhou2020kdconv,
  title={KdConv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation},
  author={Zhou, Hao and Zheng, Chujie and Huang, Kaili and Huang, Minlie and Zhu, Xiaoyan},
  journal={arXiv preprint arXiv:2004.04100},
  year={2020}
}
@misc{codeforce,
  title = {Codeforces: Results of 2020},
  author = {Mirzayanov, Mike},
  howpublished = {\url{https://codeforces.com/blog/entry/89502}}
}
@article{scialom2020mlsum,
  title={MLSUM: The multilingual summarization corpus},
  author={Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  journal={arXiv preprint arXiv:2004.14900},
  year={2020}
}
@article{narayan1808don,
  title={Don’t Give Me the Details, Just the Summary!},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={Topic-Aware Convolutional Neural Networks for Extreme Summarization. ArXiv, abs},
  year={1808}
}
@article{novikova2017e2e,
  title={The E2E dataset: New challenges for end-to-end generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  journal={arXiv preprint arXiv:1706.09254},
  year={2017}
}
@inproceedings{ferreira20202020,
  title={The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020)},
  author={Ferreira, Thiago Castro and Gardent, Claire and Ilinykh, Nikolai and Van Der Lee, Chris and Mille, Simon and Moussallem, Diego and Shimorina, Anastasia},
  booktitle={Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)},
  year={2020}
}
@article{xia2019microsoft,
  title={Microsoft Research Asia's Systems for WMT19},
  author={Xia, Yingce and Tan, Xu and Tian, Fei and Gao, Fei and Chen, Weicong and Fan, Yang and Gong, Linyuan and Leng, Yichong and Luo, Renqian and Wang, Yiren and others},
  journal={arXiv preprint arXiv:1911.06191},
  year={2019}
}
@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}
@article{zhao2018gender,
  title={Gender bias in coreference resolution: Evaluation and debiasing methods},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1804.06876},
  year={2018}
}
@inproceedings{boyd2012besting,
  title={Besting the quiz master: Crowdsourcing incremental classification games},
  author={Boyd-Graber, Jordan and Satinoff, Brianna and He, He and Daum{\'e} III, Hal},
  booktitle={Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning},
  pages={1290--1301},
  year={2012}
}
@article{parrish2021bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}
@article{Mixed_Precision,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}
@article{levine2020limits,
  title={Limits to depth efficiencies of self-attention},
  author={Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22640--22651},
  year={2020}
}
@misc{Common_Crawl,
  url = {https://commoncrawl.org/},
  title = {Common crawl}
}
@misc{Wikipedia,
  url = {https://en.wikipedia.org/wiki/Main_Page},
  title = {Wikipedia}
}
@misc{BQ_Dataset,
  url = {https://cloud.google.com/bigquery?hl=zh-cn},
  title = {Bigquery dataset}
}
@misc{Openwebtext_Dataset,
  url = {http://Skylion007.github.io/OpenWebTextCorpus},
  title = {Openwebtext corpus}
}
@misc{GPT_J_6B,
author={Ben, Wang and Aran, Komatsuzaki},
  title = {GPT-J-6B: A 6 billion parameter autoregressive language model},
  year = {2021}
}
@misc{Mesh_Transformer_JAX,
author={Ben, Wang},
  title = {Mesh-Transformer-JAX: Model-parallel implementation of transformer language model with JAX},
  year = {2021}
}

