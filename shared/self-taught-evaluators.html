<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Taught Evaluators</title>
    
    

    <meta property="og:url" content="https://www.managen.ai//shared/self-taught-evaluators.html">

    <meta property="og:title" content="Self-Taught Evaluators">
    <meta property="og:description" content="The authors show a method of using "">

    <meta property="og:type"  content="website" />
    <meta property="og:image" content="https://github.com/user-attachments/assets/89276c81-65fb-4250-9765-e8cc3abe3b9f">
    <meta property="og:url" content="https://www.managen.ai//shared/self-taught-evaluators.html">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Self-Taught Evaluators">
    <meta name="twitter:description" content="Self-Taught Evaluators">
    <meta name="twitter:image" content="https://github.com/user-attachments/assets/89276c81-65fb-4250-9765-e8cc3abe3b9f">
    <!--Import stylesheet  -->
    <link rel="stylesheet" href="../../stylesheets/share-card.css">
    <style>
        .page-url { display: block; }
        .iframe .backlink-url { display: none; }
   

     </style>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config",""),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-MPQ831MCNS",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
<script>var consent;"undefined"==typeof __md_analytics||(consent=__md_get("__consent"))&&consent.analytics&&__md_analytics()</script>
    <script>
       (function(d, w) {
          if (w.self !== w.top) {
             d.className += " iframe";
          }
       }(document.documentElement, window));

        window.onload = function() {
            var links = document.getElementsByTagName('a');
            for (var i = 0; i < links.length; i++) {
                links[i].target = '_blank';
            }
        };
        function copyToClipboard(elementId) {
            var url = document.getElementById(elementId).href;
            navigator.clipboard.writeText(url).then(function() {
                // Show copied popup
                var popup = document.getElementById("copied-popup");
                popup.classList.add("show");

                // Hide popup after 2 seconds
                setTimeout(function() { popup.classList.remove("show"); }, 2000);
            }, function(err) {
                console.error('Could not copy text: ', err);
            });
        }
    </script>
</head>
<body>
    
    <div class="blog-content">
        <h1><p><a href="https://arxiv.org/pdf/2408.02666">Self-Taught Evaluators</a></p></h1>
        <p>The authors show a method of using </p>
<blockquote>
<p>Abstract
  Model-based evaluation is at the heart of successful model development â€“ as a reward
  model for training, and as a replacement for
  human evaluation. To train such evaluators,
  the standard approach is to collect a large
  amount of human preference judgments over
  model responses, which is costly and the data
  becomes stale as models improve. In this
  work, we present an approach that aims to improve evaluators without human annotations,
  using synthetic training data only. Starting
  from unlabeled instructions, our iterative selfimprovement scheme generates contrasting
  model outputs and trains an LLM-as-a-Judge to
  produce reasoning traces and final judgments,
  repeating this training at each new iteration
  using the improved predictions. Without any
  labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70BInstruct) from 75.4 to 88.3 (88.7 with majority
  vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and
  matches the performance of the top-performing
  reward models trained with labeled examples.
<img alt="image" src="https://github.com/user-attachments/assets/89276c81-65fb-4250-9765-e8cc3abe3b9f" /></p>
</blockquote>
<p><img alt="image" src="https://github.com/user-attachments/assets/7516ba4c-1496-4e48-a5b3-cf7adfc02c25" /></p>
    </div>
    <a href="https://www.managen.ai//shared/self-taught-evaluators.html" class="page-url" id="page-url"></a>
    <!-- Clipboard copy icon/button -->
    <span class="copy-icon" id="copy-icon" class="h3" onclick="copyToClipboard('page-url' )">Share link! &#128203; </span>
    <!-- Copied popup -->
    <div class="copied-popup" id="copied-popup">Link copied!</div>
    <a href="https://www.managen.ai//Understanding/architectures/training/feedback.html" class="backlink-url" id="backlink-url">See the main site!</a>
</body>
</html>
