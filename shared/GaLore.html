<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</title>
    
    

    <meta property="og:url" content="https://www.managen.ai//shared/GaLore.html">

    <meta property="og:title" content="GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection">
    <meta property="og:description" content="  "">

    <meta property="og:type"  content="website" />
    <meta property="og:image" content="https://github.com/ianderrington/genai/assets/76016868/05166538-7af7-4239-a194-03496760dbf5">
    <meta property="og:url" content="https://www.managen.ai//shared/GaLore.html">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection">
    <meta name="twitter:description" content="GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection">
    <meta name="twitter:image" content="https://github.com/ianderrington/genai/assets/76016868/05166538-7af7-4239-a194-03496760dbf5">
    <!--Import stylesheet  -->
    <link rel="stylesheet" href="../../stylesheets/share-card.css">
    <style>
        .page-url { display: block; }
        .iframe .backlink-url { display: none; }
   

     </style>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config",""),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-MPQ831MCNS",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
<script>var consent;"undefined"==typeof __md_analytics||(consent=__md_get("__consent"))&&consent.analytics&&__md_analytics()</script>
    <script>
       (function(d, w) {
          if (w.self !== w.top) {
             d.className += " iframe";
          }
       }(document.documentElement, window));

        window.onload = function() {
            var links = document.getElementsByTagName('a');
            for (var i = 0; i < links.length; i++) {
                links[i].target = '_blank';
            }
        };
        function copyToClipboard(elementId) {
            var url = document.getElementById(elementId).href;
            navigator.clipboard.writeText(url).then(function() {
                // Show copied popup
                var popup = document.getElementById("copied-popup");
                popup.classList.add("show");

                // Hide popup after 2 seconds
                setTimeout(function() { popup.classList.remove("show"); }, 2000);
            }, function(err) {
                console.error('Could not copy text: ', err);
            });
        }
    </script>
</head>
<body>
    
    <div class="blog-content">
        <h1><p><a href="https://arxiv.org/pdf/2403.03507.pdf">GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></p></h1>
        <p><strong>Developments</strong>
"For the first time, we show that the Llama 7B LLM can be trained on a single consumer-grade GPU (RTX 4090) with only 24GB memory. This represents more than 82.5% reduction in memory for storing optimizer states during training.</p>
<p>Training LLMs from scratch currently requires huge computational resources with large memory GPUs. While there has been significant progress in reducing memory requirements during fine-tuning (e.g., LORA), they do not apply for pre-training LLMs. We design methods that overcome this obstacle and provide significant memory reduction throughout training LLMs.</p>
<p>Training LLMs often requires the use of preconditioned optimization algorithms such as Adam to achieve rapid convergence. These algorithms accumulate extensive gradient statistics, proportional to the model's parameter size, making the storage of these optimizer states the primary memory constraint during training. Instead of focusing just on engineering and system efforts to reduce memory consumption, we went back to fundamentals. </p>
<p>We looked at the slow-changing low-rank structure of the gradient matrix during training.  We introduce a novel approach that leverages the low-rank nature of gradients via Gradient Low-Rank Projection (GaLore). So instead of expressing the weight matrix as low rank, which leads to a big performance degradation during pretraining, we instead express the gradient weight matrix as low rank without performance degradation, while significantly reducing memory requirements."</p>
<p><img width="352" alt="image" src="https://github.com/ianderrington/genai/assets/76016868/05166538-7af7-4239-a194-03496760dbf5"></p>
    </div>
    <a href="https://www.managen.ai//shared/GaLore.html" class="page-url" id="page-url"></a>
    <!-- Clipboard copy icon/button -->
    <span class="copy-icon" id="copy-icon" class="h3" onclick="copyToClipboard('page-url' )">Share link! &#128203; </span>
    <!-- Copied popup -->
    <div class="copied-popup" id="copied-popup">Link copied!</div>
    <a href="https://www.managen.ai//Understanding/architectures/optimization/index.html" class="backlink-url" id="backlink-url">See the main site!</a>
</body>
</html>
